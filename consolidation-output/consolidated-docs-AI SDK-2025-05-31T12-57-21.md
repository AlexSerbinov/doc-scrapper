# 📚 Consolidated Documentation - AI SDK

## 📊 Overview

- **Total Files**: 487
- **Total Size**: 1.9 MB
- **Estimated Tokens**: ~498,682
- **Generated**: 5/31/2025, 3:57:21 PM

## 🔧 Usage Instructions

This consolidated documentation file contains the complete content of your scraped documentation.

**Recommended for:**
- 🤖 **Google Gemini Flash/Pro** (2M+ token context)
- 🤖 **ChatGPT-4 Turbo** (128K+ token context)
- 🤖 **Claude 3.5 Sonnet** (200K+ token context)
- 🤖 **Other large context LLMs**

**Instructions:**
1. Copy this entire document
2. Paste into your preferred LLM
3. Ask questions about the documentation
4. Get instant, context-aware answers!

---

## 📁 Directory Structure

```text
📄 README.md
📁 cookbook
  📁 next
    📄 caching-middleware.md
    📄 call-tools-in-parallel.md
    📄 call-tools-multiple-steps.md
    📄 call-tools.md
    📄 chat-with-pdf.md
    📄 generate-image-with-chat-prompt.md
    📄 generate-object-with-file-prompt.md
    📄 generate-object.md
    📄 generate-text-with-chat-prompt.md
    📄 generate-text.md
    📄 human-in-the-loop.md
    📄 markdown-chatbot-with-memoization.md
    📄 mcp-tools.md
    📄 render-visual-interface-in-chat.md
    📄 send-custom-body-from-use-chat.md
    📄 stream-assistant-response-with-tools.md
    📄 stream-assistant-response.md
    📄 stream-object.md
    📄 stream-text-multistep.md
    📄 stream-text-with-chat-prompt.md
    📄 stream-text-with-image-prompt.md
    📄 stream-text.md
  📁 node
    📄 call-tools-in-parallel.md
    📄 call-tools-multiple-steps.md
    📄 call-tools-with-image-prompt.md
    📄 call-tools.md
    📄 embed-text-batch.md
    📄 embed-text.md
    📄 generate-object-reasoning.md
    📄 generate-object.md
    📄 generate-text-with-chat-prompt.md
    📄 generate-text-with-image-prompt.md
    📄 generate-text.md
    📄 intercept-fetch-requests.md
    📄 local-caching-middleware.md
    📄 mcp-tools.md
    📄 retrieval-augmented-generation.md
    📄 stream-object-record-final-object.md
    📄 stream-object-record-token-usage.md
    📄 stream-object-with-image-prompt.md
    📄 stream-object.md
    📄 stream-text-with-chat-prompt.md
    📄 stream-text-with-file-prompt.md
    📄 stream-text-with-image-prompt.md
    📄 stream-text.md
    📄 web-search-agent.md
  📁 rsc
    📄 call-tools-in-parallel.md
    📄 call-tools.md
    📄 generate-object.md
    📄 generate-text-with-chat-prompt.md
    📄 generate-text.md
    📄 render-visual-interface-in-chat.md
    📄 restore-messages-from-database.md
    📄 save-messages-to-database.md
    📄 stream-assistant-response-with-tools.md
    📄 stream-assistant-response.md
    📄 stream-object.md
    📄 stream-text-with-chat-prompt.md
    📄 stream-text.md
    📄 stream-ui-record-token-usage.md
    📄 stream-updates-to-visual-interfaces.md
📄 cookbook.md
📁 docs
  📁 advanced
    📄 backpressure.md
    📄 caching.md
    📄 model-as-router.md
    📄 multiple-streamables.md
    📄 multistep-interfaces.md
    📄 prompt-engineering.md
    📄 rate-limiting.md
    📄 rendering-ui-with-language-models.md
    📄 sequential-generations.md
    📄 stopping-streams.md
    📄 vercel-deployment-guide.md
  📄 advanced.md
  📁 ai-sdk-core
    📄 embeddings.md
    📄 error-handling.md
    📄 generating-structured-data.md
    📄 generating-text.md
    📄 image-generation.md
    📄 middleware.md
    📄 overview.md
    📄 prompt-engineering.md
    📄 provider-management.md
    📄 settings.md
    📄 speech.md
    📄 telemetry.md
    📄 testing.md
    📄 tools-and-tool-calling.md
    📄 transcription.md
  📄 ai-sdk-core.md
  📁 ai-sdk-rsc
    📄 authentication.md
    📄 error-handling.md
    📄 generative-ui-state.md
    📄 loading-state.md
    📄 migrating-to-ui.md
    📄 multistep-interfaces.md
    📄 overview.md
    📄 saving-and-restoring-states.md
    📄 streaming-react-components.md
    📄 streaming-values.md
  📄 ai-sdk-rsc.md
  📁 ai-sdk-ui
    📄 chatbot-message-persistence.md
    📄 chatbot-tool-usage.md
    📄 chatbot.md
    📄 completion.md
    📄 error-handling.md
    📄 generative-user-interfaces.md
    📄 object-generation.md
    📄 openai-assistants.md
    📄 overview.md
    📄 smooth-stream-chinese.md
    📄 smooth-stream-japanese.md
    📄 stream-protocol.md
    📄 streaming-data.md
  📄 ai-sdk-ui.md
  📄 announcing-ai-sdk-5-alpha.md
  📁 foundations
    📄 agents.md
    📄 overview.md
    📄 prompts.md
    📄 providers-and-models.md
    📄 streaming.md
    📄 tools.md
  📄 foundations.md
  📁 getting-started
    📄 expo.md
    📄 navigating-the-library.md
    📄 nextjs-app-router.md
    📄 nextjs-pages-router.md
    📄 nodejs.md
    📄 nuxt.md
    📄 svelte.md
  📄 getting-started.md
  📁 guides
    📄 claude-4.md
    📄 computer-use.md
    📄 gpt-4-5.md
    📄 llama-3_1.md
    📄 multi-modal-chatbot.md
    📄 natural-language-postgres.md
    📄 o1.md
    📄 o3.md
    📄 openai-responses.md
    📄 r1.md
    📄 rag-chatbot.md
    📄 slackbot.md
    📄 sonnet-3-7.md
  📄 guides.md
  📄 introduction.md
  📁 migration-guides
    📄 migration-guide-3-1.md
    📄 migration-guide-3-2.md
    📄 migration-guide-3-3.md
    📄 migration-guide-3-4.md
    📄 migration-guide-4-0.md
    📄 migration-guide-4-1.md
    📄 migration-guide-4-2.md
    📄 versioning.md
  📄 migration-guides.md
  📁 reference
    📁 ai-sdk-core
      📄 core-message.md
      📄 cosine-similarity.md
      📄 create-id-generator.md
      📄 create-mcp-client.md
      📄 custom-provider.md
      📄 default-settings-middleware.md
      📄 embed-many.md
      📄 embed.md
      📄 extract-reasoning-middleware.md
      📄 generate-id.md
      📄 generate-image.md
      📄 generate-object.md
      📄 generate-speech.md
      📄 generate-text.md
      📄 json-schema.md
      📄 language-model-v1-middleware.md
      📄 mcp-stdio-transport.md
      📄 provider-registry.md
      📄 simulate-readable-stream.md
      📄 simulate-streaming-middleware.md
      📄 smooth-stream.md
      📄 stream-object.md
      📄 stream-text.md
      📄 tool.md
      📄 transcribe.md
      📄 valibot-schema.md
      📄 wrap-language-model.md
      📄 zod-schema.md
    📄 ai-sdk-core.md
    📁 ai-sdk-errors
      📄 ai-api-call-error.md
      📄 ai-download-error.md
      📄 ai-empty-response-body-error.md
      📄 ai-invalid-argument-error.md
      📄 ai-invalid-data-content-error.md
      📄 ai-invalid-data-content.md
      📄 ai-invalid-message-role-error.md
      📄 ai-invalid-prompt-error.md
      📄 ai-invalid-response-data-error.md
      📄 ai-invalid-tool-arguments-error.md
      📄 ai-json-parse-error.md
      📄 ai-load-api-key-error.md
      📄 ai-load-setting-error.md
      📄 ai-message-conversion-error.md
      📄 ai-no-audio-generated-error.md
      📄 ai-no-content-generated-error.md
      📄 ai-no-image-generated-error.md
      📄 ai-no-object-generated-error.md
      📄 ai-no-output-specified-error.md
      📄 ai-no-such-model-error.md
      📄 ai-no-such-provider-error.md
      📄 ai-no-such-tool-error.md
      📄 ai-no-transcript-generated-error.md
      📄 ai-retry-error.md
      📄 ai-too-many-embedding-values-for-call-error.md
      📄 ai-tool-call-repair-error.md
      📄 ai-tool-execution-error.md
      📄 ai-type-validation-error.md
      📄 ai-unsupported-functionality-error.md
    📄 ai-sdk-errors.md
    📁 ai-sdk-rsc
      📄 create-ai.md
      📄 create-streamable-ui.md
      📄 create-streamable-value.md
      📄 get-ai-state.md
      📄 get-mutable-ai-state.md
      📄 read-streamable-value.md
      📄 render.md
      📄 stream-ui.md
      📄 use-actions.md
      📄 use-ai-state.md
      📄 use-streamable-value.md
      📄 use-ui-state.md
    📄 ai-sdk-rsc.md
    📁 ai-sdk-ui
      📄 append-client-message.md
      📄 append-response-messages.md
      📄 assistant-response.md
      📄 convert-to-core-messages.md
      📄 create-data-stream-response.md
      📄 create-data-stream.md
      📄 pipe-data-stream-to-response.md
      📄 stream-data.md
      📄 use-assistant.md
      📄 use-chat.md
      📄 use-completion.md
      📄 use-object.md
    📄 ai-sdk-ui.md
    📁 stream-helpers
      📄 ai-stream.md
      📄 anthropic-stream.md
      📄 aws-bedrock-anthropic-stream.md
      📄 aws-bedrock-cohere-stream.md
      📄 aws-bedrock-llama-2-stream.md
      📄 aws-bedrock-messages-stream.md
      📄 aws-bedrock-stream.md
      📄 cohere-stream.md
      📄 google-generative-ai-stream.md
      📄 hugging-face-stream.md
      📄 inkeep-stream.md
      📄 langchain-adapter.md
      📄 langchain-stream.md
      📄 llamaindex-adapter.md
      📄 mistral-stream.md
      📄 openai-stream.md
      📄 replicate-stream.md
      📄 stream-to-response.md
      📄 streaming-text-response.md
    📄 stream-helpers.md
  📄 reference.md
  📁 troubleshooting
    📄 azure-stream-slow.md
    📄 client-side-function-calls-not-invoked.md
    📄 client-stream-error.md
    📄 jest-cannot-find-module-ai-rsc.md
    📄 model-is-not-assignable-to-type.md
    📄 nan-token-counts-openai-streaming.md
    📄 react-maximum-update-depth-exceeded.md
    📄 server-actions-in-client-components.md
    📄 strange-stream-output.md
    📄 stream-text-not-working.md
    📄 streamable-ui-errors.md
    📄 streaming-not-working-when-deployed.md
    📄 streaming-not-working-when-proxied.md
    📄 timeout-on-vercel.md
    📄 tool-invocation-missing-result.md
    📄 typescript-cannot-find-namespace-jsx.md
    📄 unclosed-streams.md
    📄 use-chat-an-error-occurred.md
    📄 use-chat-failed-to-parse-stream.md
    📄 use-chat-tools-no-response.md
  📄 troubleshooting.md
📄 getting-started.md
📄 index.md
📁 playground
  📄 anthropic_claude-3.5-haiku.md
  📄 anthropic_claude-3.7-sonnet-reasoning.md
  📄 anthropic_claude-3.7-sonnet.md
  📄 anthropic_claude-4-opus-20250514.md
  📄 anthropic_claude-4-sonnet-20250514.md
  📄 anthropic_claude-v2.md
  📄 anthropic_claude-v3-haiku.md
  📄 anthropic_claude-v3-opus.md
  📄 anthropic_claude-v3-sonnet.md
  📄 anthropic_claude-v3.5-sonnet.md
  📄 bedrock_amazon.nova-lite-v1_0.md
  📄 bedrock_amazon.nova-micro-v1_0.md
  📄 bedrock_amazon.nova-pro-v1_0.md
  📄 bedrock_claude-3-5-haiku-20241022.md
  📄 bedrock_claude-3-5-sonnet-20240620-v1.md
  📄 bedrock_claude-3-5-sonnet-20241022-v2.md
  📄 bedrock_claude-3-7-sonnet-20250219.md
  📄 bedrock_claude-3-haiku-20240307-v1.md
  📄 bedrock_claude-4-opus-20250514-v1.md
  📄 bedrock_claude-4-sonnet-20250514-v1.md
  📄 bedrock_deepseek.r1-v1.md
  📄 bedrock_meta.llama3-1-70b-instruct-v1.md
  📄 bedrock_meta.llama3-1-8b-instruct-v1.md
  📄 bedrock_meta.llama3-2-11b-instruct-v1.md
  📄 bedrock_meta.llama3-2-1b-instruct-v1.md
  📄 bedrock_meta.llama3-2-3b-instruct-v1.md
  📄 bedrock_meta.llama3-2-90b-instruct-v1.md
  📄 bedrock_meta.llama3-3-70b-instruct-v1.md
  📄 bedrock_meta.llama4-maverick-17b-instruct-v1.md
  📄 bedrock_meta.llama4-scout-17b-instruct-v1.md
  📄 cerebras_llama-3.3-70b.md
  📄 cerebras_llama-4-scout-17b-16e-instruct.md
  📄 cerebras_llama3.1-8b.md
  📄 cerebras_qwen-3-32b.md
  📄 cohere_command-a.md
  📄 cohere_command-light-nightly.md
  📄 cohere_command-nightly.md
  📄 cohere_command-r-plus.md
  📄 cohere_command-r.md
  📄 deepinfra_llama-4-maverick-17b-128e-instruct-fp8.md
  📄 deepinfra_llama-4-scout-17b-16e-instruct.md
  📄 deepinfra_qwen3-14b.md
  📄 deepinfra_qwen3-235b-a22b.md
  📄 deepinfra_qwen3-30b-a3b.md
  📄 deepinfra_qwen3-32b.md
  📄 deepseek_chat.md
  📄 deepseek_deepseek-r1-0528.md
  📄 deepseek_deepseek-r1.md
  📄 fireworks_deepseek-r1.md
  📄 fireworks_deepseek-v3.md
  📄 fireworks_firefunction-v1.md
  📄 fireworks_mixtral-8x22b-instruct.md
  📄 fireworks_mixtral-8x7b-instruct.md
  📄 fireworks_qwen3-235b-a22b.md
  📄 fireworks_qwq-32b.md
  📄 google_gemini-1.5-flash-002.md
  📄 google_gemini-1.5-flash-8b.md
  📄 google_gemini-1.5-flash.md
  📄 google_gemini-1.5-pro-002.md
  📄 google_gemini-1.5-pro.md
  📄 google_gemini-2.0-flash-001.md
  📄 google_gemini-2.0-flash-lite-preview-02-05.md
  📄 google_gemini-2.5-flash-preview-04-17.md
  📄 google_gemini-2.5-pro-preview-03-25.md
  📄 google_gemma-3-27b-it.md
  📄 groq_gemma2-9b-it.md
  📄 groq_llama-3-70b-instruct.md
  📄 groq_llama-3-8b-instruct.md
  📄 groq_llama-3.1-8b.md
  📄 groq_llama-3.2-11b-vision-preview.md
  📄 groq_llama-3.2-1b.md
  📄 groq_llama-3.2-3b.md
  📄 groq_llama-3.2-90b-vision-preview.md
  📄 groq_llama-3.3-70b-versatile.md
  📄 groq_llama-4-scout-17b-16e-instruct.md
  📄 groq_mistral-saba-24b.md
  📄 groq_qwen-qwq-32b.md
  📄 inception_mercury-coder-small.md
  📄 mistral_codestral-2501.md
  📄 mistral_ministral-3b-latest.md
  📄 mistral_ministral-8b-latest.md
  📄 mistral_mistral-large.md
  📄 mistral_mistral-small-2503.md
  📄 mistral_mistral-small.md
  📄 mistral_pixtral-12b-2409.md
  📄 mistral_pixtral-large-latest.md
  📄 openai_gpt-3.5-turbo-instruct.md
  📄 openai_gpt-3.5-turbo.md
  📄 openai_gpt-4-turbo.md
  📄 openai_gpt-4.1-mini.md
  📄 openai_gpt-4.1-nano.md
  📄 openai_gpt-4.1.md
  📄 openai_gpt-4.5-preview.md
  📄 openai_gpt-4o-mini.md
  📄 openai_gpt-4o.md
  📄 openai_o3-mini-high.md
  📄 openai_o3-mini-low.md
  📄 openai_o3-mini-medium.md
  📄 openai_o3-mini.md
  📄 openai_o3.md
  📄 openai_o4-mini.md
  📄 perplexity_sonar-pro.md
  📄 perplexity_sonar-reasoning-pro.md
  📄 perplexity_sonar-reasoning.md
  📄 perplexity_sonar.md
  📄 vertex_claude-3-5-haiku-20241022.md
  📄 vertex_claude-3-5-sonnet-20240620.md
  📄 vertex_claude-3-5-sonnet-v2-20241022.md
  📄 vertex_claude-3-7-sonnet-20250219.md
  📄 vertex_claude-3-haiku-20240307.md
  📄 vertex_claude-3-opus-20240229.md
  📄 vertex_claude-4-opus-20250514.md
  📄 vertex_claude-4-sonnet-20250514.md
  📄 vertex_gemini-2.0-flash-001.md
  📄 vertex_gemini-2.0-flash-lite-001.md
  📄 vertex_gemini-2.5-flash-preview-04-17.md
  📄 vertex_gemini-2.5-pro-preview-05-06.md
  📄 vertex_llama-3.3-70b-instruct-maas.md
  📄 vertex_llama-4-maverick-17b-128e-instruct-maas.md
  📄 vertex_llama-4-scout-17b-16e-instruct-maas.md
  📄 xai_grok-2-1212.md
  📄 xai_grok-2-vision-1212.md
  📄 xai_grok-3-beta.md
  📄 xai_grok-3-fast-beta.md
  📄 xai_grok-3-mini-beta.md
  📄 xai_grok-3-mini-fast-beta.md
  📄 xai_grok-beta.md
  📄 xai_grok-vision-beta.md
📄 playground.md
📁 providers
  📁 adapters
    📄 langchain.md
    📄 llamaindex.md
  📄 adapters.md
  📁 ai-sdk-providers
    📄 amazon-bedrock.md
    📄 anthropic.md
    📄 assemblyai.md
    📄 azure.md
    📄 cerebras.md
    📄 cohere.md
    📄 deepgram.md
    📄 deepinfra.md
    📄 deepseek.md
    📄 elevenlabs.md
    📄 fal.md
    📄 fireworks.md
    📄 gladia.md
    📄 google-generative-ai.md
    📄 google-vertex.md
    📄 groq.md
    📄 hume.md
    📄 lmnt.md
    📄 luma.md
    📄 mistral.md
    📄 openai.md
    📄 perplexity.md
    📄 replicate.md
    📄 revai.md
    📄 togetherai.md
    📄 vercel.md
    📄 xai.md
  📄 ai-sdk-providers.md
  📁 community-providers
    📄 anthropic-vertex-ai.md
    📄 azure-ai.md
    📄 chrome-ai.md
    📄 cloudflare-ai-gateway.md
    📄 cloudflare-workers-ai.md
    📄 crosshatch.md
    📄 custom-providers.md
    📄 dify.md
    📄 friendliai.md
    📄 inflection-ai.md
    📄 langdb.md
    📄 letta.md
    📄 llama-cpp.md
    📄 mem0.md
    📄 mixedbread.md
    📄 ollama.md
    📄 openrouter.md
    📄 portkey.md
    📄 qwen.md
    📄 sambanova.md
    📄 sarvam.md
    📄 spark.md
    📄 voyage-ai.md
    📄 zhipu.md
  📄 community-providers.md
  📁 observability
    📄 braintrust.md
    📄 helicone.md
    📄 laminar.md
    📄 langfuse.md
    📄 langsmith.md
    📄 langwatch.md
    📄 patronus.md
    📄 traceloop.md
    📄 weave.md
  📄 observability.md
  📁 openai-compatible-providers
    📄 baseten.md
    📄 custom-providers.md
    📄 lmstudio.md
    📄 nim.md
  📄 openai-compatible-providers.md
📄 scraping-summary.json
📄 showcase.md

```

## 📄 File Contents

### 1. `README.md`

```markdown
# Documentation Index

**Source**: https://ai-sdk.dev/docs/introduction
**Scraped**: 5/31/2025
**Total Pages**: 487

---

## Adapters

- [Adapters](providers/adapters.md)
- [LangChain](providers/adapters/langchain.md)
- [LlamaIndex](providers/adapters/llamaindex.md)

## Advanced

- [Advanced](docs/advanced.md)
- [Caching Responses](docs/advanced/caching.md)
- [Generative User Interfaces](docs/advanced/model-as-router.md)
- [Multiple Streams](docs/advanced/multiple-streamables.md)
- [Multistep Interfaces](docs/advanced/multistep-interfaces.md)
- [Prompt Engineering](docs/advanced/prompt-engineering.md)
- [Rate Limiting](docs/advanced/rate-limiting.md)
- [Rendering User Interfaces with Language Models](docs/advanced/rendering-ui-with-language-models.md)
- [Sequential Generations](docs/advanced/sequential-generations.md)
- [Stopping Streams](docs/advanced/stopping-streams.md)
- [Stream Back-pressure and Cancellation](docs/advanced/backpressure.md)
- [Vercel Deployment Guide](docs/advanced/vercel-deployment-guide.md)

## Ai-sdk-core

- [AI SDK Core](docs/ai-sdk-core.md)
- [AI SDK Core](docs/ai-sdk-core/overview.md)
- [Embeddings](docs/ai-sdk-core/embeddings.md)
- [Error Handling](docs/ai-sdk-core/error-handling.md)
- [Generating and Streaming Text](docs/ai-sdk-core/generating-text.md)
- [Generating Structured Data](docs/ai-sdk-core/generating-structured-data.md)
- [Image Generation](docs/ai-sdk-core/image-generation.md)
- [Language Model Middleware](docs/ai-sdk-core/middleware.md)
- [Prompt Engineering](docs/ai-sdk-core/prompt-engineering.md)
- [Provider & Model Management](docs/ai-sdk-core/provider-management.md)
- [Settings](docs/ai-sdk-core/settings.md)
- [Speech](docs/ai-sdk-core/speech.md)
- [Telemetry](docs/ai-sdk-core/telemetry.md)
- [Testing](docs/ai-sdk-core/testing.md)
- [Tool Calling](docs/ai-sdk-core/tools-and-tool-calling.md)
- [Transcription](docs/ai-sdk-core/transcription.md)

## Ai-sdk-providers

- [AI SDK Providers](providers/ai-sdk-providers.md)
- [Amazon Bedrock Provider](providers/ai-sdk-providers/amazon-bedrock.md)
- [Anthropic Provider](providers/ai-sdk-providers/anthropic.md)
- [AssemblyAI Provider](providers/ai-sdk-providers/assemblyai.md)
- [Azure OpenAI Provider](providers/ai-sdk-providers/azure.md)
- [Cerebras Provider](providers/ai-sdk-providers/cerebras.md)
- [Cohere Provider](providers/ai-sdk-providers/cohere.md)
- [Deepgram Provider](providers/ai-sdk-providers/deepgram.md)
- [DeepInfra Provider](providers/ai-sdk-providers/deepinfra.md)
- [DeepSeek Provider](providers/ai-sdk-providers/deepseek.md)
- [ElevenLabs Provider](providers/ai-sdk-providers/elevenlabs.md)
- [Fal Provider](providers/ai-sdk-providers/fal.md)
- [Fireworks Provider](providers/ai-sdk-providers/fireworks.md)
- [Gladia Provider](providers/ai-sdk-providers/gladia.md)
- [Google Generative AI Provider](providers/ai-sdk-providers/google-generative-ai.md)
- [Google Vertex Provider](providers/ai-sdk-providers/google-vertex.md)
- [Groq Provider](providers/ai-sdk-providers/groq.md)
- [Hume Provider](providers/ai-sdk-providers/hume.md)
- [LMNT Provider](providers/ai-sdk-providers/lmnt.md)
- [Luma Provider](providers/ai-sdk-providers/luma.md)
- [Mistral AI Provider](providers/ai-sdk-providers/mistral.md)
- [OpenAI Provider](providers/ai-sdk-providers/openai.md)
- [Perplexity Provider](providers/ai-sdk-providers/perplexity.md)
- [Replicate Provider](providers/ai-sdk-providers/replicate.md)
- [Rev.ai Provider](providers/ai-sdk-providers/revai.md)
- [Together.ai Provider](providers/ai-sdk-providers/togetherai.md)
- [Vercel Provider](providers/ai-sdk-providers/vercel.md)
- [xAI Grok Provider](providers/ai-sdk-providers/xai.md)

## Ai-sdk-rsc

- [AI SDK RSC](docs/ai-sdk-rsc.md)
- [AI SDK RSC](docs/ai-sdk-rsc/overview.md)
- [Authentication](docs/ai-sdk-rsc/authentication.md)
- [Designing Multistep Interfaces](docs/ai-sdk-rsc/multistep-interfaces.md)
- [Error Handling](docs/ai-sdk-rsc/error-handling.md)
- [Handling Loading State](docs/ai-sdk-rsc/loading-state.md)
- [Managing Generative UI State](docs/ai-sdk-rsc/generative-ui-state.md)
- [Migrating from RSC to UI](docs/ai-sdk-rsc/migrating-to-ui.md)
- [Saving and Restoring States](docs/ai-sdk-rsc/saving-and-restoring-states.md)
- [Streaming React Components](docs/ai-sdk-rsc/streaming-react-components.md)
- [Streaming Values](docs/ai-sdk-rsc/streaming-values.md)

## Ai-sdk-ui

- [AI SDK UI](docs/ai-sdk-ui.md)
- [AI SDK UI](docs/ai-sdk-ui/overview.md)
- [Chatbot](docs/ai-sdk-ui/chatbot.md)
- [Chatbot Message Persistence](docs/ai-sdk-ui/chatbot-message-persistence.md)
- [Chatbot Tool Usage](docs/ai-sdk-ui/chatbot-tool-usage.md)
- [Completion](docs/ai-sdk-ui/completion.md)
- [Error Handling](docs/ai-sdk-ui/error-handling.md)
- [Generative User Interfaces](docs/ai-sdk-ui/generative-user-interfaces.md)
- [Object Generation](docs/ai-sdk-ui/object-generation.md)
- [OpenAI Assistants](docs/ai-sdk-ui/openai-assistants.md)
- [Smooth streaming chinese text](docs/ai-sdk-ui/smooth-stream-chinese.md)
- [Smooth streaming japanese text](docs/ai-sdk-ui/smooth-stream-japanese.md)
- [Stream Protocols](docs/ai-sdk-ui/stream-protocol.md)
- [Streaming Custom Data](docs/ai-sdk-ui/streaming-data.md)

## Announcing-ai-sdk-5-alpha

- [Announcing AI SDK 5 Alpha](docs/announcing-ai-sdk-5-alpha.md)

## Anthropic:claude-3.5-haiku

- [Claude 3.5 Haiku by Anthropic on the AI Playground](playground/anthropic_claude-3.5-haiku.md)

## Anthropic:claude-3.7-sonnet

- [Claude 3.7 Sonnet by Anthropic on the AI Playground](playground/anthropic_claude-3.7-sonnet.md)

## Anthropic:claude-3.7-sonnet-reasoning

- [Claude 3.7 Sonnet Reasoning by Anthropic on the AI Playground](playground/anthropic_claude-3.7-sonnet-reasoning.md)

## Anthropic:claude-4-opus-20250514

- [Claude 4 Opus by Anthropic on the AI Playground](playground/anthropic_claude-4-opus-20250514.md)

## Anthropic:claude-4-sonnet-20250514

- [Claude 4 Sonnet by Anthropic on the AI Playground](playground/anthropic_claude-4-sonnet-20250514.md)

## Anthropic:claude-v2

- [Claude 2 by Anthropic on the AI Playground](playground/anthropic_claude-v2.md)

## Anthropic:claude-v3-haiku

- [Claude 3 Haiku by Anthropic on the AI Playground](playground/anthropic_claude-v3-haiku.md)

## Anthropic:claude-v3-opus

- [Claude 3 Opus by Anthropic on the AI Playground](playground/anthropic_claude-v3-opus.md)

## Anthropic:claude-v3-sonnet

- [Claude 3 Sonnet by Anthropic on the AI Playground](playground/anthropic_claude-v3-sonnet.md)

## Anthropic:claude-v3.5-sonnet

- [Claude 3.5 Sonnet by Anthropic on the AI Playground](playground/anthropic_claude-v3.5-sonnet.md)

## Bedrock:amazon.nova-lite-v1:0

- [Nova Lite by Amazon on the AI Playground](playground/bedrock_amazon.nova-lite-v1_0.md)

## Bedrock:amazon.nova-micro-v1:0

- [Nova Micro by Amazon on the AI Playground](playground/bedrock_amazon.nova-micro-v1_0.md)

## Bedrock:amazon.nova-pro-v1:0

- [Nova Pro by Amazon on the AI Playground](playground/bedrock_amazon.nova-pro-v1_0.md)

## Bedrock:claude-3-5-haiku-20241022

- [Claude 3.5 Haiku (Bedrock) by Amazon on the AI Playground](playground/bedrock_claude-3-5-haiku-20241022.md)

## Bedrock:claude-3-5-sonnet-20240620-v1

- [Claude 3.5 Sonnet (Bedrock) by Amazon on the AI Playground](playground/bedrock_claude-3-5-sonnet-20240620-v1.md)

## Bedrock:claude-3-5-sonnet-20241022-v2

- [Claude 3.5 Sonnet v2 (Bedrock) by Amazon on the AI Playground](playground/bedrock_claude-3-5-sonnet-20241022-v2.md)

## Bedrock:claude-3-7-sonnet-20250219

- [Claude 3.7 Sonnet (Bedrock) by Amazon on the AI Playground](playground/bedrock_claude-3-7-sonnet-20250219.md)

## Bedrock:claude-3-haiku-20240307-v1

- [Claude 3 Haiku (Bedrock) by Amazon on the AI Playground](playground/bedrock_claude-3-haiku-20240307-v1.md)

## Bedrock:claude-4-opus-20250514-v1

- [Claude 4 Opus (Bedrock) by Amazon on the AI Playground](playground/bedrock_claude-4-opus-20250514-v1.md)

## Bedrock:claude-4-sonnet-20250514-v1

- [Claude 4 Sonnet (Bedrock) by Amazon on the AI Playground](playground/bedrock_claude-4-sonnet-20250514-v1.md)

## Bedrock:deepseek.r1-v1

- [DeepSeek-R1 (Bedrock) by Amazon on the AI Playground](playground/bedrock_deepseek.r1-v1.md)

## Bedrock:meta.llama3-1-70b-instruct-v1

- [Llama 3.1 70B Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama3-1-70b-instruct-v1.md)

## Bedrock:meta.llama3-1-8b-instruct-v1

- [Llama 3.1 8B Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama3-1-8b-instruct-v1.md)

## Bedrock:meta.llama3-2-11b-instruct-v1

- [Llama 3.2 11B Vision Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama3-2-11b-instruct-v1.md)

## Bedrock:meta.llama3-2-1b-instruct-v1

- [Llama 3.2 1B Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama3-2-1b-instruct-v1.md)

## Bedrock:meta.llama3-2-3b-instruct-v1

- [Llama 3.2 3B Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama3-2-3b-instruct-v1.md)

## Bedrock:meta.llama3-2-90b-instruct-v1

- [Llama 3.2 90B Vision Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama3-2-90b-instruct-v1.md)

## Bedrock:meta.llama3-3-70b-instruct-v1

- [Llama 3.3 70B Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama3-3-70b-instruct-v1.md)

## Bedrock:meta.llama4-maverick-17b-instruct-v1

- [Llama 4 Maverick 17B Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama4-maverick-17b-instruct-v1.md)

## Bedrock:meta.llama4-scout-17b-instruct-v1

- [Llama 4 Scout 17B Instruct (Bedrock) by Amazon on the AI Playground](playground/bedrock_meta.llama4-scout-17b-instruct-v1.md)

## Cerebras:deepseek-r1-distill-llama-70b

- [DeepSeek R1 Distill Llama 70B by Cerebras on the AI Playground](playground/cerebras_deepseek-r1-distill-llama-70b.md)

## Cerebras:llama-3.3-70b

- [Llama 3.3 70B by Cerebras on the AI Playground](playground/cerebras_llama-3.3-70b.md)

## Cerebras:llama-4-scout-17b-16e-instruct

- [Llama 4 Scout by Cerebras on the AI Playground](playground/cerebras_llama-4-scout-17b-16e-instruct.md)

## Cerebras:llama3.1-8b

- [Llama 3.1 8B by Cerebras on the AI Playground](playground/cerebras_llama3.1-8b.md)

## Cerebras:qwen-3-32b

- [Qwen 3.32B by Cerebras on the AI Playground](playground/cerebras_qwen-3-32b.md)

## Cohere:command-a

- [Command A by Cohere on the AI Playground](playground/cohere_command-a.md)

## Cohere:command-light-nightly

- [Command Light Nightly by Cohere on the AI Playground](playground/cohere_command-light-nightly.md)

## Cohere:command-nightly

- [Command Nightly by Cohere on the AI Playground](playground/cohere_command-nightly.md)

## Cohere:command-r

- [Command R by Cohere on the AI Playground](playground/cohere_command-r.md)

## Cohere:command-r-plus

- [Command R+ by Cohere on the AI Playground](playground/cohere_command-r-plus.md)

## Community-providers

- [AI Gateway Provider](providers/community-providers/cloudflare-ai-gateway.md)
- [AnthropicVertex Provider](providers/community-providers/anthropic-vertex-ai.md)
- [Azure Custom Provider for AI SDK](providers/community-providers/azure-ai.md)
- [ChromeAI](providers/community-providers/chrome-ai.md)
- [Cloudflare Workers AI](providers/community-providers/cloudflare-workers-ai.md)
- [Community Providers](providers/community-providers.md)
- [Crosshatch Provider](providers/community-providers/crosshatch.md)
- [Dify Provider](providers/community-providers/dify.md)
- [FriendliAI Provider](providers/community-providers/friendliai.md)
- [LangDB](providers/community-providers/langdb.md)
- [Letta Provider](providers/community-providers/letta.md)
- [LLamaCpp Provider](providers/community-providers/llama-cpp.md)
- [Mem0 Provider](providers/community-providers/mem0.md)
- [Mixedbread Provider](providers/community-providers/mixedbread.md)
- [Ollama Provider](providers/community-providers/ollama.md)
- [OpenRouter](providers/community-providers/openrouter.md)
- [Portkey Provider](providers/community-providers/portkey.md)
- [Qwen Provider](providers/community-providers/qwen.md)
- [SambaNova Provider](providers/community-providers/sambanova.md)
- [Sarvam Provider](providers/community-providers/sarvam.md)
- [Spark Provider](providers/community-providers/spark.md)
- [Unofficial Community Provider for AI SDK - Inflection AI](providers/community-providers/inflection-ai.md)
- [Voyage AI Provider](providers/community-providers/voyage-ai.md)
- [Writing a Custom Provider](providers/community-providers/custom-providers.md)
- [Zhipu AI Provider](providers/community-providers/zhipu.md)

## Deepinfra:llama-4-maverick-17b-128e-instruct-fp8

- [Llama 4 Maverick 17B 128E Instruct FP8 by DeepInfra on the AI Playground](playground/deepinfra_llama-4-maverick-17b-128e-instruct-fp8.md)

## Deepinfra:llama-4-scout-17b-16e-instruct

- [Llama 4 Scout 17B 16E Instruct by DeepInfra on the AI Playground](playground/deepinfra_llama-4-scout-17b-16e-instruct.md)

## Deepinfra:qwen3-14b

- [Qwen3-14B by DeepInfra on the AI Playground](playground/deepinfra_qwen3-14b.md)

## Deepinfra:qwen3-235b-a22b

- [Qwen3-235B-A22B by DeepInfra on the AI Playground](playground/deepinfra_qwen3-235b-a22b.md)

## Deepinfra:qwen3-30b-a3b

- [Qwen3-30B-A3B by DeepInfra on the AI Playground](playground/deepinfra_qwen3-30b-a3b.md)

## Deepinfra:qwen3-32b

- [Qwen3-32B by DeepInfra on the AI Playground](playground/deepinfra_qwen3-32b.md)

## Deepseek:chat

- [DeepSeek-V3 by DeepSeek on the AI Playground](playground/deepseek_chat.md)

## Deepseek:deepseek-r1

- [DeepSeek R1 by DeepSeek on the AI Playground](playground/deepseek_deepseek-r1.md)

## Deepseek:deepseek-r1-0528

- [DeepSeek R1 0528 by DeepSeek on the AI Playground](playground/deepseek_deepseek-r1-0528.md)

## Fireworks:deepseek-r1

- [DeepSeek R1 by Fireworks on the AI Playground](playground/fireworks_deepseek-r1.md)

## Fireworks:deepseek-v3

- [DeepSeek-V3 by Fireworks on the AI Playground](playground/fireworks_deepseek-v3.md)

## Fireworks:firefunction-v1

- [FireFunction V1 by Fireworks on the AI Playground](playground/fireworks_firefunction-v1.md)

## Fireworks:mixtral-8x22b-instruct

- [Mixtral MoE 8x22B Instruct by Mistral on the AI Playground](playground/fireworks_mixtral-8x22b-instruct.md)

## Fireworks:mixtral-8x7b-instruct

- [Mixtral MoE 8x7B Instruct by Mistral on the AI Playground](playground/fireworks_mixtral-8x7b-instruct.md)

## Fireworks:qwen3-235b-a22b

- [Qwen3-235B-A22B by Fireworks on the AI Playground](playground/fireworks_qwen3-235b-a22b.md)

## Fireworks:qwq-32b

- [QwQ-32B by Fireworks on the AI Playground](playground/fireworks_qwq-32b.md)

## Foundations

- [Agents](docs/foundations/agents.md)
- [Foundations](docs/foundations.md)
- [Overview](docs/foundations/overview.md)
- [Prompts](docs/foundations/prompts.md)
- [Providers and Models](docs/foundations/providers-and-models.md)
- [Streaming](docs/foundations/streaming.md)
- [Tools](docs/foundations/tools.md)

## Getting-started

- [Expo Quickstart](docs/getting-started/expo.md)
- [Getting Started](docs/getting-started.md)
- [Navigating the Library](docs/getting-started/navigating-the-library.md)
- [Next.js App Router Quickstart](docs/getting-started/nextjs-app-router.md)
- [Next.js Pages Router Quickstart](docs/getting-started/nextjs-pages-router.md)
- [Node.js Quickstart](docs/getting-started/nodejs.md)
- [Svelte Quickstart](docs/getting-started/svelte.md)
- [Vue.js (Nuxt) Quickstart](docs/getting-started/nuxt.md)

## Google:gemini-1.5-flash

- [Gemini 1.5 Flash 001 by Google on the AI Playground](playground/google_gemini-1.5-flash.md)

## Google:gemini-1.5-flash-002

- [Gemini 1.5 Flash 002 by Google on the AI Playground](playground/google_gemini-1.5-flash-002.md)

## Google:gemini-1.5-flash-8b

- [Gemini 1.5 Flash 8b by Google on the AI Playground](playground/google_gemini-1.5-flash-8b.md)

## Google:gemini-1.5-pro

- [Gemini 1.5 Pro 001 by Google on the AI Playground](playground/google_gemini-1.5-pro.md)

## Google:gemini-1.5-pro-002

- [Gemini 1.5 Pro 002 by Google on the AI Playground](playground/google_gemini-1.5-pro-002.md)

## Google:gemini-2.0-flash-001

- [Gemini 2.0 Flash by Google on the AI Playground](playground/google_gemini-2.0-flash-001.md)

## Google:gemini-2.0-flash-lite-preview-02-05

- [Gemini 2.0 Flash Lite Preview by Google on the AI Playground](playground/google_gemini-2.0-flash-lite-preview-02-05.md)

## Google:gemini-2.5-flash-preview-04-17

- [Gemini 2.5 Flash Preview by Google on the AI Playground](playground/google_gemini-2.5-flash-preview-04-17.md)

## Google:gemini-2.5-pro-preview-03-25

- [Gemini 2.5 Pro Preview by Google on the AI Playground](playground/google_gemini-2.5-pro-preview-03-25.md)

## Google:gemma-3-27b-it

- [Gemma 3 27B by Google on the AI Playground](playground/google_gemma-3-27b-it.md)

## Groq:deepseek-r1-distill-llama-70b

- [DeepSeek R1 Distill Llama 70B by Groq on the AI Playground](playground/groq_deepseek-r1-distill-llama-70b.md)

## Groq:gemma2-9b-it

- [Gemma 2 9B IT by Groq on the AI Playground](playground/groq_gemma2-9b-it.md)

## Groq:llama-3-70b-instruct

- [Llama 3 70B Instruct by Groq on the AI Playground](playground/groq_llama-3-70b-instruct.md)

## Groq:llama-3-8b-instruct

- [Llama 3 8B Instruct by Groq on the AI Playground](playground/groq_llama-3-8b-instruct.md)

## Groq:llama-3.1-8b

- [Llama 3.1 8B Instant by Groq on the AI Playground](playground/groq_llama-3.1-8b.md)

## Groq:llama-3.2-11b-vision-preview

- [Llama 3.2 11B by Groq on the AI Playground](playground/groq_llama-3.2-11b-vision-preview.md)

## Groq:llama-3.2-1b

- [Llama 3.2 1B by Groq on the AI Playground](playground/groq_llama-3.2-1b.md)

## Groq:llama-3.2-3b

- [Llama 3.2 3B by Groq on the AI Playground](playground/groq_llama-3.2-3b.md)

## Groq:llama-3.2-90b-vision-preview

- [Llama 3.2 90B by Groq on the AI Playground](playground/groq_llama-3.2-90b-vision-preview.md)

## Groq:llama-3.3-70b-versatile

- [Llama 3.3 70B Versatile by Groq on the AI Playground](playground/groq_llama-3.3-70b-versatile.md)

## Groq:llama-4-scout-17b-16e-instruct

- [Llama 4 Scout 17B 16E Instruct by Groq on the AI Playground](playground/groq_llama-4-scout-17b-16e-instruct.md)

## Groq:mistral-saba-24b

- [Mistral Saba 24B by Groq on the AI Playground](playground/groq_mistral-saba-24b.md)

## Groq:qwen-qwq-32b

- [QWQ-32B by Groq on the AI Playground](playground/groq_qwen-qwq-32b.md)

## Guides

- [Building a Slack AI Chatbot with the AI SDK](docs/guides/slackbot.md)
- [Get started with Claude 3.7 Sonnet](docs/guides/sonnet-3-7.md)
- [Get started with Claude 4](docs/guides/claude-4.md)
- [Get started with Computer Use](docs/guides/computer-use.md)
- [Get started with DeepSeek R1](docs/guides/r1.md)
- [Get started with Llama 3.1](docs/guides/llama-3_1.md)
- [Get started with OpenAI GPT-4.5](docs/guides/gpt-4-5.md)
- [Get started with OpenAI o1](docs/guides/o1.md)
- [Get started with OpenAI o3-mini](docs/guides/o3.md)
- [Get started with OpenAI Responses API](docs/guides/openai-responses.md)
- [Guides](docs/guides.md)
- [Multi-Modal Chatbot](docs/guides/multi-modal-chatbot.md)
- [Natural Language Postgres Guide](docs/guides/natural-language-postgres.md)
- [RAG Chatbot Guide](docs/guides/rag-chatbot.md)

## Inception:mercury-coder-small

- [Mercury Coder Small Beta by Inception on the AI Playground](playground/inception_mercury-coder-small.md)

## Introduction

- [AI SDK](docs/introduction.md)

## Migration-guides

- [Migrate AI SDK 3.0 to 3.1](docs/migration-guides/migration-guide-3-1.md)
- [Migrate AI SDK 3.1 to 3.2](docs/migration-guides/migration-guide-3-2.md)
- [Migrate AI SDK 3.2 to 3.3](docs/migration-guides/migration-guide-3-3.md)
- [Migrate AI SDK 3.3 to 3.4](docs/migration-guides/migration-guide-3-4.md)
- [Migrate AI SDK 3.4 to 4.0](docs/migration-guides/migration-guide-4-0.md)
- [Migrate AI SDK 4.0 to 4.1](docs/migration-guides/migration-guide-4-1.md)
- [Migrate AI SDK 4.1 to 4.2](docs/migration-guides/migration-guide-4-2.md)
- [Migration Guides](docs/migration-guides.md)
- [Versioning](docs/migration-guides/versioning.md)

## Mistral:codestral-2501

- [Mistral Codestral 25.01 by Mistral on the AI Playground](playground/mistral_codestral-2501.md)

## Mistral:ministral-3b-latest

- [Ministral 3B by Mistral on the AI Playground](playground/mistral_ministral-3b-latest.md)

## Mistral:ministral-8b-latest

- [Ministral 8B by Mistral on the AI Playground](playground/mistral_ministral-8b-latest.md)

## Mistral:mistral-large

- [Mistral Large by Mistral on the AI Playground](playground/mistral_mistral-large.md)

## Mistral:mistral-small

- [Mistral Small by Mistral on the AI Playground](playground/mistral_mistral-small.md)

## Mistral:mistral-small-2503

- [Mistral Small 2503 by Mistral on the AI Playground](playground/mistral_mistral-small-2503.md)

## Mistral:pixtral-12b-2409

- [Pixtral 12B 2409 by Mistral on the AI Playground](playground/mistral_pixtral-12b-2409.md)

## Mistral:pixtral-large-latest

- [Pixtral Large by Mistral on the AI Playground](playground/mistral_pixtral-large-latest.md)

## Next

- [Caching Middleware](cookbook/next/caching-middleware.md)
- [Call Tools](cookbook/next/call-tools.md)
- [Call Tools in Multiple Steps](cookbook/next/call-tools-multiple-steps.md)
- [Call Tools in Parallel](cookbook/next/call-tools-in-parallel.md)
- [Chat with PDFs](cookbook/next/chat-with-pdf.md)
- [Generate Image with Chat Prompt](cookbook/next/generate-image-with-chat-prompt.md)
- [Generate Object](cookbook/next/generate-object.md)
- [Generate Object with File Prompt through Form Submission](cookbook/next/generate-object-with-file-prompt.md)
- [Generate Text](cookbook/next/generate-text.md)
- [Generate Text with Chat Prompt](cookbook/next/generate-text-with-chat-prompt.md)
- [Human-in-the-Loop with Next.js](cookbook/next/human-in-the-loop.md)
- [Markdown Chatbot with Memoization](cookbook/next/markdown-chatbot-with-memoization.md)
- [MCP Tools](cookbook/next/mcp-tools.md)
- [Render Visual Interface in Chat](cookbook/next/render-visual-interface-in-chat.md)
- [Send Custom Body from useChat](cookbook/next/send-custom-body-from-use-chat.md)
- [Stream Assistant Response](cookbook/next/stream-assistant-response.md)
- [Stream Assistant Response with Tools](cookbook/next/stream-assistant-response-with-tools.md)
- [Stream Object](cookbook/next/stream-object.md)
- [Stream Text](cookbook/next/stream-text.md)
- [Stream Text Multi-Step](cookbook/next/stream-text-multistep.md)
- [Stream Text with Chat Prompt](cookbook/next/stream-text-with-chat-prompt.md)
- [Stream Text with Image Prompt](cookbook/next/stream-text-with-image-prompt.md)

## Node

- [Call Tools](cookbook/node/call-tools.md)
- [Call Tools in Multiple Steps](cookbook/node/call-tools-multiple-steps.md)
- [Call Tools in Parallel](cookbook/node/call-tools-in-parallel.md)
- [Call Tools with Image Prompt](cookbook/node/call-tools-with-image-prompt.md)
- [Embed Text](cookbook/node/embed-text.md)
- [Embed Text in Batch](cookbook/node/embed-text-batch.md)
- [Generate Object](cookbook/node/generate-object.md)
- [Generate Object with a Reasoning Model](cookbook/node/generate-object-reasoning.md)
- [Generate Text](cookbook/node/generate-text.md)
- [Generate Text with Chat Prompt](cookbook/node/generate-text-with-chat-prompt.md)
- [Generate Text with Image Prompt](cookbook/node/generate-text-with-image-prompt.md)
- [Intercepting Fetch Requests](cookbook/node/intercept-fetch-requests.md)
- [Local Caching Middleware](cookbook/node/local-caching-middleware.md)
- [MCP Tools](cookbook/node/mcp-tools.md)
- [Record Final Object after Streaming Object](cookbook/node/stream-object-record-final-object.md)
- [Record Token Usage After Streaming Object](cookbook/node/stream-object-record-token-usage.md)
- [Retrieval Augmented Generation](cookbook/node/retrieval-augmented-generation.md)
- [Stream Object](cookbook/node/stream-object.md)
- [Stream Object with Image Prompt](cookbook/node/stream-object-with-image-prompt.md)
- [Stream Text](cookbook/node/stream-text.md)
- [Stream Text with Chat Prompt](cookbook/node/stream-text-with-chat-prompt.md)
- [Stream Text with File Prompt](cookbook/node/stream-text-with-file-prompt.md)
- [Stream Text with Image Prompt](cookbook/node/stream-text-with-image-prompt.md)
- [Web Search Agent](cookbook/node/web-search-agent.md)

## Observability

- [Braintrust Observability](providers/observability/braintrust.md)
- [Helicone Observability](providers/observability/helicone.md)
- [Laminar observability](providers/observability/laminar.md)
- [Langfuse Observability](providers/observability/langfuse.md)
- [LangSmith Observability](providers/observability/langsmith.md)
- [LangWatch Observability](providers/observability/langwatch.md)
- [Observability Integrations](providers/observability.md)
- [Patronus Observability](providers/observability/patronus.md)
- [Traceloop](providers/observability/traceloop.md)
- [Weave Observability](providers/observability/weave.md)

## Openai-compatible-providers

- [Baseten Provider](providers/openai-compatible-providers/baseten.md)
- [LM Studio Provider](providers/openai-compatible-providers/lmstudio.md)
- [NVIDIA NIM Provider](providers/openai-compatible-providers/nim.md)
- [OpenAI Compatible Providers](providers/openai-compatible-providers.md)
- [Writing a Custom Provider](providers/openai-compatible-providers/custom-providers.md)

## Openai:gpt-3.5-turbo

- [GPT-3.5 Turbo by OpenAI on the AI Playground](playground/openai_gpt-3.5-turbo.md)

## Openai:gpt-3.5-turbo-instruct

- [GPT-3.5 Turbo Instruct by OpenAI on the AI Playground](playground/openai_gpt-3.5-turbo-instruct.md)

## Openai:gpt-4-turbo

- [GPT-4 Turbo by OpenAI on the AI Playground](playground/openai_gpt-4-turbo.md)

## Openai:gpt-4.1

- [GPT-4.1 by OpenAI on the AI Playground](playground/openai_gpt-4.1.md)

## Openai:gpt-4.1-mini

- [GPT-4.1 mini by OpenAI on the AI Playground](playground/openai_gpt-4.1-mini.md)

## Openai:gpt-4.1-nano

- [GPT-4.1 nano by OpenAI on the AI Playground](playground/openai_gpt-4.1-nano.md)

## Openai:gpt-4.5-preview

- [GPT-4.5 Preview by OpenAI on the AI Playground](playground/openai_gpt-4.5-preview.md)

## Openai:gpt-4o

- [GPT-4o by OpenAI on the AI Playground](playground/openai_gpt-4o.md)

## Openai:gpt-4o-mini

- [GPT-4o mini by OpenAI on the AI Playground](playground/openai_gpt-4o-mini.md)

## Openai:o3

- [o3 by OpenAI on the AI Playground](playground/openai_o3.md)

## Openai:o3-mini

- [o3-mini by OpenAI on the AI Playground](playground/openai_o3-mini.md)

## Openai:o3-mini-high

- [o3-mini (High) by OpenAI on the AI Playground](playground/openai_o3-mini-high.md)

## Openai:o3-mini-low

- [o3-mini (Low) by OpenAI on the AI Playground](playground/openai_o3-mini-low.md)

## Openai:o3-mini-medium

- [o3-mini (Medium) by OpenAI on the AI Playground](playground/openai_o3-mini-medium.md)

## Openai:o4-mini

- [o4-mini by OpenAI on the AI Playground](playground/openai_o4-mini.md)

- [AI Playground | Compare top AI models side-by-side](playground.md)
- [AI SDK](index.md)
- [AI SDK Showcase](showcase.md)
- [Cookbook](cookbook.md)
- [Getting Started with the AI SDK](getting-started.md)

## Perplexity:sonar

- [Sonar by Perplexity on the AI Playground](playground/perplexity_sonar.md)

## Perplexity:sonar-pro

- [Sonar Pro by Perplexity on the AI Playground](playground/perplexity_sonar-pro.md)

## Perplexity:sonar-reasoning

- [Sonar Reasoning by Perplexity on the AI Playground](playground/perplexity_sonar-reasoning.md)

## Perplexity:sonar-reasoning-pro

- [Sonar Reasoning Pro by Perplexity on the AI Playground](playground/perplexity_sonar-reasoning-pro.md)

## Reference

- [AI SDK Core](docs/reference/ai-sdk-core.md)
- [AI SDK Errors](docs/reference/ai-sdk-errors.md)
- [AI SDK RSC](docs/reference/ai-sdk-rsc.md)
- [AI SDK UI](docs/reference/ai-sdk-ui.md)
- [AI_APICallError](docs/reference/ai-sdk-errors/ai-api-call-error.md)
- [AI_DownloadError](docs/reference/ai-sdk-errors/ai-download-error.md)
- [AI_EmptyResponseBodyError](docs/reference/ai-sdk-errors/ai-empty-response-body-error.md)
- [AI_InvalidArgumentError](docs/reference/ai-sdk-errors/ai-invalid-argument-error.md)
- [AI_InvalidDataContent](docs/reference/ai-sdk-errors/ai-invalid-data-content.md)
- [AI_InvalidDataContentError](docs/reference/ai-sdk-errors/ai-invalid-data-content-error.md)
- [AI_InvalidMessageRoleError](docs/reference/ai-sdk-errors/ai-invalid-message-role-error.md)
- [AI_InvalidPromptError](docs/reference/ai-sdk-errors/ai-invalid-prompt-error.md)
- [AI_InvalidResponseDataError](docs/reference/ai-sdk-errors/ai-invalid-response-data-error.md)
- [AI_InvalidToolArgumentsError](docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error.md)
- [AI_JSONParseError](docs/reference/ai-sdk-errors/ai-json-parse-error.md)
- [AI_LoadAPIKeyError](docs/reference/ai-sdk-errors/ai-load-api-key-error.md)
- [AI_LoadSettingError](docs/reference/ai-sdk-errors/ai-load-setting-error.md)
- [AI_MessageConversionError](docs/reference/ai-sdk-errors/ai-message-conversion-error.md)
- [AI_NoAudioGeneratedError](docs/reference/ai-sdk-errors/ai-no-audio-generated-error.md)
- [AI_NoContentGeneratedError](docs/reference/ai-sdk-errors/ai-no-content-generated-error.md)
- [AI_NoImageGeneratedError](docs/reference/ai-sdk-errors/ai-no-image-generated-error.md)
- [AI_NoObjectGeneratedError](docs/reference/ai-sdk-errors/ai-no-object-generated-error.md)
- [AI_NoOutputSpecifiedError](docs/reference/ai-sdk-errors/ai-no-output-specified-error.md)
- [AI_NoSuchModelError](docs/reference/ai-sdk-errors/ai-no-such-model-error.md)
- [AI_NoSuchProviderError](docs/reference/ai-sdk-errors/ai-no-such-provider-error.md)
- [AI_NoSuchToolError](docs/reference/ai-sdk-errors/ai-no-such-tool-error.md)
- [AI_NoTranscriptGeneratedError](docs/reference/ai-sdk-errors/ai-no-transcript-generated-error.md)
- [AI_RetryError](docs/reference/ai-sdk-errors/ai-retry-error.md)
- [AI_ToolExecutionError](docs/reference/ai-sdk-errors/ai-tool-execution-error.md)
- [AI_TooManyEmbeddingValuesForCallError](docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error.md)
- [AI_TypeValidationError](docs/reference/ai-sdk-errors/ai-type-validation-error.md)
- [AI_UnsupportedFunctionalityError](docs/reference/ai-sdk-errors/ai-unsupported-functionality-error.md)
- [AIStream](docs/reference/stream-helpers/ai-stream.md)
- [AnthropicStream](docs/reference/stream-helpers/anthropic-stream.md)
- [API Reference](docs/reference.md)
- [appendClientMessage()](docs/reference/ai-sdk-ui/append-client-message.md)
- [appendResponseMessages()](docs/reference/ai-sdk-ui/append-response-messages.md)
- [AssistantResponse](docs/reference/ai-sdk-ui/assistant-response.md)
- [AWSBedrockAnthropicMessagesStream](docs/reference/stream-helpers/aws-bedrock-messages-stream.md)
- [AWSBedrockAnthropicStream](docs/reference/stream-helpers/aws-bedrock-anthropic-stream.md)
- [AWSBedrockCohereStream](docs/reference/stream-helpers/aws-bedrock-cohere-stream.md)
- [AWSBedrockLlama2Stream](docs/reference/stream-helpers/aws-bedrock-llama-2-stream.md)
- [AWSBedrockStream](docs/reference/stream-helpers/aws-bedrock-stream.md)
- [CohereStream](docs/reference/stream-helpers/cohere-stream.md)
- [convertToCoreMessages()](docs/reference/ai-sdk-ui/convert-to-core-messages.md)
- [CoreMessage](docs/reference/ai-sdk-core/core-message.md)
- [cosineSimilarity()](docs/reference/ai-sdk-core/cosine-similarity.md)
- [createAI](docs/reference/ai-sdk-rsc/create-ai.md)
- [createDataStream](docs/reference/ai-sdk-ui/create-data-stream.md)
- [createDataStreamResponse](docs/reference/ai-sdk-ui/create-data-stream-response.md)
- [createIdGenerator()](docs/reference/ai-sdk-core/create-id-generator.md)
- [createProviderRegistry()](docs/reference/ai-sdk-core/provider-registry.md)
- [createStreamableUI](docs/reference/ai-sdk-rsc/create-streamable-ui.md)
- [createStreamableValue](docs/reference/ai-sdk-rsc/create-streamable-value.md)
- [customProvider()](docs/reference/ai-sdk-core/custom-provider.md)
- [defaultSettingsMiddleware()](docs/reference/ai-sdk-core/default-settings-middleware.md)
- [embed()](docs/reference/ai-sdk-core/embed.md)
- [embedMany()](docs/reference/ai-sdk-core/embed-many.md)
- [experimental_createMCPClient()](docs/reference/ai-sdk-core/create-mcp-client.md)
- [Experimental_StdioMCPTransport](docs/reference/ai-sdk-core/mcp-stdio-transport.md)
- [experimental_useObject()](docs/reference/ai-sdk-ui/use-object.md)
- [extractReasoningMiddleware()](docs/reference/ai-sdk-core/extract-reasoning-middleware.md)
- [generateId()](docs/reference/ai-sdk-core/generate-id.md)
- [generateImage()](docs/reference/ai-sdk-core/generate-image.md)
- [generateObject()](docs/reference/ai-sdk-core/generate-object.md)
- [generateSpeech()](docs/reference/ai-sdk-core/generate-speech.md)
- [generateText()](docs/reference/ai-sdk-core/generate-text.md)
- [getAIState](docs/reference/ai-sdk-rsc/get-ai-state.md)
- [getMutableAIState](docs/reference/ai-sdk-rsc/get-mutable-ai-state.md)
- [GoogleGenerativeAIStream](docs/reference/stream-helpers/google-generative-ai-stream.md)
- [HuggingFaceStream](docs/reference/stream-helpers/hugging-face-stream.md)
- [InkeepStream](docs/reference/stream-helpers/inkeep-stream.md)
- [jsonSchema()](docs/reference/ai-sdk-core/json-schema.md)
- [LangChainAdapter](docs/reference/stream-helpers/langchain-adapter.md)
- [LangChainStream](docs/reference/stream-helpers/langchain-stream.md)
- [LanguageModelV1Middleware](docs/reference/ai-sdk-core/language-model-v1-middleware.md)
- [LlamaIndexAdapter](docs/reference/stream-helpers/llamaindex-adapter.md)
- [MistralStream](docs/reference/stream-helpers/mistral-stream.md)
- [OpenAIStream](docs/reference/stream-helpers/openai-stream.md)
- [pipeDataStreamToResponse](docs/reference/ai-sdk-ui/pipe-data-stream-to-response.md)
- [readStreamableValue](docs/reference/ai-sdk-rsc/read-streamable-value.md)
- [Reference: Stream Helpers](docs/reference/stream-helpers.md)
- [render (Removed)](docs/reference/ai-sdk-rsc/render.md)
- [ReplicateStream](docs/reference/stream-helpers/replicate-stream.md)
- [simulateReadableStream()](docs/reference/ai-sdk-core/simulate-readable-stream.md)
- [simulateStreamingMiddleware()](docs/reference/ai-sdk-core/simulate-streaming-middleware.md)
- [smoothStream()](docs/reference/ai-sdk-core/smooth-stream.md)
- [StreamData](docs/reference/ai-sdk-ui/stream-data.md)
- [StreamingTextResponse](docs/reference/stream-helpers/streaming-text-response.md)
- [streamObject()](docs/reference/ai-sdk-core/stream-object.md)
- [streamText()](docs/reference/ai-sdk-core/stream-text.md)
- [streamToResponse](docs/reference/stream-helpers/stream-to-response.md)
- [streamUI](docs/reference/ai-sdk-rsc/stream-ui.md)
- [tool()](docs/reference/ai-sdk-core/tool.md)
- [ToolCallRepairError](docs/reference/ai-sdk-errors/ai-tool-call-repair-error.md)
- [transcribe()](docs/reference/ai-sdk-core/transcribe.md)
- [useActions](docs/reference/ai-sdk-rsc/use-actions.md)
- [useAIState](docs/reference/ai-sdk-rsc/use-ai-state.md)
- [useAssistant()](docs/reference/ai-sdk-ui/use-assistant.md)
- [useChat()](docs/reference/ai-sdk-ui/use-chat.md)
- [useCompletion()](docs/reference/ai-sdk-ui/use-completion.md)
- [useStreamableValue](docs/reference/ai-sdk-rsc/use-streamable-value.md)
- [useUIState](docs/reference/ai-sdk-rsc/use-ui-state.md)
- [valibotSchema()](docs/reference/ai-sdk-core/valibot-schema.md)
- [wrapLanguageModel()](docs/reference/ai-sdk-core/wrap-language-model.md)
- [zodSchema()](docs/reference/ai-sdk-core/zod-schema.md)

## Rsc

- [Call Tools](cookbook/rsc/call-tools.md)
- [Call Tools in Parallel](cookbook/rsc/call-tools-in-parallel.md)
- [Generate Object](cookbook/rsc/generate-object.md)
- [Generate Text](cookbook/rsc/generate-text.md)
- [Generate Text with Chat Prompt](cookbook/rsc/generate-text-with-chat-prompt.md)
- [Record Token Usage after Streaming User Interfaces](cookbook/rsc/stream-ui-record-token-usage.md)
- [Render Visual Interface in Chat](cookbook/rsc/render-visual-interface-in-chat.md)
- [Restore Messages from Database](cookbook/rsc/restore-messages-from-database.md)
- [Save Messages To Database](cookbook/rsc/save-messages-to-database.md)
- [Stream Assistant Responses](cookbook/rsc/stream-assistant-response.md)
- [Stream Assistant Responses](cookbook/rsc/stream-assistant-response-with-tools.md)
- [Stream Object](cookbook/rsc/stream-object.md)
- [Stream Text](cookbook/rsc/stream-text.md)
- [Stream Text with Chat Prompt](cookbook/rsc/stream-text-with-chat-prompt.md)
- [Stream Updates to Visual Interfaces](cookbook/rsc/stream-updates-to-visual-interfaces.md)

## Troubleshooting

- ["Only plain objects can be passed from client components" Server Action Error](docs/troubleshooting/client-stream-error.md)
- [Azure OpenAI Slow To Stream](docs/troubleshooting/azure-stream-slow.md)
- [Client-Side Function Calls Not Invoked](docs/troubleshooting/client-side-function-calls-not-invoked.md)
- [Getting Timeouts When Deploying on Vercel](docs/troubleshooting/timeout-on-vercel.md)
- [Jest: cannot find module 'ai/rsc'](docs/troubleshooting/jest-cannot-find-module-ai-rsc.md)
- [Model is not assignable to type "LanguageModelV1"](docs/troubleshooting/model-is-not-assignable-to-type.md)
- [NaN token counts when using streamText with OpenAI models](docs/troubleshooting/nan-token-counts-openai-streaming.md)
- [React error "Maximum update depth exceeded"](docs/troubleshooting/react-maximum-update-depth-exceeded.md)
- [Server Actions in Client Components](docs/troubleshooting/server-actions-in-client-components.md)
- [Streamable UI Component Error](docs/troubleshooting/streamable-ui-errors.md)
- [Streaming Not Working When Deployed](docs/troubleshooting/streaming-not-working-when-deployed.md)
- [Streaming Not Working When Proxied](docs/troubleshooting/streaming-not-working-when-proxied.md)
- [streamText is not working](docs/troubleshooting/stream-text-not-working.md)
- [Tool Invocation Missing Result Error](docs/troubleshooting/tool-invocation-missing-result.md)
- [Troubleshooting](docs/troubleshooting.md)
- [TypeScript error "Cannot find namespace 'JSX'"](docs/troubleshooting/typescript-cannot-find-namespace-jsx.md)
- [Unclosed Streams](docs/troubleshooting/unclosed-streams.md)
- [useChat "An error occurred"](docs/troubleshooting/use-chat-an-error-occurred.md)
- [useChat "Failed to Parse Stream String" Error](docs/troubleshooting/use-chat-failed-to-parse-stream.md)
- [useChat No Response with maxSteps](docs/troubleshooting/use-chat-tools-no-response.md)
- [useChat/useCompletion stream output contains 0:... instead of text](docs/troubleshooting/strange-stream-output.md)

## Vertex:claude-3-5-haiku-20241022

- [Claude 3.5 Haiku (Vertex) by Vertex on the AI Playground](playground/vertex_claude-3-5-haiku-20241022.md)

## Vertex:claude-3-5-sonnet-20240620

- [Claude 3.5 Sonnet (Vertex) by Vertex on the AI Playground](playground/vertex_claude-3-5-sonnet-20240620.md)

## Vertex:claude-3-5-sonnet-v2-20241022

- [Claude 3.5 Sonnet v2 (Vertex) by Vertex on the AI Playground](playground/vertex_claude-3-5-sonnet-v2-20241022.md)

## Vertex:claude-3-7-sonnet-20250219

- [Claude 3.7 Sonnet (Vertex) by Vertex on the AI Playground](playground/vertex_claude-3-7-sonnet-20250219.md)

## Vertex:claude-3-haiku-20240307

- [Claude 3 Haiku (Vertex) by Vertex on the AI Playground](playground/vertex_claude-3-haiku-20240307.md)

## Vertex:claude-3-opus-20240229

- [Claude 3 Opus (Vertex) by Vertex on the AI Playground](playground/vertex_claude-3-opus-20240229.md)

## Vertex:claude-4-opus-20250514

- [Claude 4 Opus (Vertex) by Vertex on the AI Playground](playground/vertex_claude-4-opus-20250514.md)

## Vertex:claude-4-sonnet-20250514

- [Claude 4 Sonnet (Vertex) by Vertex on the AI Playground](playground/vertex_claude-4-sonnet-20250514.md)

## Vertex:gemini-2.0-flash-001

- [Gemini 2.0 Flash (Vertex) by Vertex on the AI Playground](playground/vertex_gemini-2.0-flash-001.md)

## Vertex:gemini-2.0-flash-lite-001

- [Gemini 2.0 Flash Lite (Vertex) by Vertex on the AI Playground](playground/vertex_gemini-2.0-flash-lite-001.md)

## Vertex:gemini-2.5-flash-preview-04-17

- [Gemini 2.5 Flash Preview (Vertex) by Vertex on the AI Playground](playground/vertex_gemini-2.5-flash-preview-04-17.md)

## Vertex:gemini-2.5-pro-preview-05-06

- [Gemini 2.5 Pro Preview (Vertex) by Vertex on the AI Playground](playground/vertex_gemini-2.5-pro-preview-05-06.md)

## Vertex:llama-3.3-70b-instruct-maas

- [Llama 3.3 70B (Vertex) by Vertex on the AI Playground](playground/vertex_llama-3.3-70b-instruct-maas.md)

## Vertex:llama-4-maverick-17b-128e-instruct-maas

- [Llama 4 Maverick 17B 128E Instruct (Vertex) by Vertex on the AI Playground](playground/vertex_llama-4-maverick-17b-128e-instruct-maas.md)

## Vertex:llama-4-scout-17b-16e-instruct-maas

- [Llama 4 Scout 17B 16E Instruct (Vertex) by Vertex on the AI Playground](playground/vertex_llama-4-scout-17b-16e-instruct-maas.md)

## Xai:grok-2-1212

- [Grok 2 by xAI on the AI Playground](playground/xai_grok-2-1212.md)

## Xai:grok-2-vision-1212

- [Grok 2 Vision by xAI on the AI Playground](playground/xai_grok-2-vision-1212.md)

## Xai:grok-3-beta

- [Grok 3 Beta by xAI on the AI Playground](playground/xai_grok-3-beta.md)

## Xai:grok-3-fast-beta

- [Grok 3 Fast Beta by xAI on the AI Playground](playground/xai_grok-3-fast-beta.md)

## Xai:grok-3-mini-beta

- [Grok 3 Mini Beta by xAI on the AI Playground](playground/xai_grok-3-mini-beta.md)

## Xai:grok-3-mini-fast-beta

- [Grok 3 Mini Fast Beta by xAI on the AI Playground](playground/xai_grok-3-mini-fast-beta.md)

## Xai:grok-beta

- [Grok Beta by xAI on the AI Playground](playground/xai_grok-beta.md)

## Xai:grok-vision-beta

- [Grok Vision Beta by xAI on the AI Playground](playground/xai_grok-vision-beta.md)
```

### 2. `cookbook/next/caching-middleware.md`

```markdown
# Caching Middleware


---
url: https://ai-sdk.dev/cookbook/next/caching-middleware
description: Learn how to create a caching middleware with Next.js and KV.
---


# [Caching Middleware](#caching-middleware)


Let's create a simple chat interface that uses [`LanguageModelMiddleware`](/docs/ai-sdk-core/middleware) to cache the assistant's responses in fast KV storage.


## [Client](#client)


Let's create a simple chat interface that allows users to send messages to the assistant and receive responses. You will integrate the `useChat` hook from `@ai-sdk/react` to stream responses.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit, error }=useChat();if(error)return<div>{error.message}</div>;return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch"><divclassName="space-y-4">{messages.map(m=>(<divkey={m.id}className="whitespace-pre-wrap"><div><divclassName="font-bold">{m.role}</div>{m.toolInvocations ?(<pre>{JSON.stringify(m.toolInvocations,null,2)}</pre>):(<p>{m.content}</p>)}</div></div>))}</div><formonSubmit={handleSubmit}><inputclassName="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```


## [Middleware](#middleware)


Next, you will create a `LanguageModelMiddleware` that caches the assistant's responses in KV storage. `LanguageModelMiddleware` has two methods: `wrapGenerate` and `wrapStream`. `wrapGenerate` is called when using [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`generateObject`](/docs/reference/ai-sdk-core/generate-object), while `wrapStream` is called when using [`streamText`](/docs/reference/ai-sdk-core/stream-text) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object).

For `wrapGenerate`, you can cache the response directly. Instead, for `wrapStream`, you cache an array of the stream parts, which can then be used with [`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream) function to create a simulated `ReadableStream` that returns the cached response. In this way, the cached response is returned chunk-by-chunk as if it were being generated by the model. You can control the initial delay and delay between chunks by adjusting the `initialDelayInMs` and `chunkDelayInMs` parameters of `simulateReadableStream`.

ai/middleware.ts

```
import{Redis}from'@upstash/redis';import{typeLanguageModelV1,typeLanguageModelV1Middleware,typeLanguageModelV1StreamPart,  simulateReadableStream,}from'ai';const redis =newRedis({  url: process.env.KV_URL,  token: process.env.KV_TOKEN,});exportconst cacheMiddleware:LanguageModelV1Middleware={wrapGenerate:async({ doGenerate, params })=>{const cacheKey =JSON.stringify(params);const cached =(await redis.get(cacheKey))asAwaited<ReturnType<LanguageModelV1['doGenerate']>>|null;if(cached !==null){return{...cached,        response:{...cached.response,          timestamp: cached?.response?.timestamp?newDate(cached?.response?.timestamp):undefined,},};}const result =awaitdoGenerate();    redis.set(cacheKey, result);return result;},wrapStream:async({ doStream, params })=>{const cacheKey =JSON.stringify(params);// Check if the result is in the cacheconst cached =await redis.get(cacheKey);// If cached, return a simulated ReadableStream that yields the cached resultif(cached !==null){// Format the timestamps in the cached responseconst formattedChunks =(cached asLanguageModelV1StreamPart[]).map(p=>{if(p.type==='response-metadata'&& p.timestamp){return{...p, timestamp:newDate(p.timestamp)};}elsereturn p;});return{        stream:simulateReadableStream({          initialDelayInMs:0,          chunkDelayInMs:10,          chunks: formattedChunks,}),        rawCall:{ rawPrompt:null, rawSettings:{}},};}// If not cached, proceed with streamingconst{ stream,...rest }=awaitdoStream();const fullResponse:LanguageModelV1StreamPart[]=[];const transformStream =newTransformStream<LanguageModelV1StreamPart,LanguageModelV1StreamPart>({transform(chunk, controller){        fullResponse.push(chunk);        controller.enqueue(chunk);},flush(){// Store the full response in the cache after streaming is complete        redis.set(cacheKey, fullResponse);},});return{      stream: stream.pipeThrough(transformStream),...rest,};},};
```

This example uses `@upstash/redis` to store and retrieve the assistant's responses but you can use any KV storage provider you would like.


## [Server](#server)


Finally, you will create an API route for `api/chat` to handle the assistant's messages and responses. You can use your cache middleware by wrapping the model with `wrapLanguageModel` and passing the middleware as an argument.

app/api/chat/route.ts

```
import{ cacheMiddleware }from'@/ai/middleware';import{ openai }from'@ai-sdk/openai';import{ wrapLanguageModel, streamText, tool }from'ai';import{ z }from'zod';const wrappedModel =wrapLanguageModel({  model:openai('gpt-4o-mini'),  middleware: cacheMiddleware,});exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model: wrappedModel,    messages,    tools:{      weather:tool({        description:'Get the weather in a location',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>({location,          temperature:72+Math.floor(Math.random()*21)-10,}),}),},});return result.toDataStreamResponse();}
```
```

### 3. `cookbook/next/call-tools-in-parallel.md`

```markdown
# Call Tools in Parallel


---
url: https://ai-sdk.dev/cookbook/next/call-tools-in-parallel
description: Learn how to call tools in parallel using the AI SDK and Next.js
---


# [Call Tools in Parallel](#call-tools-in-parallel)


Some language models support calling tools in parallel. This is particularly useful when multiple tools are independent of each other and can be executed in parallel during the same generation step.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

What is the weather in Paris and New York?

Send Message


## [Client](#client)


Let's create a React component that imports the `useChat` hook from the `@ai-sdk/react` module. The `useChat` hook will call the `/api/chat` endpoint when the user sends a message. The endpoint will generate the assistant's response based on the conversation history and stream it to the client. If the assistant responds with a tool call, the hook will automatically display them as well.

You will use the `maxSteps` to specify the maximum number of steps that can made before the model or the user responds with a text message. In this example, you will set it to `2` to allow for another call with the tool result to happen.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, setInput, append }=useChat({    api:'/api/chat',    maxSteps:2,});return(<div><input        value={input}        onChange={event=>{setInput(event.target.value);}}        onKeyDown={asyncevent=>{if(event.key ==='Enter'){append({ content: input, role:'user'});}}}/>{messages.map((message, index)=>(<divkey={index}>{message.content}</div>))}</div>);}
```


## [Server](#server)


You will create a new route at `/api/chat` that will use the `streamText` function from the `ai` module to generate the assistant's response based on the conversation history.

You will use the [`tools`](/docs/reference/ai-sdk-core/generate-text#tools) parameter to specify a tool called `getWeather` that will get the weather for a location.

You will add the `getWeather` function and use zod to specify the schema for its parameters.

app/api/chat/route.ts

```
import{ToolInvocation, streamText }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';interfaceMessage{  role:'user'|'assistant';  content:string;  toolInvocations?:ToolInvocation[];}functiongetWeather({ city, unit }){return{ value:25, description:'Sunny'};}exportasyncfunctionPOST(req:Request){const{ messages }:{ messages:Message[]}=await req.json();const result =streamText({    model:openai('gpt-4o'),    system:'You are a helpful assistant.',    messages,    tools:{      getWeather:{        description:'Get the weather for a location',        parameters: z.object({          city: z.string().describe('The city to get the weather for'),          unit: z.enum(['C','F']).describe('The unit to display the temperature in'),}),execute:async({ city, unit })=>{const{ value, description }=getWeather({ city, unit });return`It is currently ${value}°${unit} and ${description} in ${city}!`;},},},});return result.toDataStreamResponse();}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/tools/call-tools-in-parallel/index.tsx)
```

### 4. `cookbook/next/call-tools-multiple-steps.md`

```markdown
# Call Tools in Multiple Steps


---
url: https://ai-sdk.dev/cookbook/next/call-tools-multiple-steps
description: Learn how to call tools in multiple steps using the AI SDK and Next.js
---


# [Call Tools in Multiple Steps](#call-tools-in-multiple-steps)


Some language models are great at calling tools in multiple steps to achieve a more complex task. This is particularly useful when the tools are dependent on each other and need to be executed in sequence during the same generation step.


## [Client](#client)


Let's create a React component that imports the `useChat` hook from the `@ai-sdk/react` module. The `useChat` hook will call the `/api/chat` endpoint when the user sends a message. The endpoint will generate the assistant's response based on the conversation history and stream it to the client. If the assistant responds with a tool call, the hook will automatically display them as well.

To call tools in multiple steps, you can use the `maxSteps` option to specify the maximum number of steps that can be made before the model or the user responds with a text message. In this example, you will set it to `5` to allow for multiple tool calls.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, setInput, append }=useChat({    api:'/api/chat',    maxSteps:5,});return(<div><input        value={input}        onChange={event=>{setInput(event.target.value);}}        onKeyDown={asyncevent=>{if(event.key ==='Enter'){append({ content: input, role:'user'});}}}/>{messages.map((message, index)=>(<divkey={index}>{message.content}</div>))}</div>);}
```


## [Server](#server)


You will create a new route at `/api/chat` that will use the `streamText` function from the `ai` module to generate the assistant's response based on the conversation history.

You will use the [`tools`](/docs/reference/ai-sdk-core/generate-text#tools) parameter to specify two tools called `getLocation` and `getWeather` that will first get the user's location and then use it to get the weather.

You will add the two functions mentioned earlier and use zod to specify the schema for its parameters.

app/api/chat/route.ts

```
import{ToolInvocation, streamText }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';interfaceMessage{  role:'user'|'assistant';  content:string;  toolInvocations?:ToolInvocation[];}functiongetLocation({ lat, lon }){return{ lat:37.7749, lon:-122.4194};}functiongetWeather({ lat, lon, unit }){return{ value:25, description:'Sunny'};}exportasyncfunctionPOST(req:Request){const{ messages }:{ messages:Message[]}=await req.json();const result =streamText({    model:openai('gpt-4o'),    system:'You are a helpful assistant.',    messages,    tools:{      getLocation:{        description:'Get the location of the user',        parameters: z.object({}),execute:async()=>{const{ lat, lon }=getLocation();return`Your location is at latitude ${lat} and longitude ${lon}`;},},      getWeather:{        description:'Get the weather for a location',        parameters: z.object({          lat: z.number().describe('The latitude of the location'),          lon: z.number().describe('The longitude of the location'),          unit: z.enum(['C','F']).describe('The unit to display the temperature in'),}),execute:async({ lat, lon, unit })=>{const{ value, description }=getWeather({ lat, lon, unit });return`It is currently ${value}°${unit} and ${description}!`;},},},});return result.toDataStreamResponse();}
```
```

### 5. `cookbook/next/call-tools.md`

```markdown
# Call Tools


---
url: https://ai-sdk.dev/cookbook/next/call-tools
description: Learn how to call tools using the AI SDK and Next.js
---


# [Call Tools](#call-tools)


Some models allow developers to provide a list of tools that can be called at any time during a generation. This is useful for extending the capabilites of a language model to either use logic or data to interact with systems external to the model.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

What is the weather in Paris and New York?

Send Message


## [Client](#client)


Let's create a React component that imports the `useChat` hook from the `@ai-sdk/react` module. The `useChat` hook will call the `/api/chat` endpoint when the user sends a message. The endpoint will generate the assistant's response based on the conversation history and stream it to the client. If the assistant responds with a tool call, the hook will automatically display them as well.

We will use the `maxSteps` to specify the maximum number of steps (i.e., LLM calls) that can be made to prevent infinite loops. In this example, you will set it to `2` to allow for two backend calls to happen.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, setInput, append }=useChat({    api:'/api/chat',    maxSteps:2,});return(<div><input        value={input}        onChange={event=>{setInput(event.target.value);}}        onKeyDown={asyncevent=>{if(event.key ==='Enter'){append({ content: input, role:'user'});}}}/>{messages.map((message, index)=>(<divkey={index}>{message.content}</div>))}</div>);}
```


## [Server](#server)


You will create a new route at `/api/chat` that will use the `streamText` function from the `ai` module to generate the assistant's response based on the conversation history.

You will use the [`tools`](/docs/reference/ai-sdk-core/generate-text#tools) parameter to specify a tool called `celsiusToFahrenheit` that will convert a user given value in celsius to fahrenheit.

You will also use zod to specify the schema for the `celsiusToFahrenheit` function's parameters.

app/api/chat/route.ts

```
import{ToolInvocation, streamText }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';interfaceMessage{  role:'user'|'assistant';  content:string;  toolInvocations?:ToolInvocation[];}exportasyncfunctionPOST(req: Request){const{ messages }:{ messages:Message[]}=await req.json();const result =streamText({    model:openai('gpt-4o'),    system:'You are a helpful assistant.',    messages,    tools:{      getWeather:{        description:'Get the weather for a location',        parameters: z.object({          city: z.string().describe('The city to get the weather for'),          unit: z.enum(['C','F']).describe('The unit to display the temperature in'),}),execute:async({ city, unit })=>{const weather ={            value:24,            description:'Sunny',};return`It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`;},},},});return result.toDataStreamResponse();}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/tools/call-tool/index.tsx)
```

### 6. `cookbook/next/chat-with-pdf.md`

```markdown
# Chat with PDFs


---
url: https://ai-sdk.dev/cookbook/next/chat-with-pdf
description: Learn how to build a chatbot that can understand PDFs using the AI SDK and Next.js
---


# [Chat with PDFs](#chat-with-pdfs)


Some language models like Anthropic's Claude Sonnet 3.5 and Google's Gemini 2.0 can understand PDFs and respond to questions about their contents. In this example, we'll show you how to build a chat interface that accepts PDF uploads.

This example requires a provider that supports PDFs, such as Anthropic's Claude 3.7, Google's Gemini 2.5, or OpenAI's GPT-4.1. Check the [provider documentation](/providers/ai-sdk-providers) for up-to-date support information.


## [Implementation](#implementation)



### [Server](#server)


Create a route handler that will use Anthropic's Claude model to process messages and PDFs:

app/api/chat/route.ts

```
import{ anthropic }from'@ai-sdk/anthropic';import{ streamText }from'ai';exportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:anthropic('claude-3-5-sonnet-latest'),    messages,});return result.toDataStreamResponse();}
```


### [Client](#client)


Create a chat interface that allows uploading PDFs alongside messages:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{ useRef, useState }from'react';importImagefrom'next/image';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();const[files, setFiles]=useState<FileList|undefined>(undefined);const fileInputRef =useRef<HTMLInputElement>(null);return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages.map(m=>(<divkey={m.id}className="whitespace-pre-wrap">{m.role ==='user'?'User: ':'AI: '}{m.content}<div>{m?.experimental_attachments?.filter(attachment=>                  attachment?.contentType?.startsWith('image/')|                  attachment?.contentType?.startsWith('application/pdf'),).map((attachment, index)=>                attachment.contentType?.startsWith('image/')?(<Imagekey={`${m.id}-${index}`}src={attachment.url}width={500}height={500}alt={attachment.name ??`attachment-${index}`}/>): attachment.contentType?.startsWith('application/pdf')?(<iframekey={`${m.id}-${index}`}src={attachment.url}width="500"height="600"title={attachment.name ??`attachment-${index}`}/>):null,)}</div></div>))}<formclassName="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2"onSubmit={event=>{handleSubmit(event,{            experimental_attachments: files,});setFiles(undefined);if(fileInputRef.current){            fileInputRef.current.value ='';}}}><inputtype="file"className=""onChange={event=>{if(event.target.files){setFiles(event.target.files);}}}multipleref={fileInputRef}/><inputclassName="w-full p-2"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

The code uses the `useChat` hook which handles the file upload and message streaming. The `experimental_attachments` option allows you to send files alongside messages.

Make sure to set up your environment variables with your Anthropic API key:

.env.local

```
ANTHROPIC_API_KEY=xxxxxxxxx
```

Now you can upload PDFs and ask questions about their contents. The LLM will analyze the PDF and provide relevant responses based on the document's content.
```

### 7. `cookbook/next/generate-image-with-chat-prompt.md`

```markdown
# Generate Image with Chat Prompt


---
url: https://ai-sdk.dev/cookbook/next/generate-image-with-chat-prompt
description: Learn how to generate an image with a chat prompt using the AI SDK and Next.js
---


# [Generate Image with Chat Prompt](#generate-image-with-chat-prompt)


When building a chatbot, you may want to allow the user to generate an image. This can be done by creating a tool that generates an image using the [`experimental_generateImage`](/docs/reference/ai-sdk-core/generate-image#generateimage) function from the AI SDK.


## [Server](#server)


Let's create an endpoint at `/api/chat` that generates the assistant's response based on the conversation history. You will also define a tool called `generateImage` that will generate an image based on the assistant's response.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ experimental_generateImage,Message, streamText, tool }from'ai';import{ z }from'zod';exportasyncfunctionPOST(request:Request){const{ messages }:{ messages:Message[]}=await request.json();// filter through messages and remove base64 image data to avoid sending to the modelconst formattedMessages = messages.map(m =>{if(m.role==='assistant'&& m.toolInvocations){      m.toolInvocations.forEach(ti =>{if(ti.toolName==='generateImage'&& ti.state==='result'){          ti.result.image=`redacted-for-length`;}});}return m;});const result =streamText({    model:openai('gpt-4o'),    messages: formattedMessages,    tools:{      generateImage:tool({        description:'Generate an image',        parameters: z.object({          prompt: z.string().describe('The prompt to generate the image from'),}),execute:async({ prompt })=>{const{ image }=awaitexperimental_generateImage({            model: openai.image('dall-e-3'),            prompt,});// in production, save this image to blob storage and return a URLreturn{ image: image.base64, prompt };},}),},});return result.toDataStreamResponse();}
```

In production, you should save the generated image to a blob storage and return a URL instead of the base64 image data. If you don't, the base64 image data will be sent to the model which may cause the generation to fail.


## [Client](#client)


Let's create a simple chat interface with `useChat`. You will call the `/api/chat` endpoint to generate the assistant's response. If the assistant's response contains a `generateImage` tool invocation, you will display the tool result (the image in base64 format and the prompt) using the Next `Image` component.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';importImagefrom'next/image';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch"><divclassName="space-y-4">{messages.map(m=>(<divkey={m.id}className="whitespace-pre-wrap"><divkey={m.id}><divclassName="font-bold">{m.role}</div>{m.toolInvocations ?(                m.toolInvocations.map(ti=>                  ti.toolName ==='generateImage'?(                    ti.state ==='result'?(<Imagekey={ti.toolCallId}src={`data:image/png;base64,${ti.result.image}`}alt={ti.result.prompt}height={400}width={400}/>):(<divkey={ti.toolCallId}className="animate-pulse">Generating image...</div>)):null,)):(<p>{m.content}</p>)}</div></div>))}</div><formonSubmit={handleSubmit}><inputclassName="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```
```

### 8. `cookbook/next/generate-object-with-file-prompt.md`

```markdown
# Generate Object with File Prompt through Form Submission


---
url: https://ai-sdk.dev/cookbook/next/generate-object-with-file-prompt
description: Learn how to generate object with file prompt through form submission using the AI SDK and Next.js
---


# [Generate Object with File Prompt through Form Submission](#generate-object-with-file-prompt-through-form-submission)


This feature is limited to models/providers that support PDF inputs ([Anthropic](/providers/ai-sdk-providers/anthropic#pdf-support), [Google Gemini](/providers/ai-sdk-providers/google-generative-ai#file-inputs), and [Google Vertex](/providers/ai-sdk-providers/google-vertex#file-inputs)).

With select models, you can send PDFs (files) as part of your prompt. Let's create a simple Next.js application that allows a user to upload a PDF send it to an LLM for summarization.


## [Client](#client)


On the frontend, create a form that allows the user to upload a PDF. When the form is submitted, send the PDF to the `/api/analyze` route.

```
'use client';import{ useState }from'react';exportdefaultfunctionPage(){const[description, setDescription]=useState<string>();const[loading, setLoading]=useState(false);return(<div><form        action={asyncformData=>{try{setLoading(true);const response =awaitfetch('/api/analyze',{              method:'POST',              body: formData,});setLoading(false);if(response.ok){setDescription(await response.text());}}catch(error){console.error('Analysis failed:', error);}}}><div><label>UploadImage</label><inputname="pdf"type="file"accept="application/pdf"/></div><buttontype="submit"disabled={loading}>Submit{loading &&'ing...'}</button></form>{description &&<pre>{description}</pre>}</div>);}
```


## [Server](#server)


On the server, create an API route that receives the PDF, sends it to the LLM, and returns the result. This example uses the [`generateObject`](/docs/reference/ai-sdk-core/generate-object) function to generate the summary as part of a structured output.

```
import{ generateObject }from'ai';import{ anthropic }from'@ai-sdk/anthropic';import{ z }from'zod';exportasyncfunctionPOST(request:Request){const formData =await request.formData();const file = formData.get('pdf')asFile;const result =awaitgenerateObject({    model:anthropic('claude-3-5-sonnet-latest'),    messages:[{        role:'user',        content:[{type:'text',            text:'Analyze the following PDF and generate a summary.',},{type:'file',            data:await file.arrayBuffer(),            mimeType:'application/pdf',},],},],    schema: z.object({      summary: z.string().describe('A 50 word summary of the PDF.'),}),});returnnewResponse(result.object.summary);}
```
```

### 9. `cookbook/next/generate-object.md`

```markdown
# Generate Object


---
url: https://ai-sdk.dev/cookbook/next/generate-object
description: Learn how to generate object using the AI SDK and Next.js
---


# [Generate Object](#generate-object)


Earlier functions like `generateText` and `streamText` gave us the ability to generate unstructured text. However, if you want to generate structured data like JSON, you can provide a schema that describes the structure of your desired object to the `generateObject` function.

The function requires you to provide a schema using [zod](https://zod.dev), a library for defining schemas for JavaScript objects. By using zod, you can also use it to validate the generated object and ensure that it conforms to the specified structure.

http://localhost:3000

View Notifications


## [Client](#client)


Let's create a simple React component that will make a POST request to the `/api/completion` endpoint when a button is clicked. The endpoint will return the generated object based on the input prompt and we'll display it.

app/page.tsx

```
'use client';import{ useState }from'react';exportdefaultfunctionPage(){const[generation, setGeneration]=useState();const[isLoading, setIsLoading]=useState(false);return(<div><div        onClick={async()=>{setIsLoading(true);awaitfetch('/api/completion',{            method:'POST',            body:JSON.stringify({              prompt:'Messages during finals week.',}),}).then(response=>{            response.json().then(json=>{setGeneration(json.notifications);setIsLoading(false);});});}}>Generate</div>{isLoading ?'Loading...':<pre>{JSON.stringify(generation)}</pre>}</div>);}
```


## [Server](#server)


Let's create a route handler for `/api/completion` that will generate an object based on the input prompt. The route will call the `generateObject` function from the `ai` module, which will then generate an object based on the input prompt and return it.

app/api/completion/route.ts

```
import{ generateObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';exportasyncfunctionPOST(req:Request){const{ prompt }:{ prompt:string}=await req.json();const result =awaitgenerateObject({    model:openai('gpt-4'),    system:'You generate three notifications for a messages app.',    prompt,    schema: z.object({      notifications: z.array(        z.object({          name: z.string().describe('Name of a fictional person.'),          message: z.string().describe('Do not use emojis or links.'),          minutesAgo: z.number(),}),),}),});return result.toJsonResponse();}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/basics/generate-object/index.tsx)
```

### 10. `cookbook/next/generate-text-with-chat-prompt.md`

```markdown
# Generate Text with Chat Prompt


---
url: https://ai-sdk.dev/cookbook/next/generate-text-with-chat-prompt
description: Learn how to generate text with chat prompt using the AI SDK and Next.js
---


# [Generate Text with Chat Prompt](#generate-text-with-chat-prompt)


Previously, you were able to generate text and objects using either a single message prompt, a system prompt, or a combination of both of them. However, there may be times when you want to generate text based on a series of messages.

A chat completion allows you to generate text based on a series of messages. This series of messages can be any series of interactions between any number of systems, but the most popular and relatable use case has been a series of messages that represent a conversation between a user and a model.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

Why is the sky blue?

Send Message


## [Client](#client)


Let's start by creating a simple chat interface with an input field that sends the user's message and displays the conversation history. You will call the `/api/chat` endpoint to generate the assistant's response.

app/page.tsx

```
'use client';import{CoreMessage}from'ai';import{ useState }from'react';exportdefaultfunctionPage(){const[input, setInput]=useState('');const[messages, setMessages]=useState<CoreMessage[]>([]);return(<div><input        value={input}        onChange={event=>{setInput(event.target.value);}}        onKeyDown={asyncevent=>{if(event.key ==='Enter'){setMessages(currentMessages=>[...currentMessages,{ role:'user', content: input },]);const response =awaitfetch('/api/chat',{              method:'POST',              body:JSON.stringify({                messages:[...messages,{ role:'user', content: input }],}),});const{ messages: newMessages }=await response.json();setMessages(currentMessages=>[...currentMessages,...newMessages,]);}}}/>{messages.map((message, index)=>(<divkey={`${message.role}-${index}`}>{typeof message.content ==='string'? message.content: message.content.filter(part=> part.type==='text').map((part, partIndex)=>(<divkey={partIndex}>{part.text}</div>))}</div>))}</div>);}
```


## [Server](#server)


Next, let's create the `/api/chat` endpoint that generates the assistant's response based on the conversation history.

app/api/chat/route.ts

```
import{CoreMessage, generateText }from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctionPOST(req:Request){const{ messages }:{ messages:CoreMessage[]}=await req.json();const{ response }=awaitgenerateText({    model:openai('gpt-4'),    system:'You are a helpful assistant.',    messages,});returnResponse.json({ messages: response.messages});}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/chat/generate-chat/index.tsx)
```

### 11. `cookbook/next/generate-text.md`

```markdown
# Generate Text


---
url: https://ai-sdk.dev/cookbook/next/generate-text
description: Learn how to generate text using the AI SDK and Next.js.
---


# [Generate Text](#generate-text)


A situation may arise when you need to generate text based on a prompt. For example, you may want to generate a response to a question or summarize a body of text. The `generateText` function can be used to generate text based on the input prompt.

http://localhost:3000

Answer


## [Client](#client)


Let's create a simple React component that will make a POST request to the `/api/completion` endpoint when a button is clicked. The endpoint will generate text based on the input prompt.

app/page.tsx

```
'use client';import{ useState }from'react';exportdefaultfunctionPage(){const[generation, setGeneration]=useState('');const[isLoading, setIsLoading]=useState(false);return(<div><div        onClick={async()=>{setIsLoading(true);awaitfetch('/api/completion',{            method:'POST',            body:JSON.stringify({              prompt:'Why is the sky blue?',}),}).then(response=>{            response.json().then(json=>{setGeneration(json.text);setIsLoading(false);});});}}>Generate</div>{isLoading ?'Loading...': generation}</div>);}
```


## [Server](#server)


Let's create a route handler for `/api/completion` that will generate text based on the input prompt. The route will call the `generateText` function from the `ai` module, which will then generate text based on the input prompt and return it.

app/api/completion/route.ts

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctionPOST(req:Request){const{ prompt }:{ prompt:string}=await req.json();const{ text }=awaitgenerateText({    model:openai('gpt-4'),    system:'You are a helpful assistant.',    prompt,});returnResponse.json({ text });}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/basics/generate-text/index.tsx)
```

### 12. `cookbook/next/human-in-the-loop.md`

```markdown
# Human-in-the-Loop with Next.js


---
url: https://ai-sdk.dev/cookbook/next/human-in-the-loop
description: Add a human approval step to your agentic system with Next.js and the AI SDK
---


# [Human-in-the-Loop with Next.js](#human-in-the-loop-with-nextjs)


When building agentic systems, it's important to add human-in-the-loop (HITL) functionality to ensure that users can approve actions before the system executes them. This recipe will describe how to [build a low-level solution](#adding-a-confirmation-step) and then provide an [example abstraction](#building-your-own-abstraction) you could implement and customise based on your needs.


## [Background](#background)


To understand how to implement this functionality, let's look at how tool calling works in a simple Next.js chatbot application with the AI SDK.

On the frontend, use the `useChat` hook to manage the message state and user interaction (including input and form submission handlers).

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<div><div>{messages?.map(m=>(<divkey={m.id}><strong>{`${m.role}: `}</strong>{m.parts?.map((part, i)=>{switch(part.type){case'text':return<divkey={i}>{part.text}</div>;}})}<br/></div>))}</div><formonSubmit={handleSubmit}><inputvalue={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

On the backend, create a route handler (API Route) that returns a `DataStreamResponse`. Within the execute function, call `streamText` and pass in the `messages` (sent from the client). Finally, merge the resulting generation into the data stream.

api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ createDataStreamResponse, streamText, tool }from'ai';import{ z }from'zod';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();returncreateDataStreamResponse({execute:async dataStream =>{const result =streamText({        model:openai('gpt-4o'),        messages,        tools:{          getWeatherInformation:tool({            description:'show the weather in a given city to the user',            parameters: z.object({ city: z.string()}),execute:async({}:{ city:string})=>{const weatherOptions =['sunny','cloudy','rainy','snowy'];return weatherOptions[Math.floor(Math.random()* weatherOptions.length)];},}),},});      result.mergeIntoDataStream(dataStream);},});}
```

What happens if you ask the LLM for the weather in New York?

The LLM has one tool available, `weather`, which requires a `location` to run. This tool will, as stated in the tool's `description`, "show the weather in a given city to the user". If the LLM decides that the `weather` tool could answer the user's query, it would generate a `ToolCall`, extracting the `location` from the context. The AI SDK would then run the associated `execute` function, passing in the `location` parameter, and finally returning a `ToolResult`.

To introduce a HITL step you will add a confirmation step to this process in between the `ToolCall` and the `ToolResult`.


## [Adding a Confirmation Step](#adding-a-confirmation-step)


At a high level, you will:

1.  Intercept tool calls before they are executed
2.  Render a confirmation UI with Yes/No buttons
3.  Send a temporary tool result indicating whether the user confirmed or declined
4.  On the server, check for the confirmation state in the tool result:
    -   If confirmed, execute the tool and update the result
    -   If declined, update the result with an error message
5.  Send the updated tool result back to the client to maintain state consistency


### [Forward Tool Call To The Client](#forward-tool-call-to-the-client)


To implement HITL functionality, you start by omitting the `execute` function from the tool definition. This allows the frontend to intercept the tool call and handle the responsibility of adding the final tool result to the tool call.

api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ createDataStreamResponse, streamText, tool }from'ai';import{ z }from'zod';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();returncreateDataStreamResponse({execute:async dataStream =>{const result =streamText({        model:openai('gpt-4o'),        messages,        tools:{          getWeatherInformation:tool({            description:'show the weather in a given city to the user',            parameters: z.object({ city: z.string()}),// execute function removed to stop automatic execution}),},});      result.mergeIntoDataStream(dataStream);},});}
```

Each tool call must have a corresponding tool result. If you do not add a tool result, all subsequent generations will fail.


### [Intercept Tool Call](#intercept-tool-call)


On the frontend, you map through the messages, either rendering the message content or checking for tool invocations and rendering custom UI.

You can check if the tool requiring confirmation has been called and, if so, present options to either confirm or deny the proposed tool call. This confirmation is done using the `addToolResult` function to create a tool result and append it to the associated tool call.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit, addToolResult }=useChat();return(<div><div>{messages?.map(m=>(<divkey={m.id}><strong>{`${m.role}: `}</strong>{m.parts?.map((part, i)=>{switch(part.type){case'text':return<divkey={i}>{part.text}</div>;case'tool-invocation':const toolInvocation = part.toolInvocation;const toolCallId = toolInvocation.toolCallId;// render confirmation tool (client-side tool with user interaction)if(                    toolInvocation.toolName ==='getWeatherInformation'&&                    toolInvocation.state ==='call'){return(<divkey={toolCallId}>Get weather information for{toolInvocation.args.city}?<div><buttononClick={()=>addToolResult({                                toolCallId,                                result:'Yes, confirmed.',})}>Yes</button><buttononClick={()=>addToolResult({                                toolCallId,                                result:'No, denied.',})}>No</button></div></div>);}}})}<br/></div>))}</div><formonSubmit={handleSubmit}><inputvalue={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

The `addToolResult` function will trigger a call to your route handler.


### [Handle Confirmation Response](#handle-confirmation-response)


Adding a tool result will trigger another call to your route handler. Before sending the new messages to the language model, you pull out the last message and map through the message parts to see if the tool requiring confirmation was called and whether it's in a "result" state. If those conditions are met, you check the confirmation state (the tool result state that you set on the frontend with the `addToolResult` function).

api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{  createDataStreamResponse,  formatDataStreamPart,Message,  streamText,  tool,}from'ai';import{ z }from'zod';exportasyncfunctionPOST(req:Request){const{ messages }:{ messages:Message[]}=await req.json();returncreateDataStreamResponse({execute:async dataStream =>{// pull out last messageconst lastMessage = messages[messages.length-1];      lastMessage.parts=awaitPromise.all(// map through all message parts        lastMessage.parts?.map(async part =>{if(part.type!=='tool-invocation'){return part;}const toolInvocation = part.toolInvocation;// return if tool isn't weather tool or in a result stateif(            toolInvocation.toolName!=='getWeatherInformation'|            toolInvocation.state!=='result'){return part;}// switch through tool result states (set on the frontend)switch(toolInvocation.result){case'Yes, confirmed.':{const result =awaitexecuteWeatherTool(toolInvocation.args);// forward updated tool result to the client:              dataStream.write(formatDataStreamPart('tool_result',{                  toolCallId: toolInvocation.toolCallId,                  result,}),);// update the message part:return{...part, toolInvocation:{...toolInvocation, result }};}case'No, denied.':{const result ='Error: User denied access to weather information';// forward updated tool result to the client:              dataStream.write(formatDataStreamPart('tool_result',{                  toolCallId: toolInvocation.toolCallId,                  result,}),);// update the message part:return{...part, toolInvocation:{...toolInvocation, result }};}default:return part;}})??[],);const result =streamText({        model:openai('gpt-4o'),        messages,        tools:{          getWeatherInformation:tool({            description:'show the weather in a given city to the user',            parameters: z.object({ city: z.string()}),}),},});      result.mergeIntoDataStream(dataStream);},});}asyncfunctionexecuteWeatherTool({}:{ city:string}){const weatherOptions =['sunny','cloudy','rainy','snowy'];return weatherOptions[Math.floor(Math.random()* weatherOptions.length)];}
```

In this implementation, you use simple strings like "Yes, the user confirmed" or "No, the user declined" as states. If confirmed, you execute the tool. If declined, you do not execute the tool. In both cases, you update the tool result from the arbitrary data you sent with the `addToolResult` function to either the result of the execute function or an "Execution declined" statement. You send the updated tool result back to the frontend to maintain state synchronization.

After handling the tool result, your API route continues. This triggers another generation with the updated tool result, allowing the LLM to continue attempting to solve the query.


## [Building your own abstraction](#building-your-own-abstraction)


The solution above is low-level and not very friendly to use in a production environment. You can build your own abstraction using these concepts


### [Create Utility Functions](#create-utility-functions)


utils.ts

```
import{ formatDataStreamPart,Message}from'@ai-sdk/ui-utils';import{  convertToCoreMessages,DataStreamWriter,ToolExecutionOptions,ToolSet,}from'ai';import{ z }from'zod';// Approval string to be shared across frontend and backendexportconstAPPROVAL={YES:'Yes, confirmed.',NO:'No, denied.',}asconst;functionisValidToolName<KextendsPropertyKey,Textends object>(  key:K,  obj:T,): key isK&keyofT{return key in obj;}/** * Processes tool invocations where human input is required, executing tools when authorized. * * @param options - The function options * @param options.tools - Map of tool names to Tool instances that may expose execute functions * @param options.dataStream - Data stream for sending results back to the client * @param options.messages - Array of messages to process * @param executionFunctions - Map of tool names to execute functions * @returns Promise resolving to the processed messages */exportasyncfunctionprocessToolCalls<ToolsextendsToolSet,ExecutableToolsextends{[ToolinkeyofToolsasTools[Tool]extends{ execute:Function}?never:Tool]:Tools[Tool];},>({    dataStream,    messages,}:{    tools:Tools;// used for type inference    dataStream:DataStreamWriter;    messages:Message[];},  executeFunctions:{[KinkeyofTools&keyofExecutableTools]?:(      args: z.infer<ExecutableTools[K]['parameters']>,      context:ToolExecutionOptions,)=>Promise<any>;},):Promise<Message[]>{const lastMessage = messages[messages.length-1];const parts = lastMessage.parts;if(!parts)return messages;const processedParts =awaitPromise.all(    parts.map(async part =>{// Only process tool invocations partsif(part.type!=='tool-invocation')return part;const{ toolInvocation }= part;const toolName = toolInvocation.toolName;// Only continue if we have an execute function for the tool (meaning it requires confirmation) and it's in a 'result' stateif(!(toolName in executeFunctions)| toolInvocation.state!=='result')return part;let result;if(toolInvocation.result===APPROVAL.YES){// Get the tool and check if the tool has an execute function.if(!isValidToolName(toolName, executeFunctions)|          toolInvocation.state!=='result'){return part;}const toolInstance = executeFunctions[toolName];if(toolInstance){          result =awaittoolInstance(toolInvocation.args,{            messages:convertToCoreMessages(messages),            toolCallId: toolInvocation.toolCallId,});}else{          result ='Error: No execute function found on tool';}}elseif(toolInvocation.result===APPROVAL.NO){        result ='Error: User denied access to tool execution';}else{// For any unhandled responses, return the original part.return part;}// Forward updated tool result to the client.      dataStream.write(formatDataStreamPart('tool_result',{          toolCallId: toolInvocation.toolCallId,          result,}),);// Return updated toolInvocation with the actual result.return{...part,        toolInvocation:{...toolInvocation,          result,},};}),);// Finally return the processed messagesreturn[...messages.slice(0,-1),{...lastMessage, parts: processedParts }];}exportfunctiongetToolsRequiringConfirmation<TextendsToolSet>(  tools:T,):string[]{return(Object.keys(tools)as(keyofT)[]).filter(key =>{const maybeTool = tools[key];returntypeof maybeTool.execute!=='function';})asstring[];}
```

In this file, you first declare the confirmation strings as constants so we can share them across the frontend and backend (reducing possible errors). Next, we create function called `processToolCalls` which takes in the `messages`, `tools`, and the `datastream`. It also takes in a second parameter, `executeFunction`, which is an object that maps `toolName` to the functions that will be run upon human confirmation. This function is strongly typed so:

-   it autocompletes `executableTools` - these are tools without an execute function
-   provides full type-safety for arguments and options available within the `execute` function

Unlike the low-level example, this will return a modified array of `messages` that can be passed directly to the LLM.

Finally, you declare a function called `getToolsRequiringConfirmation` that takes your tools as an argument and then will return the names of your tools without execute functions (in an array of strings). This avoids the need to manually write out and check for `toolName`'s on the frontend.

In order to use these utility functions, you will need to move tool declarations to their own file:

tools.ts

```
import{ tool }from'ai';import{ z }from'zod';const getWeatherInformation =tool({  description:'show the weather in a given city to the user',  parameters: z.object({ city: z.string()}),// no execute function, we want human in the loop});const getLocalTime =tool({  description:'get the local time for a specified location',  parameters: z.object({location: z.string()}),// including execute function -> no confirmation requiredexecute:async({location})=>{console.log(`Getting local time for ${location}`);return'10am';},});exportconst tools ={  getWeatherInformation,  getLocalTime,};
```

In this file, you have two tools, `getWeatherInformation` (requires confirmation to run) and `getLocalTime`.


### [Update Route Handler](#update-route-handler)


Update your route handler to use the `processToolCalls` utility function.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ createDataStreamResponse,Message, streamText }from'ai';import{ processToolCalls }from'./utils';import{ tools }from'./tools';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const{ messages }:{ messages:Message[]}=await req.json();returncreateDataStreamResponse({execute:async dataStream =>{// Utility function to handle tools that require human confirmation// Checks for confirmation in last message and then runs associated toolconst processedMessages =awaitprocessToolCalls({          messages,          dataStream,          tools,},{// type-safe object for tools without an execute functiongetWeatherInformation:async({ city })=>{const conditions =['sunny','cloudy','rainy','snowy'];return`The weather in ${city} is ${              conditions[Math.floor(Math.random()* conditions.length)]}.`;},},);const result =streamText({        model:openai('gpt-4o'),        messages: processedMessages,        tools,});      result.mergeIntoDataStream(dataStream);},});}
```


### [Update Frontend](#update-frontend)


Finally, update the frontend to use the new `getToolsRequiringConfirmation` function and the `APPROVAL` values:

app/page.tsx

```
'use client';import{Message, useChat }from'@ai-sdk/react';import{APPROVAL,  getToolsRequiringConfirmation,}from'../api/use-chat-human-in-the-loop/utils';import{ tools }from'../api/use-chat-human-in-the-loop/tools';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit, addToolResult }=useChat({      maxSteps:5,});const toolsRequiringConfirmation =getToolsRequiringConfirmation(tools);// used to disable input while confirmation is pendingconst pendingToolCallConfirmation = messages.some((m: Message)=>    m.parts?.some(part=>        part.type==='tool-invocation'&&        part.toolInvocation.state ==='call'&&        toolsRequiringConfirmation.includes(part.toolInvocation.toolName),),);return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages?.map((m: Message)=>(<divkey={m.id}className="whitespace-pre-wrap"><strong>{`${m.role}: `}</strong>{m.parts?.map((part, i)=>{switch(part.type){case'text':return<divkey={i}>{part.text}</div>;case'tool-invocation':const toolInvocation = part.toolInvocation;const toolCallId = toolInvocation.toolCallId;const dynamicInfoStyles ='font-mono bg-gray-100 p-1 text-sm';// render confirmation tool (client-side tool with user interaction)if(                  toolsRequiringConfirmation.includes(                    toolInvocation.toolName,)&&                  toolInvocation.state ==='call'){return(<divkey={toolCallId}className="text-gray-500">Run{' '}<spanclassName={dynamicInfoStyles}>{toolInvocation.toolName}</span>{' '}with args:{' '}<spanclassName={dynamicInfoStyles}>{JSON.stringify(toolInvocation.args)}</span><divclassName="flex gap-2 pt-2"><buttonclassName="px-4 py-2 font-bold text-white bg-blue-500 rounded hover:bg-blue-700"onClick={()=>addToolResult({                              toolCallId,                              result:APPROVAL.YES,})}>Yes</button><buttonclassName="px-4 py-2 font-bold text-white bg-red-500 rounded hover:bg-red-700"onClick={()=>addToolResult({                              toolCallId,                              result:APPROVAL.NO,})}>No</button></div></div>);}}})}<br/></div>))}<formonSubmit={handleSubmit}><inputdisabled={pendingToolCallConfirmation}className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```


## [Full Example](#full-example)


To see this code in action, check out the [`next-openai` example](https://github.com/vercel/ai/tree/main/examples/next-openai) in the AI SDK repository. Navigate to the `/use-chat-human-in-the-loop` page and associated route handler.
```

### 13. `cookbook/next/markdown-chatbot-with-memoization.md`

```markdown
# Markdown Chatbot with Memoization


---
url: https://ai-sdk.dev/cookbook/next/markdown-chatbot-with-memoization
description: Build a chatbot that renders and memoizes Markdown responses with Next.js and the AI SDK.
---


# [Markdown Chatbot with Memoization](#markdown-chatbot-with-memoization)


When building a chatbot with Next.js and the AI SDK, you'll likely want to render the model's responses in Markdown format using a library like `react-markdown`. However, this can have negative performance implications as the Markdown is re-rendered on each new token received from the streaming response.

As conversations get longer and more complex, this performance impact becomes exponentially worse since the entire conversation history is re-rendered with each new token.

This recipe uses memoization - a performance optimization technique where the results of expensive function calls are cached and reused to avoid unnecessary re-computation. In this case, parsed Markdown blocks are memoized to prevent them from being re-parsed and re-rendered on each token update, which means that once a block is fully parsed, it's cached and reused rather than being regenerated. This approach significantly improves rendering performance for long conversations by eliminating redundant parsing and rendering operations.


## [Server](#server)


On the server, you use a simple route handler that streams the response from the language model.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';exportconst maxDuration =60;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    system:'You are a helpful assistant. Respond to the user in Markdown format.',    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse();}
```


## [Memoized Markdown Component](#memoized-markdown-component)


Next, create a memoized markdown component that will take in raw Markdown text into blocks and only updates when the content actually changes. This component splits Markdown content into blocks using the `marked` library to identify discrete Markdown elements, then uses React's memoization features to optimize re-rendering by only updating blocks that have actually changed.

components/memoized-markdown.tsx

```
import{ marked }from'marked';import{ memo, useMemo }from'react';importReactMarkdownfrom'react-markdown';functionparseMarkdownIntoBlocks(markdown: string):string[]{const tokens = marked.lexer(markdown);return tokens.map(token=> token.raw);}constMemoizedMarkdownBlock=memo(({ content }:{ content: string })=>{return<ReactMarkdown>{content}</ReactMarkdown>;},(prevProps, nextProps)=>{if(prevProps.content !== nextProps.content)returnfalse;returntrue;},);MemoizedMarkdownBlock.displayName ='MemoizedMarkdownBlock';exportconstMemoizedMarkdown=memo(({ content, id }:{ content: string; id: string })=>{const blocks =useMemo(()=>parseMarkdownIntoBlocks(content),[content]);return blocks.map((block, index)=>(<MemoizedMarkdownBlockcontent={block}key={`${id}-block_${index}`}/>));},);MemoizedMarkdown.displayName ='MemoizedMarkdown';
```


## [Client](#client)


Finally, on the client, use the `useChat` hook to manage the chat state and render the chat interface. You can use the `MemoizedMarkdown` component to render the message contents in Markdown format without compromising on performance. Additionally, you can render the form in its own component so as to not trigger unnecessary re-renders of the chat messages. You can also use the `experimental_throttle` option that will throttle data updates to a specified interval, helping to manage rendering performance.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{MemoizedMarkdown}from'@/components/memoized-markdown';exportdefaultfunctionPage(){const{ messages }=useChat({    id:'chat',// Throttle the messages and data updates to 50ms:    experimental_throttle:50,});return(<div className="flex flex-col w-full max-w-xl py-24 mx-auto stretch"><div className="space-y-8 mb-4">{messages.map(message =>(<div key={message.id}><div className="font-bold mb-2">{message.role==='user'?'You':'Assistant'}</div><div className="prose space-y-2"><MemoizedMarkdown id={message.id} content={message.content}/></div></div>))}</div><MessageInput/></div>);}constMessageInput=()=>{const{ input, handleSubmit, handleInputChange }=useChat({ id:'chat'});return(<form onSubmit={handleSubmit}><input        className="fixed bottom-0 w-full max-w-xl p-2 mb-8 dark:bg-zinc-900 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"        placeholder="Say something..."        value={input}        onChange={handleInputChange}/></form>);};
```

The chat state is shared between both components by using the same `id` value. This allows you to split the form and chat messages into separate components while maintaining synchronized state.
```

### 14. `cookbook/next/mcp-tools.md`

```markdown
# MCP Tools


---
url: https://ai-sdk.dev/cookbook/next/mcp-tools
description: Learn how to use MCP tools with the AI SDK and Next.js
---


# [MCP Tools](#mcp-tools)


The AI SDK supports Model Context Protocol (MCP) tools by offering a lightweight client that exposes a `tools` method for retrieving tools from a MCP server. After use, the client should always be closed to release resources.


## [Server](#server)


Let's create a route handler for `/api/completion` that will generate text based on the input prompt and MCP tools that can be called at any time during a generation. The route will call the `streamText` function from the `ai` module, which will then generate text based on the input prompt and stream it to the client.

To use the `StreamableHTTPClientTransport`, you will need to install the official Typescript SDK for Model Context Protocol:

pnpm install @modelcontextprotocol/sdk

app/api/completion/route.ts

```
import{ experimental_createMCPClient, streamText }from'ai';import{Experimental_StdioMCPTransport}from'ai/mcp-stdio';import{ openai }from'@ai-sdk/openai';import{StreamableHTTPClientTransport}from'@modelcontextprotocol/sdk/client/streamableHttp';exportasyncfunctionPOST(req:Request){const{ prompt }:{ prompt:string}=await req.json();try{// Initialize an MCP client to connect to a `stdio` MCP server:const transport =newExperimental_StdioMCPTransport({      command:'node',      args:['src/stdio/dist/server.js'],});const stdioClient =awaitexperimental_createMCPClient({      transport,});// Alternatively, you can connect to a Server-Sent Events (SSE) MCP server:const sseClient =awaitexperimental_createMCPClient({      transport:{type:'sse',        url:'https://actions.zapier.com/mcp/[YOUR_KEY]/sse',},});// Similarly to the stdio example, you can pass in your own custom transport as long as it implements the `MCPTransport` interface (e.g. `StreamableHTTPClientTransport`):const transport =newStreamableHTTPClientTransport(newURL('http://localhost:3000/mcp'),);const customClient =awaitexperimental_createMCPClient({      transport,});const toolSetOne =await stdioClient.tools();const toolSetTwo =await sseClient.tools();const toolSetThree =await customClient.tools();const tools ={...toolSetOne,...toolSetTwo,...toolSetThree,// note: this approach causes subsequent tool sets to override tools with the same name};const response =awaitstreamText({      model:openai('gpt-4o'),      tools,      prompt,// When streaming, the client should be closed after the response is finished:onFinish:async()=>{await stdioClient.close();await sseClient.close();await customClient.close();},// Closing clients onError is optional// - Closing: Immediately frees resources, prevents hanging connections// - Not closing: Keeps connection open for retriesonError:async error =>{await stdioClient.close();await sseClient.close();await customClient.close();},});return response.toDataStreamResponse();}catch(error){returnnewResponse('Internal Server Error',{ status:500});}}
```


## [Client](#client)


Let's create a simple React component that imports the `useCompletion` hook from the `@ai-sdk/react` module. The `useCompletion` hook will call the `/api/completion` endpoint when a button is clicked. The endpoint will generate text based on the input prompt and stream it to the client.

app/page.tsx

```
'use client';import{ useCompletion }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ completion, complete }=useCompletion({    api:'/api/completion',});return(<div><divonClick={async()=>{awaitcomplete('Please schedule a call with Sonny and Robby for tomorrow at 10am ET for me!',);}}>Schedule a call</div>{completion}</div>);}
```
```

### 15. `cookbook/next/render-visual-interface-in-chat.md`

```markdown
# Render Visual Interface in Chat


---
url: https://ai-sdk.dev/cookbook/next/render-visual-interface-in-chat
description: Learn how to render visual interfaces in chat using the AI SDK and Next.js
---


# [Render Visual Interface in Chat](#render-visual-interface-in-chat)


An interesting consequence of language models that can call [tools](/docs/ai-sdk-core/tools-and-tool-calling) is that this ability can be used to render visual interfaces by streaming React components to the client.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

What is the weather in San Francisco?

Send Message


## [Client](#client)


Let's build an assistant that gets the weather for any city by calling the `getWeatherInformation` tool. Instead of returning text during the tool call, you will render a React component that displays the weather information on the client.

app/page.tsx

```
'use client';import{ToolInvocation}from'ai';import{Message, useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit, addToolResult }=useChat({      api:'/api/use-chat',      maxSteps:5,// run client-side tools that are automatically executed:asynconToolCall({ toolCall }){if(toolCall.toolName ==='getLocation'){const cities =['New York','Los Angeles','Chicago','San Francisco',];return cities[Math.floor(Math.random()* cities.length)];}},});return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch gap-4">{messages?.map((m: Message)=>(<divkey={m.id}className="whitespace-pre-wrap flex flex-col gap-1"><strong>{`${m.role}: `}</strong>{m.content}{m.toolInvocations?.map((toolInvocation: ToolInvocation)=>{const toolCallId = toolInvocation.toolCallId;// render confirmation tool (client-side tool with user interaction)if(toolInvocation.toolName ==='askForConfirmation'){return(<divkey={toolCallId}className="text-gray-500 flex flex-col gap-2">{toolInvocation.args.message}<divclassName="flex gap-2">{'result'in toolInvocation ?(<b>{toolInvocation.result}</b>):(<><buttonclassName="px-4 py-2 font-bold text-white bg-blue-500 rounded hover:bg-blue-700"onClick={()=>addToolResult({                              toolCallId,                              result:'Yes, confirmed.',})}>Yes</button><buttonclassName="px-4 py-2 font-bold text-white bg-red-500 rounded hover:bg-red-700"onClick={()=>addToolResult({                              toolCallId,                              result:'No, denied',})}>No</button></>)}</div></div>);}// other tools:return'result'in toolInvocation ?(              toolInvocation.toolName ==='getWeatherInformation'?(<divkey={toolCallId}className="flex flex-col gap-2 p-4 bg-blue-400 rounded-lg"><divclassName="flex flex-row justify-between items-center"><divclassName="text-4xl text-blue-50 font-medium">{toolInvocation.result.value}°{toolInvocation.result.unit ==='celsius'?'C':'F'}</div><divclassName="h-9 w-9 bg-amber-400 rounded-full flex-shrink-0"/></div><divclassName="flex flex-row gap-2 text-blue-50 justify-between">{toolInvocation.result.weeklyForecast.map((forecast: any)=>(<divkey={forecast.day}className="flex flex-col items-center"><divclassName="text-xs">{forecast.day}</div><div>{forecast.value}°</div></div>),)}</div></div>): toolInvocation.toolName ==='getLocation'?(<divkey={toolCallId}className="text-gray-500 bg-gray-100 rounded-lg p-4">Userisin{toolInvocation.result}.</div>):(<divkey={toolCallId}className="text-gray-500">Tool call {`${toolInvocation.toolName}: `}{toolInvocation.result}</div>)):(<divkey={toolCallId}className="text-gray-500">Calling{toolInvocation.toolName}...</div>);})}</div>))}<formonSubmit={handleSubmit}><inputclassName="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```


## [Server](#server)


api/chat.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';import{ z }from'zod';exportdefaultasyncfunctionPOST(request: Request){const{ messages }=await request.json();const result =streamText({    model:openai('gpt-4-turbo'),    messages,    tools:{// server-side tool with execute function:      getWeatherInformation:{        description:'show the weather in a given city to the user',        parameters: z.object({ city: z.string()}),execute:async({}:{ city: string })=>{return{            value:24,            unit:'celsius',            weeklyForecast:[{ day:'Monday', value:24},{ day:'Tuesday', value:25},{ day:'Wednesday', value:26},{ day:'Thursday', value:27},{ day:'Friday', value:28},{ day:'Saturday', value:29},{ day:'Sunday', value:30},],};},},// client-side tool that starts user interaction:      askForConfirmation:{        description:'Ask the user for confirmation.',        parameters: z.object({          message: z.string().describe('The message to ask for confirmation.'),}),},// client-side tool that is automatically executed on the client:      getLocation:{        description:'Get the user location. Always ask for confirmation before using this tool.',        parameters: z.object({}),},},});return result.toDataStreamResponse();}
```
```

### 16. `cookbook/next/send-custom-body-from-use-chat.md`

```markdown
# Send Custom Body from useChat


---
url: https://ai-sdk.dev/cookbook/next/send-custom-body-from-use-chat
description: Learn how to send a custom body from the useChat hook using the AI SDK and Next.js
---


# [Send Custom Body from useChat](#send-custom-body-from-usechat)


`experimental_prepareRequestBody` is an experimental feature and only available in React, Solid and Vue.

By default, `useChat` sends all messages as well as information from the request to the server. However, it is often desirable to control the body content that is sent to the server, e.g. to:

-   only send the last message
-   send additional data along with the message
-   change the structure of the request body

The `experimental_prepareRequestBody` option allows you to customize the body content that is sent to the server. The function receives the message list, the request data, and the request body from the append call. It should return the body content that will be sent to the server.


## [Example](#example)


This example shows how to only send the text of the last message to the server. This can be useful if you want to reduce the amount of data sent to the server.


### [Client](#client)


app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat({experimental_prepareRequestBody:({ messages })=>{// e.g. only the text of the last message:return messages[messages.length-1].content;},});return(<div>{messages.map(m =>(<div key={m.id}>{m.role==='user'?'User: ':'AI: '}{m.content}</div>))}<form onSubmit={handleSubmit}><input value={input} onChange={handleInputChange}/></form></div>);}
```


### [Server](#server)


We need to adjust the server to only receive the text of the last message. The rest of the message history can be loaded from storage.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai'import{ streamText }from'ai'// Allow streaming responses up to 30 secondsexportconst maxDuration =30exportasyncfunctionPOST(req: Request){// we receive only the text from the last messageconst text =await req.json()// e.g. load message history from storageconst history =awaitloadHistory()// Call the language modelconst result =streamText({    model:openai('gpt-4-turbo'),    messages:[...history,{ role:'user', content: text }]onFinish({ text }){// e.g. save the message and the response to storage}})// Respond with the streamreturn result.toDataStreamResponse()}
```
```

### 17. `cookbook/next/stream-assistant-response-with-tools.md`

```markdown
# Stream Assistant Response with Tools


---
url: https://ai-sdk.dev/cookbook/next/stream-assistant-response-with-tools
description: Learn how to stream OpenAI Assistant's response using the AI SDK and Next.js
---


# [Stream Assistant Response with Tools](#stream-assistant-response-with-tools)


Let's create a simple chat interface that allows users to send messages to the assistant and receive responses and give it the ability to use tools. You will integrate the `useAssistant` hook from `@ai-sdk/react` to stream the messages and status.

You will need to provide the list of tools on the OpenAI [Assistant Dashboard](https://platform.openai.com/assistants). You can use the following schema to create a tool to convert celsius to fahrenheit.

```
{"name":"celsiusToFahrenheit","description":"convert celsius to fahrenheit.","parameters":{"type":"object","properties":{"value":{"type":"number","description":"the value in celsius."}},"required":["value"]}}
```


## [Client](#client)


Let's create a simple chat interface that allows users to send messages to the assistant and receive responses. You will integrate the `useAssistant` hook from `@ai-sdk/react` to stream the messages and status.

app/page.tsx

```
'use client';import{Message, useAssistant }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ status, messages, input, submitMessage, handleInputChange }=useAssistant({ api:'/api/assistant'});return(<divclassName="flex flex-col gap-2"><divclassName="p-2">status:{status}</div><divclassName="flex flex-col p-2 gap-2">{messages.map((message: Message)=>(<divkey={message.id}className="flex flex-row gap-2"><divclassName="w-24 text-zinc-500">{`${message.role}: `}</div><divclassName="w-full">{message.content}</div></div>))}</div><formonSubmit={submitMessage}className="fixed bottom-0 p-2 w-full"><inputdisabled={status !=='awaiting_message'}value={input}onChange={handleInputChange}className="bg-zinc-100 w-full p-2"/></form></div>);}
```


## [Server](#server)


Next, you will create an API route for `api/assistant` to handle the assistant's messages and responses. You will use the `AssistantResponse` function from `ai` to stream the assistant's responses back to the `useAssistant` hook on the client.

app/api/assistant/route.ts

```
import{AssistantResponse}from'ai';importOpenAIfrom'openai';const openai =newOpenAI({  apiKey: process.env.OPENAI_API_KEY|'',});exportasyncfunctionPOST(req: Request){const input:{    threadId:string|null;    message:string;}=await req.json();const threadId = input.threadId ??(await openai.beta.threads.create({})).id;const createdMessage =await openai.beta.threads.messages.create(threadId,{    role:'user',    content: input.message,});returnAssistantResponse({ threadId, messageId: createdMessage.id },async({ forwardStream })=>{const runStream = openai.beta.threads.runs.stream(threadId,{        assistant_id:          process.env.ASSISTANT_ID??(()=>{thrownewError('ASSISTANT_ID is not set');})(),});let runResult =awaitforwardStream(runStream);while(        runResult?.status ==='requires_action'&&        runResult.required_action?.type==='submit_tool_outputs'){const tool_outputs =          runResult.required_action.submit_tool_outputs.tool_calls.map((toolCall: any)=>{const parameters =JSON.parse(toolCall.function.arguments);switch(toolCall.function.name){case'celsiusToFahrenheit':const celsius =parseFloat(parameters.value);const fahrenheit = celsius *(9/5)+32;return{                    tool_call_id: toolCall.id,                    output:`${celsius}°C is ${fahrenheit.toFixed(2)}°F`,};default:thrownewError(`Unknown tool call function: ${toolCall.function.name}`,);}},);        runResult =awaitforwardStream(          openai.beta.threads.runs.submitToolOutputsStream(            threadId,            runResult.id,{ tool_outputs },),);}},);}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/assistants/stream-assistant-response-with-tools/index.tsx)
```

### 18. `cookbook/next/stream-assistant-response.md`

```markdown
# Stream Assistant Response


---
url: https://ai-sdk.dev/cookbook/next/stream-assistant-response
description: Learn how to stream OpenAI Assistant's response using the AI SDK and Next.js
---


# [Stream Assistant Response](#stream-assistant-response)



## [Client](#client)


Let's create a simple chat interface that allows users to send messages to the assistant and receive responses. You will integrate the `useAssistant` hook from `@ai-sdk/react` to stream the messages and status.

app/page.tsx

```
'use client';import{Message, useAssistant }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ status, messages, input, submitMessage, handleInputChange }=useAssistant({ api:'/api/assistant'});return(<divclassName="flex flex-col gap-2"><divclassName="p-2">status:{status}</div><divclassName="flex flex-col p-2 gap-2">{messages.map((message: Message)=>(<divkey={message.id}className="flex flex-row gap-2"><divclassName="w-24 text-zinc-500">{`${message.role}: `}</div><divclassName="w-full">{message.content}</div></div>))}</div><formonSubmit={submitMessage}className="fixed bottom-0 p-2 w-full"><inputdisabled={status !=='awaiting_message'}value={input}onChange={handleInputChange}className="bg-zinc-100 w-full p-2"/></form></div>);}
```


## [Server](#server)


Next, you will create an API route for `api/assistant` to handle the assistant's messages and responses. You will use the `AssistantResponse` function from `ai` to stream the assistant's responses back to the `useAssistant` hook on the client.

app/api/assistant/route.ts

```
importOpenAIfrom'openai';import{AssistantResponse}from'ai';const openai =newOpenAI({  apiKey: process.env.OPENAI_API_KEY|'',});exportasyncfunctionPOST(req: Request){const input:{    threadId:string|null;    message:string;}=await req.json();const threadId = input.threadId ??(await openai.beta.threads.create({})).id;const createdMessage =await openai.beta.threads.messages.create(threadId,{    role:'user',    content: input.message,});returnAssistantResponse({ threadId, messageId: createdMessage.id },async({ forwardStream })=>{const runStream = openai.beta.threads.runs.stream(threadId,{        assistant_id:          process.env.ASSISTANT_ID??(()=>{thrownewError('ASSISTANT_ID environment is not set');})(),});awaitforwardStream(runStream);},);}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/assistants/stream-assistant-response/index.tsx)
```

### 19. `cookbook/next/stream-object.md`

```markdown
# Stream Object


---
url: https://ai-sdk.dev/cookbook/next/stream-object
description: Learn how to stream object using the AI SDK and Next.js
---


# [Stream Object](#stream-object)


Object generation can sometimes take a long time to complete, especially when you're generating a large schema. In such cases, it is useful to stream the object generation process to the client in real-time. This allows the client to display the generated object as it is being generated, rather than have users wait for it to complete before displaying the result.

http://localhost:3000

View Notifications


## [Object Mode](#object-mode)


The `streamObject` function allows you to specify different output strategies using the `output` parameter. By default, the output mode is set to `object`, which will generate exactly the structured object that you specify in the schema option.


### [Schema](#schema)


It is helpful to set up the schema in a separate file that is imported on both the client and server.

app/api/use-object/schema.ts

```
import{ z }from'zod';// define a schema for the notificationsexportconst notificationSchema = z.object({  notifications: z.array(    z.object({      name: z.string().describe('Name of a fictional person.'),      message: z.string().describe('Message. Do not use emojis or links.'),}),),});
```


### [Client](#client)


The client uses [`useObject`](/docs/reference/ai-sdk-ui/use-object) to stream the object generation process.

The results are partial and are displayed as they are received. Please note the code for handling `undefined` values in the JSX.

app/page.tsx

```
'use client';import{ experimental_useObject as useObject }from'@ai-sdk/react';import{ notificationSchema }from'./api/use-object/schema';exportdefaultfunctionPage(){const{ object, submit }=useObject({    api:'/api/use-object',    schema: notificationSchema,});return(<div><buttononClick={()=>submit('Messages during finals week.')}>Generate notifications</button>{object?.notifications?.map((notification, index)=>(<divkey={index}><p>{notification?.name}</p><p>{notification?.message}</p></div>))}</div>);}
```


### [Server](#server)


On the server, we use [`streamObject`](/docs/reference/ai-sdk-core/stream-object) to stream the object generation process.

app/api/use-object/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ notificationSchema }from'./schema';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const context =await req.json();const result =streamObject({    model:openai('gpt-4-turbo'),    schema: notificationSchema,    prompt:`Generate 3 notifications for a messages app in this context:`+ context,});return result.toTextStreamResponse();}
```


## [Loading State and Stopping the Stream](#loading-state-and-stopping-the-stream)


You can use the `loading` state to display a loading indicator while the object is being generated. You can also use the `stop` function to stop the object generation process.

app/page.tsx

```
'use client';import{ experimental_useObject as useObject }from'@ai-sdk/react';import{ notificationSchema }from'./api/use-object/schema';exportdefaultfunctionPage(){const{ object, submit, isLoading, stop }=useObject({    api:'/api/use-object',    schema: notificationSchema,});return(<div><buttononClick={()=>submit('Messages during finals week.')}disabled={isLoading}>Generate notifications</button>{isLoading &&(<div><div>Loading...</div><buttontype="button"onClick={()=>stop()}>Stop</button></div>)}{object?.notifications?.map((notification, index)=>(<divkey={index}><p>{notification?.name}</p><p>{notification?.message}</p></div>))}</div>);}
```


## [Array Mode](#array-mode)


The "array" output mode allows you to stream an array of objects one element at a time. This is particularly useful when generating lists of items.


### [Schema](#schema-1)


First, update the schema to generate a single object (remove the `z.array()`).

app/api/use-object/schema.ts

```
import{ z }from'zod';// define a schema for a single notificationexportconst notificationSchema = z.object({  name: z.string().describe('Name of a fictional person.'),  message: z.string().describe('Message. Do not use emojis or links.'),});
```


### [Client](#client-1)


On the client, you wrap the schema in `z.array()` to generate an array of objects.

app/page.tsx

```
'use client';import{ experimental_useObject as useObject }from'@ai-sdk/react';import{ notificationSchema }from'./api/use-object/schema';exportdefaultfunctionPage(){const{ object, submit, isLoading, stop }=useObject({    api:'/api/use-object',    schema: z.array(notificationSchema),});return(<div><buttononClick={()=>submit('Messages during finals week.')}disabled={isLoading}>Generate notifications</button>{isLoading &&(<div><div>Loading...</div><buttontype="button"onClick={()=>stop()}>Stop</button></div>)}{object?.map((notification, index)=>(<divkey={index}><p>{notification.name}</p><p>{notification.message}</p></div>))}</div>);}
```


### [Server](#server-1)


On the server, specify `output: 'array'` to generate an array of objects.

app/api/use-object/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ notificationSchema }from'./schema';exportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const context =await req.json();const result =streamObject({    model:openai('gpt-4-turbo'),    output:'array',    schema: notificationSchema,    prompt:`Generate 3 notifications for a messages app in this context:`+ context,});return result.toTextStreamResponse();}
```


## [No Schema Mode](#no-schema-mode)


The "no-schema" output mode can be used when you don't want to specify a schema, for example when the data structure is defined by a dynamic user request. When using this mode, omit the schema parameter and set `output: 'no-schema'`. The model will still attempt to generate JSON data based on the prompt.


### [Client](#client-2)


On the client, you wrap the schema in `z.array()` to generate an array of objects.

app/page.tsx

```
'use client';import{ experimental_useObject as useObject }from'@ai-sdk/react';import{ z }from'zod';exportdefaultfunctionPage(){const{ object, submit, isLoading, stop }=useObject({    api:'/api/use-object',    schema: z.unknown(),});return(<div><buttononClick={()=>submit('Messages during finals week.')}disabled={isLoading}>Generate notifications</button>{isLoading &&(<div><div>Loading...</div><buttontype="button"onClick={()=>stop()}>Stop</button></div>)}{object?.map((notification, index)=>(<divkey={index}><p>{notification.name}</p><p>{notification.message}</p></div>))}</div>);}
```


### [Server](#server-2)


On the server, specify `output: 'no-schema'`.

app/api/use-object/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ notificationSchema }from'./schema';exportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const context =await req.json();const result =streamObject({    model:openai('gpt-4-turbo'),    output:'no-schema',    prompt:`Generate 3 notifications for a messages app in this context:`+ context,});return result.toTextStreamResponse();}
```
```

### 20. `cookbook/next/stream-text-multistep.md`

```markdown
# Stream Text Multi-Step


---
url: https://ai-sdk.dev/cookbook/next/stream-text-multistep
description: Learn how to create several streamText steps with different settings
---


# [Stream Text Multi-Step](#stream-text-multi-step)


You may want to have different steps in your stream where each step has different settings, e.g. models, tools, or system prompts.

With `createDataStreamResponse` and `sendFinish` / `sendStart` options when merging into the data stream, you can control when the finish and start events are sent to the client, allowing you to have different steps in a single assistant UI message.


## [Server](#server)


app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ createDataStreamResponse, streamText, tool }from'ai';import{ z }from'zod';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();returncreateDataStreamResponse({execute:async dataStream =>{// step 1 example: forced tool callconst result1 =streamText({        model:openai('gpt-4o-mini',{ structuredOutputs:true}),        system:'Extract the user goal from the conversation.',        messages,        toolChoice:'required',// force the model to call a tool        tools:{          extractGoal:tool({            parameters: z.object({ goal: z.string()}),execute:async({ goal })=> goal,// no-op extract tool}),},});// forward the initial result to the client without the finish event:      result1.mergeIntoDataStream(dataStream,{        experimental_sendFinish:false,// omit the finish event});// note: you can use any programming construct here, e.g. if-else, loops, etc.// workflow programming is normal programming with this approach.// example: continue stream with forced tool call from previous stepconst result2 =streamText({// different system prompt, different model, no tools:        model:openai('gpt-4o'),        system:'You are a helpful assistant with a different system prompt. Repeat the extract user goal in your answer.',// continue the workflow stream with the messages from the previous step:        messages:[...convertToCoreMessages(messages),...(await result1.response).messages,],});// forward the 2nd result to the client (incl. the finish event):      result2.mergeIntoDataStream(dataStream,{        experimental_sendStart:false,// omit the start event});},});}
```


## [Client](#client)


app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<div>{messages?.map(message=>(<divkey={message.id}><strong>{`${message.role}: `}</strong>{message.parts.map((part, index)=>{switch(part.type){case'text':return<spankey={index}>{part.text}</span>;case'tool-invocation':{return(<prekey={index}>{JSON.stringify(part.toolInvocation,null,2)}</pre>);}}})}</div>))}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}/></form></div>);}
```
```

### 21. `cookbook/next/stream-text-with-chat-prompt.md`

```markdown
# Stream Text with Chat Prompt


---
url: https://ai-sdk.dev/cookbook/next/stream-text-with-chat-prompt
description: Learn how to generate text using the AI SDK and Next.js
---


# [Stream Text with Chat Prompt](#stream-text-with-chat-prompt)


Chat completion can sometimes take a long time to finish, especially when the response is big. In such cases, it is useful to stream the chat completion to the client in real-time. This allows the client to display the new message as it is being generated by the model, rather than have users wait for it to finish.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

Why is the sky blue?

Send Message


## [Client](#client)


Let's create a React component that imports the `useChat` hook from the `@ai-sdk/react` module. The `useChat` hook will call the `/api/chat` endpoint when the user sends a message. The endpoint will generate the assistant's response based on the conversation history and stream it to the client.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, setInput, append }=useChat();return(<div><input        value={input}        onChange={event=>{setInput(event.target.value);}}        onKeyDown={asyncevent=>{if(event.key ==='Enter'){append({ content: input, role:'user'});}}}/>{messages.map((message, index)=>(<divkey={index}>{message.content}</div>))}</div>);}
```


## [Server](#server)


Next, let's create the `/api/chat` endpoint that generates the assistant's response based on the conversation history.

app/api/chat/route.ts

```
import{ streamText,UIMessage}from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctionPOST(req:Request){const{ messages }:{ messages:UIMessage[]}=await req.json();const result =streamText({    model:openai('gpt-4o'),    system:'You are a helpful assistant.',    messages,});return result.toDataStreamResponse();}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/chat/stream-chat/index.tsx)
```

### 22. `cookbook/next/stream-text-with-image-prompt.md`

```markdown
# Stream Text with Image Prompt


---
url: https://ai-sdk.dev/cookbook/next/stream-text-with-image-prompt
description: Learn how to stream text with an image prompt using the AI SDK and Next.js
---


# [Stream Text with Image Prompt](#stream-text-with-image-prompt)


Vision models such as GPT-4 can process both text and images. In this example, we will show you how to send an image URL along with the user's message to the model.


## [Using Image URLs](#using-image-urls)



### [Server](#server)


We split the user's message into two parts: the text and the image URL. We then send both parts to the model. The last message is the user's message, and we add the image URL to it.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';exportconst maxDuration =60;exportasyncfunctionPOST(req: Request){// 'data' contains the additional data that you have sent:const{ messages, data }=await req.json();const initialMessages = messages.slice(0,-1);const currentMessage = messages[messages.length -1];// Call the language modelconst result =streamText({    model:openai('gpt-4-turbo'),    messages:[...initialMessages,{        role:'user',        content:[{type:'text', text: currentMessage.content },{type:'image', image:newURL(data.imageUrl)},],},],});// Respond with the streamreturn result.toDataStreamResponse();}
```


### [Client](#client)


On the client we can send the image URL along with the user's message by adding the `data` object to the `handleSubmit` function. You can replace the `imageUrl` with the actual URL of the image you want to send.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<div>{messages.map(m =>(<div key={m.id}>{m.role==='user'?'User: ':'AI: '}{m.content}</div>))}<form        onSubmit={e =>{handleSubmit(e,{            data:{ imageUrl:'https://somewhere.com/image.png'},});}}><input          value={input}          placeholder="What does the image show..."          onChange={handleInputChange}/></form></div>);}
```
```

### 23. `cookbook/next/stream-text.md`

```markdown
# Stream Text


---
url: https://ai-sdk.dev/cookbook/next/stream-text
description: Learn how to stream text using the AI SDK and Next.js
---


# [Stream Text](#stream-text)


Text generation can sometimes take a long time to complete, especially when you're generating a couple of paragraphs. In such cases, it is useful to stream the text generation process to the client in real-time. This allows the client to display the generated text as it is being generated, rather than have users wait for it to complete before displaying the result.

http://localhost:3000

Answer


## [Client](#client)


Let's create a simple React component that imports the `useCompletion` hook from the `@ai-sdk/react` module. The `useCompletion` hook will call the `/api/completion` endpoint when a button is clicked. The endpoint will generate text based on the input prompt and stream it to the client.

app/page.tsx

```
'use client';import{ useCompletion }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ completion, complete }=useCompletion({    api:'/api/completion',});return(<div><divonClick={async()=>{awaitcomplete('Why is the sky blue?');}}>Generate</div>{completion}</div>);}
```


## [Server](#server)


Let's create a route handler for `/api/completion` that will generate text based on the input prompt. The route will call the `streamText` function from the `ai` module, which will then generate text based on the input prompt and stream it to the client.

app/api/completion/route.ts

```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctionPOST(req:Request){const{ prompt }:{ prompt:string}=await req.json();const result =streamText({    model:openai('gpt-4'),    system:'You are a helpful assistant.',    prompt,});return result.toDataStreamResponse();}
```

[

View Example on GitHub

](https://github.com/vercel/ai/blob/main/examples/next-openai-pages/pages/basics/stream-text/index.tsx)
```

### 24. `cookbook/node/call-tools-in-parallel.md`

```markdown
# Call Tools in Parallel


---
url: https://ai-sdk.dev/cookbook/node/call-tools-in-parallel
description: Learn how to call tools in parallel using the AI SDK and Node
---


# [Call Tools in Parallel](#call-tools-in-parallel)


Some language models support calling tools in parallel. This is particularly useful when multiple tools are independent of each other and can be executed in parallel during the same generation step.

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const result =awaitgenerateText({  model:openai('gpt-4-turbo'),  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location}:{location:string})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),    cityAttractions:tool({      parameters: z.object({ city: z.string()}),execute:async({ city }:{ city:string})=>{if(city ==='San Francisco'){return{            attractions:['Golden Gate Bridge','Alcatraz Island',"Fisherman's Wharf",],};}else{return{ attractions:[]};}},}),},  prompt:'What is the weather in San Francisco and what attractions should I visit?',});console.log(result);
```
```

### 25. `cookbook/node/call-tools-multiple-steps.md`

```markdown
# Call Tools in Multiple Steps


---
url: https://ai-sdk.dev/cookbook/node/call-tools-multiple-steps
description: Learn how to call tools with multiple steps using the AI SDK and Node
---


# [Call Tools in Multiple Steps](#call-tools-in-multiple-steps)


Models call tools to gather information or perform actions that are not directly available to the model. When tool results are available, the model can use them to generate another response.

You can enable multi-step tool calls in `generateText` by setting the `maxSteps` option to a number greater than 1. This option specifies the maximum number of steps (i.e., LLM calls) that can be made to prevent infinite loops.

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const{ text }=awaitgenerateText({  model:openai('gpt-4-turbo'),  maxSteps:5,  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location}:{location:string})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},  prompt:'What is the weather in San Francisco?',});
```
```

### 26. `cookbook/node/call-tools-with-image-prompt.md`

```markdown
# Call Tools with Image Prompt


---
url: https://ai-sdk.dev/cookbook/node/call-tools-with-image-prompt
description: Learn how to call tools with image prompt using the AI SDK and Node
---


# [Call Tools with Image Prompt](#call-tools-with-image-prompt)


Some language models that support vision capabilities accept images as part of the prompt. Here are some of the different [formats](/docs/reference/ai-sdk-core/generate-text#content-image) you can use to include images as input.

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const result =awaitgenerateText({  model:openai('gpt-4-turbo'),  messages:[{      role:'user',      content:[{type:'text', text:'can you log this meal for me?'},{type:'image',          image:newURL('https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Cheeseburger_%2817237580619%29.jpg/640px-Cheeseburger_%2817237580619%29.jpg',),},],},],  tools:{    logFood:tool({      description:'Log a food item',      parameters: z.object({        name: z.string(),        calories: z.number(),}),execute({ name, calories }){storeInDatabase({ name, calories });},}),},});
```
```

### 27. `cookbook/node/call-tools.md`

```markdown
# Call Tools


---
url: https://ai-sdk.dev/cookbook/node/call-tools
description: Learn how to call tools using the AI SDK and Node
---


# [Call Tools](#call-tools)


Some models allow developers to provide a list of tools that can be called at any time during a generation. This is useful for extending the capabilites of a language model to either use logic or data to interact with systems external to the model.

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const result =awaitgenerateText({  model:openai('gpt-4-turbo'),  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),    cityAttractions:tool({      parameters: z.object({ city: z.string()}),}),},  prompt:'What is the weather in San Francisco and what attractions should I visit?',});
```


## [Accessing Tool Calls and Tool Results](#accessing-tool-calls-and-tool-results)


If the model decides to call a tool, it will generate a tool call. You can access the tool call by checking the `toolCalls` property on the result.

```
import{ openai }from'@ai-sdk/openai';import{ generateText, tool }from'ai';importdotenvfrom'dotenv';import{ z }from'zod';dotenv.config();asyncfunctionmain(){const result =awaitgenerateText({    model:openai('gpt-3.5-turbo'),    maxTokens:512,    tools:{      weather:tool({        description:'Get the weather in a location',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,          temperature:72+Math.floor(Math.random()*21)-10,}),}),      cityAttractions:tool({        parameters: z.object({ city: z.string()}),}),},    prompt:'What is the weather in San Francisco and what attractions should I visit?',});// typed tool calls:for(const toolCall of result.toolCalls){switch(toolCall.toolName){case'cityAttractions':{        toolCall.args.city;// stringbreak;}case'weather':{        toolCall.args.location;// stringbreak;}}}console.log(JSON.stringify(result,null,2));}main().catch(console.error);
```


## [Accessing Tool Results](#accessing-tool-results)


You can access the result of a tool call by checking the `toolResults` property on the result.

```
import{ openai }from'@ai-sdk/openai';import{ generateText, tool }from'ai';importdotenvfrom'dotenv';import{ z }from'zod';dotenv.config();asyncfunctionmain(){const result =awaitgenerateText({    model:openai('gpt-3.5-turbo'),    maxTokens:512,    tools:{      weather:tool({        description:'Get the weather in a location',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,          temperature:72+Math.floor(Math.random()*21)-10,}),}),      cityAttractions:tool({        parameters: z.object({ city: z.string()}),}),},    prompt:'What is the weather in San Francisco and what attractions should I visit?',});// typed tool results for tools with execute method:for(const toolResult of result.toolResults){switch(toolResult.toolName){case'weather':{        toolResult.args.location;// string        toolResult.result.location;// string        toolResult.result.temperature;// numberbreak;}}}console.log(JSON.stringify(result,null,2));}main().catch(console.error);
```

`toolResults` will only be available if the tool has an `execute` function.


## [Model Response](#model-response)


When using tools, it's important to note that the model won't respond with the tool call results by default. This is because the model has technically already generated its response to the prompt: the tool call. Many use cases will require the model to summarise the results of the tool call within the context of the original prompt automatically. You can achieve this by [using `maxSteps`](/examples/node/tools/call-tools-with-automatic-roundtrips) which will automatically send toolResults to the model to trigger another generation.
```

### 28. `cookbook/node/embed-text-batch.md`

```markdown
# Embed Text in Batch


---
url: https://ai-sdk.dev/cookbook/node/embed-text-batch
description: Learn how to embed multiple text using the AI SDK and Node
---


# [Embed Text in Batch](#embed-text-in-batch)


When working with large datasets or multiple pieces of text, processing embeddings one at a time can be inefficient. Batch embedding allows you to convert multiple text inputs into embeddings simultaneously, significantly improving performance and reducing API calls. This is particularly useful when processing documents, chat messages, or any collection of text that needs to be vectorized.

This example shows how to embed multiple text inputs in a single operation using the AI SDK. For single text embedding, see our [Embed Text](/cookbook/node/embed-text) example, or for a practical application, check out our [RAG example](/cookbook/node/retrieval-augmented-generation) which demonstrates how batch embeddings can be used in a document retrieval system.

```
import{ openai }from'@ai-sdk/openai';import{ embedMany }from'ai';import'dotenv/config';asyncfunctionmain(){const{ embeddings, usage }=awaitembedMany({    model: openai.embedding('text-embedding-3-small'),    values:['sunny day at the beach','rainy afternoon in the city','snowy night in the mountains',],});console.log(embeddings);console.log(usage);}main().catch(console.error);
```
```

### 29. `cookbook/node/embed-text.md`

```markdown
# Embed Text


---
url: https://ai-sdk.dev/cookbook/node/embed-text
description: Learn how to embed text using the AI SDK and Node
---


# [Embed Text](#embed-text)


Text embeddings are numerical representations of text that capture semantic meaning, allowing machines to understand and process language in a mathematical way. These vector representations are crucial for many AI applications, as they enable tasks like semantic search, document similarity comparison, and content recommendation.

This example demonstrates how to convert text into embeddings using a text embedding model. The resulting embedding is a high-dimensional vector that represents the semantic meaning of the input text. For a more practical application of embeddings, check out our [RAG example](/cookbook/node/retrieval-augmented-generation) which shows how embeddings can be used for document retrieval.

```
import{ openai }from'@ai-sdk/openai';import{ embed }from'ai';import'dotenv/config';asyncfunctionmain(){const{ embedding, usage }=awaitembed({    model: openai.embedding('text-embedding-3-small'),    value:'sunny day at the beach',});console.log(embedding);console.log(usage);}main().catch(console.error);
```
```

### 30. `cookbook/node/generate-object-reasoning.md`

```markdown
# Generate Object with a Reasoning Model


---
url: https://ai-sdk.dev/cookbook/node/generate-object-reasoning
description: Learn how to generate structured data with a reasoning model using the AI SDK and Node
---


# [Generate Object with a Reasoning Model](#generate-object-with-a-reasoning-model)


Reasoning models, like [DeepSeek's](/providers/ai-sdk-providers/deepseek) R1, are gaining popularity due to their ability to understand and generate better responses to complex queries than non-reasoning models. You may want to use these models to generate structured data. However, most (like R1 and [OpenAI's](/providers/ai-sdk-providers/openai) o1) do not support tool-calling or structured outputs.

One solution is to pass the output from a reasoning model through a smaller model that can output structured data (like gpt-4o-mini). These lightweight models can efficiently extract the structured data while adding very little overhead in terms of speed and cost.

```
import{ deepseek }from'@ai-sdk/deepseek';import{ openai }from'@ai-sdk/openai';import{ generateObject, generateText }from'ai';import'dotenv/config';import{ z }from'zod';asyncfunctionmain(){const{ text: rawOutput }=awaitgenerateText({    model:deepseek('deepseek-reasoner'),    prompt:'Predict the top 3 largest city by 2050. For each, return the name, the country, the reason why it will on the list, and the estimated population in millions.',});const{ object }=awaitgenerateObject({    model:openai('gpt-4o-mini'),    prompt:'Extract the desired information from this text: \n'+ rawOutput,    schema: z.object({      name: z.string().describe('the name of the city'),      country: z.string().describe('the name of the country'),      reason: z.string().describe('the reason why the city will be one of the largest cities by 2050',),      estimatedPopulation: z.number(),}),    output:'array',});console.log(object);}main().catch(console.error);
```
```

### 31. `cookbook/node/generate-object.md`

```markdown
# Generate Object


---
url: https://ai-sdk.dev/cookbook/node/generate-object
description: Learn how to generate structured data using the AI SDK and Node
---


# [Generate Object](#generate-object)


Earlier functions like `generateText` and `streamText` gave us the ability to generate unstructured text. However, if you want to generate structured data like JSON, you can provide a schema that describes the structure of your desired object to the `generateObject` function.

The function requires you to provide a schema using [zod](https://zod.dev), a library for defining schemas for JavaScript objects. By using zod, you can also use it to validate the generated object and ensure that it conforms to the specified structure.

```
import{ generateObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const result =awaitgenerateObject({  model:openai('gpt-4-turbo'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(        z.object({          name: z.string(),          amount: z.string(),}),),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});console.log(JSON.stringify(result.object.recipe,null,2));
```
```

### 32. `cookbook/node/generate-text-with-chat-prompt.md`

```markdown
# Generate Text with Chat Prompt


---
url: https://ai-sdk.dev/cookbook/node/generate-text-with-chat-prompt
description: Learn how to generate text with chat prompt using the AI SDK and Node
---


# [Generate Text with Chat Prompt](#generate-text-with-chat-prompt)


Previously, we were able to generate text and objects using either a single message prompt, a system prompt, or a combination of both of them. However, there may be times when you want to generate text based on a series of messages.

A chat completion allows you to generate text based on a series of messages. This series of messages can be any series of interactions between any number of systems, but the most popular and relatable use case has been a series of messages that represent a conversation between a user and a model.

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const result =awaitgenerateText({  model:openai('gpt-3.5-turbo'),  maxTokens:1024,  system:'You are a helpful chatbot.',  messages:[{      role:'user',      content:'Hello!',},{      role:'assistant',      content:'Hello! How can I help you today?',},{      role:'user',      content:'I need help with my computer.',},],});console.log(result.text);
```
```

### 33. `cookbook/node/generate-text-with-image-prompt.md`

```markdown
# Generate Text with Image Prompt


---
url: https://ai-sdk.dev/cookbook/node/generate-text-with-image-prompt
description: Learn how to generate text with image prompt using the AI SDK and Node
---


# [Generate Text with Image Prompt](#generate-text-with-image-prompt)


Some language models that support vision capabilities accept images as part of the prompt. Here are some of the different [formats](/docs/reference/ai-sdk-core/generate-text#content-image) you can use to include images as input.


## [URL](#url)


```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const result =awaitgenerateText({  model:openai('gpt-4-turbo'),  maxTokens:512,  messages:[{      role:'user',      content:[{type:'text',          text:'what are the red things in this image?',},{type:'image',          image:newURL('https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/2024_Solar_Eclipse_Prominences.jpg/720px-2024_Solar_Eclipse_Prominences.jpg',),},],},],});console.log(result);
```


## [File Buffer](#file-buffer)


```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';importfsfrom'fs';const result =awaitgenerateText({  model:openai('gpt-4-turbo'),  maxTokens:512,  messages:[{      role:'user',      content:[{type:'text',          text:'what are the red things in this image?',},{type:'image',          image: fs.readFileSync('./node/attachments/eclipse.jpg'),},],},],});console.log(result);
```
```

### 34. `cookbook/node/generate-text.md`

```markdown
# Generate Text


---
url: https://ai-sdk.dev/cookbook/node/generate-text
description: Learn how to generate text using the AI SDK and Node
---


# [Generate Text](#generate-text)


The most basic LLM use case is generating text based on a prompt. For example, you may want to generate a response to a question or summarize a body of text. The `generateText` function can be used to generate text based on the input prompt.

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const result =awaitgenerateText({  model:openai('gpt-3.5-turbo'),  prompt:'Why is the sky blue?',});console.log(result);
```
```

### 35. `cookbook/node/intercept-fetch-requests.md`

```markdown
# Intercepting Fetch Requests


---
url: https://ai-sdk.dev/cookbook/node/intercept-fetch-requests
description: Learn how to intercept fetch requests using the AI SDK and Node
---


# [Intercepting Fetch Requests](#intercepting-fetch-requests)


Many providers support setting a custom `fetch` function using the `fetch` argument in their factory function.

A custom `fetch` function can be used to intercept and modify requests before they are sent to the provider's API, and to intercept and modify responses before they are returned to the caller.

Use cases for intercepting requests include:

-   Logging requests and responses
-   Adding authentication headers
-   Modifying request bodies
-   Caching responses
-   Using a custom HTTP client


## [Example](#example)


```
import{ generateText }from'ai';import{ createOpenAI }from'@ai-sdk/openai';const openai =createOpenAI({// example fetch wrapper that logs the input to the API call:fetch:async(url, options)=>{console.log('URL', url);console.log('Headers',JSON.stringify(options!.headers,null,2));console.log(`Body ${JSON.stringify(JSON.parse(options!.body!as string),null,2)}`,);returnawaitfetch(url, options);},});const{ text }=awaitgenerateText({  model:openai('gpt-3.5-turbo'),  prompt:'Why is the sky blue?',});
```
```

### 36. `cookbook/node/local-caching-middleware.md`

```markdown
# Local Caching Middleware


---
url: https://ai-sdk.dev/cookbook/node/local-caching-middleware
description: Learn how to create a caching middleware for local development.
---


# [Local Caching Middleware](#local-caching-middleware)


When developing AI applications, you'll often find yourself repeatedly making the same API calls during development. This can lead to increased costs and slower development cycles. A caching middleware allows you to store responses locally and reuse them when the same inputs are provided.

This approach is particularly useful in two scenarios:

1.  **Iterating on UI/UX** - When you're focused on styling and user experience, you don't want to regenerate AI responses for every code change.
2.  **Working on evals** - When developing evals, you need to repeatedly test the same prompts, but don't need new generations each time.


## [Implementation](#implementation)


In this implementation, you create a JSON file to store responses. When a request is made, you first check if you have already seen this exact request. If you have, you return the cached response immediately (as a one-off generation or chunks of tokens). If not, you trigger the generation, save the response, and return it.

Make sure to add the path of your local cache to your `.gitignore` so you do not commit it.


### [How it works](#how-it-works)


For regular generations, you store and retrieve complete responses. Instead, the streaming implementation captures each token as it arrives, stores the full sequence, and on cache hits uses the SDK's `simulateReadableStream` utility to recreate the token-by-token streaming experience at a controlled speed (defaults to 10ms between chunks).

This approach gives you the best of both worlds:

-   Instant responses for repeated queries
-   Preserved streaming behavior for UI development

The middleware handles all transformations needed to make cached responses indistinguishable from fresh ones, including normalizing tool calls and fixing timestamp formats.


### [Middleware](#middleware)


```
import{typeLanguageModelV1,typeLanguageModelV1Middleware,LanguageModelV1Prompt,typeLanguageModelV1StreamPart,  simulateReadableStream,  wrapLanguageModel,}from'ai';import'dotenv/config';importfsfrom'fs';importpathfrom'path';constCACHE_FILE= path.join(process.cwd(),'.cache/ai-cache.json');exportconstcached=(model:LanguageModelV1)=>wrapLanguageModel({    middleware: cacheMiddleware,    model,});constensureCacheFile=()=>{const cacheDir = path.dirname(CACHE_FILE);if(!fs.existsSync(cacheDir)){    fs.mkdirSync(cacheDir,{ recursive:true});}if(!fs.existsSync(CACHE_FILE)){    fs.writeFileSync(CACHE_FILE,'{}');}};constgetCachedResult=(key:string| object)=>{ensureCacheFile();const cacheKey =typeof key ==='object'?JSON.stringify(key): key;try{const cacheContent = fs.readFileSync(CACHE_FILE,'utf-8');const cache =JSON.parse(cacheContent);const result = cache[cacheKey];return result ??null;}catch(error){console.error('Cache error:', error);returnnull;}};constupdateCache=(key:string, value:any)=>{ensureCacheFile();try{const cache =JSON.parse(fs.readFileSync(CACHE_FILE,'utf-8'));const updatedCache ={...cache,[key]: value };    fs.writeFileSync(CACHE_FILE,JSON.stringify(updatedCache,null,2));console.log('Cache updated for key:', key);}catch(error){console.error('Failed to update cache:', error);}};constcleanPrompt=(prompt:LanguageModelV1Prompt)=>{return prompt.map(m =>{if(m.role==='assistant'){return m.content.map(part =>        part.type==='tool-call'?{...part, toolCallId:'cached'}: part,);}if(m.role==='tool'){return m.content.map(tc =>({...tc,        toolCallId:'cached',        result:{},}));}return m;});};exportconst cacheMiddleware:LanguageModelV1Middleware={wrapGenerate:async({ doGenerate, params })=>{const cacheKey =JSON.stringify({...cleanPrompt(params.prompt),      _function:'generate',});console.log('Cache Key:', cacheKey);const cached =getCachedResult(cacheKey)asAwaited<ReturnType<LanguageModelV1['doGenerate']>>|null;if(cached && cached !==null){console.log('Cache Hit');return{...cached,        response:{...cached.response,          timestamp: cached?.response?.timestamp?newDate(cached?.response?.timestamp):undefined,},};}console.log('Cache Miss');const result =awaitdoGenerate();updateCache(cacheKey, result);return result;},wrapStream:async({ doStream, params })=>{const cacheKey =JSON.stringify({...cleanPrompt(params.prompt),      _function:'stream',});console.log('Cache Key:', cacheKey);// Check if the result is in the cacheconst cached =getCachedResult(cacheKey);// If cached, return a simulated ReadableStream that yields the cached resultif(cached && cached !==null){console.log('Cache Hit');// Format the timestamps in the cached responseconst formattedChunks =(cached asLanguageModelV1StreamPart[]).map(p =>{if(p.type==='response-metadata'&& p.timestamp){return{...p, timestamp:newDate(p.timestamp)};}elsereturn p;});return{        stream:simulateReadableStream({          initialDelayInMs:0,          chunkDelayInMs:10,          chunks: formattedChunks,}),        rawCall:{ rawPrompt:null, rawSettings:{}},};}console.log('Cache Miss');// If not cached, proceed with streamingconst{ stream,...rest }=awaitdoStream();const fullResponse:LanguageModelV1StreamPart[]=[];const transformStream =newTransformStream<LanguageModelV1StreamPart,LanguageModelV1StreamPart>({transform(chunk, controller){        fullResponse.push(chunk);        controller.enqueue(chunk);},flush(){// Store the full response in the cache after streaming is completeupdateCache(cacheKey, fullResponse);},});return{      stream: stream.pipeThrough(transformStream),...rest,};},};
```


## [Using the Middleware](#using-the-middleware)


The middleware can be easily integrated into your existing AI SDK setup:

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';import'dotenv/config';import{ cached }from'../middleware/your-cache-middleware';asyncfunctionmain(){const result =streamText({    model:cached(openai('gpt-4o')),    maxTokens:512,    temperature:0.3,    maxRetries:5,    prompt:'Invent a new holiday and describe its traditions.',});forawait(const textPart of result.textStream){    process.stdout.write(textPart);}console.log();console.log('Token usage:',await result.usage);console.log('Finish reason:',await result.finishReason);}main().catch(console.error);
```


## [Considerations](#considerations)


When using this caching middleware, keep these points in mind:

1.  **Development Only** - This approach is intended for local development, not production environments
2.  **Cache Invalidation** - You'll need to clear the cache (delete the cache file) when you want fresh responses
3.  **Multi-Step Flows** - When using `maxSteps`, be aware that caching occurs at the individual language model response level, not across the entire execution flow. This means that while the model's generation is cached, the tool call is not and will run on each generation.
```

### 37. `cookbook/node/mcp-tools.md`

```markdown
# MCP Tools


---
url: https://ai-sdk.dev/cookbook/node/mcp-tools
description: Learn how to use MCP tools with the AI SDK and Node
---


# [MCP Tools](#mcp-tools)


The AI SDK supports Model Context Protocol (MCP) tools by offering a lightweight client that exposes a `tools` method for retrieving tools from a MCP server. After use, the client should always be closed to release resources.

```
import{ experimental_createMCPClient, generateText }from'ai';import{Experimental_StdioMCPTransport}from'ai/mcp-stdio';import{ openai }from'@ai-sdk/openai';let clientOne;let clientTwo;let clientThree;try{// Initialize an MCP client to connect to a `stdio` MCP server:const transport =newExperimental_StdioMCPTransport({    command:'node',    args:['src/stdio/dist/server.js'],});  clientOne =awaitexperimental_createMCPClient({    transport,});// Alternatively, you can connect to a Server-Sent Events (SSE) MCP server:  clientTwo =awaitexperimental_createMCPClient({    transport:{type:'sse',      url:'http://localhost:3000/sse',},});// Similarly to the stdio example, you can pass in your own custom transport as long as it implements the `MCPTransport` interface:const transport =newMyCustomTransport({// ...});  clientThree =awaitexperimental_createMCPClient({    transport,});const toolSetOne =await clientOne.tools();const toolSetTwo =await clientTwo.tools();const toolSetThree =await clientThree.tools();const tools ={...toolSetOne,...toolSetTwo,...toolSetThree,// note: this approach causes subsequent tool sets to override tools with the same name};const response =awaitgenerateText({    model:openai('gpt-4o'),    tools,    messages:[{        role:'user',        content:'Find products under $100',},],});console.log(response.text);}catch(error){console.error(error);}finally{awaitPromise.all([clientOne.close(), clientTwo.close()]);}
```
```

### 38. `cookbook/node/retrieval-augmented-generation.md`

```markdown
# Retrieval Augmented Generation


---
url: https://ai-sdk.dev/cookbook/node/retrieval-augmented-generation
description: Learn how to use retrieval augmented generation using the AI SDK and Node
---


# [Retrieval Augmented Generation](#retrieval-augmented-generation)


Retrieval Augmented Generation (RAG) is a technique that enhances the capabilities of language models by providing them with relevant information from external sources during the generation process. This approach allows the model to access and incorporate up-to-date or specific knowledge that may not be present in its original training data.

This example uses [the following essay](https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt) as an input (`essay.txt`). This example uses a simple in-memory vector database to store and retrieve relevant information. For a more in-depth guide, check out the [RAG Chatbot Guide](/docs/guides/rag-chatbot) which will show you how to build a RAG chatbot with [Next.js](https://nextjs.org), [Drizzle ORM](https://orm.drizzle.team/) and [Postgres](https://postgresql.org).

```
importfsfrom'fs';importpathfrom'path';importdotenvfrom'dotenv';import{ openai }from'@ai-sdk/openai';import{ cosineSimilarity, embed, embedMany, generateText }from'ai';dotenv.config();asyncfunctionmain(){const db:{ embedding:number[]; value:string}[]=[];const essay = fs.readFileSync(path.join(__dirname,'essay.txt'),'utf8');const chunks = essay.split('.').map(chunk => chunk.trim()).filter(chunk => chunk.length>0&& chunk !=='\n');const{ embeddings }=awaitembedMany({    model: openai.embedding('text-embedding-3-small'),    values: chunks,});  embeddings.forEach((e, i)=>{    db.push({      embedding: e,      value: chunks[i],});});const input ='What were the two main things the author worked on before college?';const{ embedding }=awaitembed({    model: openai.embedding('text-embedding-3-small'),    value: input,});const context = db.map(item =>({document: item,      similarity:cosineSimilarity(embedding, item.embedding),})).sort((a, b)=> b.similarity- a.similarity).slice(0,3).map(r => r.document.value).join('\n');const{ text }=awaitgenerateText({    model:openai('gpt-4o'),    prompt:`Answer the following question based only on the provided context:${context}             Question: ${input}`,});console.log(text);}main().catch(console.error);
```
```

### 39. `cookbook/node/stream-object-record-final-object.md`

```markdown
# Record Final Object after Streaming Object


---
url: https://ai-sdk.dev/cookbook/node/stream-object-record-final-object
description: Learn how to record the final object after streaming an object using the AI SDK and Node
---


# [Record Final Object after Streaming Object](#record-final-object-after-streaming-object)


When you're streaming structured data, you may want to record the final object for logging or other purposes.


## [`onFinish` Callback](#onfinish-callback)


You can use the `onFinish` callback to record the final object. It is called when the stream is finished.

The `object` parameter contains the final object, or `undefined` if the type validation fails. There is also an `error` parameter that contains error when e.g. the object does not match the schema.

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ z }from'zod';const result =streamObject({  model:openai('gpt-4-turbo'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.string()),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',onFinish({ object, error }){// handle type validation failure (when the object does not match the schema):if(object ===undefined){console.error('Error:', error);return;}console.log('Final object:',JSON.stringify(object,null,2));},});
```


## [`object` Promise](#object-promise)


The [`streamObject`](/docs/reference/ai-sdk-core/stream-object) result contains an `object` promise that resolves to the final object. The object is fully typed. When the type validation according to the schema fails, the promise will be rejected with a `TypeValidationError`.

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ z }from'zod';const result =streamObject({  model:openai('gpt-4-turbo'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.string()),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});result.object.then(({ recipe })=>{// do something with the fully typed, final object:console.log('Recipe:',JSON.stringify(recipe,null,2));}).catch(error =>{// handle type validation failure// (when the object does not match the schema):console.error(error);});// note: the stream needs to be consumed because of backpressureforawait(const partialObject of result.partialObjectStream){}
```
```

### 40. `cookbook/node/stream-object-record-token-usage.md`

```markdown
# Record Token Usage After Streaming Object


---
url: https://ai-sdk.dev/cookbook/node/stream-object-record-token-usage
description: Learn how to record token usage when streaming structured data using the AI SDK and Node
---


# [Record Token Usage After Streaming Object](#record-token-usage-after-streaming-object)


When you're streaming structured data with [`streamObject`](/docs/reference/ai-sdk-core/stream-object), you may want to record the token usage for billing purposes.


## [`onFinish` Callback](#onfinish-callback)


You can use the `onFinish` callback to record token usage. It is called when the stream is finished.

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ z }from'zod';const result =streamObject({  model:openai('gpt-4-turbo'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.string()),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',onFinish({ usage }){console.log('Token usage:', usage);},});
```


## [`usage` Promise](#usage-promise)


The [`streamObject`](/docs/reference/ai-sdk-core/stream-object) result contains a `usage` promise that resolves to the total token usage.

```
import{ openai }from'@ai-sdk/openai';import{ streamObject,TokenUsage}from'ai';import{ z }from'zod';const result =streamObject({  model:openai('gpt-4-turbo'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.string()),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});// your custom function to record token usage:functionrecordTokenUsage({  promptTokens,  completionTokens,  totalTokens,}:TokenUsage){console.log('Prompt tokens:', promptTokens);console.log('Completion tokens:', completionTokens);console.log('Total tokens:', totalTokens);}// use as promise:result.usage.then(recordTokenUsage);// use with async/await:recordTokenUsage(await result.usage);// note: the stream needs to be consumed because of backpressureforawait(const partialObject of result.partialObjectStream){}
```
```

### 41. `cookbook/node/stream-object-with-image-prompt.md`

```markdown
# Stream Object with Image Prompt


---
url: https://ai-sdk.dev/cookbook/node/stream-object-with-image-prompt
description: Learn how to stream structured data with an image prompt using the AI SDK and Node
---


# [Stream Object with Image Prompt](#stream-object-with-image-prompt)


Some language models that support vision capabilities accept images as part of the prompt. Here are some of the different [formats](/docs/reference/ai-sdk-core/generate-text#content-image) you can use to include images as input.


## [URL](#url)


```
import{ streamObject }from'ai';import{ openai }from'@ai-sdk/openai';importdotenvfrom'dotenv';import{ z }from'zod';dotenv.config();asyncfunctionmain(){const{ partialObjectStream }=streamObject({    model:openai('gpt-4-turbo'),    maxTokens:512,    schema: z.object({      stamps: z.array(        z.object({          country: z.string(),          date: z.string(),}),),}),    messages:[{        role:'user',        content:[{type:'text',            text:'list all the stamps in these passport pages?',},{type:'image',            image:newURL('https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/WW2_Spanish_official_passport.jpg/1498px-WW2_Spanish_official_passport.jpg',),},],},],});forawait(const partialObject of partialObjectStream){console.clear();console.log(partialObject);}}main();
```


## [File Buffer](#file-buffer)


```
import{ streamObject }from'ai';import{ openai }from'@ai-sdk/openai';importdotenvfrom'dotenv';import{ z }from'zod';dotenv.config();asyncfunctionmain(){const{ partialObjectStream }=streamObject({    model:openai('gpt-4-turbo'),    maxTokens:512,    schema: z.object({      stamps: z.array(        z.object({          country: z.string(),          date: z.string(),}),),}),    messages:[{        role:'user',        content:[{type:'text',            text:'list all the stamps in these passport pages?',},{type:'image',            image: fs.readFileSync('./node/attachments/eclipse.jpg'),},],},],});forawait(const partialObject of partialObjectStream){console.clear();console.log(partialObject);}}main();
```
```

### 42. `cookbook/node/stream-object.md`

```markdown
# Stream Object


---
url: https://ai-sdk.dev/cookbook/node/stream-object
description: Learn how to stream structured data using the AI SDK and Node
---


# [Stream Object](#stream-object)


Object generation can sometimes take a long time to complete, especially when you're generating a large schema.

In Generative UI use cases, it is useful to stream the object to the client in real-time to render UIs as the object is being generated. You can use the [`streamObject`](/docs/reference/ai-sdk-core/stream-object) function to generate partial object streams.

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ z }from'zod';const{ partialObjectStream }=streamObject({  model:openai('gpt-4-turbo'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.string()),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});forawait(const partialObject of partialObjectStream){console.clear();console.log(partialObject);}
```
```

### 43. `cookbook/node/stream-text-with-chat-prompt.md`

```markdown
# Stream Text with Chat Prompt


---
url: https://ai-sdk.dev/cookbook/node/stream-text-with-chat-prompt
description: Learn how to stream text with chat prompt using the AI SDK and Node
---


# [Stream Text with Chat Prompt](#stream-text-with-chat-prompt)


Text generation can sometimes take a long time to finish, especially when the response is big. In such cases, it is useful to stream the chat completion to the client in real-time. This allows the client to display the new message as it is being generated by the model, rather than have users wait for it to finish.

```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';const result =streamText({  model:openai('gpt-3.5-turbo'),  maxTokens:1024,  system:'You are a helpful chatbot.',  messages:[{      role:'user',      content:'Hello!',},{      role:'assistant',      content:'Hello! How can I help you today?',},{      role:'user',      content:'I need help with my computer.',},],});forawait(const textPart of result.textStream){console.log(textPart);}
```
```

### 44. `cookbook/node/stream-text-with-file-prompt.md`

```markdown
# Stream Text with File Prompt


---
url: https://ai-sdk.dev/cookbook/node/stream-text-with-file-prompt
description: Learn how to stream text with file prompt using the AI SDK and Node
---


# [Stream Text with File Prompt](#stream-text-with-file-prompt)


Working with files in AI applications often requires analyzing documents, processing structured data, or extracting information from various file formats. File prompts allow you to send file content directly to the model, enabling tasks like document analysis, data extraction, or generating responses based on file contents.

```
import{ anthropic }from'@ai-sdk/anthropic';import{ streamText }from'ai';import'dotenv/config';importfsfrom'node:fs';asyncfunctionmain(){const result =streamText({    model:anthropic('claude-3-5-sonnet-20241022'),    messages:[{        role:'user',        content:[{type:'text',            text:'What is an embedding model according to this document?',},{type:'file',            data: fs.readFileSync('./data/ai.pdf'),            mimeType:'application/pdf',},],},],});forawait(const textPart of result.textStream){    process.stdout.write(textPart);}}main().catch(console.error);
```
```

### 45. `cookbook/node/stream-text-with-image-prompt.md`

```markdown
# Stream Text with Image Prompt


---
url: https://ai-sdk.dev/cookbook/node/stream-text-with-image-prompt
description: Learn how to stream text with image prompt using the AI SDK and Node
---


# [Stream Text with Image Prompt](#stream-text-with-image-prompt)


Vision-language models can analyze images alongside text prompts to generate responses about visual content. This multimodal approach allows for rich interactions where you can ask questions about images, request descriptions, or analyze visual details. The combination of image and text inputs enables more sophisticated AI applications like visual question answering and image analysis.

```
import{ anthropic }from'@ai-sdk/anthropic';import{ streamText }from'ai';import'dotenv/config';importfsfrom'node:fs';asyncfunctionmain(){const result =streamText({    model:anthropic('claude-3-5-sonnet-20240620'),    messages:[{        role:'user',        content:[{type:'text', text:'Describe the image in detail.'},{type:'image', image: fs.readFileSync('./data/comic-cat.png')},],},],});forawait(const textPart of result.textStream){    process.stdout.write(textPart);}}main().catch(console.error);
```
```

### 46. `cookbook/node/stream-text.md`

```markdown
# Stream Text


---
url: https://ai-sdk.dev/cookbook/node/stream-text
description: Learn how to stream text using the AI SDK and Node
---


# [Stream Text](#stream-text)


Text generation can sometimes take a long time to complete, especially when you're generating a couple of paragraphs. In such cases, it is useful to stream the text to the client in real-time. This allows the client to display the generated text as it is being generated, rather than have users wait for it to complete before displaying the result.

```
Introducing "Joyful Hearts Day" - a holiday dedicated to spreading love, joy, and kindness to others.On Joyful Hearts Day, people exchange handmade cards, gifts, and acts of kindness to show appreciation and love for their friends, family, and community members. It is a day to focus on positivity and gratitude, spreading happiness and warmth to those around us.Traditions include decorating homes and public spaces with hearts and bright colors, hosting community events such as charity drives, volunteer projects, and festive gatherings. People also participate in random acts of kindness, such as paying for someone's coffee, leaving encouraging notes for strangers, or simply offering a helping hand to those in need.One of the main traditions of Joyful Hearts Day is the "Heart Exchange" where people write heartfelt messages to loved ones and exchange them in person or through mail. These messages can be words of encouragement, expressions of gratitude, or simply a reminder of how much they are loved.Overall, Joyful Hearts Day is a day to celebrate love, kindness, and positivity, and to spread joy and happiness to all those around us. It is a reminder to appreciate the people in our lives and to make the world a brighter and more loving place.
```


## [Without reader](#without-reader)


```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';const result =streamText({  model:openai('gpt-3.5-turbo'),  maxTokens:512,  temperature:0.3,  maxRetries:5,  prompt:'Invent a new holiday and describe its traditions.',});forawait(const textPart of result.textStream){console.log(textPart);}
```


## [With reader](#with-reader)


```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';const result =streamText({  model:openai('gpt-3.5-turbo'),  maxTokens:512,  temperature:0.3,  maxRetries:5,  prompt:'Invent a new holiday and describe its traditions.',});const reader = result.textStream.getReader();while(true){const{ done, value }=await reader.read();if(done){break;}console.log(value);}
```
```

### 47. `cookbook/node/web-search-agent.md`

```markdown
# Web Search Agent


---
url: https://ai-sdk.dev/cookbook/node/web-search-agent
description: Learn how to build an agent that has access to web with the AI SDK and Node
---


# [Web Search Agent](#web-search-agent)


There are two approaches you can take to building a web search agent with the AI SDK:

1.  Use a model that has native web-searching capabilities
2.  Create a tool to access the web and return search results.

Both approaches have their advantages and disadvantages. Models with native search capabilities tend to be faster and there is no additional cost to make the search. The disadvantage is that you have less control over what is being searched, and the functionality is limited to models that support it.

instead, by creating a tool, you can achieve more flexibility and greater control over your search queries. It allows you to customize your search strategy, specify search parameters, and you can use it with any LLM that supports tool calling. This approach will incur additional costs for the search API you use, but gives you complete control over the search experience.


## [Using native web-search](#using-native-web-search)


There are several models that offer native web-searching capabilities (Perplexity, OpenAI, Gemini). Let's look at how you could build a Web Search Agent across providers.


### [OpenAI Responses API](#openai-responses-api)


OpenAI's Responses API has a built-in web search tool that can be used to search the web and return search results. This tool is called `web_search_preview` and is accessed via the `openai` provider.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const{ text, sources }=awaitgenerateText({  model: openai.responses('gpt-4o-mini'),  prompt:'What happened in San Francisco last week?',  tools:{    web_search_preview: openai.tools.webSearchPreview(),},});console.log(text);console.log(sources);
```


### [Perplexity](#perplexity)


Perplexity's Sonar models combines real-time web search with natural language processing. Each response is grounded in current web data and includes detailed citations.

```
import{ perplexity }from'@ai-sdk/perplexity';import{ generateText }from'ai';const{ text, sources }=awaitgenerateText({  model:perplexity('sonar-pro'),  prompt:'What are the latest developments in quantum computing?',});console.log(text);console.log(sources);
```


### [Gemini](#gemini)


With compatible Gemini models, you can enable search grounding to give the model access to the latest information using Google search.

```
import{ google }from'@ai-sdk/google';import{ generateText }from'ai';const{ text, sources, providerMetadata }=awaitgenerateText({  model:google('gemini-1.5-pro',{    useSearchGrounding:true,}),  prompt:'List the top 5 San Francisco news from the past week.'+'You must include the date of each article.',});console.log(text);console.log(sources);// access the grounding metadata.const metadata = providerMetadata?.google;const groundingMetadata = metadata?.groundingMetadata;const safetyRatings = metadata?.safetyRatings;
```


## [Building a web search tool](#building-a-web-search-tool)


Let's look at how you can build tools that search the web and return results. These tools can be used with any model that supports tool calling, giving you maximum flexibility and control over your search experience. We'll examine several search API options that can be integrated as tools in your agent.

Unlike the native web search examples where searching is built into the model, using web search tools requires multiple steps. The language model will make two generations - the first to call the relevant web search tool (extracting search queries from the context), and the second to process the results and generate a response. This multi-step process is handled automatically when you set `maxSteps` to a value greater than 1.

By using `maxSteps`, you can automatically send tool results back to the language model alongside the original question, enabling the model to respond with information relevant to the user's query based on the search results. This creates a seamless experience where the agent can search the web and incorporate those findings into its response.


### [Exa](#exa)


[Exa](https://exa.ai/) is a search API designed for AI. Let's look at how you could implement a search tool using Exa:

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';importExafrom'exa-js';exportconst exa =newExa(process.env.EXA_API_KEY);exportconst webSearch =tool({  description:'Search the web for up-to-date information',  parameters: z.object({    query: z.string().min(1).max(100).describe('The search query'),}),execute:async({ query })=>{const{ results }=await exa.searchAndContents(query,{      livecrawl:'always',      numResults:3,});return results.map(result =>({      title: result.title,      url: result.url,      content: result.text.slice(0,1000),// take just the first 1000 characters      publishedDate: result.publishedDate,}));},});const{ text }=awaitgenerateText({  model:openai('gpt-4o-mini'),// can be any model that supports tools  prompt:'What happened in San Francisco last week?',  tools:{    webSearch,},  maxSteps:2,});
```


### [Firecrawl](#firecrawl)


[Firecrawl](https://firecrawl.dev) provides an API for web scraping and crawling. Let's look at how you could implement a scraping tool using Firecrawl:

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';importFirecrawlAppfrom'@mendable/firecrawl-js';import'dotenv/config';const app =newFirecrawlApp({ apiKey: process.env.FIRECRAWL_API_KEY});exportconst webSearch =tool({  description:'Search the web for up-to-date information',  parameters: z.object({    urlToCrawl: z.string().url().min(1).max(100).describe('The URL to crawl (including http:// or https://)'),}),execute:async({ urlToCrawl })=>{const crawlResponse =await app.crawlUrl(urlToCrawl,{      limit:1,      scrapeOptions:{        formats:['markdown','html'],},});if(!crawlResponse.success){thrownewError(`Failed to crawl: ${crawlResponse.error}`);}return crawlResponse.data;},});constmain=async()=>{const{ text }=awaitgenerateText({    model:openai('gpt-4o-mini'),// can be any model that supports tools    prompt:'Get the latest blog post from vercel.com/blog',    tools:{      webSearch,},    maxSteps:2,});console.log(text);};main();
```
```

### 48. `cookbook/rsc/call-tools-in-parallel.md`

```markdown
# Call Tools in Parallel


---
url: https://ai-sdk.dev/cookbook/rsc/call-tools-in-parallel
description: Learn how to tools in parallel text using the AI SDK and React Server Components.
---


# [Call Tools in Parallel](#call-tools-in-parallel)


Some language models support calling tools in parallel. This is particularly useful when multiple tools are independent of each other and can be executed in parallel during the same generation step.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

What is the weather in Paris and New York?

Send Message


## [Client](#client)


Let's modify our previous example to call `getWeather` tool for multiple cities in parallel.

app/page.tsx

```
'use client';import{ useState }from'react';import{Message, continueConversation }from'./actions';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[conversation, setConversation]=useState<Message[]>([]);const[input, setInput]=useState<string>('');return(<div><div>{conversation.map((message, index)=>(<divkey={index}>{message.role}:{message.content}</div>))}</div><div><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttononClick={async()=>{const{ messages }=awaitcontinueConversation([...conversation,{ role:'user', content: input },]);setConversation(messages);}}>SendMessage</button></div></div>);}
```


## [Server](#server)


Let's update the tools object to now use the `getWeather` function instead.

app/actions.ts

```
'use server';import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';exportinterfaceMessage{  role:'user'|'assistant';  content:string;}functiongetWeather({ city, unit }){// This function would normally make an// API request to get the weather.return{ value:25, description:'Sunny'};}exportasyncfunctioncontinueConversation(history:Message[]){'use server';const{ text, toolResults }=awaitgenerateText({    model:openai('gpt-3.5-turbo'),    system:'You are a friendly weather assistant!',    messages: history,    tools:{      getWeather:{        description:'Get the weather for a location',        parameters: z.object({          city: z.string().describe('The city to get the weather for'),          unit: z.enum(['C','F']).describe('The unit to display the temperature in'),}),execute:async({ city, unit })=>{const weather =getWeather({ city, unit });return`It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`;},},},});return{    messages:[...history,{        role:'assistant'asconst,        content:          text | toolResults.map(toolResult => toolResult.result).join('\n'),},],};}
```
```

### 49. `cookbook/rsc/call-tools.md`

```markdown
# Call Tools


---
url: https://ai-sdk.dev/cookbook/rsc/call-tools
description: Learn how to call tools using the AI SDK and React Server Components.
---


# [Call Tools](#call-tools)


Some models allow developers to provide a list of tools that can be called at any time during a generation. This is useful for extending the capabilites of a language model to either use logic or data to interact with systems external to the model.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

What is 24 celsius in fahrenheit?

Send Message


## [Client](#client)


Let's create a simple conversation between a user and model and place a button that will call `continueConversation`.

app/page.tsx

```
'use client';import{ useState }from'react';import{Message, continueConversation }from'./actions';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[conversation, setConversation]=useState<Message[]>([]);const[input, setInput]=useState<string>('');return(<div><div>{conversation.map((message, index)=>(<divkey={index}>{message.role}:{message.content}</div>))}</div><div><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttononClick={async()=>{const{ messages }=awaitcontinueConversation([...conversation,{ role:'user', content: input },]);setConversation(messages);}}>SendMessage</button></div></div>);}
```


## [Server](#server)


Now, let's implement the `continueConversation` action that uses `generateText` to generate a response to the user's question. We will use the [`tools`](/docs/reference/ai-sdk-core/generate-text#tools) parameter to specify our own function called `celsiusToFahrenheit` that will convert a user given value in celsius to fahrenheit.

We will use zod to specify the schema for the `celsiusToFahrenheit` function's parameters.

app/actions.ts

```
'use server';import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';exportinterfaceMessage{  role:'user'|'assistant';  content:string;}exportasyncfunctioncontinueConversation(history: Message[]){'use server';const{ text, toolResults }=awaitgenerateText({    model:openai('gpt-3.5-turbo'),    system:'You are a friendly assistant!',    messages: history,    tools:{      celsiusToFahrenheit:{        description:'Converts celsius to fahrenheit',        parameters: z.object({          value: z.string().describe('The value in celsius'),}),execute:async({ value })=>{const celsius =parseFloat(value);const fahrenheit = celsius *(9/5)+32;return`${celsius}°C is ${fahrenheit.toFixed(2)}°F`;},},},});return{    messages:[...history,{        role:'assistant'asconst,        content:          text | toolResults.map(toolResult=> toolResult.result).join('\n'),},],};}
```
```

### 50. `cookbook/rsc/generate-object.md`

```markdown
# Generate Object


---
url: https://ai-sdk.dev/cookbook/rsc/generate-object
description: Learn how to generate object using the AI SDK and React Server Components.
---


# [Generate Object](#generate-object)


This example uses React Server Components (RSC). If you want to client side rendering and hooks instead, check out the ["generate object" example with useState](/examples/next-pages/basics/generating-object).

Earlier functions like `generateText` and `streamText` gave us the ability to generate unstructured text. However, if you want to generate structured data like JSON, you can provide a schema that describes the structure of your desired object to the `generateObject` function.

The function requires you to provide a schema using [zod](https://zod.dev), a library for defining schemas for JavaScript objects. By using zod, you can also use it to validate the generated object and ensure that it conforms to the specified structure.

http://localhost:3000

View Notifications


## [Client](#client)


Let's create a simple React component that will call the `getNotifications` function when a button is clicked. The function will generate a list of notifications as described in the schema.

app/page.tsx

```
'use client';import{ useState }from'react';import{ getNotifications }from'./actions';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[generation, setGeneration]=useState<string>('');return(<div><buttononClick={async()=>{const{ notifications }=awaitgetNotifications('Messages during finals week.',);setGeneration(JSON.stringify(notifications,null,2));}}>ViewNotifications</button><pre>{generation}</pre></div>);}
```


## [Server](#server)


Now let's implement the `getNotifications` function. We'll use the `generateObject` function to generate the list of notifications based on the schema we defined earlier.

app/actions.ts

```
'use server';import{ generateObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';exportasyncfunctiongetNotifications(input:string){'use server';const{ object: notifications }=awaitgenerateObject({    model:openai('gpt-4-turbo'),    system:'You generate three notifications for a messages app.',    prompt: input,    schema: z.object({      notifications: z.array(        z.object({          name: z.string().describe('Name of a fictional person.'),          message: z.string().describe('Do not use emojis or links.'),          minutesAgo: z.number(),}),),}),});return{ notifications };}
```
```

### 51. `cookbook/rsc/generate-text-with-chat-prompt.md`

```markdown
# Generate Text with Chat Prompt


---
url: https://ai-sdk.dev/cookbook/rsc/generate-text-with-chat-prompt
description: Learn how to generate text with chat prompt using the AI SDK and React Server Components.
---


# [Generate Text with Chat Prompt](#generate-text-with-chat-prompt)


Previously, we were able to generate text and objects using either a single message prompt, a system prompt, or a combination of both of them. However, there may be times when you want to generate text based on a series of messages.

A chat completion allows you to generate text based on a series of messages. This series of messages can be any series of interactions between any number of systems, but the most popular and relatable use case has been a series of messages that represent a conversation between a user and a model.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

Why is the sky blue?

Send Message


## [Client](#client)


Let's create a simple conversation between a user and a model, and place a button that will call `continueConversation`.

app/page.tsx

```
'use client';import{ useState }from'react';import{Message, continueConversation }from'./actions';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[conversation, setConversation]=useState<Message[]>([]);const[input, setInput]=useState<string>('');return(<div><div>{conversation.map((message, index)=>(<divkey={index}>{message.role}:{message.content}</div>))}</div><div><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttononClick={async()=>{const{ messages }=awaitcontinueConversation([...conversation,{ role:'user', content: input },]);setConversation(messages);}}>SendMessage</button></div></div>);}
```


## [Server](#server)


Now, let's implement the `continueConversation` function that will insert the user's message into the conversation and generate a response.

app/actions.ts

```
'use server';import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';exportinterfaceMessage{  role:'user'|'assistant';  content:string;}exportasyncfunctioncontinueConversation(history:Message[]){'use server';const{ text }=awaitgenerateText({    model:openai('gpt-3.5-turbo'),    system:'You are a friendly assistant!',    messages: history,});return{    messages:[...history,{        role:'assistant'asconst,        content: text,},],};}
```
```

### 52. `cookbook/rsc/generate-text.md`

```markdown
# Generate Text


---
url: https://ai-sdk.dev/cookbook/rsc/generate-text
description: Learn how to generate text using the AI SDK and React Server Components.
---


# [Generate Text](#generate-text)


This example uses React Server Components (RSC). If you want to client side rendering and hooks instead, check out the ["generate text" example with useState](/examples/next-pages/basics/generating-text).

A situation may arise when you need to generate text based on a prompt. For example, you may want to generate a response to a question or summarize a body of text. The `generateText` function can be used to generate text based on the input prompt.

http://localhost:3000

Answer


## [Client](#client)


Let's create a simple React component that will call the `getAnswer` function when a button is clicked. The `getAnswer` function will call the `generateText` function from the `ai` module, which will then generate text based on the input prompt.

app/page.tsx

```
'use client';import{ useState }from'react';import{ getAnswer }from'./actions';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[generation, setGeneration]=useState<string>('');return(<div><buttononClick={async()=>{const{ text }=awaitgetAnswer('Why is the sky blue?');setGeneration(text);}}>Answer</button><div>{generation}</div></div>);}
```


## [Server](#server)


On the server side, we need to implement the `getAnswer` function, which will call the `generateText` function from the `ai` module. The `generateText` function will generate text based on the input prompt.

app/actions.ts

```
'use server';import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctiongetAnswer(question:string){const{ text, finishReason, usage }=awaitgenerateText({    model:openai('gpt-3.5-turbo'),    prompt: question,});return{ text, finishReason, usage };}
```
```

### 53. `cookbook/rsc/render-visual-interface-in-chat.md`

```markdown
# Render Visual Interface in Chat


---
url: https://ai-sdk.dev/cookbook/rsc/render-visual-interface-in-chat
description: Learn how to generate text using the AI SDK and React Server Components.
---


# [Render Visual Interface in Chat](#render-visual-interface-in-chat)


We've now seen how a language model can call a function and render a component based on a conversation with the user.

When we define multiple functions in [`tools`](/docs/reference/ai-sdk-core/generate-text#tools), it is possible for the model to reason out the right functions to call based on whatever the user's intent is. This means that you can write a bunch of functions without the burden of implementing complex routing logic to run them.


## [Client](#client)


app/page.tsx

```
'use client';import{ useState }from'react';import{ClientMessage}from'./actions';import{ useActions, useUIState }from'ai/rsc';import{ generateId }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[input, setInput]=useState<string>('');const[conversation, setConversation]=useUIState();const{ continueConversation }=useActions();return(<div><div>{conversation.map((message: ClientMessage)=>(<divkey={message.id}>{message.role}:{message.display}</div>))}</div><div><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttononClick={async()=>{setConversation((currentConversation: ClientMessage[])=>[...currentConversation,{ id:generateId(), role:'user', display: input },]);const message =awaitcontinueConversation(input);setConversation((currentConversation: ClientMessage[])=>[...currentConversation,              message,]);}}>SendMessage</button></div></div>);}
```

components/stock.tsx

```
exportasyncfunctionStock({ symbol, numOfMonths }){const data =awaitfetch(`https://api.example.com/stock/${symbol}/${numOfMonths}`,);return(<div><div>{symbol}</div><div>{data.timeline.map(data=>(<div><div>{data.date}</div><div>{data.value}</div></div>))}</div></div>);}
```

components/flight.tsx

```
exportasyncfunctionFlight({ flightNumber }){const data =awaitfetch(`https://api.example.com/flight/${flightNumber}`);return(<div><div>{flightNumber}</div><div>{data.status}</div><div>{data.source}</div><div>{data.destination}</div></div>);}
```


## [Server](#server)


app/actions.tsx

```
'use server';import{ getMutableAIState, streamUI }from'ai/rsc';import{ openai }from'@ai-sdk/openai';import{ReactNode}from'react';import{ z }from'zod';import{ generateId }from'ai';import{Stock}from'@/components/stock';import{Flight}from'@/components/flight';exportinterfaceServerMessage{  role:'user'|'assistant';  content:string;}exportinterfaceClientMessage{  id:string;  role:'user'|'assistant';  display:ReactNode;}exportasyncfunctioncontinueConversation(input: string,):Promise<ClientMessage>{'use server';const history =getMutableAIState();const result =awaitstreamUI({    model:openai('gpt-3.5-turbo'),    messages:[...history.get(),{ role:'user', content: input }],text:({ content, done })=>{if(done){        history.done((messages: ServerMessage[])=>[...messages,{ role:'assistant', content },]);}return<div>{content}</div>;},    tools:{      showStockInformation:{        description:'Get stock information for symbol for the last numOfMonths months',        parameters: z.object({symbol: z.string().describe('The stock symbol to get information for'),          numOfMonths: z.number().describe('The number of months to get historical information for'),}),generate:async({ symbol, numOfMonths })=>{          history.done((messages: ServerMessage[])=>[...messages,{              role:'assistant',              content:`Showing stock information for ${symbol}`,},]);return<Stocksymbol={symbol}numOfMonths={numOfMonths}/>;},},      showFlightStatus:{        description:'Get the status of a flight',        parameters: z.object({          flightNumber: z.string().describe('The flight number to get status for'),}),generate:async({ flightNumber })=>{          history.done((messages: ServerMessage[])=>[...messages,{              role:'assistant',              content:`Showing flight status for ${flightNumber}`,},]);return<FlightflightNumber={flightNumber}/>;},},},});return{    id:generateId(),    role:'assistant',    display: result.value,};}
```

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ServerMessage,ClientMessage, continueConversation }from'./actions';exportconstAI=createAI<ServerMessage[],ClientMessage[]>({  actions:{    continueConversation,},  initialAIState:[],  initialUIState:[],});
```
```

### 54. `cookbook/rsc/restore-messages-from-database.md`

```markdown
# Restore Messages from Database


---
url: https://ai-sdk.dev/cookbook/rsc/restore-messages-from-database
description: Learn how to restore messages from an external database using the AI SDK and React Server Components
---


# [Restore Messages from Database](#restore-messages-from-database)


When building AI applications, you might want to restore previous conversations from a database to allow users to continue their conversations or review past interactions. The AI SDK provides mechanisms to restore conversation state through `initialAIState` and `onGetUIState`.


## [Client](#client)


app/layout.tsx

```
import{ServerMessage}from'./actions';import{AI}from'./ai';exportdefaultfunctionRootLayout({  children,}: Readonly<{  children: React.ReactNode;}>){// Fetch stored messages from your databaseconst savedMessages:ServerMessage[]=getSavedMessages();return(<htmllang="en"><body><AIinitialAIState={savedMessages}initialUIState={[]}>{children}</AI></body></html>);}
```

app/page.tsx

```
'use client';import{ useState, useEffect }from'react';import{ClientMessage}from'./actions';import{ useActions, useUIState }from'ai/rsc';import{ generateId }from'ai';exportdefaultfunctionHome(){const[conversation, setConversation]=useUIState();const[input, setInput]=useState<string>('');const{ continueConversation }=useActions();return(<div><divclassName="conversation-history">{conversation.map((message: ClientMessage)=>(<divkey={message.id}className={`message ${message.role}`}>{message.role}:{message.display}</div>))}</div><divclassName="input-area"><inputtype="text"value={input}onChange={e=>setInput(e.target.value)}placeholder="Type your message..."/><buttononClick={async()=>{// Add user message to UIsetConversation((currentConversation: ClientMessage[])=>[...currentConversation,{ id:generateId(), role:'user', display: input },]);// Get AI responseconst message =awaitcontinueConversation(input);// Add AI response to UIsetConversation((currentConversation: ClientMessage[])=>[...currentConversation,              message,]);setInput('');}}>Send</button></div></div>);}
```


## [Server](#server)


The server-side implementation handles the restoration of messages and their transformation into the appropriate format for display.

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ServerMessage,ClientMessage, continueConversation }from'./actions';import{Stock}from'@ai-studio/components/stock';import{ generateId }from'ai';exportconstAI=createAI<ServerMessage[],ClientMessage[]>({  actions:{    continueConversation,},onGetUIState:async()=>{'use server';// Get the current AI state (stored messages)const history:ServerMessage[]=getAIState();// Transform server messages into client messagesreturn history.map(({ role, content })=>({      id:generateId(),      role,      display:        role ==='function'?<Stock{...JSON.parse(content)}/>: content,}));},});
```

app/actions.tsx

```
'use server';import{ getAIState }from'ai/rsc';exportinterfaceServerMessage{  role:'user'|'assistant'|'function';  content:string;}exportinterfaceClientMessage{  id:string;  role:'user'|'assistant'|'function';  display:ReactNode;}// Function to get saved messages from databaseexportasyncfunctiongetSavedMessages():Promise<ServerMessage[]>{'use server';// Implement your database fetching logic herereturnawaitfetchMessagesFromDatabase();}
```
```

### 55. `cookbook/rsc/save-messages-to-database.md`

```markdown
# Save Messages To Database


---
url: https://ai-sdk.dev/cookbook/rsc/save-messages-to-database
description: Learn how to save messages to an external database using the AI SDK and React Server Components
---


# [Save Messages To Database](#save-messages-to-database)


Sometimes conversations with language models can get interesting and you might want to save the state of so you can revisit it or continue the conversation later.

`createAI` has an experimental callback function called `onSetAIState` that gets called whenever the AI state changes. You can use this to save the AI state to a file or a database.


## [Client](#client)


app/layout.tsx

```
import{ServerMessage}from'./actions';import{AI}from'./ai';exportdefaultfunctionRootLayout({  children,}: Readonly<{  children: React.ReactNode;}>){// get chat history from databaseconst history:ServerMessage[]=getChat();return(<htmllang="en"><body><AIinitialAIState={history}initialUIState={[]}>{children}</AI></body></html>);}
```

app/page.tsx

```
'use client';import{ useState }from'react';import{ClientMessage}from'./actions';import{ useActions, useUIState }from'ai/rsc';import{ generateId }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[input, setInput]=useState<string>('');const[conversation, setConversation]=useUIState();const{ continueConversation }=useActions();return(<div><div>{conversation.map((message: ClientMessage)=>(<divkey={message.id}>{message.role}:{message.display}</div>))}</div><div><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttononClick={async()=>{setConversation((currentConversation: ClientMessage[])=>[...currentConversation,{ id:generateId(), role:'user', display: input },]);const message =awaitcontinueConversation(input);setConversation((currentConversation: ClientMessage[])=>[...currentConversation,              message,]);}}>SendMessage</button></div></div>);}
```


## [Server](#server)


We will use the callback function to listen to state changes and save the conversation once we receive a `done` event.

app/actions.tsx

```
'use server';import{ getAIState, getMutableAIState, streamUI }from'ai/rsc';import{ openai }from'@ai-sdk/openai';import{ReactNode}from'react';import{ z }from'zod';import{ generateId }from'ai';import{Stock}from'@ai-studio/components/stock';exportinterfaceServerMessage{  role:'user'|'assistant'|'function';  content:string;}exportinterfaceClientMessage{  id:string;  role:'user'|'assistant'|'function';  display:ReactNode;}exportasyncfunctioncontinueConversation(input: string,):Promise<ClientMessage>{'use server';const history =getMutableAIState();const result =awaitstreamUI({    model:openai('gpt-3.5-turbo'),    messages:[...history.get(),{ role:'user', content: input }],text:({ content, done })=>{if(done){        history.done([...history.get(),{ role:'user', content: input },{ role:'assistant', content },]);}return<div>{content}</div>;},    tools:{      showStockInformation:{        description:'Get stock information for symbol for the last numOfMonths months',        parameters: z.object({symbol: z.string().describe('The stock symbol to get information for'),          numOfMonths: z.number().describe('The number of months to get historical information for'),}),generate:async({ symbol, numOfMonths })=>{          history.done([...history.get(),{              role:'function',              name:'showStockInformation',              content:JSON.stringify({symbol, numOfMonths }),},]);return<Stocksymbol={symbol}numOfMonths={numOfMonths}/>;},},},});return{    id:generateId(),    role:'assistant',    display: result.value,};}
```

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ServerMessage,ClientMessage, continueConversation }from'./actions';exportconstAI=createAI<ServerMessage[],ClientMessage[]>({  actions:{    continueConversation,},onSetAIState:async({ state, done })=>{'use server';if(done){saveChat(state);}},onGetUIState:async()=>{'use server';const history:ServerMessage[]=getAIState();return history.map(({ role, content })=>({      id:generateId(),      role,      display:        role ==='function'?<Stock{...JSON.parse(content)}/>: content,}));},});
```
```

### 56. `cookbook/rsc/stream-assistant-response-with-tools.md`

```markdown
# Stream Assistant Responses


---
url: https://ai-sdk.dev/cookbook/rsc/stream-assistant-response-with-tools
description: Learn how to generate text using the AI SDK and React Server Components.
---


# [Stream Assistant Responses](#stream-assistant-responses)


In this example, you'll learn how to stream responses along with tool calls from OpenAI's [Assistant API](https://platform.openai.com/docs/assistants/overview) using `ai/rsc`.


## [Client](#client)


In your client component, you will create a simple chat interface that allows users to send messages to the assistant and receive responses. The assistant's responses will be streamed in two parts: the status of the current run and the text content of the messages.

app/page.tsx

```
'use client';import{ useState }from'react';import{ClientMessage, submitMessage }from'./actions';import{ useActions }from'ai/rsc';exportdefaultfunctionHome(){const[input, setInput]=useState('');const[messages, setMessages]=useState<ClientMessage[]>([]);const{ submitMessage }=useActions();consthandleSubmission=async()=>{setMessages(currentMessages=>[...currentMessages,{        id:'123',        status:'user.message.created',        text: input,        gui:null,},]);const response =awaitsubmitMessage(input);setMessages(currentMessages=>[...currentMessages, response]);setInput('');};return(<divclassName="flex flex-col-reverse"><divclassName="flex flex-row gap-2 p-2 bg-zinc-100 w-full"><inputclassName="bg-zinc-100 w-full p-2 outline-none"value={input}onChange={event=>setInput(event.target.value)}placeholder="Ask a question"onKeyDown={event=>{if(event.key ==='Enter'){handleSubmission();}}}/><buttonclassName="p-2 bg-zinc-900 text-zinc-100 rounded-md"onClick={handleSubmission}>Send</button></div><divclassName="flex flex-col h-[calc(100dvh-56px)] overflow-y-scroll"><div>{messages.map(message=>(<divkey={message.id}className="flex flex-col gap-1 border-b p-2"><divclassName="flex flex-row justify-between"><divclassName="text-sm text-zinc-500">{message.status}</div></div><divclassName="flex flex-col gap-2">{message.gui}</div><div>{message.text}</div></div>))}</div></div></div>);}
```

app/message.tsx

```
'use client';import{StreamableValue, useStreamableValue }from'ai/rsc';exportfunctionMessage({ textStream }:{ textStream: StreamableValue }){const[text]=useStreamableValue(textStream);return<div>{text}</div>;}
```


## [Server](#server)


In your server action, you will create a function called `submitMessage` that adds the user's message to the thread. The function will create a new thread if one does not exist and add the user's message to the thread. If a thread already exists, the function will add the user's message to the existing thread. The function will then create a run and stream the assistant's response to the client. Furthermore, the run queue is used to manage multiple runs in the same thread during the lifetime of the server action.

In case the assistant requires a tool call, the server action will handle the tool call and return the output to the assistant. In this example, the assistant requires a tool call to search for emails. The server action will search for emails based on the `query` and `has_attachments` parameters and return the output to the both the assistant and the client.

app/actions.tsx

```
'use server';import{ generateId }from'ai';import{ createStreamableUI, createStreamableValue }from'ai/rsc';import{OpenAI}from'openai';import{ReactNode}from'react';import{ searchEmails }from'./function';import{Message}from'./message';const openai =newOpenAI({  apiKey: process.env.OPENAI_API_KEY,});exportinterfaceClientMessage{  id:string;  status:ReactNode;  text:ReactNode;  gui:ReactNode;}constASSISTANT_ID='asst_xxxx';letTHREAD_ID='';letRUN_ID='';exportasyncfunctionsubmitMessage(question: string):Promise<ClientMessage>{const status =createStreamableUI('thread.init');const textStream =createStreamableValue('');const textUIStream =createStreamableUI(<MessagetextStream={textStream.value}/>,);const gui =createStreamableUI();const runQueue =[];(async()=>{if(THREAD_ID){await openai.beta.threads.messages.create(THREAD_ID,{        role:'user',        content: question,});const run =await openai.beta.threads.runs.create(THREAD_ID,{        assistant_id:ASSISTANT_ID,        stream:true,});      runQueue.push({ id:generateId(), run });}else{const run =await openai.beta.threads.createAndRun({        assistant_id:ASSISTANT_ID,        stream:true,        thread:{          messages:[{ role:'user', content: question }],},});      runQueue.push({ id:generateId(), run });}while(runQueue.length >0){const latestRun = runQueue.shift();if(latestRun){forawait(const delta of latestRun.run){const{ data, event }= delta;          status.update(event);if(event ==='thread.created'){THREAD_ID= data.id;}elseif(event ==='thread.run.created'){RUN_ID= data.id;}elseif(event ==='thread.message.delta'){            data.delta.content?.map((part: any)=>{if(part.type==='text'){if(part.text){                  textStream.append(part.text.value);}}});}elseif(event ==='thread.run.requires_action'){if(data.required_action){if(data.required_action.type==='submit_tool_outputs'){const{ tool_calls }= data.required_action.submit_tool_outputs;const tool_outputs =[];for(const tool_call of tool_calls){const{ id: toolCallId,function: fn }= tool_call;const{ name, arguments: args }= fn;if(name ==='search_emails'){const{ query, has_attachments }=JSON.parse(args);                    gui.append(<divclassName="flex flex-row gap-2 items-center"><div>Searchingfor emails:{query}, has_attachments:{has_attachments ?'true':'false'}</div></div>,);awaitnewPromise(resolve=>setTimeout(resolve,2000));const fakeEmails =searchEmails({ query, has_attachments });                    gui.append(<divclassName="flex flex-col gap-2">{fakeEmails.map(email=>(<divkey={email.id}className="p-2 bg-zinc-100 rounded-md flex flex-row gap-2 items-center justify-between"><divclassName="flex flex-row gap-2 items-center"><div>{email.subject}</div></div><divclassName="text-zinc-500">{email.date}</div></div>))}</div>,);                    tool_outputs.push({                      tool_call_id: toolCallId,                      output:JSON.stringify(fakeEmails),});}}const nextRun:any=await openai.beta.threads.runs.submitToolOutputs(THREAD_ID,RUN_ID,{                      tool_outputs,                      stream:true,},);                runQueue.push({ id:generateId(), run: nextRun });}}}elseif(event ==='thread.run.failed'){console.log(data);}}}}    status.done();    textUIStream.done();    gui.done();})();return{    id:generateId(),    status: status.value,    text: textUIStream.value,    gui: gui.value,};}
```

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ submitMessage }from'./actions';exportconstAI=createAI({  actions:{    submitMessage,},  initialAIState:[],  initialUIState:[],});
```

And finally, make sure to update your layout component to wrap the children with the `AI` component.

app/layout.tsx

```
import{ReactNode}from'react';import{AI}from'./ai';exportdefaultfunctionLayout({ children }:{ children: ReactNode }){return<AI>{children}</AI>;}
```
```

### 57. `cookbook/rsc/stream-assistant-response.md`

```markdown
# Stream Assistant Responses


---
url: https://ai-sdk.dev/cookbook/rsc/stream-assistant-response
description: Learn how to generate text using the AI SDK and React Server Components.
---


# [Stream Assistant Responses](#stream-assistant-responses)


In this example, you'll learn how to stream responses from OpenAI's [Assistant API](https://platform.openai.com/docs/assistants/overview) using `ai/rsc`.


## [Client](#client)


In your client component, you will create a simple chat interface that allows users to send messages to the assistant and receive responses. The assistant's responses will be streamed in two parts: the status of the current run and the text content of the messages.

app/page.tsx

```
'use client';import{ useState }from'react';import{ClientMessage}from'./actions';import{ useActions }from'ai/rsc';exportdefaultfunctionHome(){const[input, setInput]=useState('');const[messages, setMessages]=useState<ClientMessage[]>([]);const{ submitMessage }=useActions();consthandleSubmission=async()=>{setMessages(currentMessages=>[...currentMessages,{        id:'123',        status:'user.message.created',        text: input,        gui:null,},]);const response =awaitsubmitMessage(input);setMessages(currentMessages=>[...currentMessages, response]);setInput('');};return(<divclassName="flex flex-col-reverse"><divclassName="flex flex-row gap-2 p-2 bg-zinc-100 w-full"><inputclassName="bg-zinc-100 w-full p-2 outline-none"value={input}onChange={event=>setInput(event.target.value)}placeholder="Ask a question"onKeyDown={event=>{if(event.key ==='Enter'){handleSubmission();}}}/><buttonclassName="p-2 bg-zinc-900 text-zinc-100 rounded-md"onClick={handleSubmission}>Send</button></div><divclassName="flex flex-col h-[calc(100dvh-56px)] overflow-y-scroll"><div>{messages.map(message=>(<divkey={message.id}className="flex flex-col gap-1 border-b p-2"><divclassName="flex flex-row justify-between"><divclassName="text-sm text-zinc-500">{message.status}</div></div><div>{message.text}</div></div>))}</div></div></div>);}
```

app/message.tsx

```
'use client';import{StreamableValue, useStreamableValue }from'ai/rsc';exportfunctionMessage({ textStream }:{ textStream: StreamableValue }){const[text]=useStreamableValue(textStream);return<div>{text}</div>;}
```


## [Server](#server)


In your server action, you will create a function called `submitMessage` that adds the user's message to the thread. The function will create a new thread if one does not exist and add the user's message to the thread. If a thread already exists, the function will add the user's message to the existing thread. The function will then create a run and stream the assistant's response to the client. Furthermore, the run queue is used to manage multiple runs in the same thread during the lifetime of the server action.

app/actions.tsx

```
'use server';import{ generateId }from'ai';import{ createStreamableUI, createStreamableValue }from'ai/rsc';import{OpenAI}from'openai';import{ReactNode}from'react';import{Message}from'./message';const openai =newOpenAI({  apiKey: process.env.OPENAI_API_KEY,});exportinterfaceClientMessage{  id:string;  status:ReactNode;  text:ReactNode;}constASSISTANT_ID='asst_xxxx';letTHREAD_ID='';letRUN_ID='';exportasyncfunctionsubmitMessage(question: string):Promise<ClientMessage>{const statusUIStream =createStreamableUI('thread.init');const textStream =createStreamableValue('');const textUIStream =createStreamableUI(<MessagetextStream={textStream.value}/>,);const runQueue =[];(async()=>{if(THREAD_ID){await openai.beta.threads.messages.create(THREAD_ID,{        role:'user',        content: question,});const run =await openai.beta.threads.runs.create(THREAD_ID,{        assistant_id:ASSISTANT_ID,        stream:true,});      runQueue.push({ id:generateId(), run });}else{const run =await openai.beta.threads.createAndRun({        assistant_id:ASSISTANT_ID,        stream:true,        thread:{          messages:[{ role:'user', content: question }],},});      runQueue.push({ id:generateId(), run });}while(runQueue.length >0){const latestRun = runQueue.shift();if(latestRun){forawait(const delta of latestRun.run){const{ data, event }= delta;          statusUIStream.update(event);if(event ==='thread.created'){THREAD_ID= data.id;}elseif(event ==='thread.run.created'){RUN_ID= data.id;}elseif(event ==='thread.message.delta'){            data.delta.content?.map(part=>{if(part.type==='text'){if(part.text){                  textStream.append(part.text.value asstring);}}});}elseif(event ==='thread.run.failed'){console.error(data);}}}}    statusUIStream.done();    textStream.done();})();return{    id:generateId(),    status: statusUIStream.value,    text: textUIStream.value,};}
```

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ submitMessage }from'./actions';exportconstAI=createAI({  actions:{    submitMessage,},  initialAIState:[],  initialUIState:[],});
```

And finally, make sure to update your layout component to wrap the children with the `AI` component.

app/layout.tsx

```
import{ReactNode}from'react';import{AI}from'./ai';exportdefaultfunctionLayout({ children }:{ children: ReactNode }){return<AI>{children}</AI>;}
```
```

### 58. `cookbook/rsc/stream-object.md`

```markdown
# Stream Object


---
url: https://ai-sdk.dev/cookbook/rsc/stream-object
description: Learn how to stream object using the AI SDK and React Server Components.
---


# [Stream Object](#stream-object)


This example uses React Server Components (RSC). If you want to client side rendering and hooks instead, check out the ["streaming object generation" example with useObject](/examples/next-pages/basics/streaming-object-generation).

Object generation can sometimes take a long time to complete, especially when you're generating a large schema. In such cases, it is useful to stream the object generation process to the client in real-time. This allows the client to display the generated object as it is being generated, rather than have users wait for it to complete before displaying the result.

http://localhost:3000

View Notifications


## [Client](#client)


Let's create a simple React component that will call the `getNotifications` function when a button is clicked. The function will generate a list of notifications as described in the schema.

app/page.tsx

```
'use client';import{ useState }from'react';import{ generate }from'./actions';import{ readStreamableValue }from'ai/rsc';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[generation, setGeneration]=useState<string>('');return(<div><button        onClick={async()=>{const{ object }=awaitgenerate('Messages during finals week.');forawait(const partialObject ofreadStreamableValue(object)){if(partialObject){setGeneration(JSON.stringify(partialObject.notifications,null,2),);}}}}>Ask</button><pre>{generation}</pre></div>);}
```


## [Server](#server)


Now let's implement the `getNotifications` function. We'll use the `generateObject` function to generate the list of fictional notifications based on the schema we defined earlier.

app/actions.ts

```
'use server';import{ streamObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ createStreamableValue }from'ai/rsc';import{ z }from'zod';exportasyncfunctiongenerate(input:string){'use server';const stream =createStreamableValue();(async()=>{const{ partialObjectStream }=streamObject({      model:openai('gpt-4-turbo'),      system:'You generate three notifications for a messages app.',      prompt: input,      schema: z.object({        notifications: z.array(          z.object({            name: z.string().describe('Name of a fictional person.'),            message: z.string().describe('Do not use emojis or links.'),            minutesAgo: z.number(),}),),}),});forawait(const partialObject of partialObjectStream){      stream.update(partialObject);}    stream.done();})();return{ object: stream.value};}
```
```

### 59. `cookbook/rsc/stream-text-with-chat-prompt.md`

```markdown
# Stream Text with Chat Prompt


---
url: https://ai-sdk.dev/cookbook/rsc/stream-text-with-chat-prompt
description: Learn how to stream text with chat prompt using the AI SDK and React Server Components.
---


# [Stream Text with Chat Prompt](#stream-text-with-chat-prompt)


Chat completion can sometimes take a long time to finish, especially when the response is big. In such cases, it is useful to stream the chat completion to the client in real-time. This allows the client to display the new message as it is being generated by the model, rather than have users wait for it to finish.

http://localhost:3000

User: How is it going?

Assistant: All good, how may I help you?

Why is the sky blue?

Send Message


## [Client](#client)


Let's create a simple conversation between a user and a model, and place a button that will call `continueConversation`.

app/page.tsx

```
'use client';import{ useState }from'react';import{Message, continueConversation }from'./actions';import{ readStreamableValue }from'ai/rsc';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[conversation, setConversation]=useState<Message[]>([]);const[input, setInput]=useState<string>('');return(<div><div>{conversation.map((message, index)=>(<divkey={index}>{message.role}:{message.content}</div>))}</div><div><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><button          onClick={async()=>{const{ messages, newMessage }=awaitcontinueConversation([...conversation,{ role:'user', content: input },]);let textContent ='';forawait(const delta ofreadStreamableValue(newMessage)){              textContent =`${textContent}${delta}`;setConversation([...messages,{ role:'assistant', content: textContent },]);}}}>SendMessage</button></div></div>);}
```


## [Server](#server)


Now, let's implement the `continueConversation` function that will insert the user's message into the conversation and stream back the new message.

app/actions.ts

```
'use server';import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';import{ createStreamableValue }from'ai/rsc';exportinterfaceMessage{  role:'user'|'assistant';  content:string;}exportasyncfunctioncontinueConversation(history:Message[]){'use server';const stream =createStreamableValue();(async()=>{const{ textStream }=streamText({      model:openai('gpt-3.5-turbo'),      system:"You are a dude that doesn't drop character until the DVD commentary.",      messages: history,});forawait(const text of textStream){      stream.update(text);}    stream.done();})();return{    messages: history,    newMessage: stream.value,};}
```
```

### 60. `cookbook/rsc/stream-text.md`

```markdown
# Stream Text


---
url: https://ai-sdk.dev/cookbook/rsc/stream-text
description: Learn how to stream text using the AI SDK and React Server Components.
---


# [Stream Text](#stream-text)


This example uses React Server Components (RSC). If you want to client side rendering and hooks instead, check out the ["stream text" example with useCompletion](/examples/next-pages/basics/streaming-text-generation).

Text generation can sometimes take a long time to complete, especially when you're generating a couple of paragraphs. In such cases, it is useful to stream the text generation process to the client in real-time. This allows the client to display the generated text as it is being generated, rather than have users wait for it to complete before displaying the result.

http://localhost:3000

Answer


## [Client](#client)


Let's create a simple React component that will call the `generate` function when a button is clicked. The `generate` function will call the `streamText` function, which will then generate text based on the input prompt. To consume the stream of text in the client, we will use the `readStreamableValue` function from the `ai/rsc` module.

app/page.tsx

```
'use client';import{ useState }from'react';import{ generate }from'./actions';import{ readStreamableValue }from'ai/rsc';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[generation, setGeneration]=useState<string>('');return(<div><button        onClick={async()=>{const{ output }=awaitgenerate('Why is the sky blue?');forawait(const delta ofreadStreamableValue(output)){setGeneration(currentGeneration=>`${currentGeneration}${delta}`);}}}>Ask</button><div>{generation}</div></div>);}
```


## [Server](#server)


On the server side, we need to implement the `generate` function, which will call the `streamText` function. The `streamText` function will generate text based on the input prompt. In order to stream the text generation to the client, we will use `createStreamableValue` that can wrap any changeable value and stream it to the client.

Using DevTools, we can see the text generation being streamed to the client in real-time.

app/actions.ts

```
'use server';import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';import{ createStreamableValue }from'ai/rsc';exportasyncfunctiongenerate(input:string){const stream =createStreamableValue('');(async()=>{const{ textStream }=streamText({      model:openai('gpt-3.5-turbo'),      prompt: input,});forawait(const delta of textStream){      stream.update(delta);}    stream.done();})();return{ output: stream.value};}
```
```

### 61. `cookbook/rsc/stream-ui-record-token-usage.md`

```markdown
# Record Token Usage after Streaming User Interfaces


---
url: https://ai-sdk.dev/cookbook/rsc/stream-ui-record-token-usage
description: Learn how to record token usage after streaming user interfaces using the AI SDK and React Server Components
---


# [Record Token Usage after Streaming User Interfaces](#record-token-usage-after-streaming-user-interfaces)


When you're streaming structured data with [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui), you may want to record the token usage for billing purposes.


## [`onFinish` Callback](#onfinish-callback)


You can use the `onFinish` callback to record token usage. It is called when the stream is finished.

app/page.tsx

```
'use client';import{ useState }from'react';import{ClientMessage}from'./actions';import{ useActions, useUIState }from'ai/rsc';import{ generateId }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[input, setInput]=useState<string>('');const[conversation, setConversation]=useUIState();const{ continueConversation }=useActions();return(<div><div>{conversation.map((message: ClientMessage)=>(<divkey={message.id}>{message.role}:{message.display}</div>))}</div><div><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttononClick={async()=>{setConversation((currentConversation: ClientMessage[])=>[...currentConversation,{ id:generateId(), role:'user', display: input },]);const message =awaitcontinueConversation(input);setConversation((currentConversation: ClientMessage[])=>[...currentConversation,              message,]);}}>SendMessage</button></div></div>);}
```


## [Server](#server)


app/actions.tsx

```
'use server';import{ createAI, getMutableAIState, streamUI }from'ai/rsc';import{ openai }from'@ai-sdk/openai';import{ReactNode}from'react';import{ z }from'zod';import{ generateId }from'ai';exportinterfaceServerMessage{  role:'user'|'assistant';  content:string;}exportinterfaceClientMessage{  id:string;  role:'user'|'assistant';  display:ReactNode;}exportasyncfunctioncontinueConversation(input: string,):Promise<ClientMessage>{'use server';const history =getMutableAIState();const result =awaitstreamUI({    model:openai('gpt-3.5-turbo'),    messages:[...history.get(),{ role:'user', content: input }],text:({ content, done })=>{if(done){        history.done((messages: ServerMessage[])=>[...messages,{ role:'assistant', content },]);}return<div>{content}</div>;},    tools:{      deploy:{        description:'Deploy repository to vercel',        parameters: z.object({          repositoryName: z.string().describe('The name of the repository, example: vercel/ai-chatbot'),}),generate:asyncfunction*({ repositoryName }){yield<div>Cloning repository {repositoryName}...</div>;// [!code highlight:5]awaitnewPromise(resolve=>setTimeout(resolve,3000));yield<div>Building repository {repositoryName}...</div>;awaitnewPromise(resolve=>setTimeout(resolve,2000));return<div>{repositoryName} deployed!</div>;},},},onFinish:({ usage })=>{const{ promptTokens, completionTokens, totalTokens }= usage;// your own logic, e.g. for saving the chat history or recording usageconsole.log('Prompt tokens:', promptTokens);console.log('Completion tokens:', completionTokens);console.log('Total tokens:', totalTokens);},});return{    id:generateId(),    role:'assistant',    display: result.value,};}
```

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ServerMessage,ClientMessage, continueConversation }from'./actions';exportconstAI=createAI<ServerMessage[],ClientMessage[]>({  actions:{    continueConversation,},  initialAIState:[],  initialUIState:[],});
```
```

### 62. `cookbook/rsc/stream-updates-to-visual-interfaces.md`

```markdown
# Stream Updates to Visual Interfaces


---
url: https://ai-sdk.dev/cookbook/rsc/stream-updates-to-visual-interfaces
description: Learn how to generate text using the AI SDK and React Server Components.
---


# [Stream Updates to Visual Interfaces](#stream-updates-to-visual-interfaces)


In our previous example we've been streaming react components from the server to the client. By streaming the components, we open up the possibility to update these components based on state changes that occur in the server.


## [Client](#client)


app/page.tsx

```
'use client';import{ useState }from'react';import{ClientMessage}from'./actions';import{ useActions, useUIState }from'ai/rsc';import{ generateId }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[input, setInput]=useState<string>('');const[conversation, setConversation]=useUIState();const{ continueConversation }=useActions();return(<div><div>{conversation.map((message: ClientMessage)=>(<divkey={message.id}>{message.role}:{message.display}</div>))}</div><div><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttononClick={async()=>{setConversation((currentConversation: ClientMessage[])=>[...currentConversation,{ id:generateId(), role:'user', display: input },]);const message =awaitcontinueConversation(input);setConversation((currentConversation: ClientMessage[])=>[...currentConversation,              message,]);}}>SendMessage</button></div></div>);}
```


## [Server](#server)


app/actions.tsx

```
'use server';import{ getMutableAIState, streamUI }from'ai/rsc';import{ openai }from'@ai-sdk/openai';import{ReactNode}from'react';import{ z }from'zod';import{ generateId }from'ai';exportinterfaceServerMessage{  role:'user'|'assistant';  content:string;}exportinterfaceClientMessage{  id:string;  role:'user'|'assistant';  display:ReactNode;}exportasyncfunctioncontinueConversation(input: string,):Promise<ClientMessage>{'use server';const history =getMutableAIState();const result =awaitstreamUI({    model:openai('gpt-3.5-turbo'),    messages:[...history.get(),{ role:'user', content: input }],text:({ content, done })=>{if(done){        history.done((messages: ServerMessage[])=>[...messages,{ role:'assistant', content },]);}return<div>{content}</div>;},    tools:{      deploy:{        description:'Deploy repository to vercel',        parameters: z.object({          repositoryName: z.string().describe('The name of the repository, example: vercel/ai-chatbot'),}),generate:asyncfunction*({ repositoryName }){yield<div>Cloning repository {repositoryName}...</div>;// [!code highlight:5]awaitnewPromise(resolve=>setTimeout(resolve,3000));yield<div>Building repository {repositoryName}...</div>;awaitnewPromise(resolve=>setTimeout(resolve,2000));return<div>{repositoryName} deployed!</div>;},},},});return{    id:generateId(),    role:'assistant',    display: result.value,};}
```

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ServerMessage,ClientMessage, continueConversation }from'./actions';exportconstAI=createAI<ServerMessage[],ClientMessage[]>({  actions:{    continueConversation,},  initialAIState:[],  initialUIState:[],});
```
```

### 63. `cookbook.md`

```markdown
# Cookbook


---
url: https://ai-sdk.dev/cookbook
description: Open-source collection of examples, guides, and templates for building with the AI SDK.
---

[

AI SDK

](/)

Announcing AI SDK 5 Alpha!

[Learn more](https://ai-sdk.dev/docs/announcing-ai-sdk-5-alpha)


# Cookbook


An open-source collection of recipes, guides, and templates for building with the AI SDK.


# Guides


[

Build a Natural Language Postgres Editor

](/docs/guides/natural-language-postgres)[

Get Started with Computer Use

](/docs/guides/computer-use)[

Get Started with OpenAI o1

](/docs/guides/o1)[

Get Started with Llama 3.1

](/docs/guides/llama-3_1)

Show More


# Recipes


Next.js

Next.jsNode.jsReact Server ComponentsAPI Servers

streaming

13

tool use

5

chat

4

assistant

2

multimodal

2

structured data

2

image generation

1

tools

1

caching

1

middleware

1

pdf

1

chatbot

1

markdown

1

multi-modal

1

agent

1

mcp

1

agents

1

generative user interface

1

[

Generate Text

](cookbook/next/generate-text)[

Generate Text with Chat Prompt

streamingchat

](cookbook/next/generate-text-with-chat-prompt)[

Generate Image with Chat Prompt

streamingchat

](cookbook/next/generate-image-with-chat-prompt)[

Stream Text

streaming

](cookbook/next/stream-text)[

Stream Text with Chat Prompt

streamingchat

](cookbook/next/stream-text-with-chat-prompt)[

Stream Text with Image Prompt

streamingmultimodal

](cookbook/next/stream-text-with-image-prompt)[

Chat with PDFs

pdfmultimodal

](cookbook/next/chat-with-pdf)[

streamText Multi-Step Cookbook

streaming

](cookbook/next/stream-text-multistep)[

Markdown Chatbot with Memoization

streamingchatbot

](cookbook/next/markdown-chatbot-with-memoization)[

Generate Object

structured data

](cookbook/next/generate-object)[

Generate Object with File Prompt through Form Submission

multi-modal

](cookbook/next/generate-object-with-file-prompt)[

Stream Object

streamingstructured data

](cookbook/next/stream-object)[

Call Tools

tool use

](cookbook/next/call-tools)[

Call Tools in Parallel

streamingtool use

](cookbook/next/call-tools-in-parallel)[

Call Tools in Multiple Steps

streamingtool use

](cookbook/next/call-tools-multiple-steps)[

Model Context Protocol (MCP) Tools

tool useagent

](cookbook/next/mcp-tools)[

Human-in-the-Loop with Next.js

agentstool use

](cookbook/next/human-in-the-loop)[

Send Custom Body from useChat

chat

](cookbook/next/send-custom-body-from-use-chat)[

Render Visual Interface in Chat

generative user interface

](cookbook/next/render-visual-interface-in-chat)[

Stream Assistant Response

streamingassistant

](cookbook/next/stream-assistant-response)[

Stream Assistant Response with Tools

streamingassistant

](cookbook/next/stream-assistant-response-with-tools)[

Caching Middleware

streamingcaching

](cookbook/next/caching-middleware)


# Templates


We've built some [templates](https://vercel.com/templates?type=ai) that include AI SDK integrations for different use cases, providers, and frameworks. You can use these templates to get started with your AI-powered application.


### Starter Kits


[

Chatbot Starter Template

Uses the AI SDK and Next.js. Features persistence, multi-modal chat, and more.

](https://vercel.com/templates/next.js/nextjs-ai-chatbot)[

Internal Knowledge Base (RAG)

Uses AI SDK Language Model Middleware for RAG and enforcing guardrails.

](https://vercel.com/templates/next.js/ai-sdk-internal-knowledge-base)[

Multi-Modal Chat

Uses Next.js and AI SDK useChat hook for multi-modal message chat interface.

](https://vercel.com/templates/next.js/multi-modal-chatbot)[

Semantic Image Search

An AI semantic image search app template built with Next.js, AI SDK, and Postgres.

](https://vercel.com/templates/next.js/semantic-image-search)[

Natural Language PostgreSQL

Query PostgreSQL using natural language with AI SDK and GPT-4o.

](https://vercel.com/templates/next.js/natural-language-postgres)


### Feature Exploration


[

Feature Flags Example

AI SDK with Next.js, Feature Flags, and Edge Config for dynamic model switching.

](https://vercel.com/templates/next.js/ai-sdk-feature-flags-edge-config)[

Chatbot with Telemetry

AI SDK chatbot with OpenTelemetry support.

](https://vercel.com/templates/next.js/ai-chatbot-telemetry)[

Structured Object Streaming

Uses AI SDK useObject hook to stream structured object generation.

](https://vercel.com/templates/next.js/use-object)[

Multi-Step Tools

Uses AI SDK streamText function to handle multiple tool steps automatically.

](https://vercel.com/templates/next.js/ai-sdk-roundtrips)


### Frameworks


[

Next.js OpenAI Starter

Uses OpenAI GPT-4, AI SDK, and Next.js.

](https://github.com/vercel/ai/tree/main/examples/next-openai)[

Nuxt OpenAI Starter

Uses OpenAI GPT-4, AI SDK, and Nuxt.js.

](https://github.com/vercel/ai/tree/main/examples/nuxt-openai)[

SvelteKit OpenAI Starter

Uses OpenAI GPT-4, AI SDK, and SvelteKit.

](https://github.com/vercel/ai/tree/main/examples/sveltekit-openai)[

Solid OpenAI Starter

Uses OpenAI GPT-4, AI SDK, and Solid.

](https://github.com/vercel/ai/tree/main/examples/solidstart-openai)


### Generative UI


[

Gemini Chatbot

Uses Google Gemini, AI SDK, and Next.js.

](https://vercel.com/templates/next.js/gemini-ai-chatbot)[

Generative UI with RSC (experimental)

Uses Next.js, AI SDK, and streamUI to create generative UIs with React Server Components.

](https://vercel.com/templates/next.js/rsc-genui)


### Security


[

Bot Protection

Uses Kasada, OpenAI GPT-4, AI SDK, and Next.js.

](https://vercel.com/templates/next.js/advanced-ai-bot-protection)[

Rate Limiting

Uses Vercel KV, OpenAI GPT-4, AI SDK, and Next.js.

](https://github.com/vercel/ai/tree/main/examples/next-openai-upstash-rate-limits)
```

### 64. `docs/advanced/backpressure.md`

```markdown
# Stream Back-pressure and Cancellation


---
url: https://ai-sdk.dev/docs/advanced/backpressure
description: How to handle backpressure and cancellation when working with the AI SDK
---


# [Stream Back-pressure and Cancellation](#stream-back-pressure-and-cancellation)


This page focuses on understanding back-pressure and cancellation when working with streams. You do not need to know this information to use the AI SDK, but for those interested, it offers a deeper dive on why and how the SDK optimally streams responses.

In the following sections, we'll explore back-pressure and cancellation in the context of a simple example program. We'll discuss the issues that can arise from an eager approach and demonstrate how a lazy approach can resolve them.


## [Back-pressure and Cancellation with Streams](#back-pressure-and-cancellation-with-streams)


Let's begin by setting up a simple example program:

```
// A generator that will yield positive integersasyncfunction*integers(){let i =1;while(true){    console.log(`yielding ${i}`);yield i++;awaitsleep(100);}}functionsleep(ms){returnnewPromise(resolve=>setTimeout(resolve, ms));}// Wraps a generator into a ReadableStreamfunctioncreateStream(iterator){returnnewReadableStream({asyncstart(controller){forawait(const v of iterator){        controller.enqueue(v);}      controller.close();},});}// Collect data from streamasyncfunctionrun(){// Set up a stream of integersconst stream =createStream(integers());// Read values from our streamconst reader = stream.getReader();for(let i =0; i <10_000; i++){// we know our stream is infinite, so there's no need to check `done`.const{ value }=await reader.read();    console.log(`read ${value}`);awaitsleep(1_000);}}run();
```

In this example, we create an async-generator that yields positive integers, a `ReadableStream` that wraps our integer generator, and a reader which will read values out of our stream. Notice, too, that our integer generator logs out `"yielding ${i}"`, and our reader logs out `"read ${value}"`. Both take an arbitrary amount of time to process data, represented with a 100ms sleep in our generator, and a 1sec sleep in our reader.


## [Back-pressure](#back-pressure)


If you were to run this program, you'd notice something funny. We'll see roughly 10 "yield" logs for every "read" log. This might seem obvious, the generator can push values 10x faster than the reader can pull them out. But it represents a problem, our `stream` has to maintain an ever expanding queue of items that have been pushed in but not pulled out.

The problem stems from the way we wrap our generator into a stream. Notice the use of `for await (…)` inside our `start` handler. This is an **eager** for-loop, and it is constantly running to get the next value from our generator to be enqueued in our stream. This means our stream does not respect back-pressure, the signal from the consumer to the producer that more values aren't needed *yet*. We've essentially spawned a thread that will perpetually push more data into the stream, one that runs as fast as possible to push new data immediately. Worse, there's no way to signal to this thread to stop running when we don't need additional data.

To fix this, `ReadableStream` allows a `pull` handler. `pull` is called every time the consumer attempts to read more data from our stream (if there's no data already queued internally). But it's not enough to just move the `for await(…)` into `pull`, we also need to convert from an eager enqueuing to a **lazy** one. By making these 2 changes, we'll be able to react to the consumer. If they need more data, we can easily produce it, and if they don't, then we don't need to spend any time doing unnecessary work.

```
functioncreateStream(iterator){returnnewReadableStream({asyncpull(controller){const{ value, done }=await iterator.next();if(done){        controller.close();}else{        controller.enqueue(value);}},});}
```

Our `createStream` is a little more verbose now, but the new code is important. First, we need to manually call our `iterator.next()` method. This returns a `Promise` for an object with the type signature `{ done: boolean, value: T }`. If `done` is `true`, then we know that our iterator won't yield any more values and we must `close` the stream (this allows the consumer to know that the stream is also finished producing values). Else, we need to `enqueue` our newly produced value.

When we run this program, we see that our "yield" and "read" logs are now paired. We're no longer yielding 10x integers for every read! And, our stream now only needs to maintain 1 item in its internal buffer. We've essentially given control to the consumer, so that it's responsible for producing new values as it needs it. Neato!


## [Cancellation](#cancellation)


Let's go back to our initial eager example, with 1 small edit. Now instead of reading 10,000 integers, we're only going to read 3:

```
// A generator that will yield positive integersasyncfunction*integers(){let i =1;while(true){    console.log(`yielding ${i}`);yield i++;awaitsleep(100);}}functionsleep(ms){returnnewPromise(resolve=>setTimeout(resolve, ms));}// Wraps a generator into a ReadableStreamfunctioncreateStream(iterator){returnnewReadableStream({asyncstart(controller){forawait(const v of iterator){        controller.enqueue(v);}      controller.close();},});}// Collect data from streamasyncfunctionrun(){// Set up a stream that of integersconst stream =createStream(integers());// Read values from our streamconst reader = stream.getReader();// We're only reading 3 items this time:for(let i =0; i <3; i++){// we know our stream is infinite, so there's no need to check `done`.const{ value }=await reader.read();    console.log(`read ${value}`);awaitsleep(1000);}}run();
```

We're back to yielding 10x the number of values read. But notice now, after we've read 3 values, we're continuing to yield new values. We know that our reader will never read another value, but our stream doesn't! The eager `for await (…)` will continue forever, loudly enqueuing new values into our stream's buffer and increasing our memory usage until it consumes all available program memory.

The fix to this is exactly the same: use `pull` and manual iteration. By producing values ***lazily***, we tie the lifetime of our integer generator to the lifetime of the reader. Once the reads stop, the yields will stop too:

```
// Wraps a generator into a ReadableStreamfunctioncreateStream(iterator){returnnewReadableStream({asyncpull(controller){const{ value, done }=await iterator.next();if(done){        controller.close();}else{        controller.enqueue(value);}},});}
```

Since the solution is the same as implementing back-pressure, it shows that they're just 2 facets of the same problem: Pushing values into a stream should be done **lazily**, and doing it eagerly results in expected problems.


## [Tying Stream Laziness to AI Responses](#tying-stream-laziness-to-ai-responses)


Now let's imagine you're integrating AIBot service into your product. Users will be able to prompt "count from 1 to infinity", the browser will fetch your AI API endpoint, and your servers connect to AIBot to get a response. But "infinity" is, well, infinite. The response will never end!

After a few seconds, the user gets bored and navigates away. Or maybe you're doing local development and a hot-module reload refreshes your page. The browser will have ended its connection to the API endpoint, but will your server end its connection with AIBot?

If you used the eager `for await (...)` approach, then the connection is still running and your server is asking for more and more data from AIBot. Our server spawned a "thread" and there's no signal when we can end the eager pulls. Eventually, the server is going to run out of memory (remember, there's no active fetch connection to read the buffering responses and free them).

With the lazy approach, this is taken care of for you. Because the stream will only request new data from AIBot when the consumer requests it, navigating away from the page naturally frees all resources. The fetch connection aborts and the server can clean up the response. The `ReadableStream` tied to that response can now be garbage collected. When that happens, the connection it holds to AIBot can then be freed.
```

### 65. `docs/advanced/caching.md`

```markdown
# Caching Responses


---
url: https://ai-sdk.dev/docs/advanced/caching
description: How to handle caching when working with the AI SDK
---


# [Caching Responses](#caching-responses)


Depending on the type of application you're building, you may want to cache the responses you receive from your AI provider, at least temporarily.


## [Using Language Model Middleware (Recommended)](#using-language-model-middleware-recommended)


The recommended approach to caching responses is using [language model middleware](/docs/ai-sdk-core/middleware) and the [`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream) function.

Language model middleware is a way to enhance the behavior of language models by intercepting and modifying the calls to the language model. Let's see how you can use language model middleware to cache responses.

ai/middleware.ts

```
import{Redis}from'@upstash/redis';import{typeLanguageModelV1,typeLanguageModelV1Middleware,typeLanguageModelV1StreamPart,  simulateReadableStream,}from'ai';const redis =newRedis({  url: process.env.KV_URL,  token: process.env.KV_TOKEN,});exportconst cacheMiddleware:LanguageModelV1Middleware={wrapGenerate:async({ doGenerate, params })=>{const cacheKey =JSON.stringify(params);const cached =(await redis.get(cacheKey))asAwaited<ReturnType<LanguageModelV1['doGenerate']>>|null;if(cached !==null){return{...cached,        response:{...cached.response,          timestamp: cached?.response?.timestamp?newDate(cached?.response?.timestamp):undefined,},};}const result =awaitdoGenerate();    redis.set(cacheKey, result);return result;},wrapStream:async({ doStream, params })=>{const cacheKey =JSON.stringify(params);// Check if the result is in the cacheconst cached =await redis.get(cacheKey);// If cached, return a simulated ReadableStream that yields the cached resultif(cached !==null){// Format the timestamps in the cached responseconst formattedChunks =(cached asLanguageModelV1StreamPart[]).map(p =>{if(p.type==='response-metadata'&& p.timestamp){return{...p, timestamp:newDate(p.timestamp)};}elsereturn p;});return{        stream:simulateReadableStream({          initialDelayInMs:0,          chunkDelayInMs:10,          chunks: formattedChunks,}),        rawCall:{ rawPrompt:null, rawSettings:{}},};}// If not cached, proceed with streamingconst{ stream,...rest }=awaitdoStream();const fullResponse:LanguageModelV1StreamPart[]=[];const transformStream =newTransformStream<LanguageModelV1StreamPart,LanguageModelV1StreamPart>({transform(chunk, controller){        fullResponse.push(chunk);        controller.enqueue(chunk);},flush(){// Store the full response in the cache after streaming is complete        redis.set(cacheKey, fullResponse);},});return{      stream: stream.pipeThrough(transformStream),...rest,};},};
```

This example uses `@upstash/redis` to store and retrieve the assistant's responses but you can use any KV storage provider you would like.

`LanguageModelMiddleware` has two methods: `wrapGenerate` and `wrapStream`. `wrapGenerate` is called when using [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`generateObject`](/docs/reference/ai-sdk-core/generate-object), while `wrapStream` is called when using [`streamText`](/docs/reference/ai-sdk-core/stream-text) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object).

For `wrapGenerate`, you can cache the response directly. Instead, for `wrapStream`, you cache an array of the stream parts, which can then be used with [`simulateReadableStream`](/docs/ai-sdk-core/testing#simulate-data-stream-protocol-responses) function to create a simulated `ReadableStream` that returns the cached response. In this way, the cached response is returned chunk-by-chunk as if it were being generated by the model. You can control the initial delay and delay between chunks by adjusting the `initialDelayInMs` and `chunkDelayInMs` parameters of `simulateReadableStream`.

You can see a full example of caching with Redis in a Next.js application in our [Caching Middleware Recipe](/cookbook/next/caching-middleware).


## [Using Lifecycle Callbacks](#using-lifecycle-callbacks)


Alternatively, each AI SDK Core function has special lifecycle callbacks you can use. The one of interest is likely `onFinish`, which is called when the generation is complete. This is where you can cache the full response.

Here's an example of how you can implement caching using Vercel KV and Next.js to cache the OpenAI response for 1 hour:

This example uses [Upstash Redis](https://upstash.com/docs/redis/overall/getstarted) and Next.js to cache the response for 1 hour.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ formatDataStreamPart, streamText }from'ai';import{Redis}from'@upstash/redis';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;const redis =newRedis({  url: process.env.KV_URL,  token: process.env.KV_TOKEN,});exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();// come up with a key based on the request:const key =JSON.stringify(messages);// Check if we have a cached responseconst cached =await redis.get(key);if(cached !=null){returnnewResponse(formatDataStreamPart('text', cached),{      status:200,      headers:{'Content-Type':'text/plain'},});}// Call the language model:const result =streamText({    model:openai('gpt-4o'),    messages,asynconFinish({ text }){// Cache the response text:await redis.set(key, text);await redis.expire(key,60*60);},});// Respond with the streamreturn result.toDataStreamResponse();}
```
```

### 66. `docs/advanced/model-as-router.md`

```markdown
# Generative User Interfaces


---
url: https://ai-sdk.dev/docs/advanced/model-as-router
description: Generative User Interfaces and Language Models as Routers
---


# [Generative User Interfaces](#generative-user-interfaces)


Since language models can render user interfaces as part of their generations, the resulting model generations are referred to as generative user interfaces.

In this section we will learn more about generative user interfaces and their impact on the way AI applications are built.


## [Deterministic Routes and Probabilistic Routing](#deterministic-routes-and-probabilistic-routing)


Generative user interfaces are not deterministic in nature because they depend on the model's generation output. Since these generations are probabilistic in nature, it is possible for every user query to result in a different user interface.

Users expect their experience using your application to be predictable, so non-deterministic user interfaces can sound like a bad idea at first. However, language models can be set up to limit their generations to a particular set of outputs using their ability to call functions.

When language models are provided with a set of function definitions and instructed to execute any of them based on user query, they do either one of the following things:

-   Execute a function that is most relevant to the user query.
-   Not execute any function if the user query is out of bounds of the set of functions available to them.

app/actions.ts

```
constsendMessage=(prompt: string)=>generateText({    model:'gpt-3.5-turbo',    system:'you are a friendly weather assistant!',    prompt,    tools:{      getWeather:{        description:'Get the weather in a location',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location }:{ location: string })=>({location,          temperature:72+Math.floor(Math.random()*21)-10,}),},},});sendMessage('What is the weather in San Francisco?');// getWeather is calledsendMessage('What is the weather in New York?');// getWeather is calledsendMessage('What events are happening in London?');// No function is called
```

This way, it is possible to ensure that the generations result in deterministic outputs, while the choice a model makes still remains to be probabilistic.

This emergent ability exhibited by a language model to choose whether a function needs to be executed or not based on a user query is believed to be models emulating "reasoning".

As a result, the combination of language models being able to reason which function to execute as well as render user interfaces at the same time gives you the ability to build applications where language models can be used as a router.


## [Language Models as Routers](#language-models-as-routers)


Historically, developers had to write routing logic that connected different parts of an application to be navigable by a user and complete a specific task.

In web applications today, most of the routing logic takes place in the form of routes:

-   `/login` would navigate you to a page with a login form.
-   `/user/john` would navigate you to a page with profile details about John.
-   `/api/events?limit=5` would display the five most recent events from an events database.

While routes help you build web applications that connect different parts of an application into a seamless user experience, it can also be a burden to manage them as the complexity of applications grow.

Next.js has helped reduce complexity in developing with routes by introducing:

-   File-based routing system
-   Dynamic routing
-   API routes
-   Middleware
-   App router, and so on...

With language models becoming better at reasoning, we believe that there is a future where developers only write core application specific components while models take care of routing them based on the user's state in an application.

With generative user interfaces, the language model decides which user interface to render based on the user's state in the application, giving users the flexibility to interact with your application in a conversational manner instead of navigating through a series of predefined routes.


### [Routing by parameters](#routing-by-parameters)


For routes like:

-   `/profile/[username]`
-   `/search?q=[query]`
-   `/media/[id]`

that have segments dependent on dynamic data, the language model can generate the correct parameters and render the user interface.

For example, when you're in a search application, you can ask the language model to search for artworks from different artists. The language model will call the search function with the artist's name as a parameter and render the search results.

Art made by Van Gogh?

searchImages("Van Gogh")

Here are a few of his notable works

Starry Night

Sunflowers

Olive Trees

Wow, these look great! How about Monet?

searchImages("Monet")

Sure! Here are a few of his paintings

Frau im Gartenfrau

Cliff Walk

Waves

Media Search

Let your users see more than words can say by rendering components directly within your search experience.


### [Routing by sequence](#routing-by-sequence)


For actions that require a sequence of steps to be completed by navigating through different routes, the language model can generate the correct sequence of routes to complete in order to fulfill the user's request.

For example, when you're in a calendar application, you can ask the language model to schedule a happy hour evening with your friends. The language model will then understand your request and will perform the right sequence of [tool calls](/docs/ai-sdk-core/tools-and-tool-calling) to:

1.  Lookup your calendar
2.  Lookup your friends' calendars
3.  Determine the best time for everyone
4.  Search for nearby happy hour spots
5.  Create an event and send out invites to your friends

I'd like to get drinks with Max tomorrow evening after studio!

searchContacts("Max")

max

@mleiter

shu

@shuding

getEvents("2023-10-18", \["jrmy", "mleiter"\])

4PM

5PM

6PM

7PM

studio

4-6 PM

searchNearby("Bar")

wild colonial

200m

the eddy

1.3km

createEvent("2023-10-18", \["jrmy", "mleiter"\])

4PM

5PM

6PM

7PM

studio

4-6 PM

Drinks at Wild Colonial

6-7 PM

Exciting! Max is free around that time and Wild Colonial is right around the corner, would you like me to mark it on your calendar?

Sure, sounds good!

Planning an Event

The model calls functions and generates interfaces based on user intent, acting like a router.

Just by defining functions to lookup contacts, pull events from a calendar, and search for nearby locations, the model is able to sequentially navigate the routes for you.

To learn more, check out these [examples](/examples/next-app/interface) using the `streamUI` function to stream generative user interfaces to the client based on the response from the language model.
```

### 67. `docs/advanced/multiple-streamables.md`

```markdown
# Multiple Streams


---
url: https://ai-sdk.dev/docs/advanced/multiple-streamables
description: Learn to handle multiple streamables in your application.
---


# [Multiple Streams](#multiple-streams)



## [Multiple Streamable UIs](#multiple-streamable-uis)


The AI SDK RSC APIs allow you to compose and return any number of streamable UIs, along with other data, in a single request. This can be useful when you want to decouple the UI into smaller components and stream them separately.

```
'use server';import{ createStreamableUI }from'ai/rsc';exportasyncfunctiongetWeather(){const weatherUI =createStreamableUI();const forecastUI =createStreamableUI();  weatherUI.update(<div>Loading weather...</div>);  forecastUI.update(<div>Loading forecast...</div>);getWeatherData().then(weatherData=>{    weatherUI.done(<div>{weatherData}</div>);});getForecastData().then(forecastData=>{    forecastUI.done(<div>{forecastData}</div>);});// Return both streamable UIs and other data fields.return{    requestedAt:Date.now(),    weather: weatherUI.value,    forecast: forecastUI.value,};}
```

The client side code is similar to the previous example, but the [tool call](/docs/ai-sdk-core/tools-and-tool-calling) will return the new data structure with the weather and forecast UIs. Depending on the speed of getting weather and forecast data, these two components might be updated independently.


## [Nested Streamable UIs](#nested-streamable-uis)


You can stream UI components within other UI components. This allows you to create complex UIs that are built up from smaller, reusable components. In the example below, we pass a `historyChart` streamable as a prop to a `StockCard` component. The StockCard can render the `historyChart` streamable, and it will automatically update as the server responds with new data.

```
asyncfunctiongetStockHistoryChart({ symbol: string }){'use server';const ui =createStreamableUI(<Spinner/>);// We need to wrap this in an async IIFE to avoid blocking.(async()=>{const price =awaitgetStockPrice({symbol});// Show a spinner as the history chart for now.const historyChart =createStreamableUI(<Spinner/>);    ui.done(<StockCardhistoryChart={historyChart.value}price={price}/>);// Getting the history data and then update that part of the UI.const historyData =awaitfetch('https://my-stock-data-api.com');    historyChart.done(<HistoryChartdata={historyData}/>);})();return ui;}
```
```

### 68. `docs/advanced/multistep-interfaces.md`

```markdown
# Multistep Interfaces


---
url: https://ai-sdk.dev/docs/advanced/multistep-interfaces
description: Concepts behind building multistep interfaces
---


# [Multistep Interfaces](#multistep-interfaces)


Multistep interfaces refer to user interfaces that require multiple independent steps to be executed in order to complete a specific task.

In order to understand multistep interfaces, it is important to understand two concepts:

-   Tool composition
-   Application context

**Tool composition** is the process of combining multiple [tools](/docs/ai-sdk-core/tools-and-tool-calling) to create a new tool. This is a powerful concept that allows you to break down complex tasks into smaller, more manageable steps.

**Application context** refers to the state of the application at any given point in time. This includes the user's input, the output of the language model, and any other relevant information.

When designing multistep interfaces, you need to consider how the tools in your application can be composed together to form a coherent user experience as well as how the application context changes as the user progresses through the interface.


## [Application Context](#application-context)


The application context can be thought of as the conversation history between the user and the language model. The richer the context, the more information the model has to generate relevant responses.

In the context of multistep interfaces, the application context becomes even more important. This is because **the user's input in one step may affect the output of the model in the next step**.

For example, consider a meal logging application that helps users track their daily food intake. The language model is provided with the following tools:

-   `log_meal` takes in parameters like the name of the food, the quantity, and the time of consumption to log a meal.
-   `delete_meal` takes in the name of the meal to be deleted.

When the user logs a meal, the model generates a response confirming the meal has been logged.

```
User: Log a chicken shawarma for lunch.Tool: log_meal("chicken shawarma", "250g", "12:00 PM")Model: Chicken shawarma has been logged for lunch.
```

Now when the user decides to delete the meal, the model should be able to reference the previous step to identify the meal to be deleted.

```
User: Log a chicken shawarma for lunch.Tool: log_meal("chicken shawarma", "250g", "12:00 PM")Model: Chicken shawarma has been logged for lunch.......User: I skipped lunch today, can you update my log?Tool: delete_meal("chicken shawarma")Model: Chicken shawarma has been deleted from your log.
```

In this example, managing the application context is important for the model to generate the correct response. The model needs to have information about the previous actions in order for it to use generate the parameters for the `delete_meal` tool.


## [Tool Composition](#tool-composition)


Tool composition is the process of combining multiple tools to create a new tool. This involves defining the inputs and outputs of each tool, as well as how they interact with each other.

The design of how these tools can be composed together to form a multistep interface is crucial to both the user experience of your application and the model's ability to generate the correct output.

For example, consider a flight booking assistant that can help users book flights. The assistant can be designed to have the following tools:

-   `searchFlights`: Searches for flights based on the user's query.
-   `lookupFlight`: Looks up details of a specific flight based on the flight number.
-   `bookFlight`: Books a flight based on the user's selection.

The `searchFlights` tool is called when the user wants to lookup flights for a specific route. This would typically mean the tool should be able to take in parameters like the origin and destination of the flight.

The `lookupFlight` tool is called when the user wants to get more details about a specific flight. This would typically mean the tool should be able to take in parameters like the flight number and return information about seat availability.

The `bookFlight` tool is called when the user decides to book a flight. In order to identify the flight to book, the tool should be able to take in parameters like the flight number, trip date, and passenger details.

So the conversation between the user and the model could look like this:

```
User: I want to book a flight from New York to London.Tool: searchFlights("New York", "London")Model: Here are the available flights from New York to London.User: I want to book flight number BA123 on 12th December for myself and my wife.Tool: lookupFlight("BA123") -> "4 seats available"Model: Sure, there are seats available! Can you provide the names of the passengers?User: John Doe and Jane Doe.Tool: bookFlight("BA123", "12th December", ["John Doe", "Jane Doe"])Model: Your flight has been booked!
```

In the last tool call, the `bookFlight` tool does not include passenger details as it is not available in the application context. As a result, it requests the user to provide the passenger details before proceeding with the booking.

Looking up passenger information could've been another tool that the model could've called before calling the `bookFlight` tool assuming that the user is logged into the application. This way, the model does not have to ask the user for the passenger details and can proceed with the booking.

```
User: I want to book a flight from New York to London.Tool: searchFlights("New York", "London")Model: Here are the available flights from New York to London.User: I want to book flight number BA123 on 12th December for myself an my wife.Tool: lookupContacts() -> ["John Doe", "Jane Doe"]Tool: bookFlight("BA123", "12th December", ["John Doe", "Jane Doe"])Model: Your flight has been booked!
```

The `lookupContacts` tool is called before the `bookFlight` tool to ensure that the passenger details are available in the application context when booking the flight. This way, the model can reduce the number of steps required from the user and use its ability to call tools that populate its context and use that information to complete the booking process.

Now, let's introduce another tool called `lookupBooking` that can be used to show booking details by taking in the name of the passenger as parameter. This tool can be composed with the existing tools to provide a more complete user experience.

```
User: What's the status of my wife's upcoming flight?Tool: lookupContacts() -> ["John Doe", "Jane Doe"]Tool: lookupBooking("Jane Doe") -> "BA123 confirmed"Tool: lookupFlight("BA123") -> "Flight BA123 is scheduled to depart on 12th December."Model: Your wife's flight BA123 is confirmed and scheduled to depart on 12th December.
```

In this example, the `lookupBooking` tool is used to provide the user with the status of their wife's upcoming flight. By composing this tool with the existing tools, the model is able to generate a response that includes the booking status and the departure date of the flight without requiring the user to provide additional information.

As a result, the more tools you design that can be composed together, the more complex and powerful your application can become.
```

### 69. `docs/advanced/prompt-engineering.md`

```markdown
# Prompt Engineering


---
url: https://ai-sdk.dev/docs/advanced/prompt-engineering
description: Learn how to engineer prompts for LLMs with the AI SDK
---


# [Prompt Engineering](#prompt-engineering)



## [What is a Large Language Model (LLM)?](#what-is-a-large-language-model-llm)


A Large Language Model is essentially a prediction engine that takes a sequence of words as input and aims to predict the most likely sequence to follow. It does this by assigning probabilities to potential next sequences and then selecting one. The model continues to generate sequences until it meets a specified stopping criterion.

These models learn by training on massive text corpuses, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well. However, it's crucial to understand that the generated sequences, while often seeming plausible, can sometimes be random and not grounded in reality. As these models become more accurate, many surprising abilities and applications emerge.


## [What is a prompt?](#what-is-a-prompt)


Prompts are the starting points for LLMs. They are the inputs that trigger the model to generate text. The scope of prompt engineering involves not just crafting these prompts but also understanding related concepts such as hidden prompts, tokens, token limits, and the potential for prompt hacking, which includes phenomena like jailbreaks and leaks.


## [Why is prompt engineering needed?](#why-is-prompt-engineering-needed)


Prompt engineering currently plays a pivotal role in shaping the responses of LLMs. It allows us to tweak the model to respond more effectively to a broader range of queries. This includes the use of techniques like semantic search, command grammars, and the ReActive model architecture. The performance, context window, and cost of LLMs varies between models and model providers which adds further constraints to the mix. For example, the GPT-4 model is more expensive than GPT-3.5-turbo and significantly slower, but it can also be more effective at certain tasks. And so, like many things in software engineering, there is a trade-offs between cost and performance.

To assist with comparing and tweaking LLMs, we've built an AI playground that allows you to compare the performance of different models side-by-side online. When you're ready, you can even generate code with the AI SDK to quickly use your prompt and your selected model into your own applications.


## [Example: Build a Slogan Generator](#example-build-a-slogan-generator)



### [Start with an instruction](#start-with-an-instruction)


Imagine you want to build a slogan generator for marketing campaigns. Creating catchy slogans isn't always straightforward!

First, you'll need a prompt that makes it clear what you want. Let's start with an instruction. Submit this prompt to generate your first completion.

Create a slogan for a coffee shop.

Generate

...

Not bad! Now, try making your instruction more specific.

Create a slogan for an organic coffee shop.

Generate

...

Introducing a single descriptive term to our prompt influences the completion. Essentially, crafting your prompt is the means by which you "instruct" or "program" the model.


### [Include examples](#include-examples)


Clear instructions are key for quality outcomes, but that might not always be enough. Let's try to enhance your instruction further.

Create three slogans for a coffee shop with live music.

Generate

...

These slogans are fine, but could be even better. It appears the model overlooked the 'live' part in our prompt. Let's change it slightly to generate more appropriate suggestions.

Often, it's beneficial to both demonstrate and tell the model your requirements. Incorporating examples in your prompt can aid in conveying patterns or subtleties. Test this prompt that carries a few examples.

Create three slogans for a business with unique features. Business: Bookstore with cats Slogans: "Purr-fect Pages", "Books and Whiskers", "Novels and Nuzzles" Business: Gym with rock climbing Slogans: "Peak Performance", "Reach New Heights", "Climb Your Way Fit" Business: Coffee shop with live music Slogans:

Generate

...

Great! Incorporating examples of expected output for a certain input prompted the model to generate the kind of names we aimed for.


### [Tweak your settings](#tweak-your-settings)


Apart from designing prompts, you can influence completions by tweaking model settings. A crucial setting is the **temperature**.

You might have seen that the same prompt, when repeated, yielded the same or nearly the same completions. This happens when your temperature is at 0.

Attempt to re-submit the identical prompt a few times with temperature set to 1.

Temperature

Create three slogans for a business with unique features. Business: Bookstore with cats Slogans: "Purr-fect Pages", "Books and Whiskers", "Novels and Nuzzles" Business: Gym with rock climbing Slogans: "Peak Performance", "Reach New Heights", "Climb Your Way Fit" Business: Coffee shop with live music Slogans:

Generate

...

Notice the difference? With a temperature above 0, the same prompt delivers varied completions each time.

Keep in mind that the model forecasts the text most likely to follow the preceding text. Temperature, a value from 0 to 1, essentially governs the model's confidence level in making these predictions. A lower temperature implies lesser risks, leading to more precise and deterministic completions. A higher temperature yields a broader range of completions.

For your slogan generator, you might want a large pool of name suggestions. A moderate temperature of 0.6 should serve well.


## [Recommended Resources](#recommended-resources)


Prompt Engineering is evolving rapidly, with new methods and research papers surfacing every week. Here are some resources that we've found useful for learning about and experimenting with prompt engineering:

-   [The Vercel AI Playground](/playground)
-   [Brex Prompt Engineering](https://github.com/brexhq/prompt-engineering)
-   [Prompt Engineering Guide by Dair AI](https://www.promptingguide.ai/)
```

### 70. `docs/advanced/rate-limiting.md`

```markdown
# Rate Limiting


---
url: https://ai-sdk.dev/docs/advanced/rate-limiting
description: Learn how to rate limit your application.
---


# [Rate Limiting](#rate-limiting)


Rate limiting helps you protect your APIs from abuse. It involves setting a maximum threshold on the number of requests a client can make within a specified timeframe. This simple technique acts as a gatekeeper, preventing excessive usage that can degrade service performance and incur unnecessary costs.


## [Rate Limiting with Vercel KV and Upstash Ratelimit](#rate-limiting-with-vercel-kv-and-upstash-ratelimit)


In this example, you will protect an API endpoint using [Vercel KV](https://vercel.com/storage/kv) and [Upstash Ratelimit](https://github.com/upstash/ratelimit).

app/api/generate/route.ts

```
import kv from'@vercel/kv';import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';import{Ratelimit}from'@upstash/ratelimit';import{NextRequest}from'next/server';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;// Create Rate limitconst ratelimit =newRatelimit({  redis: kv,  limiter:Ratelimit.fixedWindow(5,'30s'),});exportasyncfunctionPOST(req: NextRequest){// call ratelimit with request ipconst ip = req.ip ??'ip';const{ success, remaining }=await ratelimit.limit(ip);// block the request if unsuccessfullif(!success){returnnewResponse('Ratelimited!',{ status:429});}const{ messages }=await req.json();const result =streamText({    model:openai('gpt-3.5-turbo'),    messages,});return result.toDataStreamResponse();}
```


## [Simplify API Protection](#simplify-api-protection)


With Vercel KV and Upstash Ratelimit, it is possible to protect your APIs from such attacks with ease. To learn more about how Ratelimit works and how it can be configured to your needs, see [Ratelimit Documentation](https://upstash.com/docs/oss/sdks/ts/ratelimit/overview).
```

### 71. `docs/advanced/rendering-ui-with-language-models.md`

```markdown
# Rendering User Interfaces with Language Models


---
url: https://ai-sdk.dev/docs/advanced/rendering-ui-with-language-models
description: Rendering UI with Language Models
---


# [Rendering User Interfaces with Language Models](#rendering-user-interfaces-with-language-models)


Language models generate text, so at first it may seem like you would only need to render text in your application.

app/actions.tsx

```
const text =generateText({  model:openai('gpt-3.5-turbo'),  system:'You are a friendly assistant',  prompt:'What is the weather in SF?',  tools:{    getWeather:{      description:'Get the weather for a location',      parameters: z.object({        city: z.string().describe('The city to get the weather for'),        unit: z.enum(['C','F']).describe('The unit to display the temperature in'),}),execute:async({ city, unit })=>{const weather =getWeather({ city, unit });return`It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`;},},},});
```

Above, the language model is passed a [tool](/docs/ai-sdk-core/tools-and-tool-calling) called `getWeather` that returns the weather information as text. However, instead of returning text, if you return a JSON object that represents the weather information, you can use it to render a React component instead.

app/action.ts

```
const text =generateText({  model:openai('gpt-3.5-turbo'),  system:'You are a friendly assistant',  prompt:'What is the weather in SF?',  tools:{    getWeather:{      description:'Get the weather for a location',      parameters: z.object({        city: z.string().describe('The city to get the weather for'),        unit: z.enum(['C','F']).describe('The unit to display the temperature in'),}),execute:async({ city, unit })=>{const weather =getWeather({ city, unit });const{ temperature, unit, description, forecast }= weather;return{          temperature,          unit,          description,          forecast,};},},},});
```

Now you can use the object returned by the `getWeather` function to conditionally render a React component `<WeatherCard/>` that displays the weather information by passing the object as props.

app/page.tsx

```
return(<div>{messages.map(message=>{if(message.role ==='function'){const{ name, content }= messageconst{ temperature, unit, description, forecast }= content;return(<WeatherCardweather={{              temperature:47,              unit:'F',              description:'sunny'              forecast,}}/>)}})}</div>)
```

Here's a little preview of what that might look like.

What is the weather in SF?

getWeather("San Francisco")

Thursday, March 7

47°

sunny

7am

48°

8am

50°

9am

52°

10am

54°

11am

56°

12pm

58°

1pm

60°

Thanks!

Weather

An example of an assistant that renders the weather information in a streamed component.

Rendering interfaces as part of language model generations elevates the user experience of your application, allowing people to interact with language models beyond text.

They also make it easier for you to interpret [sequential tool calls](/docs/ai-sdk-rsc/multistep-interfaces) that take place in multiple steps and help identify and debug where the model reasoned incorrectly.


## [Rendering Multiple User Interfaces](#rendering-multiple-user-interfaces)


To recap, an application has to go through the following steps to render user interfaces as part of model generations:

1.  The user prompts the language model.
2.  The language model generates a response that includes a tool call.
3.  The tool call returns a JSON object that represents the user interface.
4.  The response is sent to the client.
5.  The client receives the response and checks if the latest message was a tool call.
6.  If it was a tool call, the client renders the user interface based on the JSON object returned by the tool call.

Most applications have multiple tools that are called by the language model, and each tool can return a different user interface.

For example, a tool that searches for courses can return a list of courses, while a tool that searches for people can return a list of people. As this list grows, the complexity of your application will grow as well and it can become increasingly difficult to manage these user interfaces.

app/page.tsx

```
{  message.role ==='tool'?(    message.name ==='api-search-course'?(<Coursescourses={message.content}/>): message.name ==='api-search-profile'?(<Peoplepeople={message.content}/>): message.name ==='api-meetings'?(<Meetingsmeetings={message.content}/>): message.name ==='api-search-building'?(<Buildingsbuildings={message.content}/>): message.name ==='api-events'?(<Eventsevents={message.content}/>): message.name ==='api-meals'?(<Mealsmeals={message.content}/>):null):(<div>{message.content}</div>);}
```


## [Rendering User Interfaces on the Server](#rendering-user-interfaces-on-the-server)


The **AI SDK RSC (`ai/rsc`)** takes advantage of RSCs to solve the problem of managing all your React components on the client side, allowing you to render React components on the server and stream them to the client.

Rather than conditionally rendering user interfaces on the client based on the data returned by the language model, you can directly stream them from the server during a model generation.

app/action.ts

```
import{ createStreamableUI }from'ai/rsc'const uiStream =createStreamableUI();const text =generateText({  model:openai('gpt-3.5-turbo'),  system:'you are a friendly assistant'  prompt:'what is the weather in SF?'  tools:{    getWeather:{      description:'Get the weather for a location',      parameters: z.object({        city: z.string().describe('The city to get the weather for'),        unit: z.enum(['C','F']).describe('The unit to display the temperature in')}),execute:async({ city, unit })=>{const weather =getWeather({ city, unit })const{ temperature, unit, description, forecast }= weather        uiStream.done(<WeatherCardweather={{              temperature:47,              unit:'F',              description:'sunny'              forecast,}}/>)}}}})return{  display: uiStream.value}
```

The [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) function belongs to the `ai/rsc` module and creates a stream that can send React components to the client.

On the server, you render the `<WeatherCard/>` component with the props passed to it, and then stream it to the client. On the client side, you only need to render the UI that is streamed from the server.

app/page.tsx

```
return(<div>{messages.map(message=>(<div>{message.display}</div>))}</div>);
```

Now the steps involved are simplified:

1.  The user prompts the language model.
2.  The language model generates a response that includes a tool call.
3.  The tool call renders a React component along with relevant props that represent the user interface.
4.  The response is streamed to the client and rendered directly.

> **Note:** You can also render text on the server and stream it to the client using React Server Components. This way, all operations from language model generation to UI rendering can be done on the server, while the client only needs to render the UI that is streamed from the server.

Check out this [example](/examples/next-app/interface/stream-component-updates) for a full illustration of how to stream component updates with React Server Components in Next.js App Router.
```

### 72. `docs/advanced/sequential-generations.md`

```markdown
# Sequential Generations


---
url: https://ai-sdk.dev/docs/advanced/sequential-generations
description: Learn how to implement sequential generations ("chains") with the AI SDK
---


# [Sequential Generations](#sequential-generations)


When working with the AI SDK, you may want to create sequences of generations (often referred to as "chains" or "pipes"), where the output of one becomes the input for the next. This can be useful for creating more complex AI-powered workflows or for breaking down larger tasks into smaller, more manageable steps.


## [Example](#example)


In a sequential chain, the output of one generation is directly used as input for the next generation. This allows you to create a series of dependent generations, where each step builds upon the previous one.

Here's an example of how you can implement sequential actions:

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';asyncfunctionsequentialActions(){// Generate blog post ideasconst ideasGeneration =awaitgenerateText({    model:openai('gpt-4o'),    prompt:'Generate 10 ideas for a blog post about making spaghetti.',});console.log('Generated Ideas:\n', ideasGeneration);// Pick the best ideaconst bestIdeaGeneration =awaitgenerateText({    model:openai('gpt-4o'),    prompt:`Here are some blog post ideas about making spaghetti:${ideasGeneration}Pick the best idea from the list above and explain why it's the best.`,});console.log('\nBest Idea:\n', bestIdeaGeneration);// Generate an outlineconst outlineGeneration =awaitgenerateText({    model:openai('gpt-4o'),    prompt:`We've chosen the following blog post idea about making spaghetti:${bestIdeaGeneration}Create a detailed outline for a blog post based on this idea.`,});console.log('\nBlog Post Outline:\n', outlineGeneration);}sequentialActions().catch(console.error);
```

In this example, we first generate ideas for a blog post, then pick the best idea, and finally create an outline based on that idea. Each step uses the output from the previous step as input for the next generation.
```

### 73. `docs/advanced/stopping-streams.md`

```markdown
# Stopping Streams


---
url: https://ai-sdk.dev/docs/advanced/stopping-streams
description: Learn how to cancel streams with the AI SDK
---


# [Stopping Streams](#stopping-streams)


Cancelling ongoing streams is often needed. For example, users might want to stop a stream when they realize that the response is not what they want.

The different parts of the AI SDK support cancelling streams in different ways.


## [AI SDK Core](#ai-sdk-core)


The AI SDK functions have an `abortSignal` argument that you can use to cancel a stream. You would use this if you want to cancel a stream from the server side to the LLM API, e.g. by forwarding the `abortSignal` from the request.

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';exportasyncfunctionPOST(req: Request){const{ prompt }=await req.json();const result =streamText({    model:openai('gpt-4-turbo'),    prompt,// forward the abort signal:    abortSignal: req.signal,});return result.toTextStreamResponse();}
```


## [AI SDK UI](#ai-sdk-ui)


The hooks, e.g. `useChat` or `useCompletion`, provide a `stop` helper function that can be used to cancel a stream. This will cancel the stream from the client side to the server.

```
'use client';import{ useCompletion }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ input, completion, stop, status, handleSubmit, handleInputChange }=useCompletion();return(<div>{(status ==='submitted'| status ==='streaming')&&(<buttontype="button"onClick={()=>stop()}>Stop</button>)}{completion}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}/></form></div>);}
```


## [AI SDK RSC](#ai-sdk-rsc)


The AI SDK RSC does not currently support stopping streams.
```

### 74. `docs/advanced/vercel-deployment-guide.md`

```markdown
# Vercel Deployment Guide


---
url: https://ai-sdk.dev/docs/advanced/vercel-deployment-guide
description: Learn how to deploy an AI application to production on Vercel
---


# [Vercel Deployment Guide](#vercel-deployment-guide)


In this guide, you will deploy an AI application to [Vercel](https://vercel.com) using [Next.js](https://nextjs.org) (App Router).

Vercel is a platform for developers that provides the tools, workflows, and infrastructure you need to build and deploy your web apps faster, without the need for additional configuration.

Vercel allows for automatic deployments on every branch push and merges onto the production branch of your GitHub, GitLab, and Bitbucket projects. It is a great option for deploying your AI application.


## [Before You Begin](#before-you-begin)


To follow along with this guide, you will need:

-   a Vercel account
-   an account with a Git provider (this tutorial will use [Github](https://github.com))
-   an OpenAI API key

This guide will teach you how to deploy the application you built in the Next.js (App Router) quickstart tutorial to Vercel. If you haven’t completed the quickstart guide, you can start with [this repo](https://github.com/vercel-labs/ai-sdk-deployment-guide).


## [Commit Changes](#commit-changes)


Vercel offers a powerful git-centered workflow that automatically deploys your application to production every time you push to your repository’s main branch.

Before committing your local changes, make sure that you have a `.gitignore`. Within your `.gitignore`, ensure that you are excluding your environment variables (`.env`) and your node modules (`node_modules`).

If you have any local changes, you can commit them by running the following commands:

```
gitadd.git commit -m "init"
```


## [Create Git Repo](#create-git-repo)


You can create a GitHub repository from within your terminal, or on [github.com](https://github.com/). For this tutorial, you will use the GitHub CLI ([more info here](https://cli.github.com/)).

To create your GitHub repository:

1.  Navigate to [github.com](http://github.com/)
2.  In the top right corner, click the "plus" icon and select "New repository"
3.  Pick a name for your repository (this can be anything)
4.  Click "Create repository"

Once you have created your repository, GitHub will redirect you to your new repository.

1.  Scroll down the page and copy the commands under the title "...or push an existing repository from the command line"
2.  Go back to the terminal, paste and then run the commands

Note: if you run into the error "error: remote origin already exists.", this is because your local repository is still linked to the repository you cloned. To "unlink", you can run the following command:

```
rm -rf .gitgit initgitadd.git commit -m "init"
```

Rerun the code snippet from the previous step.


## [Import Project in Vercel](#import-project-in-vercel)


On the [New Project](https://vercel.com/new) page, under the **Import Git Repository** section, select the Git provider that you would like to import your project from. Follow the prompts to sign in to your GitHub account.

Once you have signed in, you should see your newly created repository from the previous step in the "Import Git Repository" section. Click the "Import" button next to that project.


### [Add Environment Variables](#add-environment-variables)


Your application stores uses environment secrets to store your OpenAI API key using a `.env.local` file locally in development. To add this API key to your production deployment, expand the "Environment Variables" section and paste in your `.env.local` file. Vercel will automatically parse your variables and enter them in the appropriate `key:value` format.


### [Deploy](#deploy)


Press the **Deploy** button. Vercel will create the Project and deploy it based on the chosen configurations.


### [Enjoy the confetti!](#enjoy-the-confetti)


To view your deployment, select the Project in the dashboard and then select the **Domain**. This page is now visible to anyone who has the URL.


## [Considerations](#considerations)


When deploying an AI application, there are infrastructure-related considerations to be aware of.


### [Function Duration](#function-duration)


In most cases, you will call the large language model (LLM) on the server. By default, Vercel serverless functions have a maximum duration of 10 seconds on the Hobby Tier. Depending on your prompt, it can take an LLM more than this limit to complete a response. If the response is not resolved within this limit, the server will throw an error.

You can specify the maximum duration of your Vercel function using [route segment config](https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config). To update your maximum duration, add the following route segment config to the top of your route handler or the page which is calling your server action.

```
exportconst maxDuration =30;
```

You can increase the max duration to 60 seconds on the Hobby Tier. For other tiers, [see the documentation](https://vercel.com/docs/functions/runtimes#max-duration) for limits.


## [Security Considerations](#security-considerations)


Given the high cost of calling an LLM, it's important to have measures in place that can protect your application from abuse.


### [Rate Limit](#rate-limit)


Rate limiting is a method used to regulate network traffic by defining a maximum number of requests that a client can send to a server within a given time frame.

Follow [this guide](https://vercel.com/guides/securing-ai-app-rate-limiting) to add rate limiting to your application.


### [Firewall](#firewall)


A firewall helps protect your applications and websites from DDoS attacks and unauthorized access.

[Vercel Firewall](https://vercel.com/docs/security/vercel-firewall) is a set of tools and infrastructure, created specifically with security in mind. It automatically mitigates DDoS attacks and Enterprise teams can get further customization for their site, including dedicated support and custom rules for IP blocking.


## [Troubleshooting](#troubleshooting)


-   Streaming not working when [proxied](/docs/troubleshooting/streaming-not-working-when-proxied)
-   Experiencing [Timeouts](/docs/troubleshooting/timeout-on-vercel)
```

### 75. `docs/advanced.md`

```markdown
# Advanced


---
url: https://ai-sdk.dev/docs/advanced
description: Learn how to use advanced functionality within the AI SDK and RSC API.
---


# [Advanced](#advanced)


This section covers advanced topics and concepts for the AI SDK and RSC API. Working with LLMs often requires a different mental model compared to traditional software development.

After these concepts, you should have a better understanding of the paradigms behind the AI SDK and RSC API, and how to use them to build more AI applications.
```

### 76. `docs/ai-sdk-core/embeddings.md`

```markdown
# Embeddings


---
url: https://ai-sdk.dev/docs/ai-sdk-core/embeddings
description: Learn how to embed values with the AI SDK.
---


# [Embeddings](#embeddings)


Embeddings are a way to represent words, phrases, or images as vectors in a high-dimensional space. In this space, similar words are close to each other, and the distance between words can be used to measure their similarity.


## [Embedding a Single Value](#embedding-a-single-value)


The AI SDK provides the [`embed`](/docs/reference/ai-sdk-core/embed) function to embed single values, which is useful for tasks such as finding similar words or phrases or clustering text. You can use it with embeddings models, e.g. `openai.embedding('text-embedding-3-large')` or `mistral.embedding('mistral-embed')`.

```
import{ embed }from'ai';import{ openai }from'@ai-sdk/openai';// 'embedding' is a single embedding object (number[])const{ embedding }=awaitembed({  model: openai.embedding('text-embedding-3-small'),  value:'sunny day at the beach',});
```


## [Embedding Many Values](#embedding-many-values)


When loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG), it is often useful to embed many values at once (batch embedding).

The AI SDK provides the [`embedMany`](/docs/reference/ai-sdk-core/embed-many) function for this purpose. Similar to `embed`, you can use it with embeddings models, e.g. `openai.embedding('text-embedding-3-large')` or `mistral.embedding('mistral-embed')`.

```
import{ openai }from'@ai-sdk/openai';import{ embedMany }from'ai';// 'embeddings' is an array of embedding objects (number[][]).// It is sorted in the same order as the input values.const{ embeddings }=awaitembedMany({  model: openai.embedding('text-embedding-3-small'),  values:['sunny day at the beach','rainy afternoon in the city','snowy night in the mountains',],});
```


## [Embedding Similarity](#embedding-similarity)


After embedding values, you can calculate the similarity between them using the [`cosineSimilarity`](/docs/reference/ai-sdk-core/cosine-similarity) function. This is useful to e.g. find similar words or phrases in a dataset. You can also rank and filter related items based on their similarity.

```
import{ openai }from'@ai-sdk/openai';import{ cosineSimilarity, embedMany }from'ai';const{ embeddings }=awaitembedMany({  model: openai.embedding('text-embedding-3-small'),  values:['sunny day at the beach','rainy afternoon in the city'],});console.log(`cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,);
```


## [Token Usage](#token-usage)


Many providers charge based on the number of tokens used to generate embeddings. Both `embed` and `embedMany` provide token usage information in the `usage` property of the result object:

```
import{ openai }from'@ai-sdk/openai';import{ embed }from'ai';const{ embedding, usage }=awaitembed({  model: openai.embedding('text-embedding-3-small'),  value:'sunny day at the beach',});console.log(usage);// { tokens: 10 }
```


## [Settings](#settings)



### [Retries](#retries)


Both `embed` and `embedMany` accept an optional `maxRetries` parameter of type `number` that you can use to set the maximum number of retries for the embedding process. It defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.

```
import{ openai }from'@ai-sdk/openai';import{ embed }from'ai';const{ embedding }=awaitembed({  model: openai.embedding('text-embedding-3-small'),  value:'sunny day at the beach',  maxRetries:0,// Disable retries});
```


### [Abort Signals and Timeouts](#abort-signals-and-timeouts)


Both `embed` and `embedMany` accept an optional `abortSignal` parameter of type [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal) that you can use to abort the embedding process or set a timeout.

```
import{ openai }from'@ai-sdk/openai';import{ embed }from'ai';const{ embedding }=awaitembed({  model: openai.embedding('text-embedding-3-small'),  value:'sunny day at the beach',  abortSignal:AbortSignal.timeout(1000),// Abort after 1 second});
```


### [Custom Headers](#custom-headers)


Both `embed` and `embedMany` accept an optional `headers` parameter of type `Record<string, string>` that you can use to add custom headers to the embedding request.

```
import{ openai }from'@ai-sdk/openai';import{ embed }from'ai';const{ embedding }=awaitembed({  model: openai.embedding('text-embedding-3-small'),  value:'sunny day at the beach',  headers:{'X-Custom-Header':'custom-value'},});
```


## [Embedding Providers & Models](#embedding-providers--models)


Several providers offer embedding models:

Provider

Model

Embedding Dimensions

[OpenAI](/providers/ai-sdk-providers/openai#embedding-models)

`text-embedding-3-large`

3072

[OpenAI](/providers/ai-sdk-providers/openai#embedding-models)

`text-embedding-3-small`

1536

[OpenAI](/providers/ai-sdk-providers/openai#embedding-models)

`text-embedding-ada-002`

1536

[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#embedding-models)

`text-embedding-004`

768

[Mistral](/providers/ai-sdk-providers/mistral#embedding-models)

`mistral-embed`

1024

[Cohere](/providers/ai-sdk-providers/cohere#embedding-models)

`embed-english-v3.0`

1024

[Cohere](/providers/ai-sdk-providers/cohere#embedding-models)

`embed-multilingual-v3.0`

1024

[Cohere](/providers/ai-sdk-providers/cohere#embedding-models)

`embed-english-light-v3.0`

384

[Cohere](/providers/ai-sdk-providers/cohere#embedding-models)

`embed-multilingual-light-v3.0`

384

[Cohere](/providers/ai-sdk-providers/cohere#embedding-models)

`embed-english-v2.0`

4096

[Cohere](/providers/ai-sdk-providers/cohere#embedding-models)

`embed-english-light-v2.0`

1024

[Cohere](/providers/ai-sdk-providers/cohere#embedding-models)

`embed-multilingual-v2.0`

768

[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#embedding-models)

`amazon.titan-embed-text-v1`

1024

[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#embedding-models)

`amazon.titan-embed-text-v2:0`

1024
```

### 77. `docs/ai-sdk-core/error-handling.md`

```markdown
# Error Handling


---
url: https://ai-sdk.dev/docs/ai-sdk-core/error-handling
description: Learn how to handle errors in the AI SDK Core
---


# [Error Handling](#error-handling)



## [Handling regular errors](#handling-regular-errors)


Regular errors are thrown and can be handled using the `try/catch` block.

```
import{ generateText }from'ai';try{const{ text }=awaitgenerateText({    model: yourModel,    prompt:'Write a vegetarian lasagna recipe for 4 people.',});}catch(error){// handle error}
```

See [Error Types](/docs/reference/ai-sdk-errors) for more information on the different types of errors that may be thrown.


## [Handling streaming errors (simple streams)](#handling-streaming-errors-simple-streams)


When errors occur during streams that do not support error chunks, the error is thrown as a regular error. You can handle these errors using the `try/catch` block.

```
import{ generateText }from'ai';try{const{ textStream }=streamText({    model: yourModel,    prompt:'Write a vegetarian lasagna recipe for 4 people.',});forawait(const textPart of textStream){    process.stdout.write(textPart);}}catch(error){// handle error}
```


## [Handling streaming errors (streaming with `error` support)](#handling-streaming-errors-streaming-with-error-support)


Full streams support error parts. You can handle those parts similar to other parts. It is recommended to also add a try-catch block for errors that happen outside of the streaming.

```
import{ generateText }from'ai';try{const{ fullStream }=streamText({    model: yourModel,    prompt:'Write a vegetarian lasagna recipe for 4 people.',});forawait(const part of fullStream){switch(part.type){// ... handle other part typescase'error':{const error = part.error;// handle errorbreak;}}}}catch(error){// handle error}
```
```

### 78. `docs/ai-sdk-core/generating-structured-data.md`

```markdown
# Generating Structured Data


---
url: https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data
description: Learn how to generate structured data with the AI SDK.
---


# [Generating Structured Data](#generating-structured-data)


While text generation can be useful, your use case will likely call for generating structured data. For example, you might want to extract information from text, classify data, or generate synthetic data.

Many language models are capable of generating structured data, often defined as using "JSON modes" or "tools". However, you need to manually provide schemas and then validate the generated data as LLMs can produce incorrect or incomplete structured data.

The AI SDK standardises structured object generation across model providers with the [`generateObject`](/docs/reference/ai-sdk-core/generate-object) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object) functions. You can use both functions with different output strategies, e.g. `array`, `object`, or `no-schema`, and with different generation modes, e.g. `auto`, `tool`, or `json`. You can use [Zod schemas](/docs/reference/ai-sdk-core/zod-schema), [Valibot](/docs/reference/ai-sdk-core/valibot-schema), or [JSON schemas](/docs/reference/ai-sdk-core/json-schema) to specify the shape of the data that you want, and the AI model will generate data that conforms to that structure.

You can pass Zod objects directly to the AI SDK functions or use the `zodSchema` helper function.


## [Generate Object](#generate-object)


The `generateObject` generates structured data from a prompt. The schema is also used to validate the generated data, ensuring type safety and correctness.

```
import{ generateObject }from'ai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model: yourModel,  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.object({ name: z.string(), amount: z.string()})),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});
```

See `generateObject` in action with [these examples](#more-examples)


### [Accessing response headers & body](#accessing-response-headers--body)


Sometimes you need access to the full response from the model provider, e.g. to access some provider-specific headers or body content.

You can access the raw response headers and body using the `response` property:

```
import{ generateText }from'ai';const result =awaitgenerateText({// ...});console.log(JSON.stringify(result.response.headers,null,2));console.log(JSON.stringify(result.response.body,null,2));
```


## [Stream Object](#stream-object)


Given the added complexity of returning structured data, model response time can be unacceptable for your interactive use case. With the [`streamObject`](/docs/reference/ai-sdk-core/stream-object) function, you can stream the model's response as it is generated.

```
import{ streamObject }from'ai';const{ partialObjectStream }=streamObject({// ...});// use partialObjectStream as an async iterableforawait(const partialObject of partialObjectStream){console.log(partialObject);}
```

You can use `streamObject` to stream generated UIs in combination with React Server Components (see [Generative UI](../ai-sdk-rsc))) or the [`useObject`](/docs/reference/ai-sdk-ui/use-object) hook.

See `streamObject` in action with [these examples](#more-examples)


### [`onError` callback](#onerror-callback)


`streamObject` immediately starts streaming. Errors become part of the stream and are not thrown to prevent e.g. servers from crashing.

To log errors, you can provide an `onError` callback that is triggered when an error occurs.

```
import{ streamObject }from'ai';const result =streamObject({// ...onError({ error }){console.error(error);// your error logging logic here},});
```


## [Output Strategy](#output-strategy)


You can use both functions with different output strategies, e.g. `array`, `object`, or `no-schema`.


### [Object](#object)


The default output strategy is `object`, which returns the generated data as an object. You don't need to specify the output strategy if you want to use the default.


### [Array](#array)


If you want to generate an array of objects, you can set the output strategy to `array`. When you use the `array` output strategy, the schema specifies the shape of an array element. With `streamObject`, you can also stream the generated array elements using `elementStream`.

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ z }from'zod';const{ elementStream }=streamObject({  model:openai('gpt-4-turbo'),  output:'array',  schema: z.object({    name: z.string(),class: z.string().describe('Character class, e.g. warrior, mage, or thief.'),    description: z.string(),}),  prompt:'Generate 3 hero descriptions for a fantasy role playing game.',});forawait(const hero of elementStream){console.log(hero);}
```


### [Enum](#enum)


If you want to generate a specific enum value, e.g. for classification tasks, you can set the output strategy to `enum` and provide a list of possible values in the `enum` parameter.

Enum output is only available with `generateObject`.

```
import{ generateObject }from'ai';const{ object }=awaitgenerateObject({  model: yourModel,  output:'enum',enum:['action','comedy','drama','horror','sci-fi'],  prompt:'Classify the genre of this movie plot: '+'"A group of astronauts travel through a wormhole in search of a '+'new habitable planet for humanity."',});
```


### [No Schema](#no-schema)


In some cases, you might not want to use a schema, for example when the data is a dynamic user request. You can use the `output` setting to set the output format to `no-schema` in those cases and omit the schema parameter.

```
import{ openai }from'@ai-sdk/openai';import{ generateObject }from'ai';const{ object }=awaitgenerateObject({  model:openai('gpt-4-turbo'),  output:'no-schema',  prompt:'Generate a lasagna recipe.',});
```


## [Generation Mode](#generation-mode)


While some models (like OpenAI) natively support object generation, others require alternative methods, like modified [tool calling](/docs/ai-sdk-core/tools-and-tool-calling). The `generateObject` function allows you to specify the method it will use to return structured data.

-   `auto`: The provider will choose the best mode for the model. This recommended mode is used by default.
-   `tool`: A tool with the JSON schema as parameters is provided and the provider is instructed to use it.
-   `json`: The response format is set to JSON when supported by the provider, e.g. via json modes or grammar-guided generation. If grammar-guided generation is not supported, the JSON schema and instructions to generate JSON that conforms to the schema are injected into the system prompt.

Please note that not every provider supports all generation modes. Some providers do not support object generation at all.


## [Schema Name and Description](#schema-name-and-description)


You can optionally specify a name and description for the schema. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.

```
import{ generateObject }from'ai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model: yourModel,  schemaName:'Recipe',  schemaDescription:'A recipe for a dish.',  schema: z.object({    name: z.string(),    ingredients: z.array(z.object({ name: z.string(), amount: z.string()})),    steps: z.array(z.string()),}),  prompt:'Generate a lasagna recipe.',});
```


## [Error Handling](#error-handling)


When `generateObject` cannot generate a valid object, it throws a [`AI_NoObjectGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-object-generated-error).

This error occurs when the AI provider fails to generate a parsable object that conforms to the schema. It can arise due to the following reasons:

-   The model failed to generate a response.
-   The model generated a response that could not be parsed.
-   The model generated a response that could not be validated against the schema.

The error preserves the following information to help you log the issue:

-   `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.
-   `response`: Metadata about the language model response, including response id, timestamp, and model.
-   `usage`: Request token usage.
-   `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.

```
import{ generateObject,NoObjectGeneratedError}from'ai';try{awaitgenerateObject({ model, schema, prompt });}catch(error){if(NoObjectGeneratedError.isInstance(error)){console.log('NoObjectGeneratedError');console.log('Cause:', error.cause);console.log('Text:', error.text);console.log('Response:', error.response);console.log('Usage:', error.usage);}}
```


## [Repairing Invalid or Malformed JSON](#repairing-invalid-or-malformed-json)


The `repairText` function is experimental and may change in the future.

Sometimes the model will generate invalid or malformed JSON. You can use the `repairText` function to attempt to repair the JSON.

It receives the error, either a `JSONParseError` or a `TypeValidationError`, and the text that was generated by the model. You can then attempt to repair the text and return the repaired text.

```
import{ generateObject }from'ai';const{ object }=awaitgenerateObject({  model,  schema,  prompt,experimental_repairText:async({ text, error })=>{// example: add a closing brace to the textreturn text +'}';},});
```


## [Structured outputs with `generateText` and `streamText`](#structured-outputs-with-generatetext-and-streamtext)


You can generate structured data with `generateText` and `streamText` by using the `experimental_output` setting.

Some models, e.g. those by OpenAI, support structured outputs and tool calling at the same time. This is only possible with `generateText` and `streamText`.

Structured output generation with `generateText` and `streamText` is experimental and may change in the future.


### [`generateText`](#generatetext)


```
// experimental_output is a structured object that matches the schema:const{ experimental_output }=awaitgenerateText({// ...  experimental_output:Output.object({    schema: z.object({      name: z.string(),      age: z.number().nullable().describe('Age of the person.'),      contact: z.object({type: z.literal('email'),        value: z.string(),}),      occupation: z.object({type: z.literal('employed'),        company: z.string(),        position: z.string(),}),}),}),  prompt:'Generate an example person for testing.',});
```


### [`streamText`](#streamtext)


```
// experimental_partialOutputStream contains generated partial objects:const{ experimental_partialOutputStream }=awaitstreamText({// ...  experimental_output:Output.object({    schema: z.object({      name: z.string(),      age: z.number().nullable().describe('Age of the person.'),      contact: z.object({type: z.literal('email'),        value: z.string(),}),      occupation: z.object({type: z.literal('employed'),        company: z.string(),        position: z.string(),}),}),}),  prompt:'Generate an example person for testing.',});
```


## [More Examples](#more-examples)


You can see `generateObject` and `streamObject` in action using various frameworks in the following examples:


### [`generateObject`](#generateobject)


[

Learn to generate objects in Node.js

](/examples/node/generating-structured-data/generate-object)[

Learn to generate objects in Next.js with Route Handlers (AI SDK UI)

](/examples/next-pages/basics/generating-object)[

Learn to generate objects in Next.js with Server Actions (AI SDK RSC)

](/examples/next-app/basics/generating-object)


### [`streamObject`](#streamobject)


[

Learn to stream objects in Node.js

](/examples/node/streaming-structured-data/stream-object)[

Learn to stream objects in Next.js with Route Handlers (AI SDK UI)

](/examples/next-pages/basics/streaming-object-generation)[

Learn to stream objects in Next.js with Server Actions (AI SDK RSC)

](/examples/next-app/basics/streaming-object-generation)
```

### 79. `docs/ai-sdk-core/generating-text.md`

```markdown
# Generating and Streaming Text


---
url: https://ai-sdk.dev/docs/ai-sdk-core/generating-text
description: Learn how to generate text with the AI SDK.
---


# [Generating and Streaming Text](#generating-and-streaming-text)


Large language models (LLMs) can generate text in response to a prompt, which can contain instructions and information to process. For example, you can ask a model to come up with a recipe, draft an email, or summarize a document.

The AI SDK Core provides two functions to generate text and stream it from LLMs:

-   [`generateText`](#generatetext): Generates text for a given prompt and model.
-   [`streamText`](#streamtext): Streams text from a given prompt and model.

Advanced LLM features such as [tool calling](./tools-and-tool-calling) and [structured data generation](./generating-structured-data) are built on top of text generation.


## [`generateText`](#generatetext)


You can generate text using the [`generateText`](/docs/reference/ai-sdk-core/generate-text) function. This function is ideal for non-interactive use cases where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.

```
import{ generateText }from'ai';const{ text }=awaitgenerateText({  model: yourModel,  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

You can use more [advanced prompts](./prompts) to generate text with more complex instructions and content:

```
import{ generateText }from'ai';const{ text }=awaitgenerateText({  model: yourModel,  system:'You are a professional writer. '+'You write simple, clear, and concise content.',  prompt:`Summarize the following article in 3-5 sentences: ${article}`,});
```

The result object of `generateText` contains several promises that resolve when all required data is available:

-   `result.text`: The generated text.
-   `result.reasoning`: The reasoning text of the model (only available for some models).
-   `result.sources`: Sources that have been used as input to generate the response (only available for some models).
-   `result.finishReason`: The reason the model finished generating text.
-   `result.usage`: The usage of the model during text generation.


### [Accessing response headers & body](#accessing-response-headers--body)


Sometimes you need access to the full response from the model provider, e.g. to access some provider-specific headers or body content.

You can access the raw response headers and body using the `response` property:

```
import{ generateText }from'ai';const result =awaitgenerateText({// ...});console.log(JSON.stringify(result.response.headers,null,2));console.log(JSON.stringify(result.response.body,null,2));
```


## [`streamText`](#streamtext)


Depending on your model and prompt, it can take a large language model (LLM) up to a minute to finish generating its response. This delay can be unacceptable for interactive use cases such as chatbots or real-time applications, where users expect immediate responses.

AI SDK Core provides the [`streamText`](/docs/reference/ai-sdk-core/stream-text) function which simplifies streaming text from LLMs:

```
import{ streamText }from'ai';const result =streamText({  model: yourModel,  prompt:'Invent a new holiday and describe its traditions.',});// example: use textStream as an async iterableforawait(const textPart of result.textStream){console.log(textPart);}
```

`result.textStream` is both a `ReadableStream` and an `AsyncIterable`.

`streamText` immediately starts streaming and suppresses errors to prevent server crashes. Use the `onError` callback to log errors.

You can use `streamText` on its own or in combination with [AI SDK UI](/examples/next-pages/basics/streaming-text-generation) and [AI SDK RSC](/examples/next-app/basics/streaming-text-generation). The result object contains several helper functions to make the integration into [AI SDK UI](/docs/ai-sdk-ui) easier:

-   `result.toDataStreamResponse()`: Creates a data stream HTTP response (with tool calls etc.) that can be used in a Next.js App Router API route.
-   `result.pipeDataStreamToResponse()`: Writes data stream delta output to a Node.js response-like object.
-   `result.toTextStreamResponse()`: Creates a simple text stream HTTP response.
-   `result.pipeTextStreamToResponse()`: Writes text delta output to a Node.js response-like object.

`streamText` is using backpressure and only generates tokens as they are requested. You need to consume the stream in order for it to finish.

It also provides several promises that resolve when the stream is finished:

-   `result.text`: The generated text.
-   `result.reasoning`: The reasoning text of the model (only available for some models).
-   `result.sources`: Sources that have been used as input to generate the response (only available for some models).
-   `result.finishReason`: The reason the model finished generating text.
-   `result.usage`: The usage of the model during text generation.


### [`onError` callback](#onerror-callback)


`streamText` immediately starts streaming to enable sending data without waiting for the model. Errors become part of the stream and are not thrown to prevent e.g. servers from crashing.

To log errors, you can provide an `onError` callback that is triggered when an error occurs.

```
import{ streamText }from'ai';const result =streamText({  model: yourModel,  prompt:'Invent a new holiday and describe its traditions.',onError({ error }){console.error(error);// your error logging logic here},});
```


### [`onChunk` callback](#onchunk-callback)


When using `streamText`, you can provide an `onChunk` callback that is triggered for each chunk of the stream.

It receives the following chunk types:

-   `text-delta`
-   `reasoning`
-   `source`
-   `tool-call`
-   `tool-result`
-   `tool-call-streaming-start` (when `toolCallStreaming` is enabled)
-   `tool-call-delta` (when `toolCallStreaming` is enabled)

```
import{ streamText }from'ai';const result =streamText({  model: yourModel,  prompt:'Invent a new holiday and describe its traditions.',onChunk({ chunk }){// implement your own logic here, e.g.:if(chunk.type==='text-delta'){console.log(chunk.text);}},});
```


### [`onFinish` callback](#onfinish-callback)


When using `streamText`, you can provide an `onFinish` callback that is triggered when the stream is finished ( [API Reference](/docs/reference/ai-sdk-core/stream-text#on-finish) ). It contains the text, usage information, finish reason, messages, and more:

```
import{ streamText }from'ai';const result =streamText({  model: yourModel,  prompt:'Invent a new holiday and describe its traditions.',onFinish({ text, finishReason, usage, response }){// your own logic, e.g. for saving the chat history or recording usageconst messages = response.messages;// messages that were generated},});
```


### [`fullStream` property](#fullstream-property)


You can read a stream with all events using the `fullStream` property. This can be useful if you want to implement your own UI or handle the stream in a different way. Here is an example of how to use the `fullStream` property:

```
import{ streamText }from'ai';import{ z }from'zod';const result =streamText({  model: yourModel,  tools:{    cityAttractions:{      parameters: z.object({ city: z.string()}),execute:async({ city })=>({        attractions:['attraction1','attraction2','attraction3'],}),},},  prompt:'What are some San Francisco tourist attractions?',});forawait(const part of result.fullStream){switch(part.type){case'text-delta':{// handle text delta herebreak;}case'reasoning':{// handle reasoning herebreak;}case'source':{// handle source herebreak;}case'tool-call':{switch(part.toolName){case'cityAttractions':{// handle tool call herebreak;}}break;}case'tool-result':{switch(part.toolName){case'cityAttractions':{// handle tool result herebreak;}}break;}case'finish':{// handle finish herebreak;}case'error':{// handle error herebreak;}}}
```


### [Stream transformation](#stream-transformation)


You can use the `experimental_transform` option to transform the stream. This is useful for e.g. filtering, changing, or smoothing the text stream.

The transformations are applied before the callbacks are invoked and the promises are resolved. If you e.g. have a transformation that changes all text to uppercase, the `onFinish` callback will receive the transformed text.


#### [Smoothing streams](#smoothing-streams)


The AI SDK Core provides a [`smoothStream` function](/docs/reference/ai-sdk-core/smooth-stream) that can be used to smooth out text streaming.

```
import{ smoothStream, streamText }from'ai';const result =streamText({  model,  prompt,  experimental_transform:smoothStream(),});
```


#### [Custom transformations](#custom-transformations)


You can also implement your own custom transformations. The transformation function receives the tools that are available to the model, and returns a function that is used to transform the stream. Tools can either be generic or limited to the tools that you are using.

Here is an example of how to implement a custom transformation that converts all text to uppercase:

```
const upperCaseTransform =<TOOLSextendsToolSet>()=>(options:{ tools:TOOLS;stopStream:()=>void})=>newTransformStream<TextStreamPart<TOOLS>,TextStreamPart<TOOLS>>({transform(chunk, controller){        controller.enqueue(// for text-delta chunks, convert the text to uppercase:          chunk.type==='text-delta'?{...chunk, textDelta: chunk.textDelta.toUpperCase()}: chunk,);},});
```

You can also stop the stream using the `stopStream` function. This is e.g. useful if you want to stop the stream when model guardrails are violated, e.g. by generating inappropriate content.

When you invoke `stopStream`, it is important to simulate the `step-finish` and `finish` events to guarantee that a well-formed stream is returned and all callbacks are invoked.

```
const stopWordTransform =<TOOLSextendsToolSet>()=>({ stopStream }:{stopStream:()=>void})=>newTransformStream<TextStreamPart<TOOLS>,TextStreamPart<TOOLS>>({// note: this is a simplified transformation for testing;// in a real-world version more there would need to be// stream buffering and scanning to correctly emit prior text// and to detect all STOP occurrences.transform(chunk, controller){if(chunk.type!=='text-delta'){          controller.enqueue(chunk);return;}if(chunk.textDelta.includes('STOP')){// stop the streamstopStream();// simulate the step-finish event          controller.enqueue({type:'step-finish',            finishReason:'stop',            logprobs:undefined,            usage:{              completionTokens:NaN,              promptTokens:NaN,              totalTokens:NaN,},            request:{},            response:{              id:'response-id',              modelId:'mock-model-id',              timestamp:newDate(0),},            warnings:[],            isContinued:false,});// simulate the finish event          controller.enqueue({type:'finish',            finishReason:'stop',            logprobs:undefined,            usage:{              completionTokens:NaN,              promptTokens:NaN,              totalTokens:NaN,},            response:{              id:'response-id',              modelId:'mock-model-id',              timestamp:newDate(0),},});return;}        controller.enqueue(chunk);},});
```


#### [Multiple transformations](#multiple-transformations)


You can also provide multiple transformations. They are applied in the order they are provided.

```
const result =streamText({  model,  prompt,  experimental_transform:[firstTransform, secondTransform],});
```


## [Sources](#sources)


Some providers such as [Perplexity](/providers/ai-sdk-providers/perplexity#sources) and [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.

Currently sources are limited to web pages that ground the response. You can access them using the `sources` property of the result.

Each `url` source contains the following properties:

-   `id`: The ID of the source.
-   `url`: The URL of the source.
-   `title`: The optional title of the source.
-   `providerMetadata`: Provider metadata for the source.

When you use `generateText`, you can access the sources using the `sources` property:

```
const result =awaitgenerateText({  model:google('gemini-2.0-flash-exp',{ useSearchGrounding:true}),  prompt:'List the top 5 San Francisco news from the past week.',});for(const source of result.sources){if(source.sourceType==='url'){console.log('ID:', source.id);console.log('Title:', source.title);console.log('URL:', source.url);console.log('Provider metadata:', source.providerMetadata);console.log();}}
```

When you use `streamText`, you can access the sources using the `fullStream` property:

```
const result =streamText({  model:google('gemini-2.0-flash-exp',{ useSearchGrounding:true}),  prompt:'List the top 5 San Francisco news from the past week.',});forawait(const part of result.fullStream){if(part.type==='source'&& part.source.sourceType ==='url'){console.log('ID:', part.source.id);console.log('Title:', part.source.title);console.log('URL:', part.source.url);console.log('Provider metadata:', part.source.providerMetadata);console.log();}}
```

The sources are also available in the `result.sources` promise.


## [Generating Long Text](#generating-long-text)


Most language models have an output limit that is much shorter than their context window. This means that you cannot generate long text in one go, but it is possible to add responses back to the input and continue generating to create longer text.

`generateText` and `streamText` support such continuations for long text generation using the experimental `continueSteps` setting:

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const{  text,// combined text  usage,// combined usage of all steps}=awaitgenerateText({  model:openai('gpt-4o'),// 4096 output tokens  maxSteps:5,// enable multi-step calls  experimental_continueSteps:true,  prompt:'Write a book about Roman history, '+'from the founding of the city of Rome '+'to the fall of the Western Roman Empire. '+'Each chapter MUST HAVE at least 1000 words.',});
```

When `experimental_continueSteps` is enabled, only full words are streamed in `streamText`, and both `generateText` and `streamText` might drop the trailing tokens of some calls to prevent whitespace issues.

Some models might not always stop correctly on their own and keep generating until `maxSteps` is reached. You can hint the model to stop by e.g. using a system message such as "Stop when sufficient information was provided."


## [Examples](#examples)


You can see `generateText` and `streamText` in action using various frameworks in the following examples:


### [`generateText`](#generatetext-1)


[

Learn to generate text in Node.js

](/examples/node/generating-text/generate-text)[

Learn to generate text in Next.js with Route Handlers (AI SDK UI)

](/examples/next-pages/basics/generating-text)[

Learn to generate text in Next.js with Server Actions (AI SDK RSC)

](/examples/next-app/basics/generating-text)


### [`streamText`](#streamtext-1)


[

Learn to stream text in Node.js

](/examples/node/generating-text/stream-text)[

Learn to stream text in Next.js with Route Handlers (AI SDK UI)

](/examples/next-pages/basics/streaming-text-generation)[

Learn to stream text in Next.js with Server Actions (AI SDK RSC)

](/examples/next-app/basics/streaming-text-generation)
```

### 80. `docs/ai-sdk-core/image-generation.md`

```markdown
# Image Generation


---
url: https://ai-sdk.dev/docs/ai-sdk-core/image-generation
description: Learn how to generate images with the AI SDK.
---


# [Image Generation](#image-generation)


Image generation is an experimental feature.

The AI SDK provides the [`generateImage`](/docs/reference/ai-sdk-core/generate-image) function to generate images based on a given prompt using an image model.

```
import{ experimental_generateImage as generateImage }from'ai';import{ openai }from'@ai-sdk/openai';const{ image }=awaitgenerateImage({  model: openai.image('dall-e-3'),  prompt:'Santa Claus driving a Cadillac',});
```

You can access the image data using the `base64` or `uint8Array` properties:

```
const base64 = image.base64;// base64 image dataconst uint8Array = image.uint8Array;// Uint8Array image data
```


## [Settings](#settings)



### [Size and Aspect Ratio](#size-and-aspect-ratio)


Depending on the model, you can either specify the size or the aspect ratio.


##### [Size](#size)


The size is specified as a string in the format `{width}x{height}`. Models only support a few sizes, and the supported sizes are different for each model and provider.

```
import{ experimental_generateImage as generateImage }from'ai';import{ openai }from'@ai-sdk/openai';const{ image }=awaitgenerateImage({  model: openai.image('dall-e-3'),  prompt:'Santa Claus driving a Cadillac',  size:'1024x1024',});
```


##### [Aspect Ratio](#aspect-ratio)


The aspect ratio is specified as a string in the format `{width}:{height}`. Models only support a few aspect ratios, and the supported aspect ratios are different for each model and provider.

```
import{ experimental_generateImage as generateImage }from'ai';import{ vertex }from'@ai-sdk/google-vertex';const{ image }=awaitgenerateImage({  model: vertex.image('imagen-3.0-generate-002'),  prompt:'Santa Claus driving a Cadillac',  aspectRatio:'16:9',});
```


### [Generating Multiple Images](#generating-multiple-images)


`generateImage` also supports generating multiple images at once:

```
import{ experimental_generateImage as generateImage }from'ai';import{ openai }from'@ai-sdk/openai';const{ images }=awaitgenerateImage({  model: openai.image('dall-e-2'),  prompt:'Santa Claus driving a Cadillac',  n:4,// number of images to generate});
```

`generateImage` will automatically call the model as often as needed (in parallel) to generate the requested number of images.

Each image model has an internal limit on how many images it can generate in a single API call. The AI SDK manages this automatically by batching requests appropriately when you request multiple images using the `n` parameter. By default, the SDK uses provider-documented limits (for example, DALL-E 3 can only generate 1 image per call, while DALL-E 2 supports up to 10).

If needed, you can override this behavior using the `maxImagesPerCall` setting when configuring your model. This is particularly useful when working with new or custom models where the default batch size might not be optimal:

```
const model = openai.image('dall-e-2',{  maxImagesPerCall:5,// Override the default batch size});const{ images }=awaitgenerateImage({  model,  prompt:'Santa Claus driving a Cadillac',  n:10,// Will make 2 calls of 5 images each});
```


### [Providing a Seed](#providing-a-seed)


You can provide a seed to the `generateImage` function to control the output of the image generation process. If supported by the model, the same seed will always produce the same image.

```
import{ experimental_generateImage as generateImage }from'ai';import{ openai }from'@ai-sdk/openai';const{ image }=awaitgenerateImage({  model: openai.image('dall-e-3'),  prompt:'Santa Claus driving a Cadillac',  seed:1234567890,});
```


### [Provider-specific Settings](#provider-specific-settings)


Image models often have provider- or even model-specific settings. You can pass such settings to the `generateImage` function using the `providerOptions` parameter. The options for the provider (`openai` in the example below) become request body properties.

```
import{ experimental_generateImage as generateImage }from'ai';import{ openai }from'@ai-sdk/openai';const{ image }=awaitgenerateImage({  model: openai.image('dall-e-3'),  prompt:'Santa Claus driving a Cadillac',  size:'1024x1024',  providerOptions:{    openai:{ style:'vivid', quality:'hd'},},});
```


### [Abort Signals and Timeouts](#abort-signals-and-timeouts)


`generateImage` accepts an optional `abortSignal` parameter of type [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal) that you can use to abort the image generation process or set a timeout.

```
import{ openai }from'@ai-sdk/openai';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: openai.image('dall-e-3'),  prompt:'Santa Claus driving a Cadillac',  abortSignal:AbortSignal.timeout(1000),// Abort after 1 second});
```


### [Custom Headers](#custom-headers)


`generateImage` accepts an optional `headers` parameter of type `Record<string, string>` that you can use to add custom headers to the image generation request.

```
import{ openai }from'@ai-sdk/openai';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: openai.image('dall-e-3'),  value:'sunny day at the beach',  headers:{'X-Custom-Header':'custom-value'},});
```


### [Warnings](#warnings)


If the model returns warnings, e.g. for unsupported parameters, they will be available in the `warnings` property of the response.

```
const{ image, warnings }=awaitgenerateImage({  model: openai.image('dall-e-3'),  prompt:'Santa Claus driving a Cadillac',});
```


### [Error Handling](#error-handling)


When `generateImage` cannot generate a valid image, it throws a [`AI_NoImageGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-image-generated-error).

This error occurs when the AI provider fails to generate an image. It can arise due to the following reasons:

-   The model failed to generate a response
-   The model generated a response that could not be parsed

The error preserves the following information to help you log the issue:

-   `responses`: Metadata about the image model responses, including timestamp, model, and headers.
-   `cause`: The cause of the error. You can use this for more detailed error handling

```
import{ generateImage,NoImageGeneratedError}from'ai';try{awaitgenerateImage({ model, prompt });}catch(error){if(NoImageGeneratedError.isInstance(error)){console.log('NoImageGeneratedError');console.log('Cause:', error.cause);console.log('Responses:', error.responses);}}
```


## [Generating Images with Language Models](#generating-images-with-language-models)


Some language models such as Google `gemini-2.0-flash-exp` support multi-modal outputs including images. With such models, you can access the generated images using the `files` property of the response.

```
import{ google }from'@ai-sdk/google';import{ generateText }from'ai';const result =awaitgenerateText({  model:google('gemini-2.0-flash-exp'),  providerOptions:{    google:{ responseModalities:['TEXT','IMAGE']},},  prompt:'Generate an image of a comic cat',});for(const file of result.files){if(file.mimeType.startsWith('image/')){// The file object provides multiple data formats:// Access images as base64 string, Uint8Array binary data, or check type// - file.base64: string (data URL format)// - file.uint8Array: Uint8Array (binary data)// - file.mimeType: string (e.g. "image/png")}}
```


## [Image Models](#image-models)


Provider

Model

Support sizes (`width x height`) or aspect ratios (`width : height`)

[xAI Grok](/providers/ai-sdk-providers/xai#image-models)

`grok-2-image`

1024x768 (default)

[OpenAI](/providers/ai-sdk-providers/openai#image-models)

`gpt-image-1`

1024x1024, 1536x1024, 1024x1536

[OpenAI](/providers/ai-sdk-providers/openai#image-models)

`dall-e-3`

1024x1024, 1792x1024, 1024x1792

[OpenAI](/providers/ai-sdk-providers/openai#image-models)

`dall-e-2`

256x256, 512x512, 1024x1024

[Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#image-models)

`amazon.nova-canvas-v1:0`

320-4096 (multiples of 16), 1:4 to 4:1, max 4.2M pixels

[Fal](/providers/ai-sdk-providers/fal#image-models)

`fal-ai/flux/dev`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Fal](/providers/ai-sdk-providers/fal#image-models)

`fal-ai/flux-lora`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Fal](/providers/ai-sdk-providers/fal#image-models)

`fal-ai/fast-sdxl`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Fal](/providers/ai-sdk-providers/fal#image-models)

`fal-ai/flux-pro/v1.1-ultra`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Fal](/providers/ai-sdk-providers/fal#image-models)

`fal-ai/ideogram/v2`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Fal](/providers/ai-sdk-providers/fal#image-models)

`fal-ai/recraft-v3`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Fal](/providers/ai-sdk-providers/fal#image-models)

`fal-ai/stable-diffusion-3.5-large`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Fal](/providers/ai-sdk-providers/fal#image-models)

`fal-ai/hyper-sdxl`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)

`stabilityai/sd3.5`

1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21

[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)

`black-forest-labs/FLUX-1.1-pro`

256-1440 (multiples of 32)

[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)

`black-forest-labs/FLUX-1-schnell`

256-1440 (multiples of 32)

[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)

`black-forest-labs/FLUX-1-dev`

256-1440 (multiples of 32)

[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)

`black-forest-labs/FLUX-pro`

256-1440 (multiples of 32)

[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)

`stabilityai/sd3.5-medium`

1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21

[DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)

`stabilityai/sdxl-turbo`

1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21

[Replicate](/providers/ai-sdk-providers/replicate)

`black-forest-labs/flux-schnell`

1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9

[Replicate](/providers/ai-sdk-providers/replicate)

`recraft-ai/recraft-v3`

1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024

[Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models)

`imagen-3.0-generate-002`

1:1, 3:4, 4:3, 9:16, 16:9

[Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models)

`imagen-3.0-fast-generate-001`

1:1, 3:4, 4:3, 9:16, 16:9

[Fireworks](/providers/ai-sdk-providers/fireworks#image-models)

`accounts/fireworks/models/flux-1-dev-fp8`

1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9

[Fireworks](/providers/ai-sdk-providers/fireworks#image-models)

`accounts/fireworks/models/flux-1-schnell-fp8`

1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9

[Fireworks](/providers/ai-sdk-providers/fireworks#image-models)

`accounts/fireworks/models/playground-v2-5-1024px-aesthetic`

640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640

[Fireworks](/providers/ai-sdk-providers/fireworks#image-models)

`accounts/fireworks/models/japanese-stable-diffusion-xl`

640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640

[Fireworks](/providers/ai-sdk-providers/fireworks#image-models)

`accounts/fireworks/models/playground-v2-1024px-aesthetic`

640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640

[Fireworks](/providers/ai-sdk-providers/fireworks#image-models)

`accounts/fireworks/models/SSD-1B`

640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640

[Fireworks](/providers/ai-sdk-providers/fireworks#image-models)

`accounts/fireworks/models/stable-diffusion-xl-1024-v1-0`

640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640

[Luma](/providers/ai-sdk-providers/luma#image-models)

`photon-1`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Luma](/providers/ai-sdk-providers/luma#image-models)

`photon-flash-1`

1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`stabilityai/stable-diffusion-xl-base-1.0`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1-dev`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1-dev-lora`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1-schnell`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1-canny`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1-depth`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1-redux`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1.1-pro`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1-pro`

512x512, 768x768, 1024x1024

[Together.ai](/providers/ai-sdk-providers/togetherai#image-models)

`black-forest-labs/FLUX.1-schnell-Free`

512x512, 768x768, 1024x1024

Above are a small subset of the image models supported by the AI SDK providers. For more, see the respective provider documentation.
```

### 81. `docs/ai-sdk-core/middleware.md`

```markdown
# Language Model Middleware


---
url: https://ai-sdk.dev/docs/ai-sdk-core/middleware
description: Learn how to use middleware to enhance the behavior of language models
---


# [Language Model Middleware](#language-model-middleware)


Language model middleware is a way to enhance the behavior of language models by intercepting and modifying the calls to the language model.

It can be used to add features like guardrails, RAG, caching, and logging in a language model agnostic way. Such middleware can be developed and distributed independently from the language models that they are applied to.


## [Using Language Model Middleware](#using-language-model-middleware)


You can use language model middleware with the `wrapLanguageModel` function. It takes a language model and a language model middleware and returns a new language model that incorporates the middleware.

```
import{ wrapLanguageModel }from'ai';const wrappedLanguageModel =wrapLanguageModel({  model: yourModel,  middleware: yourLanguageModelMiddleware,});
```

The wrapped language model can be used just like any other language model, e.g. in `streamText`:

```
const result =streamText({  model: wrappedLanguageModel,  prompt:'What cities are in the United States?',});
```


## [Multiple middlewares](#multiple-middlewares)


You can provide multiple middlewares to the `wrapLanguageModel` function. The middlewares will be applied in the order they are provided.

```
const wrappedLanguageModel =wrapLanguageModel({  model: yourModel,  middleware:[firstMiddleware, secondMiddleware],});// applied as: firstMiddleware(secondMiddleware(yourModel))
```


## [Built-in Middleware](#built-in-middleware)


The AI SDK comes with several built-in middlewares that you can use to configure language models:

-   `extractReasoningMiddleware`: Extracts reasoning information from the generated text and exposes it as a `reasoning` property on the result.
-   `simulateStreamingMiddleware`: Simulates streaming behavior with responses from non-streaming language models.
-   `defaultSettingsMiddleware`: Applies default settings to a language model.


### [Extract Reasoning](#extract-reasoning)


Some providers and models expose reasoning information in the generated text using special tags, e.g. <think> and </think>.

The `extractReasoningMiddleware` function can be used to extract this reasoning information and expose it as a `reasoning` property on the result.

```
import{ wrapLanguageModel, extractReasoningMiddleware }from'ai';const model =wrapLanguageModel({  model: yourModel,  middleware:extractReasoningMiddleware({ tagName:'think'}),});
```

You can then use that enhanced model in functions like `generateText` and `streamText`.

The `extractReasoningMiddleware` function also includes a `startWithReasoning` option. When set to `true`, the reasoning tag will be prepended to the generated text. This is useful for models that do not include the reasoning tag at the beginning of the response. For more details, see the [DeepSeek R1 guide](/docs/guides/r1#deepseek-r1-middleware).


### [Simulate Streaming](#simulate-streaming)


The `simulateStreamingMiddleware` function can be used to simulate streaming behavior with responses from non-streaming language models. This is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.

```
import{ wrapLanguageModel, simulateStreamingMiddleware }from'ai';const model =wrapLanguageModel({  model: yourModel,  middleware:simulateStreamingMiddleware(),});
```


### [Default Settings](#default-settings)


The `defaultSettingsMiddleware` function can be used to apply default settings to a language model.

```
import{ wrapLanguageModel, defaultSettingsMiddleware }from'ai';const model =wrapLanguageModel({  model: yourModel,  middleware:defaultSettingsMiddleware({    settings:{      temperature:0.5,      maxTokens:800,// note: use providerMetadata instead of providerOptions here:      providerMetadata:{ openai:{ store:false}},},}),});
```


## [Implementing Language Model Middleware](#implementing-language-model-middleware)


Implementing language model middleware is advanced functionality and requires a solid understanding of the [language model specification](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v1/language-model-v1.ts).

You can implement any of the following three function to modify the behavior of the language model:

1.  `transformParams`: Transforms the parameters before they are passed to the language model, for both `doGenerate` and `doStream`.
2.  `wrapGenerate`: Wraps the `doGenerate` method of the [language model](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v1/language-model-v1.ts). You can modify the parameters, call the language model, and modify the result.
3.  `wrapStream`: Wraps the `doStream` method of the [language model](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v1/language-model-v1.ts). You can modify the parameters, call the language model, and modify the result.

Here are some examples of how to implement language model middleware:


## [Examples](#examples)


These examples are not meant to be used in production. They are just to show how you can use middleware to enhance the behavior of language models.


### [Logging](#logging)


This example shows how to log the parameters and generated text of a language model call.

```
importtype{LanguageModelV1Middleware,LanguageModelV1StreamPart}from'ai';exportconst yourLogMiddleware:LanguageModelV1Middleware={wrapGenerate:async({ doGenerate, params })=>{console.log('doGenerate called');console.log(`params: ${JSON.stringify(params,null,2)}`);const result =awaitdoGenerate();console.log('doGenerate finished');console.log(`generated text: ${result.text}`);return result;},wrapStream:async({ doStream, params })=>{console.log('doStream called');console.log(`params: ${JSON.stringify(params,null,2)}`);const{ stream,...rest }=awaitdoStream();let generatedText ='';const transformStream =newTransformStream<LanguageModelV1StreamPart,LanguageModelV1StreamPart>({transform(chunk, controller){if(chunk.type==='text-delta'){          generatedText += chunk.textDelta;}        controller.enqueue(chunk);},flush(){console.log('doStream finished');console.log(`generated text: ${generatedText}`);},});return{      stream: stream.pipeThrough(transformStream),...rest,};},};
```


### [Caching](#caching)


This example shows how to build a simple cache for the generated text of a language model call.

```
importtype{LanguageModelV1Middleware}from'ai';const cache =newMap<string,any>();exportconst yourCacheMiddleware:LanguageModelV1Middleware={wrapGenerate:async({ doGenerate, params })=>{const cacheKey =JSON.stringify(params);if(cache.has(cacheKey)){return cache.get(cacheKey);}const result =awaitdoGenerate();    cache.set(cacheKey, result);return result;},// here you would implement the caching logic for streaming};
```


### [Retrieval Augmented Generation (RAG)](#retrieval-augmented-generation-rag)


This example shows how to use RAG as middleware.

Helper functions like `getLastUserMessageText` and `findSources` are not part of the AI SDK. They are just used in this example to illustrate the concept of RAG.

```
importtype{LanguageModelV1Middleware}from'ai';exportconst yourRagMiddleware:LanguageModelV1Middleware={transformParams:async({ params })=>{const lastUserMessageText =getLastUserMessageText({      prompt: params.prompt,});if(lastUserMessageText ==null){return params;// do not use RAG (send unmodified parameters)}const instruction ='Use the following information to answer the question:\n'+findSources({ text: lastUserMessageText }).map(chunk =>JSON.stringify(chunk)).join('\n');returnaddToLastUserMessage({ params, text: instruction });},};
```


### [Guardrails](#guardrails)


Guard rails are a way to ensure that the generated text of a language model call is safe and appropriate. This example shows how to use guardrails as middleware.

```
importtype{LanguageModelV1Middleware}from'ai';exportconst yourGuardrailMiddleware:LanguageModelV1Middleware={wrapGenerate:async({ doGenerate })=>{const{ text,...rest }=awaitdoGenerate();// filtering approach, e.g. for PII or other sensitive information:const cleanedText = text?.replace(/badword/g,'<REDACTED>');return{ text: cleanedText,...rest };},// here you would implement the guardrail logic for streaming// Note: streaming guardrails are difficult to implement, because// you do not know the full content of the stream until it's finished.};
```


## [Configuring Per Request Custom Metadata](#configuring-per-request-custom-metadata)


To send and access custom metadata in Middleware, you can use `providerOptions`. This is useful when building logging middleware where you want to pass additional context like user IDs, timestamps, or other contextual data that can help with tracking and debugging.

```
import{ openai }from'@ai-sdk/openai';import{ generateText, wrapLanguageModel,LanguageModelV1Middleware}from'ai';exportconst yourLogMiddleware:LanguageModelV1Middleware={wrapGenerate:async({ doGenerate, params })=>{console.log('METADATA', params?.providerMetadata?.yourLogMiddleware);const result =awaitdoGenerate();return result;},};const{ text }=awaitgenerateText({  model:wrapLanguageModel({    model:openai('gpt-4o'),    middleware: yourLogMiddleware,}),  prompt:'Invent a new holiday and describe its traditions.',  providerOptions:{    yourLogMiddleware:{      hello:'world',},},});console.log(text);
```
```

### 82. `docs/ai-sdk-core/overview.md`

```markdown
# AI SDK Core


---
url: https://ai-sdk.dev/docs/ai-sdk-core/overview
description: An overview of AI SDK Core.
---


# [AI SDK Core](#ai-sdk-core)


Large Language Models (LLMs) are advanced programs that can understand, create, and engage with human language on a large scale. They are trained on vast amounts of written material to recognize patterns in language and predict what might come next in a given piece of text.

AI SDK Core **simplifies working with LLMs by offering a standardized way of integrating them into your app** - so you can focus on building great AI applications for your users, not waste time on technical details.

For example, here’s how you can generate text with various models using the AI SDK:

xAI

OpenAI

Anthropic

Google

Custom

import { generateText } from "ai"

import { xai } from "@ai-sdk/xai"

const { text } = await generateText({

model: xai("grok-3-beta"),

prompt: "What is love?"

})

Love is a universal emotion that is characterized by feelings of affection, attachment, and warmth towards someone or something. It is a complex and multifaceted experience that can take many different forms, including romantic love, familial love, platonic love, and self-love.


## [AI SDK Core Functions](#ai-sdk-core-functions)


AI SDK Core has various functions designed for [text generation](./generating-text), [structured data generation](./generating-structured-data), and [tool usage](./tools-and-tool-calling). These functions take a standardized approach to setting up [prompts](./prompts) and [settings](./settings), making it easier to work with different models.

-   [`generateText`](/docs/ai-sdk-core/generating-text): Generates text and [tool calls](./tools-and-tool-calling). This function is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.
-   [`streamText`](/docs/ai-sdk-core/generating-text): Stream text and tool calls. You can use the `streamText` function for interactive use cases such as [chat bots](/docs/ai-sdk-ui/chatbot) and [content streaming](/docs/ai-sdk-ui/completion).
-   [`generateObject`](/docs/ai-sdk-core/generating-structured-data): Generates a typed, structured object that matches a [Zod](https://zod.dev/) schema. You can use this function to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.
-   [`streamObject`](/docs/ai-sdk-core/generating-structured-data): Stream a structured object that matches a Zod schema. You can use this function to [stream generated UIs](/docs/ai-sdk-ui/object-generation).


## [API Reference](#api-reference)


Please check out the [AI SDK Core API Reference](/docs/reference/ai-sdk-core) for more details on each function.
```

### 83. `docs/ai-sdk-core/prompt-engineering.md`

```markdown
# Prompt Engineering


---
url: https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering
description: Learn how to develop prompts with AI SDK Core.
---


# [Prompt Engineering](#prompt-engineering)



## [Tips](#tips)



### [Prompts for Tools](#prompts-for-tools)


When you create prompts that include tools, getting good results can be tricky as the number and complexity of your tools increases.

Here are a few tips to help you get the best results:

1.  Use a model that is strong at tool calling, such as `gpt-4` or `gpt-4-turbo`. Weaker models will often struggle to call tools effectively and flawlessly.
2.  Keep the number of tools low, e.g. to 5 or less.
3.  Keep the complexity of the tool parameters low. Complex Zod schemas with many nested and optional elements, unions, etc. can be challenging for the model to work with.
4.  Use semantically meaningful names for your tools, parameters, parameter properties, etc. The more information you pass to the model, the better it can understand what you want.
5.  Add `.describe("...")` to your Zod schema properties to give the model hints about what a particular property is for.
6.  When the output of a tool might be unclear to the model and there are dependencies between tools, use the `description` field of a tool to provide information about the output of the tool execution.
7.  You can include example input/outputs of tool calls in your prompt to help the model understand how to use the tools. Keep in mind that the tools work with JSON objects, so the examples should use JSON.

In general, the goal should be to give the model all information it needs in a clear way.


### [Tool & Structured Data Schemas](#tool--structured-data-schemas)


The mapping from Zod schemas to LLM inputs (typically JSON schema) is not always straightforward, since the mapping is not one-to-one.


#### [Zod Dates](#zod-dates)


Zod expects JavaScript Date objects, but models return dates as strings. You can specify and validate the date format using `z.string().datetime()` or `z.string().date()`, and then use a Zod transformer to convert the string to a Date object.

```
const result =awaitgenerateObject({  model:openai('gpt-4-turbo'),  schema: z.object({    events: z.array(      z.object({        event: z.string(),        date: z.string().date().transform(value =>newDate(value)),}),),}),  prompt:'List 5 important events from the year 2000.',});
```


## [Debugging](#debugging)



### [Inspecting Warnings](#inspecting-warnings)


Not all providers support all AI SDK features. Providers either throw exceptions or return warnings when they do not support a feature. To check if your prompt, tools, and settings are handled correctly by the provider, you can check the call warnings:

```
const result =awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Hello, world!',});console.log(result.warnings);
```


### [HTTP Request Bodies](#http-request-bodies)


You can inspect the raw HTTP request bodies for models that expose them, e.g. [OpenAI](/providers/ai-sdk-providers/openai). This allows you to inspect the exact payload that is sent to the model provider in the provider-specific way.

Request bodies are available via the `request.body` property of the response:

```
const result =awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Hello, world!',});console.log(result.request.body);
```
```

### 84. `docs/ai-sdk-core/provider-management.md`

```markdown
# Provider & Model Management


---
url: https://ai-sdk.dev/docs/ai-sdk-core/provider-management
description: Learn how to work with multiple providers and models
---


# [Provider & Model Management](#provider--model-management)


When you work with multiple providers and models, it is often desirable to manage them in a central place and access the models through simple string ids.

The AI SDK offers [custom providers](/docs/reference/ai-sdk-core/custom-provider) and a [provider registry](/docs/reference/ai-sdk-core/provider-registry) for this purpose:

-   With **custom providers**, you can pre-configure model settings, provide model name aliases, and limit the available models.
-   The **provider registry** lets you mix multiple providers and access them through simple string ids.

You can mix and match custom providers, the provider registry, and [middleware](/docs/ai-sdk-core/middleware) in your application.


## [Custom Providers](#custom-providers)


You can create a [custom provider](/docs/reference/ai-sdk-core/custom-provider) using `customProvider`.


### [Example: custom model settings](#example-custom-model-settings)


You might want to override the default model settings for a provider or provide model name aliases with pre-configured settings.

```
import{ openai as originalOpenAI }from'@ai-sdk/openai';import{ customProvider }from'ai';// custom provider with different model settings:exportconst openai =customProvider({  languageModels:{// replacement model with custom settings:'gpt-4o':originalOpenAI('gpt-4o',{ structuredOutputs:true}),// alias model with custom settings:'gpt-4o-mini-structured':originalOpenAI('gpt-4o-mini',{      structuredOutputs:true,}),},  fallbackProvider: originalOpenAI,});
```


### [Example: model name alias](#example-model-name-alias)


You can also provide model name aliases, so you can update the model version in one place in the future:

```
import{ anthropic as originalAnthropic }from'@ai-sdk/anthropic';import{ customProvider }from'ai';// custom provider with alias names:exportconst anthropic =customProvider({  languageModels:{    opus:originalAnthropic('claude-3-opus-20240229'),    sonnet:originalAnthropic('claude-3-5-sonnet-20240620'),    haiku:originalAnthropic('claude-3-haiku-20240307'),},  fallbackProvider: originalAnthropic,});
```


### [Example: limit available models](#example-limit-available-models)


You can limit the available models in the system, even if you have multiple providers.

```
import{ anthropic }from'@ai-sdk/anthropic';import{ openai }from'@ai-sdk/openai';import{ customProvider }from'ai';exportconst myProvider =customProvider({  languageModels:{'text-medium':anthropic('claude-3-5-sonnet-20240620'),'text-small':openai('gpt-4o-mini'),'structure-medium':openai('gpt-4o',{ structuredOutputs:true}),'structure-fast':openai('gpt-4o-mini',{ structuredOutputs:true}),},  embeddingModels:{    emdedding: openai.textEmbeddingModel('text-embedding-3-small'),},// no fallback provider});
```


## [Provider Registry](#provider-registry)


You can create a [provider registry](/docs/reference/ai-sdk-core/provider-registry) with multiple providers and models using `createProviderRegistry`.


### [Setup](#setup)


registry.ts

```
import{ anthropic }from'@ai-sdk/anthropic';import{ createOpenAI }from'@ai-sdk/openai';import{ createProviderRegistry }from'ai';exportconst registry =createProviderRegistry({// register provider with prefix and default setup:  anthropic,// register provider with prefix and custom setup:  openai:createOpenAI({    apiKey: process.env.OPENAI_API_KEY,}),});
```


### [Setup with Custom Separator](#setup-with-custom-separator)


By default, the registry uses `:` as the separator between provider and model IDs. You can customize this separator:

registry.ts

```
import{ anthropic }from'@ai-sdk/anthropic';import{ openai }from'@ai-sdk/openai';exportconst customSeparatorRegistry =createProviderRegistry({    anthropic,    openai,},{ separator:' > '},);
```


### [Example: Use language models](#example-use-language-models)


You can access language models by using the `languageModel` method on the registry. The provider id will become the prefix of the model id: `providerId:modelId`.

```
import{ generateText }from'ai';import{ registry }from'./registry';const{ text }=awaitgenerateText({  model: registry.languageModel('openai:gpt-4-turbo'),// default separator// or with custom separator:// model: customSeparatorRegistry.languageModel('openai > gpt-4-turbo'),  prompt:'Invent a new holiday and describe its traditions.',});
```


### [Example: Use text embedding models](#example-use-text-embedding-models)


You can access text embedding models by using the `textEmbeddingModel` method on the registry. The provider id will become the prefix of the model id: `providerId:modelId`.

```
import{ embed }from'ai';import{ registry }from'./registry';const{ embedding }=awaitembed({  model: registry.textEmbeddingModel('openai:text-embedding-3-small'),  value:'sunny day at the beach',});
```


### [Example: Use image models](#example-use-image-models)


You can access image models by using the `imageModel` method on the registry. The provider id will become the prefix of the model id: `providerId:modelId`.

```
import{ generateImage }from'ai';import{ registry }from'./registry';const{ image }=awaitgenerateImage({  model: registry.imageModel('openai:dall-e-3'),  prompt:'A beautiful sunset over a calm ocean',});
```


## [Combining Custom Providers, Provider Registry, and Middleware](#combining-custom-providers-provider-registry-and-middleware)


The central idea of provider management is to set up a file that contains all the providers and models you want to use. You may want to pre-configure model settings, provide model name aliases, limit the available models, and more.

Here is an example that implements the following concepts:

-   pass through a full provider with a namespace prefix (here: `xai > *`)
-   setup an OpenAI-compatible provider with custom api key and base URL (here: `custom > *`)
-   setup model name aliases (here: `anthropic > fast`, `anthropic > writing`, `anthropic > reasoning`)
-   pre-configure model settings (here: `anthropic > reasoning`)
-   validate the provider-specific options (here: `AnthropicProviderOptions`)
-   use a fallback provider (here: `anthropic > *`)
-   limit a provider to certain models without a fallback (here: `groq > gemma2-9b-it`, `groq > qwen-qwq-32b`)
-   define a custom separator for the provider registry (here: `>`)

```
import{ anthropic,AnthropicProviderOptions}from'@ai-sdk/anthropic';import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ xai }from'@ai-sdk/xai';import{ groq }from'@ai-sdk/groq';import{  createProviderRegistry,  customProvider,  defaultSettingsMiddleware,  wrapLanguageModel,}from'ai';exportconst registry =createProviderRegistry({// pass through a full provider with a namespace prefix    xai,// access an OpenAI-compatible provider with custom setup    custom:createOpenAICompatible({      name:'provider-name',      apiKey: process.env.CUSTOM_API_KEY,      baseURL:'https://api.custom.com/v1',}),// setup model name aliases    anthropic:customProvider({      languageModels:{        fast:anthropic('claude-3-haiku-20240307'),// simple model        writing:anthropic('claude-3-7-sonnet-20250219'),// extended reasoning model configuration:        reasoning:wrapLanguageModel({          model:anthropic('claude-3-7-sonnet-20250219'),          middleware:defaultSettingsMiddleware({            settings:{              maxTokens:100000,// example default setting              providerMetadata:{                anthropic:{                  thinking:{type:'enabled',                    budgetTokens:32000,},} satisfies AnthropicProviderOptions,},},}),}),},      fallbackProvider: anthropic,}),// limit a provider to certain models without a fallback    groq:customProvider({      languageModels:{'gemma2-9b-it':groq('gemma2-9b-it'),'qwen-qwq-32b':groq('qwen-qwq-32b'),},}),},{ separator:' > '},);// usage:const model = registry.languageModel('anthropic > reasoning');
```
```

### 85. `docs/ai-sdk-core/settings.md`

```markdown
# Settings


---
url: https://ai-sdk.dev/docs/ai-sdk-core/settings
description: Learn how to configure the AI SDK.
---


# [Settings](#settings)


Large language models (LLMs) typically provide settings to augment their output.

All AI SDK functions support the following common settings in addition to the model, the [prompt](./prompts), and additional provider-specific settings:

```
const result =awaitgenerateText({  model: yourModel,  maxTokens:512,  temperature:0.3,  maxRetries:5,  prompt:'Invent a new holiday and describe its traditions.',});
```

Some providers do not support all common settings. If you use a setting with a provider that does not support it, a warning will be generated. You can check the `warnings` property in the result object to see if any warnings were generated.


### [`maxTokens`](#maxtokens)


Maximum number of tokens to generate.


### [`temperature`](#temperature)


Temperature setting.

The value is passed through to the provider. The range depends on the provider and model. For most providers, `0` means almost deterministic results, and higher values mean more randomness.

It is recommended to set either `temperature` or `topP`, but not both.


### [`topP`](#topp)


Nucleus sampling.

The value is passed through to the provider. The range depends on the provider and model. For most providers, nucleus sampling is a number between 0 and 1. E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.

It is recommended to set either `temperature` or `topP`, but not both.


### [`topK`](#topk)


Only sample from the top K options for each subsequent token.

Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use `temperature`.


### [`presencePenalty`](#presencepenalty)


The presence penalty affects the likelihood of the model to repeat information that is already in the prompt.

The value is passed through to the provider. The range depends on the provider and model. For most providers, `0` means no penalty.


### [`frequencyPenalty`](#frequencypenalty)


The frequency penalty affects the likelihood of the model to repeatedly use the same words or phrases.

The value is passed through to the provider. The range depends on the provider and model. For most providers, `0` means no penalty.


### [`stopSequences`](#stopsequences)


The stop sequences to use for stopping the text generation.

If set, the model will stop generating text when one of the stop sequences is generated. Providers may have limits on the number of stop sequences.


### [`seed`](#seed)


It is the seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.


### [`maxRetries`](#maxretries)


Maximum number of retries. Set to 0 to disable retries. Default: `2`.


### [`abortSignal`](#abortsignal)


An optional abort signal that can be used to cancel the call.

The abort signal can e.g. be forwarded from a user interface to cancel the call, or to define a timeout.


#### [Example: Timeout](#example-timeout)


```
const result =awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Invent a new holiday and describe its traditions.',  abortSignal:AbortSignal.timeout(5000),// 5 seconds});
```


### [`headers`](#headers)


Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.

You can use the request headers to provide additional information to the provider, depending on what the provider supports. For example, some observability providers support headers such as `Prompt-Id`.

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const result =awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Invent a new holiday and describe its traditions.',  headers:{'Prompt-Id':'my-prompt-id',},});
```

The `headers` setting is for request-specific headers. You can also set `headers` in the provider configuration. These headers will be sent with every request made by the provider.
```

### 86. `docs/ai-sdk-core/speech.md`

```markdown
# Speech


---
url: https://ai-sdk.dev/docs/ai-sdk-core/speech
description: Learn how to generate speech from text with the AI SDK.
---


# [Speech](#speech)


Speech is an experimental feature.

The AI SDK provides the [`generateSpeech`](/docs/reference/ai-sdk-core/generate-speech) function to generate speech from text using a speech model.

```
import{ experimental_generateSpeech as generateSpeech }from'ai';import{ openai }from'@ai-sdk/openai';import{ readFile }from'fs/promises';const audio =awaitgenerateSpeech({  model: openai.speech('tts-1'),  text:'Hello, world!',  voice:'alloy',});
```

To access the generated audio:

```
const audio = audio.audioData;// audio data e.g. Uint8Array
```


## [Settings](#settings)



### [Provider-Specific settings](#provider-specific-settings)


You can set model-specific settings with the `providerOptions` parameter.

```
import{ experimental_generateSpeech as generateSpeech }from'ai';import{ openai }from'@ai-sdk/openai';import{ readFile }from'fs/promises';const audio =awaitgenerateSpeech({  model: openai.speech('tts-1'),  text:'Hello, world!',  providerOptions:{    openai:{// ...},},});
```


### [Abort Signals and Timeouts](#abort-signals-and-timeouts)


`generateSpeech` accepts an optional `abortSignal` parameter of type [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal) that you can use to abort the speech generation process or set a timeout.

```
import{ openai }from'@ai-sdk/openai';import{ experimental_generateSpeech as generateSpeech }from'ai';import{ readFile }from'fs/promises';const audio =awaitgenerateSpeech({  model: openai.speech('tts-1'),  text:'Hello, world!',  abortSignal:AbortSignal.timeout(1000),// Abort after 1 second});
```


### [Custom Headers](#custom-headers)


`generateSpeech` accepts an optional `headers` parameter of type `Record<string, string>` that you can use to add custom headers to the speech generation request.

```
import{ openai }from'@ai-sdk/openai';import{ experimental_generateSpeech as generateSpeech }from'ai';import{ readFile }from'fs/promises';const audio =awaitgenerateSpeech({  model: openai.speech('tts-1'),  text:'Hello, world!',  headers:{'X-Custom-Header':'custom-value'},});
```


### [Warnings](#warnings)


Warnings (e.g. unsupported parameters) are available on the `warnings` property.

```
import{ openai }from'@ai-sdk/openai';import{ experimental_generateSpeech as generateSpeech }from'ai';import{ readFile }from'fs/promises';const audio =awaitgenerateSpeech({  model: openai.speech('tts-1'),  text:'Hello, world!',});const warnings = audio.warnings;
```


### [Error Handling](#error-handling)


When `generateSpeech` cannot generate a valid audio, it throws a [`AI_NoAudioGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-audio-generated-error).

This error can arise for any the following reasons:

-   The model failed to generate a response
-   The model generated a response that could not be parsed

The error preserves the following information to help you log the issue:

-   `responses`: Metadata about the speech model responses, including timestamp, model, and headers.
-   `cause`: The cause of the error. You can use this for more detailed error handling.

```
import{  experimental_generateSpeech as generateSpeech,AI_NoAudioGeneratedError,}from'ai';import{ openai }from'@ai-sdk/openai';import{ readFile }from'fs/promises';try{awaitgenerateSpeech({    model: openai.speech('tts-1'),    text:'Hello, world!',});}catch(error){if(AI_NoAudioGeneratedError.isInstance(error)){console.log('AI_NoAudioGeneratedError');console.log('Cause:', error.cause);console.log('Responses:', error.responses);}}
```


## [Speech Models](#speech-models)


Provider

Model

[OpenAI](/providers/ai-sdk-providers/openai#speech-models)

`tts-1`

[OpenAI](/providers/ai-sdk-providers/openai#speech-models)

`tts-1-hd`

[OpenAI](/providers/ai-sdk-providers/openai#speech-models)

`gpt-4o-mini-tts`

[LMNT](/providers/ai-sdk-providers/lmnt#speech-models)

`aurora`

[LMNT](/providers/ai-sdk-providers/lmnt#speech-models)

`blizzard`

[Hume](/providers/ai-sdk-providers/hume#speech-models)

`default`

Above are a small subset of the speech models supported by the AI SDK providers. For more, see the respective provider documentation.
```

### 87. `docs/ai-sdk-core/telemetry.md`

```markdown
# Telemetry


---
url: https://ai-sdk.dev/docs/ai-sdk-core/telemetry
description: Using OpenTelemetry with AI SDK Core
---


# [Telemetry](#telemetry)


AI SDK Telemetry is experimental and may change in the future.

The AI SDK uses [OpenTelemetry](https://opentelemetry.io/) to collect telemetry data. OpenTelemetry is an open-source observability framework designed to provide standardized instrumentation for collecting telemetry data.

Check out the [AI SDK Observability Integrations](/providers/observability) to see providers that offer monitoring and tracing for AI SDK applications.


## [Enabling telemetry](#enabling-telemetry)


For Next.js applications, please follow the [Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry) to enable telemetry first.

You can then use the `experimental_telemetry` option to enable telemetry on specific function calls while the feature is experimental:

```
const result =awaitgenerateText({  model:openai('gpt-4-turbo'),  prompt:'Write a short story about a cat.',  experimental_telemetry:{ isEnabled:true},});
```

When telemetry is enabled, you can also control if you want to record the input values and the output values for the function. By default, both are enabled. You can disable them by setting the `recordInputs` and `recordOutputs` options to `false`.

Disabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons. You might for example want to disable recording inputs if they contain sensitive information.


## [Telemetry Metadata](#telemetry-metadata)


You can provide a `functionId` to identify the function that the telemetry data is for, and `metadata` to include additional information in the telemetry data.

```
const result =awaitgenerateText({  model:openai('gpt-4-turbo'),  prompt:'Write a short story about a cat.',  experimental_telemetry:{    isEnabled:true,    functionId:'my-awesome-function',    metadata:{      something:'custom',      someOtherThing:'other-value',},},});
```


## [Custom Tracer](#custom-tracer)


You may provide a `tracer` which must return an OpenTelemetry `Tracer`. This is useful in situations where you want your traces to use a `TracerProvider` other than the one provided by the `@opentelemetry/api` singleton.

```
const tracerProvider =newNodeTracerProvider();const result =awaitgenerateText({  model:openai('gpt-4-turbo'),  prompt:'Write a short story about a cat.',  experimental_telemetry:{    isEnabled:true,    tracer: tracerProvider.getTracer('ai'),},});
```


## [Collected Data](#collected-data)



### [generateText function](#generatetext-function)


`generateText` records 3 types of spans:

-   `ai.generateText` (span): the full length of the generateText call. It contains 1 or more `ai.generateText.doGenerate` spans. It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:

    -   `operation.name`: `ai.generateText` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.generateText"`
    -   `ai.prompt`: the prompt that was used when calling `generateText`
    -   `ai.response.text`: the text that was generated
    -   `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)
    -   `ai.response.finishReason`: the reason why the generation finished
    -   `ai.settings.maxSteps`: the maximum number of steps that were set
-   `ai.generateText.doGenerate` (span): a provider doGenerate call. It can contain `ai.toolCall` spans. It contains the [call LLM span information](#call-llm-span-information) and the following attributes:

    -   `operation.name`: `ai.generateText.doGenerate` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.generateText.doGenerate"`
    -   `ai.prompt.format`: the format of the prompt
    -   `ai.prompt.messages`: the messages that were passed into the provider
    -   `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined`. Function tools have a `name`, `description` (optional), and `parameters` (JSON schema). Provider-defined tools have a `name`, `id`, and `args` (Record).
    -   `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.
    -   `ai.response.text`: the text that was generated
    -   `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)
    -   `ai.response.finishReason`: the reason why the generation finished
-   `ai.toolCall` (span): a tool call that is made as part of the generateText call. See [Tool call spans](#tool-call-spans) for more details.



### [streamText function](#streamtext-function)


`streamText` records 3 types of spans and 2 types of events:

-   `ai.streamText` (span): the full length of the streamText call. It contains a `ai.streamText.doStream` span. It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:

    -   `operation.name`: `ai.streamText` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.streamText"`
    -   `ai.prompt`: the prompt that was used when calling `streamText`
    -   `ai.response.text`: the text that was generated
    -   `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)
    -   `ai.response.finishReason`: the reason why the generation finished
    -   `ai.settings.maxSteps`: the maximum number of steps that were set
-   `ai.streamText.doStream` (span): a provider doStream call. This span contains an `ai.stream.firstChunk` event and `ai.toolCall` spans. It contains the [call LLM span information](#call-llm-span-information) and the following attributes:

    -   `operation.name`: `ai.streamText.doStream` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.streamText.doStream"`
    -   `ai.prompt.format`: the format of the prompt
    -   `ai.prompt.messages`: the messages that were passed into the provider
    -   `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined`. Function tools have a `name`, `description` (optional), and `parameters` (JSON schema). Provider-defined tools have a `name`, `id`, and `args` (Record).
    -   `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.
    -   `ai.response.text`: the text that was generated
    -   `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)
    -   `ai.response.msToFirstChunk`: the time it took to receive the first chunk in milliseconds
    -   `ai.response.msToFinish`: the time it took to receive the finish part of the LLM stream in milliseconds
    -   `ai.response.avgCompletionTokensPerSecond`: the average number of completion tokens per second
    -   `ai.response.finishReason`: the reason why the generation finished
-   `ai.toolCall` (span): a tool call that is made as part of the generateText call. See [Tool call spans](#tool-call-spans) for more details.

-   `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.

    -   `ai.response.msToFirstChunk`: the time it took to receive the first chunk
-   `ai.stream.finish` (event): an event that is emitted when the finish part of the LLM stream is received.


It also records a `ai.stream.firstChunk` event when the first chunk of the stream is received.


### [generateObject function](#generateobject-function)


`generateObject` records 2 types of spans:

-   `ai.generateObject` (span): the full length of the generateObject call. It contains 1 or more `ai.generateObject.doGenerate` spans. It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:

    -   `operation.name`: `ai.generateObject` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.generateObject"`
    -   `ai.prompt`: the prompt that was used when calling `generateObject`
    -   `ai.schema`: Stringified JSON schema version of the schema that was passed into the `generateObject` function
    -   `ai.schema.name`: the name of the schema that was passed into the `generateObject` function
    -   `ai.schema.description`: the description of the schema that was passed into the `generateObject` function
    -   `ai.response.object`: the object that was generated (stringified JSON)
    -   `ai.settings.mode`: the object generation mode, e.g. `json`
    -   `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`
-   `ai.generateObject.doGenerate` (span): a provider doGenerate call. It contains the [call LLM span information](#call-llm-span-information) and the following attributes:

    -   `operation.name`: `ai.generateObject.doGenerate` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.generateObject.doGenerate"`
    -   `ai.prompt.format`: the format of the prompt
    -   `ai.prompt.messages`: the messages that were passed into the provider
    -   `ai.response.object`: the object that was generated (stringified JSON)
    -   `ai.settings.mode`: the object generation mode
    -   `ai.response.finishReason`: the reason why the generation finished


### [streamObject function](#streamobject-function)


`streamObject` records 2 types of spans and 1 type of event:

-   `ai.streamObject` (span): the full length of the streamObject call. It contains 1 or more `ai.streamObject.doStream` spans. It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:

    -   `operation.name`: `ai.streamObject` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.streamObject"`
    -   `ai.prompt`: the prompt that was used when calling `streamObject`
    -   `ai.schema`: Stringified JSON schema version of the schema that was passed into the `streamObject` function
    -   `ai.schema.name`: the name of the schema that was passed into the `streamObject` function
    -   `ai.schema.description`: the description of the schema that was passed into the `streamObject` function
    -   `ai.response.object`: the object that was generated (stringified JSON)
    -   `ai.settings.mode`: the object generation mode, e.g. `json`
    -   `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`
-   `ai.streamObject.doStream` (span): a provider doStream call. This span contains an `ai.stream.firstChunk` event. It contains the [call LLM span information](#call-llm-span-information) and the following attributes:

    -   `operation.name`: `ai.streamObject.doStream` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.streamObject.doStream"`
    -   `ai.prompt.format`: the format of the prompt
    -   `ai.prompt.messages`: the messages that were passed into the provider
    -   `ai.settings.mode`: the object generation mode
    -   `ai.response.object`: the object that was generated (stringified JSON)
    -   `ai.response.msToFirstChunk`: the time it took to receive the first chunk
    -   `ai.response.finishReason`: the reason why the generation finished
-   `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.

    -   `ai.response.msToFirstChunk`: the time it took to receive the first chunk


### [embed function](#embed-function)


`embed` records 2 types of spans:

-   `ai.embed` (span): the full length of the embed call. It contains 1 `ai.embed.doEmbed` spans. It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:

    -   `operation.name`: `ai.embed` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.embed"`
    -   `ai.value`: the value that was passed into the `embed` function
    -   `ai.embedding`: a JSON-stringified embedding
-   `ai.embed.doEmbed` (span): a provider doEmbed call. It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:

    -   `operation.name`: `ai.embed.doEmbed` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.embed.doEmbed"`
    -   `ai.values`: the values that were passed into the provider (array)
    -   `ai.embeddings`: an array of JSON-stringified embeddings


### [embedMany function](#embedmany-function)


`embedMany` records 2 types of spans:

-   `ai.embedMany` (span): the full length of the embedMany call. It contains 1 or more `ai.embedMany.doEmbed` spans. It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:

    -   `operation.name`: `ai.embedMany` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.embedMany"`
    -   `ai.values`: the values that were passed into the `embedMany` function
    -   `ai.embeddings`: an array of JSON-stringified embedding
-   `ai.embedMany.doEmbed` (span): a provider doEmbed call. It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:

    -   `operation.name`: `ai.embedMany.doEmbed` and the functionId that was set through `telemetry.functionId`
    -   `ai.operationId`: `"ai.embedMany.doEmbed"`
    -   `ai.values`: the values that were sent to the provider
    -   `ai.embeddings`: an array of JSON-stringified embeddings for each value


## [Span Details](#span-details)



### [Basic LLM span information](#basic-llm-span-information)


Many spans that use LLMs (`ai.generateText`, `ai.generateText.doGenerate`, `ai.streamText`, `ai.streamText.doStream`, `ai.generateObject`, `ai.generateObject.doGenerate`, `ai.streamObject`, `ai.streamObject.doStream`) contain the following attributes:

-   `resource.name`: the functionId that was set through `telemetry.functionId`
-   `ai.model.id`: the id of the model
-   `ai.model.provider`: the provider of the model
-   `ai.request.headers.*`: the request headers that were passed in through `headers`
-   `ai.settings.maxRetries`: the maximum number of retries that were set
-   `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`
-   `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`
-   `ai.usage.completionTokens`: the number of completion tokens that were used
-   `ai.usage.promptTokens`: the number of prompt tokens that were used


### [Call LLM span information](#call-llm-span-information)


Spans that correspond to individual LLM calls (`ai.generateText.doGenerate`, `ai.streamText.doStream`, `ai.generateObject.doGenerate`, `ai.streamObject.doStream`) contain [basic LLM span information](#basic-llm-span-information) and the following attributes:

-   `ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.
-   `ai.response.id`: the id of the response. Uses the ID from the provider when available.
-   `ai.response.timestamp`: the timestamp of the response. Uses the timestamp from the provider when available.
-   [Semantic Conventions for GenAI operations](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/)
    -   `gen_ai.system`: the provider that was used
    -   `gen_ai.request.model`: the model that was requested
    -   `gen_ai.request.temperature`: the temperature that was set
    -   `gen_ai.request.max_tokens`: the maximum number of tokens that were set
    -   `gen_ai.request.frequency_penalty`: the frequency penalty that was set
    -   `gen_ai.request.presence_penalty`: the presence penalty that was set
    -   `gen_ai.request.top_k`: the topK parameter value that was set
    -   `gen_ai.request.top_p`: the topP parameter value that was set
    -   `gen_ai.request.stop_sequences`: the stop sequences
    -   `gen_ai.response.finish_reasons`: the finish reasons that were returned by the provider
    -   `gen_ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.
    -   `gen_ai.response.id`: the id of the response. Uses the ID from the provider when available.
    -   `gen_ai.usage.input_tokens`: the number of prompt tokens that were used
    -   `gen_ai.usage.output_tokens`: the number of completion tokens that were used


### [Basic embedding span information](#basic-embedding-span-information)


Many spans that use embedding models (`ai.embed`, `ai.embed.doEmbed`, `ai.embedMany`, `ai.embedMany.doEmbed`) contain the following attributes:

-   `ai.model.id`: the id of the model
-   `ai.model.provider`: the provider of the model
-   `ai.request.headers.*`: the request headers that were passed in through `headers`
-   `ai.settings.maxRetries`: the maximum number of retries that were set
-   `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`
-   `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`
-   `ai.usage.tokens`: the number of tokens that were used
-   `resource.name`: the functionId that was set through `telemetry.functionId`


### [Tool call spans](#tool-call-spans)


Tool call spans (`ai.toolCall`) contain the following attributes:

-   `operation.name`: `"ai.toolCall"`
-   `ai.operationId`: `"ai.toolCall"`
-   `ai.toolCall.name`: the name of the tool
-   `ai.toolCall.id`: the id of the tool call
-   `ai.toolCall.args`: the parameters of the tool call
-   `ai.toolCall.result`: the result of the tool call. Only available if the tool call is successful and the result is serializable.
```

### 88. `docs/ai-sdk-core/testing.md`

```markdown
# Testing


---
url: https://ai-sdk.dev/docs/ai-sdk-core/testing
description: Learn how to use AI SDK Core mock providers for testing.
---


# [Testing](#testing)


Testing language models can be challenging, because they are non-deterministic and calling them is slow and expensive.

To enable you to unit test your code that uses the AI SDK, the AI SDK Core includes mock providers and test helpers. You can import the following helpers from `ai/test`:

-   `MockEmbeddingModelV1`: A mock embedding model using the [embedding model v1 specification](https://github.com/vercel/ai/blob/main/packages/provider/src/embedding-model/v1/embedding-model-v1.ts).
-   `MockLanguageModelV1`: A mock language model using the [language model v1 specification](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v1/language-model-v1.ts).
-   `mockId`: Provides an incrementing integer ID.
-   `mockValues`: Iterates over an array of values with each call. Returns the last value when the array is exhausted.
-   [`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream): Simulates a readable stream with delays.

With mock providers and test helpers, you can control the output of the AI SDK and test your code in a repeatable and deterministic way without actually calling a language model provider.


## [Examples](#examples)


You can use the test helpers with the AI Core functions in your unit tests:


### [generateText](#generatetext)


```
import{ generateText }from'ai';import{MockLanguageModelV1}from'ai/test';const result =awaitgenerateText({  model:newMockLanguageModelV1({doGenerate:async()=>({      rawCall:{ rawPrompt:null, rawSettings:{}},      finishReason:'stop',      usage:{ promptTokens:10, completionTokens:20},      text:`Hello, world!`,}),}),  prompt:'Hello, test!',});
```


### [streamText](#streamtext)


```
import{ streamText, simulateReadableStream }from'ai';import{MockLanguageModelV1}from'ai/test';const result =streamText({  model:newMockLanguageModelV1({doStream:async()=>({      stream:simulateReadableStream({        chunks:[{type:'text-delta', textDelta:'Hello'},{type:'text-delta', textDelta:', '},{type:'text-delta', textDelta:`world!`},{type:'finish',            finishReason:'stop',            logprobs:undefined,            usage:{ completionTokens:10, promptTokens:3},},],}),      rawCall:{ rawPrompt:null, rawSettings:{}},}),}),  prompt:'Hello, test!',});
```


### [generateObject](#generateobject)


```
import{ generateObject }from'ai';import{MockLanguageModelV1}from'ai/test';import{ z }from'zod';const result =awaitgenerateObject({  model:newMockLanguageModelV1({    defaultObjectGenerationMode:'json',doGenerate:async()=>({      rawCall:{ rawPrompt:null, rawSettings:{}},      finishReason:'stop',      usage:{ promptTokens:10, completionTokens:20},      text:`{"content":"Hello, world!"}`,}),}),  schema: z.object({ content: z.string()}),  prompt:'Hello, test!',});
```


### [streamObject](#streamobject)


```
import{ streamObject, simulateReadableStream }from'ai';import{MockLanguageModelV1}from'ai/test';import{ z }from'zod';const result =streamObject({  model:newMockLanguageModelV1({    defaultObjectGenerationMode:'json',doStream:async()=>({      stream:simulateReadableStream({        chunks:[{type:'text-delta', textDelta:'{ '},{type:'text-delta', textDelta:'"content": '},{type:'text-delta', textDelta:`"Hello, `},{type:'text-delta', textDelta:`world`},{type:'text-delta', textDelta:`!"`},{type:'text-delta', textDelta:' }'},{type:'finish',            finishReason:'stop',            logprobs:undefined,            usage:{ completionTokens:10, promptTokens:3},},],}),      rawCall:{ rawPrompt:null, rawSettings:{}},}),}),  schema: z.object({ content: z.string()}),  prompt:'Hello, test!',});
```


### [Simulate Data Stream Protocol Responses](#simulate-data-stream-protocol-responses)


You can also simulate [Data Stream Protocol](/docs/ai-sdk-ui/stream-protocol#data-stream-protocol) responses for testing, debugging, or demonstration purposes.

Here is a Next example:

route.ts

```
import{ simulateReadableStream }from'ai';exportasyncfunctionPOST(req:Request){returnnewResponse(simulateReadableStream({      initialDelayInMs:1000,// Delay before the first chunk      chunkDelayInMs:300,// Delay between chunks      chunks:[`0:"This"\n`,`0:" is an"\n`,`0:"example."\n`,`e:{"finishReason":"stop","usage":{"promptTokens":20,"completionTokens":50},"isContinued":false}\n`,`d:{"finishReason":"stop","usage":{"promptTokens":20,"completionTokens":50}}\n`,],}).pipeThrough(newTextEncoderStream()),{      status:200,      headers:{'X-Vercel-AI-Data-Stream':'v1','Content-Type':'text/plain; charset=utf-8',},},);}
```
```

### 89. `docs/ai-sdk-core/tools-and-tool-calling.md`

```markdown
# Tool Calling


---
url: https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling
description: Learn about tool calling and multi-step calls (using maxSteps) with AI SDK Core.
---


# [Tool Calling](#tool-calling)


As covered under Foundations, [tools](/docs/foundations/tools) are objects that can be called by the model to perform a specific task. AI SDK Core tools contain three elements:

-   **`description`**: An optional description of the tool that can influence when the tool is picked.
-   **`parameters`**: A [Zod schema](/docs/foundations/tools#schemas) or a [JSON schema](/docs/reference/ai-sdk-core/json-schema) that defines the parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.
-   **`execute`**: An optional async function that is called with the arguments from the tool call. It produces a value of type `RESULT` (generic type). It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.

You can use the [`tool`](/docs/reference/ai-sdk-core/tool) helper function to infer the types of the `execute` parameters.

The `tools` parameter of `generateText` and `streamText` is an object that has the tool names as keys and the tools as values:

```
import{ z }from'zod';import{ generateText, tool }from'ai';const result =awaitgenerateText({  model: yourModel,  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},  prompt:'What is the weather in San Francisco?',});
```

When a model uses a tool, it is called a "tool call" and the output of the tool is called a "tool result".

Tool calling is not restricted to only text generation. You can also use it to render user interfaces (Generative UI).


## [Multi-Step Calls (using maxSteps)](#multi-step-calls-using-maxsteps)


With the `maxSteps` setting, you can enable multi-step calls in `generateText` and `streamText`. When `maxSteps` is set to a number greater than 1 and the model generates a tool call, the AI SDK will trigger a new generation passing in the tool result until there are no further tool calls or the maximum number of tool steps is reached.

To decide what value to set for `maxSteps`, consider the most complex task the call might handle and the number of sequential steps required for completion, rather than just the number of available tools.

By default, when you use `generateText` or `streamText`, it triggers a single generation (`maxSteps: 1`). This works well for many use cases where you can rely on the model's training data to generate a response. However, when you provide tools, the model now has the choice to either generate a normal text response, or generate a tool call. If the model generates a tool call, it's generation is complete and that step is finished.

You may want the model to generate text after the tool has been executed, either to summarize the tool results in the context of the users query. In many cases, you may also want the model to use multiple tools in a single response. This is where multi-step calls come in.

You can think of multi-step calls in a similar way to a conversation with a human. When you ask a question, if the person does not have the requisite knowledge in their common knowledge (a model's training data), the person may need to look up information (use a tool) before they can provide you with an answer. In the same way, the model may need to call a tool to get the information it needs to answer your question where each generation (tool call or text generation) is a step.


### [Example](#example)


In the following example, there are two steps:

1.  **Step 1**
    1.  The prompt `'What is the weather in San Francisco?'` is sent to the model.
    2.  The model generates a tool call.
    3.  The tool call is executed.
2.  **Step 2**
    1.  The tool result is sent to the model.
    2.  The model generates a response considering the tool result.

```
import{ z }from'zod';import{ generateText, tool }from'ai';const{ text, steps }=awaitgenerateText({  model: yourModel,  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},  maxSteps:5,// allow up to 5 steps  prompt:'What is the weather in San Francisco?',});
```

You can use `streamText` in a similar way.


### [Steps](#steps)


To access intermediate tool calls and results, you can use the `steps` property in the result object or the `streamText` `onFinish` callback. It contains all the text, tool calls, tool results, and more from each step.


#### [Example: Extract tool results from all steps](#example-extract-tool-results-from-all-steps)


```
import{ generateText }from'ai';const{ steps }=awaitgenerateText({  model:openai('gpt-4-turbo'),  maxSteps:10,// ...});// extract all tool calls from the steps:const allToolCalls = steps.flatMap(step => step.toolCalls);
```


### [`onStepFinish` callback](#onstepfinish-callback)


When using `generateText` or `streamText`, you can provide an `onStepFinish` callback that is triggered when a step is finished, i.e. all text deltas, tool calls, and tool results for the step are available. When you have multiple steps, the callback is triggered for each step.

```
import{ generateText }from'ai';const result =awaitgenerateText({// ...onStepFinish({ text, toolCalls, toolResults, finishReason, usage }){// your own logic, e.g. for saving the chat history or recording usage},});
```


### [`experimental_prepareStep` callback](#experimental_preparestep-callback)


The `experimental_prepareStep` callback is experimental and may change in the future. It is only available in the `generateText` function.

The `experimental_prepareStep` callback is called before a step is started.

It is called with the following parameters:

-   `model`: The model that was passed into `generateText`.
-   `maxSteps`: The maximum number of steps that was passed into `generateText`.
-   `stepNumber`: The number of the step that is being executed.
-   `steps`: The steps that have been executed so far.

You can use it to provide different settings for a step.

```
import{ generateText }from'ai';const result =awaitgenerateText({// ...experimental_prepareStep:async({ model, stepNumber, maxSteps, steps })=>{if(stepNumber ===0){return{// use a different model for this step:        model: modelForThisParticularStep,// force a tool choice for this step:        toolChoice:{type:'tool', toolName:'tool1'},// limit the tools that are available for this step:        experimental_activeTools:['tool1'],};}// when nothing is returned, the default settings are used},});
```


## [Response Messages](#response-messages)


Adding the generated assistant and tool messages to your conversation history is a common task, especially if you are using multi-step tool calls.

Both `generateText` and `streamText` have a `response.messages` property that you can use to add the assistant and tool messages to your conversation history. It is also available in the `onFinish` callback of `streamText`.

The `response.messages` property contains an array of `CoreMessage` objects that you can add to your conversation history:

```
import{ generateText }from'ai';const messages:CoreMessage[]=[// ...];const{ response }=awaitgenerateText({// ...  messages,});// add the response messages to your conversation history:messages.push(...response.messages);// streamText: ...((await response).messages)
```


## [Tool Choice](#tool-choice)


You can use the `toolChoice` setting to influence when a tool is selected. It supports the following settings:

-   `auto` (default): the model can choose whether and which tools to call.
-   `required`: the model must call a tool. It can choose which tool to call.
-   `none`: the model must not call tools
-   `{ type: 'tool', toolName: string (typed) }`: the model must call the specified tool

```
import{ z }from'zod';import{ generateText, tool }from'ai';const result =awaitgenerateText({  model: yourModel,  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},  toolChoice:'required',// force the model to call a tool  prompt:'What is the weather in San Francisco?',});
```


## [Tool Execution Options](#tool-execution-options)


When tools are called, they receive additional options as a second parameter.


### [Tool Call ID](#tool-call-id)


The ID of the tool call is forwarded to the tool execution. You can use it e.g. when sending tool-call related information with stream data.

```
import{StreamData, streamText, tool }from'ai';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();const data =newStreamData();const result =streamText({// ...    messages,    tools:{      myTool:tool({// ...execute:async(args,{ toolCallId })=>{// return e.g. custom status for tool call          data.appendMessageAnnotation({type:'tool-status',            toolCallId,            status:'in-progress',});// ...},}),},onFinish(){      data.close();},});return result.toDataStreamResponse({ data });}
```


### [Messages](#messages)


The messages that were sent to the language model to initiate the response that contained the tool call are forwarded to the tool execution. You can access them in the second parameter of the `execute` function. In multi-step calls, the messages contain the text, tool calls, and tool results from all previous steps.

```
import{ generateText, tool }from'ai';const result =awaitgenerateText({// ...  tools:{    myTool:tool({// ...execute:async(args,{ messages })=>{// use the message history in e.g. calls to other language modelsreturn something;},}),},});
```


### [Abort Signals](#abort-signals)


The abort signals from `generateText` and `streamText` are forwarded to the tool execution. You can access them in the second parameter of the `execute` function and e.g. abort long-running computations or forward them to fetch calls inside tools.

```
import{ z }from'zod';import{ generateText, tool }from'ai';const result =awaitgenerateText({  model: yourModel,  abortSignal: myAbortSignal,// signal that will be forwarded to tools  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string()}),execute:async({location},{ abortSignal })=>{returnfetch(`https://api.weatherapi.com/v1/current.json?q=${location}`,{ signal: abortSignal },// forward the abort signal to fetch);},}),},  prompt:'What is the weather in San Francisco?',});
```


## [Types](#types)


Modularizing your code often requires defining types to ensure type safety and reusability. To enable this, the AI SDK provides several helper types for tools, tool calls, and tool results.

You can use them to strongly type your variables, function parameters, and return types in parts of the code that are not directly related to `streamText` or `generateText`.

Each tool call is typed with `ToolCall<NAME extends string, ARGS>`, depending on the tool that has been invoked. Similarly, the tool results are typed with `ToolResult<NAME extends string, ARGS, RESULT>`.

The tools in `streamText` and `generateText` are defined as a `ToolSet`. The type inference helpers `ToolCallUnion<TOOLS extends ToolSet>` and `ToolResultUnion<TOOLS extends ToolSet>` can be used to extract the tool call and tool result types from the tools.

```
import{ openai }from'@ai-sdk/openai';import{ToolCallUnion,ToolResultUnion, generateText, tool }from'ai';import{ z }from'zod';const myToolSet ={  firstTool:tool({    description:'Greets the user',    parameters: z.object({ name: z.string()}),execute:async({ name })=>`Hello, ${name}!`,}),  secondTool:tool({    description:'Tells the user their age',    parameters: z.object({ age: z.number()}),execute:async({ age })=>`You are ${age} years old!`,}),};typeMyToolCall=ToolCallUnion<typeof myToolSet>;typeMyToolResult=ToolResultUnion<typeof myToolSet>;asyncfunctiongenerateSomething(prompt:string):Promise<{  text:string;  toolCalls:Array<MyToolCall>;// typed tool calls  toolResults:Array<MyToolResult>;// typed tool results}>{returngenerateText({    model:openai('gpt-4o'),    tools: myToolSet,    prompt,});}
```


## [Handling Errors](#handling-errors)


The AI SDK has three tool-call related errors:

-   [`NoSuchToolError`](/docs/reference/ai-sdk-errors/ai-no-such-tool-error): the model tries to call a tool that is not defined in the tools object
-   [`InvalidToolArgumentsError`](/docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error): the model calls a tool with arguments that do not match the tool's parameters
-   [`ToolExecutionError`](/docs/reference/ai-sdk-errors/ai-tool-execution-error): an error that occurred during tool execution
-   [`ToolCallRepairError`](/docs/reference/ai-sdk-errors/ai-tool-call-repair-error): an error that occurred during tool call repair


### [`generateText`](#generatetext)


`generateText` throws errors and can be handled using a `try`/`catch` block:

```
try{const result =awaitgenerateText({//...});}catch(error){if(NoSuchToolError.isInstance(error)){// handle the no such tool error}elseif(InvalidToolArgumentsError.isInstance(error)){// handle the invalid tool arguments error}elseif(ToolExecutionError.isInstance(error)){// handle the tool execution error}else{// handle other errors}}
```


### [`streamText`](#streamtext)


`streamText` sends the errors as part of the full stream. The error parts contain the error object.

When using `toDataStreamResponse`, you can pass an `getErrorMessage` function to extract the error message from the error part and forward it as part of the data stream response:

```
const result =streamText({// ...});return result.toDataStreamResponse({getErrorMessage: error =>{if(NoSuchToolError.isInstance(error)){return'The model tried to call a unknown tool.';}elseif(InvalidToolArgumentsError.isInstance(error)){return'The model called a tool with invalid arguments.';}elseif(ToolExecutionError.isInstance(error)){return'An error occurred during tool execution.';}else{return'An unknown error occurred.';}},});
```


## [Tool Call Repair](#tool-call-repair)


The tool call repair feature is experimental and may change in the future.

Language models sometimes fail to generate valid tool calls, especially when the parameters are complex or the model is smaller.

You can use the `experimental_repairToolCall` function to attempt to repair the tool call with a custom function.

You can use different strategies to repair the tool call:

-   Use a model with structured outputs to generate the arguments.
-   Send the messages, system prompt, and tool schema to a stronger model to generate the arguments.
-   Provide more specific repair instructions based on which tool was called.


### [Example: Use a model with structured outputs for repair](#example-use-a-model-with-structured-outputs-for-repair)


```
import{ openai }from'@ai-sdk/openai';import{ generateObject, generateText,NoSuchToolError, tool }from'ai';const result =awaitgenerateText({  model,  tools,  prompt,experimental_repairToolCall:async({    toolCall,    tools,    parameterSchema,    error,})=>{if(NoSuchToolError.isInstance(error)){returnnull;// do not attempt to fix invalid tool names}const tool = tools[toolCall.toolNameaskeyoftypeof tools];const{ object: repairedArgs }=awaitgenerateObject({      model:openai('gpt-4o',{ structuredOutputs:true}),      schema: tool.parameters,      prompt:[`The model tried to call the tool "${toolCall.toolName}"`+` with the following arguments:`,JSON.stringify(toolCall.args),`The tool accepts the following schema:`,JSON.stringify(parameterSchema(toolCall)),'Please fix the arguments.',].join('\n'),});return{...toolCall, args:JSON.stringify(repairedArgs)};},});
```


### [Example: Use the re-ask strategy for repair](#example-use-the-re-ask-strategy-for-repair)


```
import{ openai }from'@ai-sdk/openai';import{ generateObject, generateText,NoSuchToolError, tool }from'ai';const result =awaitgenerateText({  model,  tools,  prompt,experimental_repairToolCall:async({    toolCall,    tools,    error,    messages,    system,})=>{const result =awaitgenerateText({      model,      system,      messages:[...messages,{          role:'assistant',          content:[{type:'tool-call',              toolCallId: toolCall.toolCallId,              toolName: toolCall.toolName,              args: toolCall.args,},],},{          role:'tool'asconst,          content:[{type:'tool-result',              toolCallId: toolCall.toolCallId,              toolName: toolCall.toolName,              result: error.message,},],},],      tools,});const newToolCall = result.toolCalls.find(      newToolCall => newToolCall.toolName=== toolCall.toolName,);return newToolCall !=null?{          toolCallType:'function'asconst,          toolCallId: toolCall.toolCallId,          toolName: toolCall.toolName,          args:JSON.stringify(newToolCall.args),}:null;},});
```


## [Active Tools](#active-tools)


The `activeTools` property is experimental and may change in the future.

Language models can only handle a limited number of tools at a time, depending on the model. To allow for static typing using a large number of tools and limiting the available tools to the model at the same time, the AI SDK provides the `experimental_activeTools` property.

It is an array of tool names that are currently active. By default, the value is `undefined` and all tools are active.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:openai('gpt-4o'),  tools: myToolSet,  experimental_activeTools:['firstTool'],});
```


## [Multi-modal Tool Results](#multi-modal-tool-results)


Multi-modal tool results are experimental and only supported by Anthropic.

In order to send multi-modal tool results, e.g. screenshots, back to the model, they need to be converted into a specific format.

AI SDK Core tools have an optional `experimental_toToolResultContent` function that converts the tool result into a content part.

Here is an example for converting a screenshot into a content part:

```
const result =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-20241022'),  tools:{    computer: anthropic.tools.computer_20241022({// ...asyncexecute({ action, coordinate, text }){switch(action){case'screenshot':{return{type:'image',              data: fs.readFileSync('./data/screenshot-editor.png').toString('base64'),};}default:{return`executed ${action}`;}}},// map to tool result content for LLM consumption:experimental_toToolResultContent(result){returntypeof result ==='string'?[{type:'text', text: result }]:[{type:'image', data: result.data, mimeType:'image/png'}];},}),},// ...});
```


## [Extracting Tools](#extracting-tools)


Once you start having many tools, you might want to extract them into separate files. The `tool` helper function is crucial for this, because it ensures correct type inference.

Here is an example of an extracted tool:

tools/weather-tool.ts

```
import{ tool }from'ai';import{ z }from'zod';// the `tool` helper function ensures correct type inference:exportconst weatherTool =tool({  description:'Get the weather in a location',  parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,    temperature:72+Math.floor(Math.random()*21)-10,}),});
```


## [MCP Tools](#mcp-tools)


The MCP tools feature is experimental and may change in the future.

The AI SDK supports connecting to [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers to access their tools. This enables your AI applications to discover and use tools across various services through a standardized interface.


### [Initializing an MCP Client](#initializing-an-mcp-client)


Create an MCP client using either:

-   `SSE` (Server-Sent Events): Uses HTTP-based real-time communication, better suited for remote servers that need to send data over the network
-   `stdio`: Uses standard input and output streams for communication, ideal for local tool servers running on the same machine (like CLI tools or local services)
-   Custom transport: Bring your own transport by implementing the `MCPTransport` interface, ideal when implementing transports from MCP's official Typescript SDK (e.g. `StreamableHTTPClientTransport`)


#### [SSE Transport](#sse-transport)


The SSE can be configured using a simple object with a `type` and `url` property:

```
import{ experimental_createMCPClient as createMCPClient }from'ai';const mcpClient =awaitcreateMCPClient({  transport:{type:'sse',    url:'https://my-server.com/sse',// optional: configure HTTP headers, e.g. for authentication    headers:{Authorization:'Bearer my-api-key',},},});
```


#### [Stdio Transport](#stdio-transport)


The Stdio transport requires importing the `StdioMCPTransport` class from the `ai/mcp-stdio` package:

```
import{ experimental_createMCPClient as createMCPClient }from'ai';import{Experimental_StdioMCPTransportasStdioMCPTransport}from'ai/mcp-stdio';const mcpClient =awaitcreateMCPClient({  transport:newStdioMCPTransport({    command:'node',    args:['src/stdio/dist/server.js'],}),});
```


#### [Custom Transport](#custom-transport)


You can also bring your own transport, as long as it implements the `MCPTransport` interface. Below is an example of using the new `StreamableHTTPClientTransport` from MCP's official Typescript SDK:

```
import{MCPTransport,  experimental_createMCPClient as createMCPClient,}from'ai';import{StreamableHTTPClientTransport}from'@modelcontextprotocol/sdk/client/streamableHttp';const url =newURL('http://localhost:3000/mcp');const mcpClient =awaitcreateMCPClient({  transport:newStreamableHTTPClientTransport(url,{    sessionId:'session_123',}),});
```

The client returned by the `experimental_createMCPClient` function is a lightweight client intended for use in tool conversion. It currently does not support all features of the full MCP client, such as: authorization, session management, resumable streams, and receiving notifications.


#### [Closing the MCP Client](#closing-the-mcp-client)


After initialization, you should close the MCP client based on your usage pattern:

-   For short-lived usage (e.g., single requests), close the client when the response is finished
-   For long-running clients (e.g., command line apps), keep the client open but ensure it's closed when the application terminates

When streaming responses, you can close the client when the LLM response has finished. For example, when using `streamText`, you should use the `onFinish` callback:

```
const mcpClient =awaitexperimental_createMCPClient({// ...});const tools =await mcpClient.tools();const result =awaitstreamText({  model:openai('gpt-4o'),  tools,  prompt:'What is the weather in Brooklyn, New York?',onFinish:async()=>{await mcpClient.close();},});
```

When generating responses without streaming, you can use try/finally or cleanup functions in your framework:

```
let mcpClient:MCPClient|undefined;try{  mcpClient =awaitexperimental_createMCPClient({// ...});}finally{await mcpClient?.close();}
```


### [Using MCP Tools](#using-mcp-tools)


The client's `tools` method acts as an adapter between MCP tools and AI SDK tools. It supports two approaches for working with tool schemas:


#### [Schema Discovery](#schema-discovery)


The simplest approach where all tools offered by the server are listed, and input parameter types are inferred based the schemas provided by the server:

```
const tools =await mcpClient.tools();
```

**Pros:**

-   Simpler to implement
-   Automatically stays in sync with server changes

**Cons:**

-   No TypeScript type safety during development
-   No IDE autocompletion for tool parameters
-   Errors only surface at runtime
-   Loads all tools from the server


#### [Schema Definition](#schema-definition)


You can also define the tools and their input schemas explicitly in your client code:

```
import{ z }from'zod';const tools =await mcpClient.tools({  schemas:{'get-data':{      parameters: z.object({        query: z.string().describe('The data query'),        format: z.enum(['json','text']).optional(),}),},// For tools with zero arguments, you should use an empty object:'tool-with-no-args':{      parameters: z.object({}),},},});
```

**Pros:**

-   Control over which tools are loaded
-   Full TypeScript type safety
-   Better IDE support with autocompletion
-   Catch parameter mismatches during development

**Cons:**

-   Need to manually keep schemas in sync with server
-   More code to maintain

When you define `schemas`, the client will only pull the explicitly defined tools, even if the server offers additional tools. This can be beneficial for:

-   Keeping your application focused on the tools it needs
-   Reducing unnecessary tool loading
-   Making your tool dependencies explicit


## [Examples](#examples)


You can see tools in action using various frameworks in the following examples:

[

Learn to use tools in Node.js

](/cookbook/node/call-tools)[

Learn to use tools in Next.js with Route Handlers

](/cookbook/next/call-tools)[

Learn to use MCP tools in Node.js

](/cookbook/node/mcp-tools)
```

### 90. `docs/ai-sdk-core/transcription.md`

```markdown
# Transcription


---
url: https://ai-sdk.dev/docs/ai-sdk-core/transcription
description: Learn how to transcribe audio with the AI SDK.
---


# [Transcription](#transcription)


Transcription is an experimental feature.

The AI SDK provides the [`transcribe`](/docs/reference/ai-sdk-core/transcribe) function to transcribe audio using a transcription model.

```
import{ experimental_transcribe as transcribe }from'ai';import{ openai }from'@ai-sdk/openai';import{ readFile }from'fs/promises';const transcript =awaittranscribe({  model: openai.transcription('whisper-1'),  audio:awaitreadFile('audio.mp3'),});
```

The `audio` property can be a `Uint8Array`, `ArrayBuffer`, `Buffer`, `string` (base64 encoded audio data), or a `URL`.

To access the generated transcript:

```
const text = transcript.text;// transcript text e.g. "Hello, world!"const segments = transcript.segments;// array of segments with start and end times, if availableconst language = transcript.language;// language of the transcript e.g. "en", if availableconst durationInSeconds = transcript.durationInSeconds;// duration of the transcript in seconds, if available
```


## [Settings](#settings)



### [Provider-Specific settings](#provider-specific-settings)


Transcription models often have provider or model-specific settings which you can set using the `providerOptions` parameter.

```
import{ experimental_transcribe as transcribe }from'ai';import{ openai }from'@ai-sdk/openai';import{ readFile }from'fs/promises';const transcript =awaittranscribe({  model: openai.transcription('whisper-1'),  audio:awaitreadFile('audio.mp3'),  providerOptions:{    openai:{      timestampGranularities:['word'],},},});
```


### [Abort Signals and Timeouts](#abort-signals-and-timeouts)


`transcribe` accepts an optional `abortSignal` parameter of type [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal) that you can use to abort the transcription process or set a timeout.

```
import{ openai }from'@ai-sdk/openai';import{ experimental_transcribe as transcribe }from'ai';import{ readFile }from'fs/promises';const transcript =awaittranscribe({  model: openai.transcription('whisper-1'),  audio:awaitreadFile('audio.mp3'),  abortSignal:AbortSignal.timeout(1000),// Abort after 1 second});
```


### [Custom Headers](#custom-headers)


`transcribe` accepts an optional `headers` parameter of type `Record<string, string>` that you can use to add custom headers to the transcription request.

```
import{ openai }from'@ai-sdk/openai';import{ experimental_transcribe as transcribe }from'ai';import{ readFile }from'fs/promises';const transcript =awaittranscribe({  model: openai.transcription('whisper-1'),  audio:awaitreadFile('audio.mp3'),  headers:{'X-Custom-Header':'custom-value'},});
```


### [Warnings](#warnings)


Warnings (e.g. unsupported parameters) are available on the `warnings` property.

```
import{ openai }from'@ai-sdk/openai';import{ experimental_transcribe as transcribe }from'ai';import{ readFile }from'fs/promises';const transcript =awaittranscribe({  model: openai.transcription('whisper-1'),  audio:awaitreadFile('audio.mp3'),});const warnings = transcript.warnings;
```


### [Error Handling](#error-handling)


When `transcribe` cannot generate a valid transcript, it throws a [`AI_NoTranscriptGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error).

This error can arise for any the following reasons:

-   The model failed to generate a response
-   The model generated a response that could not be parsed

The error preserves the following information to help you log the issue:

-   `responses`: Metadata about the transcription model responses, including timestamp, model, and headers.
-   `cause`: The cause of the error. You can use this for more detailed error handling.

```
import{  experimental_transcribe as transcribe,NoTranscriptGeneratedError,}from'ai';import{ openai }from'@ai-sdk/openai';import{ readFile }from'fs/promises';try{awaittranscribe({    model: openai.transcription('whisper-1'),    audio:awaitreadFile('audio.mp3'),});}catch(error){if(NoTranscriptGeneratedError.isInstance(error)){console.log('NoTranscriptGeneratedError');console.log('Cause:', error.cause);console.log('Responses:', error.responses);}}
```


## [Transcription Models](#transcription-models)


Provider

Model

[OpenAI](/providers/ai-sdk-providers/openai#transcription-models)

`whisper-1`

[OpenAI](/providers/ai-sdk-providers/openai#transcription-models)

`gpt-4o-transcribe`

[OpenAI](/providers/ai-sdk-providers/openai#transcription-models)

`gpt-4o-mini-transcribe`

[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#transcription-models)

`scribe_v1`

[ElevenLabs](/providers/ai-sdk-providers/elevenlabs#transcription-models)

`scribe_v1_experimental`

[Groq](/providers/ai-sdk-providers/groq#transcription-models)

`whisper-large-v3-turbo`

[Groq](/providers/ai-sdk-providers/groq#transcription-models)

`distil-whisper-large-v3-en`

[Groq](/providers/ai-sdk-providers/groq#transcription-models)

`whisper-large-v3`

[Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)

`whisper-1`

[Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)

`gpt-4o-transcribe`

[Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)

`gpt-4o-mini-transcribe`

[Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)

`machine`

[Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)

`low_cost`

[Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)

`fusion`

[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)

`base` (+ variants)

[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)

`enhanced` (+ variants)

[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)

`nova` (+ variants)

[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)

`nova-2` (+ variants)

[Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)

`nova-3` (+ variants)

[Gladia](/providers/ai-sdk-providers/gladia#transcription-models)

`default`

[AssemblyAI](/providers/ai-sdk-providers/assemblyai#transcription-models)

`best`

[AssemblyAI](/providers/ai-sdk-providers/assemblyai#transcription-models)

`nano`

[Fal](/providers/ai-sdk-providers/fal#transcription-models)

`whisper`

[Fal](/providers/ai-sdk-providers/fal#transcription-models)

`wizper`

Above are a small subset of the transcription models supported by the AI SDK providers. For more, see the respective provider documentation.
```

### 91. `docs/ai-sdk-core.md`

```markdown
# AI SDK Core


---
url: https://ai-sdk.dev/docs/ai-sdk-core
description: Learn about AI SDK Core.
---


# [AI SDK Core](#ai-sdk-core)


[

Overview

Learn about AI SDK Core and how to work with Large Language Models (LLMs).

](/docs/ai-sdk-core/overview)[

Generating Text

Learn how to generate text.

](/docs/ai-sdk-core/generating-text)[

Generating Structured Data

Learn how to generate structured data.

](/docs/ai-sdk-core/generating-structured-data)[

Tool Calling

Learn how to do tool calling with AI SDK Core.

](/docs/ai-sdk-core/tools-and-tool-calling)[

Prompt Engineering

Learn how to write prompts with AI SDK Core.

](/docs/ai-sdk-core/prompt-engineering)[

Settings

Learn how to set up settings for language models generations.

](/docs/ai-sdk-core/settings)[

Embeddings

Learn how to use embeddings with AI SDK Core.

](/docs/ai-sdk-core/embeddings)[

Image Generation

Learn how to generate images with AI SDK Core.

](/docs/ai-sdk-core/image-generation)[

Transcription

Learn how to transcribe audio with AI SDK Core.

](/docs/ai-sdk-core/transcription)[

Speech

Learn how to generate speech with AI SDK Core.

](/docs/ai-sdk-core/speech)[

Provider Management

Learn how to work with multiple providers.

](/docs/ai-sdk-core/provider-management)[

Middleware

Learn how to use middleware with AI SDK Core.

](/docs/ai-sdk-core/middleware)[

Error Handling

Learn how to handle errors with AI SDK Core.

](/docs/ai-sdk-core/error-handling)[

Testing

Learn how to test with AI SDK Core.

](/docs/ai-sdk-core/testing)[

Telemetry

Learn how to use telemetry with AI SDK Core.

](/docs/ai-sdk-core/telemetry)
```

### 92. `docs/ai-sdk-rsc/authentication.md`

```markdown
# Authentication


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/authentication
description: Learn how to authenticate with the AI SDK.
---


# [Authentication](#authentication)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

The RSC API makes extensive use of [`Server Actions`](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations) to power streaming values and UI from the server.

Server Actions are exposed as public, unprotected endpoints. As a result, you should treat Server Actions as you would public-facing API endpoints and ensure that the user is authorized to perform the action before returning any data.

app/actions.tsx

```
'use server';import{ cookies }from'next/headers';import{ createStremableUI }from'ai/rsc';import{ validateToken }from'../utils/auth';exportconstgetWeather=async()=>{const token =cookies().get('token');if(!token |!validateToken(token)){return{      error:'This action requires authentication',};}const streamableDisplay =createStreamableUI(null);  streamableDisplay.update(<Skeleton/>);  streamableDisplay.done(<Weather/>);return{    display: streamableDisplay.value,};};
```
```

### 93. `docs/ai-sdk-rsc/error-handling.md`

```markdown
# Error Handling


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/error-handling
description: Learn how to handle errors with the AI SDK.
---


# [Error Handling](#error-handling)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

Two categories of errors can occur when working with the RSC API: errors while streaming user interfaces and errors while streaming other values.


## [Handling UI Errors](#handling-ui-errors)


To handle errors while generating UI, the [`streamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) object exposes an `error()` method.

app/actions.tsx

```
'use server';import{ createStreamableUI }from'ai/rsc';exportasyncfunctiongetStreamedUI(){const ui =createStreamableUI();(async()=>{    ui.update(<div>loading</div>);const data =awaitfetchData();    ui.done(<div>{data}</div>);})().catch(e=>{    ui.error(<div>Error:{e.message}</div>);});return ui.value;}
```

With this method, you can catch any error with the stream, and return relevant UI. On the client, you can also use a [React Error Boundary](https://react.dev/reference/react/Component#catching-rendering-errors-with-an-error-boundary) to wrap the streamed component and catch any additional errors.

app/page.tsx

```
import{ getStreamedUI }from'@/actions';import{ useState }from'react';import{ErrorBoundary}from'./ErrorBoundary';exportdefaultfunctionPage(){const[streamedUI, setStreamedUI]=useState(null);return(<div><buttononClick={async()=>{const newUI =awaitgetStreamedUI();setStreamedUI(newUI);}}>What does the newUI look like?</button><ErrorBoundary>{streamedUI}</ErrorBoundary></div>);}
```


## [Handling Other Errors](#handling-other-errors)


To handle other errors while streaming, you can return an error object that the receiver can use to determine why the failure occurred.

app/actions.tsx

```
'use server';import{ createStreamableValue }from'ai/rsc';import{ fetchData, emptyData }from'../utils/data';exportconstgetStreamedData=async()=>{const streamableData =createStreamableValue<string>(emptyData);try{(()=>{const data1 =awaitfetchData();      streamableData.update(data1);const data2 =awaitfetchData();      streamableData.update(data2);const data3 =awaitfetchData();      streamableData.done(data3);})();return{ data: streamableData.value };}catch(e){return{ error: e.message };}};
```
```

### 94. `docs/ai-sdk-rsc/generative-ui-state.md`

```markdown
# Managing Generative UI State


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/generative-ui-state
description: Overview of the AI and UI states
---


# [Managing Generative UI State](#managing-generative-ui-state)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

State is an essential part of any application. State is particularly important in AI applications as it is passed to large language models (LLMs) on each request to ensure they have the necessary context to produce a great generation. Traditional chatbots are text-based and have a structure that mirrors that of any chat application.

For example, in a chatbot, state is an array of `messages` where each `message` has:

-   `id`: a unique identifier
-   `role`: who sent the message (user/assistant/system/tool)
-   `content`: the content of the message

This state can be rendered in the UI and sent to the model without any modifications.

With Generative UI, the model can now return a React component, rather than a plain text message. The client can render that component without issue, but that state can't be sent back to the model because React components aren't serialisable. So, what can you do?

**The solution is to split the state in two, where one (AI State) becomes a proxy for the other (UI State)**.

One way to understand this concept is through a Lego analogy. Imagine a 10,000 piece Lego model that, once built, cannot be easily transported because it is fragile. By taking the model apart, it can be easily transported, and then rebuilt following the steps outlined in the instructions pamphlet. In this way, the instructions pamphlet is a proxy to the physical structure. Similarly, AI State provides a serialisable (JSON) representation of your UI that can be passed back and forth to the model.


## [What is AI and UI State?](#what-is-ai-and-ui-state)


The RSC API simplifies how you manage AI State and UI State, providing a robust way to keep them in sync between your database, server and client.


### [AI State](#ai-state)


AI State refers to the state of your application in a serialisable format that will be used on the server and can be shared with the language model.

For a chat app, the AI State is the conversation history (messages) between the user and the assistant. Components generated by the model would be represented in a JSON format as a tool alongside any necessary props. AI State can also be used to store other values and meta information such as `createdAt` for each message and `chatId` for each conversation. The LLM reads this history so it can generate the next message. This state serves as the source of truth for the current application state.

**Note**: AI state can be accessed/modified from both the server and the client.


### [UI State](#ui-state)


UI State refers to the state of your application that is rendered on the client. It is a fully client-side state (similar to `useState`) that can store anything from Javascript values to React elements. UI state is a list of actual UI elements that are rendered on the client.

**Note**: UI State can only be accessed client-side.


## [Using AI / UI State](#using-ai--ui-state)



### [Creating the AI Context](#creating-the-ai-context)


AI SDK RSC simplifies managing AI and UI state across your application by providing several hooks. These hooks are powered by [React context](https://react.dev/reference/react/hooks#context-hooks) under the hood.

Notably, this means you do not have to pass the message history to the server explicitly for each request. You also can access and update your application state in any child component of the context provider. As you begin building [multistep generative interfaces](/docs/ai-sdk-rsc/multistep-interfaces), this will be particularly helpful.

To use `ai/rsc` to manage AI and UI State in your application, you can create a React context using [`createAI`](/docs/reference/ai-sdk-rsc/create-ai):

app/actions.tsx

```
// Define the AI state and UI state typesexporttypeServerMessage={  role:'user'|'assistant';  content:string;};exporttypeClientMessage={  id:string;  role:'user'|'assistant';  display:ReactNode;};exportconst sendMessage =async(input:string):Promise<ClientMessage>=>{"use server"...}
```

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ClientMessage,ServerMessage, sendMessage }from'./actions';exporttypeAIState=ServerMessage[];exporttypeUIState=ClientMessage[];// Create the AI provider with the initial states and allowed actionsexportconstAI=createAI<AIState,UIState>({  initialAIState:[],  initialUIState:[],  actions:{    sendMessage,},});
```

You must pass Server Actions to the `actions` object.

In this example, you define types for AI State and UI State, respectively.

Next, wrap your application with your newly created context. With that, you can get and set AI and UI State across your entire application.

app/layout.tsx

```
import{typeReactNode}from'react';import{AI}from'./ai';exportdefaultfunctionRootLayout({  children,}: Readonly<{ children: ReactNode }>){return(<AI><htmllang="en"><body>{children}</body></html></AI>);}
```


## [Reading UI State in Client](#reading-ui-state-in-client)


The UI state can be accessed in Client Components using the [`useUIState`](/docs/reference/ai-sdk-rsc/use-ui-state) hook provided by the RSC API. The hook returns the current UI state and a function to update the UI state like React's `useState`.

app/page.tsx

```
'use client';import{ useUIState }from'ai/rsc';exportdefaultfunctionPage(){const[messages, setMessages]=useUIState();return(<ul>{messages.map(message=>(<likey={message.id}>{message.display}</li>))}</ul>);}
```


## [Reading AI State in Client](#reading-ai-state-in-client)


The AI state can be accessed in Client Components using the [`useAIState`](/docs/reference/ai-sdk-rsc/use-ai-state) hook provided by the RSC API. The hook returns the current AI state.

app/page.tsx

```
'use client';import{ useAIState }from'ai/rsc';exportdefaultfunctionPage(){const[messages, setMessages]=useAIState();return(<ul>{messages.map(message=>(<likey={message.id}>{message.content}</li>))}</ul>);}
```


## [Reading AI State on Server](#reading-ai-state-on-server)


The AI State can be accessed within any Server Action provided to the `createAI` context using the [`getAIState`](/docs/reference/ai-sdk-rsc/get-ai-state) function. It returns the current AI state as a read-only value:

app/actions.ts

```
import{ getAIState }from'ai/rsc';exportasyncfunctionsendMessage(message: string){'use server';const history =getAIState();const response =awaitgenerateText({    model:openai('gpt-3.5-turbo'),    messages:[...history,{ role:'user', content: message }],});return response;}
```

Remember, you can only access state within actions that have been passed to the `createAI` context within the `actions` key.


## [Updating AI State on Server](#updating-ai-state-on-server)


The AI State can also be updated from within your Server Action with the [`getMutableAIState`](/docs/reference/ai-sdk-rsc/get-mutable-ai-state) function. This function is similar to `getAIState`, but it returns the state with methods to read and update it:

app/actions.ts

```
import{ getMutableAIState }from'ai/rsc';exportasyncfunctionsendMessage(message: string){'use server';const history =getMutableAIState();// Update the AI state with the new user message.  history.update([...history.get(),{ role:'user', content: message }]);const response =awaitgenerateText({    model:openai('gpt-3.5-turbo'),    messages: history.get(),});// Update the AI state again with the response from the model.  history.done([...history.get(),{ role:'assistant', content: response }]);return response;}
```

It is important to update the AI State with new responses using `.update()` and `.done()` to keep the conversation history in sync.


## [Calling Server Actions from the Client](#calling-server-actions-from-the-client)


To call the `sendMessage` action from the client, you can use the [`useActions`](/docs/reference/ai-sdk-rsc/use-actions) hook. The hook returns all the available Actions that were provided to `createAI`:

app/page.tsx

```
'use client';import{ useActions, useUIState }from'ai/rsc';import{AI}from'./ai';exportdefaultfunctionPage(){const{ sendMessage }=useActions<typeofAI>();const[messages, setMessages]=useUIState();consthandleSubmit=asyncevent=>{    event.preventDefault();setMessages([...messages,{ id:Date.now(), role:'user', display: event.target.message.value },]);const response =awaitsendMessage(event.target.message.value);setMessages([...messages,{ id:Date.now(), role:'assistant', display: response },]);};return(<><ul>{messages.map(message=>(<likey={message.id}>{message.display}</li>))}</ul><formonSubmit={handleSubmit}><inputtype="text"name="message"/><buttontype="submit">Send</button></form></>);}
```

When the user submits a message, the `sendMessage` action is called with the message content. The response from the action is then added to the UI state, updating the displayed messages.

Important! Don't forget to update the UI State after you call your Server Action otherwise the streamed component will not show in the UI.

To learn more, check out this [example](/examples/next-app/state-management/ai-ui-states) on managing AI and UI state using `ai/rsc`.

Next, you will learn how you can save and restore state with `ai/rsc`.
```

### 95. `docs/ai-sdk-rsc/loading-state.md`

```markdown
# Handling Loading State


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/loading-state
description: Overview of handling loading state with AI SDK RSC
---


# [Handling Loading State](#handling-loading-state)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

Given that responses from language models can often take a while to complete, it's crucial to be able to show loading state to users. This provides visual feedback that the system is working on their request and helps maintain a positive user experience.

There are three approaches you can take to handle loading state with the AI SDK RSC:

-   Managing loading state similar to how you would in a traditional Next.js application. This involves setting a loading state variable in the client and updating it when the response is received.
-   Streaming loading state from the server to the client. This approach allows you to track loading state on a more granular level and provide more detailed feedback to the user.
-   Streaming loading component from the server to the client. This approach allows you to stream a React Server Component to the client while awaiting the model's response.


## [Handling Loading State on the Client](#handling-loading-state-on-the-client)



### [Client](#client)


Let's create a simple Next.js page that will call the `generateResponse` function when the form is submitted. The function will take in the user's prompt (`input`) and then generate a response (`response`). To handle the loading state, use the `loading` state variable. When the form is submitted, set `loading` to `true`, and when the response is received, set it back to `false`. While the response is being streamed, the input field will be disabled.

app/page.tsx

```
'use client';import{ useState }from'react';import{ generateResponse }from'./actions';import{ readStreamableValue }from'ai/rsc';// Force the page to be dynamic and allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[input, setInput]=useState<string>('');const[generation, setGeneration]=useState<string>('');const[loading, setLoading]=useState<boolean>(false);return(<div><div>{generation}</div><form        onSubmit={asynce=>{          e.preventDefault();setLoading(true);const response =awaitgenerateResponse(input);let textContent ='';forawait(const delta ofreadStreamableValue(response)){            textContent =`${textContent}${delta}`;setGeneration(textContent);}setInput('');setLoading(false);}}><inputtype="text"value={input}disabled={loading}className="disabled:opacity-50"onChange={event=>{setInput(event.target.value);}}/><button>SendMessage</button></form></div>);}
```


### [Server](#server)


Now let's implement the `generateResponse` function. Use the `streamText` function to generate a response to the input.

app/actions.ts

```
'use server';import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';import{ createStreamableValue }from'ai/rsc';exportasyncfunctiongenerateResponse(prompt:string){const stream =createStreamableValue();(async()=>{const{ textStream }=streamText({      model:openai('gpt-4o'),      prompt,});forawait(const text of textStream){      stream.update(text);}    stream.done();})();return stream.value;}
```


## [Streaming Loading State from the Server](#streaming-loading-state-from-the-server)


If you are looking to track loading state on a more granular level, you can create a new streamable value to store a custom variable and then read this on the frontend. Let's update the example to create a new streamable value for tracking loading state:


### [Server](#server-1)


app/actions.ts

```
'use server';import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';import{ createStreamableValue }from'ai/rsc';exportasyncfunctiongenerateResponse(prompt:string){const stream =createStreamableValue();const loadingState =createStreamableValue({ loading:true});(async()=>{const{ textStream }=streamText({      model:openai('gpt-4o'),      prompt,});forawait(const text of textStream){      stream.update(text);}    stream.done();    loadingState.done({ loading:false});})();return{ response: stream.value, loadingState: loadingState.value};}
```


### [Client](#client-1)


app/page.tsx

```
'use client';import{ useState }from'react';import{ generateResponse }from'./actions';import{ readStreamableValue }from'ai/rsc';// Force the page to be dynamic and allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[input, setInput]=useState<string>('');const[generation, setGeneration]=useState<string>('');const[loading, setLoading]=useState<boolean>(false);return(<div><div>{generation}</div><form        onSubmit={asynce=>{          e.preventDefault();setLoading(true);const{ response, loadingState }=awaitgenerateResponse(input);let textContent ='';forawait(const responseDelta ofreadStreamableValue(response)){            textContent =`${textContent}${responseDelta}`;setGeneration(textContent);}forawait(const loadingDelta ofreadStreamableValue(loadingState)){if(loadingDelta){setLoading(loadingDelta.loading);}}setInput('');setLoading(false);}}><inputtype="text"value={input}disabled={loading}className="disabled:opacity-50"onChange={event=>{setInput(event.target.value);}}/><button>SendMessage</button></form></div>);}
```

This allows you to provide more detailed feedback about the generation process to your users.


## [Streaming Loading Components with `streamUI`](#streaming-loading-components-with-streamui)


If you are using the [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function, you can stream the loading state to the client in the form of a React component. `streamUI` supports the usage of [JavaScript generator functions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*) , which allow you to yield some value (in this case a React component) while some other blocking work completes.


## [Server](#server-2)


```
'use server';import{ openai }from'@ai-sdk/openai';import{ streamUI }from'ai/rsc';exportasyncfunctiongenerateResponse(prompt:string){const result =awaitstreamUI({    model:openai('gpt-4o'),    prompt,text:asyncfunction*({ content }){yield<div>loading...</div>;return<div>{content}</div>;},});return result.value;}
```

Remember to update the file from `.ts` to `.tsx` because you are defining a React component in the `streamUI` function.


## [Client](#client-2)


```
'use client';import{ useState }from'react';import{ generateResponse }from'./actions';import{ readStreamableValue }from'ai/rsc';// Force the page to be dynamic and allow streaming responses up to 30 secondsexportconst maxDuration =30;exportdefaultfunctionHome(){const[input, setInput]=useState<string>('');const[generation, setGeneration]=useState<React.ReactNode>();return(<div><div>{generation}</div><formonSubmit={asynce=>{          e.preventDefault();const result =awaitgenerateResponse(input);setGeneration(result);setInput('');}}><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><button>SendMessage</button></form></div>);}
```
```

### 96. `docs/ai-sdk-rsc/migrating-to-ui.md`

```markdown
# Migrating from RSC to UI


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/migrating-to-ui
description: Learn how to migrate from AI SDK RSC to AI SDK UI.
---


# [Migrating from RSC to UI](#migrating-from-rsc-to-ui)


This guide helps you migrate from AI SDK RSC to AI SDK UI.


## [Background](#background)


The AI SDK has two packages that help you build the frontend for your applications – [AI SDK UI](/docs/ai-sdk-ui) and [AI SDK RSC](/docs/ai-sdk-rsc).

We introduced support for using [React Server Components](https://react.dev/reference/rsc/server-components) (RSC) within the AI SDK to simplify building generative user interfaces for frameworks that support RSC.

However, given we're pushing the boundaries of this technology, AI SDK RSC currently faces significant limitations that make it unsuitable for stable production use.

-   It is not possible to abort a stream using server actions. This will be improved in future releases of React and Next.js [(1122)](https://github.com/vercel/ai/issues/1122).
-   When using `createStreamableUI` and `streamUI`, components remount on `.done()`, causing them to flicker [(2939)](https://github.com/vercel/ai/issues/2939).
-   Many suspense boundaries can lead to crashes [(2843)](https://github.com/vercel/ai/issues/2843).
-   Using `createStreamableUI` can lead to quadratic data transfer. You can avoid this using createStreamableValue instead, and rendering the component client-side.
-   Closed RSC streams cause update issues [(3007)](https://github.com/vercel/ai/issues/3007).

Due to these limitations, AI SDK RSC is marked as experimental, and we do not recommend using it for stable production environments.

As a result, we strongly recommend migrating to AI SDK UI, which has undergone extensive development to provide a more stable and production grade experience.

In building [v0](https://v0.dev), we have invested considerable time exploring how to create the best chat experience on the web. AI SDK UI ships with many of these best practices and commonly used patterns like [language model middleware](/docs/ai-sdk-core/middleware), [multi-step tool calls](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls), [attachments](/docs/ai-sdk-ui/chatbot#attachments-experimental), [telemetry](/docs/ai-sdk-core/telemetry), [provider registry](/docs/ai-sdk-core/provider-management#provider-registry), and many more. These features have been considerately designed into a neat abstraction that you can use to reliably integrate AI into your applications.


## [Streaming Chat Completions](#streaming-chat-completions)



### [Basic Setup](#basic-setup)


The `streamUI` function executes as part of a server action as illustrated below.


#### [Before: Handle generation and rendering in a single server action](#before-handle-generation-and-rendering-in-a-single-server-action)


@/app/actions.tsx

```
import{ openai }from'@ai-sdk/openai';import{ getMutableAIState, streamUI }from'ai/rsc';exportasyncfunctionsendMessage(message: string){'use server';const messages =getMutableAIState('messages');  messages.update([...messages.get(),{ role:'user', content: message }]);const{ value: stream }=awaitstreamUI({    model:openai('gpt-4o'),    system:'you are a friendly assistant!',    messages: messages.get(),text:asyncfunction*({ content, done }){// process text},    tools:{// tool definitions},});return stream;}
```


#### [Before: Call server action and update UI state](#before-call-server-action-and-update-ui-state)


The chat interface calls the server action. The response is then saved using the `useUIState` hook.

@/app/page.tsx

```
'use client';import{ useState,ReactNode}from'react';import{ useActions, useUIState }from'ai/rsc';exportdefaultfunctionPage(){const{ sendMessage }=useActions();const[input, setInput]=useState('');const[messages, setMessages]=useUIState();return(<div>{messages.map(message=> message)}<formonSubmit={async()=>{const response:ReactNode=awaitsendMessage(input);setMessages(msgs=>[...msgs, response]);}}><inputtype="text"/><buttontype="submit">Submit</button></form></div>);}
```

The `streamUI` function combines generating text and rendering the user interface. To migrate to AI SDK UI, you need to **separate these concerns** – streaming generations with `streamText` and rendering the UI with `useChat`.


#### [After: Replace server action with route handler](#after-replace-server-action-with-route-handler)


The `streamText` function executes as part of a route handler and streams the response to the client. The `useChat` hook on the client decodes this stream and renders the response within the chat interface.

@/app/api/chat/route.ts

```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctionPOST(request){const{ messages }=await request.json();const result =streamText({    model:openai('gpt-4o'),    system:'you are a friendly assistant!',    messages,    tools:{// tool definitions},});return result.toDataStreamResponse();}
```


#### [After: Update client to use chat hook](#after-update-client-to-use-chat-hook)


@/app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, setInput, handleSubmit }=useChat();return(<div>{messages.map(message=>(<divkey={message.id}><div>{message.role}</div><div>{message.content}</div></div>))}<formonSubmit={handleSubmit}><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttontype="submit">Send</button></form></div>);}
```


### [Parallel Tool Calls](#parallel-tool-calls)


In AI SDK RSC, `streamUI` does not support parallel tool calls. You will have to use a combination of `streamText`, `createStreamableUI` and `createStreamableValue`.

With AI SDK UI, `useChat` comes with built-in support for parallel tool calls. You can define multiple tools in the `streamText` and have them called them in parallel. The `useChat` hook will then handle the parallel tool calls for you automatically.


### [Multi-Step Tool Calls](#multi-step-tool-calls)


In AI SDK RSC, `streamUI` does not support multi-step tool calls. You will have to use a combination of `streamText`, `createStreamableUI` and `createStreamableValue`.

With AI SDK UI, `useChat` comes with built-in support for multi-step tool calls. You can set `maxSteps` in the `streamText` function to define the number of steps the language model can make in a single call. The `useChat` hook will then handle the multi-step tool calls for you automatically.


### [Generative User Interfaces](#generative-user-interfaces)


The `streamUI` function uses `tools` as a way to execute functions based on user input and renders React components based on the function output to go beyond text in the chat interface.


#### [Before: Render components within the server action and stream to client](#before-render-components-within-the-server-action-and-stream-to-client)


@/app/actions.tsx

```
import{ z }from'zod';import{ streamUI }from'ai/rsc';import{ openai }from'@ai-sdk/openai';import{ getWeather }from'@/utils/queries';import{Weather}from'@/components/weather';const{ value: stream }=awaitstreamUI({  model:openai('gpt-4o'),  system:'you are a friendly assistant!',  messages,text:asyncfunction*({ content, done }){// process text},  tools:{    displayWeather:{      description:'Display the weather for a location',      parameters: z.object({        latitude: z.number(),        longitude: z.number(),}),generate:asyncfunction*({ latitude, longitude }){yield<div>Loading weather...</div>;const{ value, unit }=awaitgetWeather({ latitude, longitude });return<Weathervalue={value}unit={unit}/>;},},},});
```

As mentioned earlier, `streamUI` generates text and renders the React component in a single server action call.


#### [After: Replace with route handler and stream props data to client](#after-replace-with-route-handler-and-stream-props-data-to-client)


The `streamText` function streams the props data as response to the client, while `useChat` decode the stream as `toolInvocations` and renders the chat interface.

@/app/api/chat/route.ts

```
import{ z }from'zod';import{ openai }from'@ai-sdk/openai';import{ getWeather }from'@/utils/queries';import{ streamText }from'ai';exportasyncfunctionPOST(request){const{ messages }=await request.json();const result =streamText({    model:openai('gpt-4o'),    system:'you are a friendly assistant!',    messages,    tools:{      displayWeather:{        description:'Display the weather for a location',        parameters: z.object({          latitude: z.number(),          longitude: z.number(),}),execute:asyncfunction({ latitude, longitude }){const props =awaitgetWeather({ latitude, longitude });return props;},},},});return result.toDataStreamResponse();}
```


#### [After: Update client to use chat hook and render components using tool invocations](#after-update-client-to-use-chat-hook-and-render-components-using-tool-invocations)


@/app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{Weather}from'@/components/weather';exportdefaultfunctionPage(){const{ messages, input, setInput, handleSubmit }=useChat();return(<div>{messages.map(message=>(<divkey={message.id}><div>{message.role}</div><div>{message.content}</div><div>{message.toolInvocations.map(toolInvocation=>{const{ toolName, toolCallId, state }= toolInvocation;if(state ==='result'){const{ result }= toolInvocation;return(<divkey={toolCallId}>{toolName ==='displayWeather'?(<WeatherweatherAtLocation={result}/>):null}</div>);}else{return(<divkey={toolCallId}>{toolName ==='displayWeather'?(<div>Loading weather...</div>):null}</div>);}})}</div></div>))}<formonSubmit={handleSubmit}><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttontype="submit">Send</button></form></div>);}
```


### [Handling Client Interactions](#handling-client-interactions)


With AI SDK RSC, components streamed to the client can trigger subsequent generations by calling the relevant server action using the `useActions` hooks. This is possible as long as the component is a descendant of the `<AI/>` context provider.


#### [Before: Use actions hook to send messages](#before-use-actions-hook-to-send-messages)


@/app/components/list-flights.tsx

```
'use client';import{ useActions, useUIState }from'ai/rsc';exportfunctionListFlights({ flights }){const{ sendMessage }=useActions();const[_, setMessages]=useUIState();return(<div>{flights.map(flight=>(<divkey={flight.id}onClick={async()=>{const response =awaitsendMessage(`I would like to choose flight ${flight.id}!`,);setMessages(msgs=>[...msgs, response]);}}>{flight.name}</div>))}</div>);}
```


#### [After: Use another chat hook with same ID from the component](#after-use-another-chat-hook-with-same-id-from-the-component)


After switching to AI SDK UI, these messages are synced by initializing the `useChat` hook in the component with the same `id` as the parent component.

@/app/components/list-flights.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportfunctionListFlights({ chatId, flights }){const{ append }=useChat({    id: chatId,    body:{ id: chatId },    maxSteps:5,});return(<div>{flights.map(flight=>(<div          key={flight.id}          onClick={async()=>{awaitappend({              role:'user',              content:`I would like to choose flight ${flight.id}!`,});}}>{flight.name}</div>))}</div>);}
```


### [Loading Indicators](#loading-indicators)


In AI SDK RSC, you can use the `initial` parameter of `streamUI` to define the component to display while the generation is in progress.


#### [Before: Use `loading` to show loading indicator](#before-use-loading-to-show-loading-indicator)


@/app/actions.tsx

```
import{ openai }from'@ai-sdk/openai';import{ streamUI }from'ai/rsc';const{ value: stream }=awaitstreamUI({  model:openai('gpt-4o'),  system:'you are a friendly assistant!',  messages,  initial:<div>Loading...</div>,text:asyncfunction*({ content, done }){// process text},  tools:{// tool definitions},});return stream;
```

With AI SDK UI, you can use the tool invocation state to show a loading indicator while the tool is executing.


#### [After: Use tool invocation state to show loading indicator](#after-use-tool-invocation-state-to-show-loading-indicator)


@/app/components/message.tsx

```
'use client';exportfunctionMessage({ role, content, toolInvocations }){return(<div><div>{role}</div><div>{content}</div>{toolInvocations &&(<div>{toolInvocations.map(toolInvocation=>{const{ toolName, toolCallId, state }= toolInvocation;if(state ==='result'){const{ result }= toolInvocation;return(<divkey={toolCallId}>{toolName ==='getWeather'?(<WeatherweatherAtLocation={result}/>):null}</div>);}else{return(<divkey={toolCallId}>{toolName ==='getWeather'?(<WeatherisLoading={true}/>):(<div>Loading...</div>)}</div>);}})}</div>)}</div>);}
```


### [Saving Chats](#saving-chats)


Before implementing `streamUI` as a server action, you should create an `<AI/>` provider and wrap your application at the root layout to sync the AI and UI states. During initialization, you typically use the `onSetAIState` callback function to track updates to the AI state and save it to the database when `done(...)` is called.


#### [Before: Save chats using callback function of context provider](#before-save-chats-using-callback-function-of-context-provider)


@/app/actions.ts

```
import{ createAI }from'ai/rsc';import{ saveChat }from'@/utils/queries';exportconstAI=createAI({  initialAIState:{},  initialUIState:{},  actions:{// server actions},onSetAIState:async({ state, done })=>{'use server';if(done){awaitsaveChat(state);}},});
```


#### [After: Save chats using callback function of `streamText`](#after-save-chats-using-callback-function-of-streamtext)


With AI SDK UI, you will save chats using the `onFinish` callback function of `streamText` in your route handler.

@/app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ saveChat }from'@/utils/queries';import{ streamText, convertToCoreMessages }from'ai';exportasyncfunctionPOST(request){const{ id, messages }=await request.json();const coreMessages =convertToCoreMessages(messages);const result =streamText({    model:openai('gpt-4o'),    system:'you are a friendly assistant!',    messages: coreMessages,onFinish:async({ response })=>{try{awaitsaveChat({          id,          messages:[...coreMessages,...response.messages],});}catch(error){console.error('Failed to save chat');}},});return result.toDataStreamResponse();}
```


### [Restoring Chats](#restoring-chats)


When using AI SDK RSC, the `useUIState` hook contains the UI state of the chat. When restoring a previously saved chat, the UI state needs to be loaded with messages.

Similar to how you typically save chats in AI SDK RSC, you should use the `onGetUIState` callback function to retrieve the chat from the database, convert it into UI state, and return it to be accessible through `useUIState`.


#### [Before: Load chat from database using callback function of context provider](#before-load-chat-from-database-using-callback-function-of-context-provider)


@/app/actions.ts

```
import{ createAI }from'ai/rsc';import{ loadChatFromDB, convertToUIState }from'@/utils/queries';exportconstAI=createAI({  actions:{// server actions},onGetUIState:async()=>{'use server';const chat =awaitloadChatFromDB();const uiState =convertToUIState(chat);return uiState;},});
```

AI SDK UI uses the `messages` field of `useChat` to store messages. To load messages when `useChat` is mounted, you should use `initialMessages`.

As messages are typically loaded from the database, we can use a server actions inside a Page component to fetch an older chat from the database during static generation and pass the messages as props to the `<Chat/>` component.


#### [After: Load chat from database during static generation of page](#after-load-chat-from-database-during-static-generation-of-page)


@/app/chat/\[id\]/page.tsx

```
import{Chat}from'@/app/components/chat';import{ getChatById }from'@/utils/queries';// link to example implementation: https://github.com/vercel/ai-chatbot/blob/00b125378c998d19ef60b73fe576df0fe5a0e9d4/lib/utils.ts#L87-L127import{ convertToUIMessages }from'@/utils/functions';exportdefaultasyncfunctionPage({ params }:{ params: any }){const{ id }= params;const chatFromDb =awaitgetChatById({ id });const chat:Chat={...chatFromDb,    messages:convertToUIMessages(chatFromDb.messages),};return<Chatkey={id}id={chat.id}initialMessages={chat.messages}/>;}
```


#### [After: Pass chat messages as props and load into chat hook](#after-pass-chat-messages-as-props-and-load-into-chat-hook)


@/app/components/chat.tsx

```
'use client';import{Message}from'ai';import{ useChat }from'@ai-sdk/react';exportfunctionChat({  id,  initialMessages,}:{  id;  initialMessages: Array<Message>;}){const{ messages }=useChat({    id,    initialMessages,});return(<div>{messages.map(message=>(<divkey={message.id}><div>{message.role}</div><div>{message.content}</div></div>))}</div>);}
```


## [Streaming Object Generation](#streaming-object-generation)


The `createStreamableValue` function streams any serializable data from the server to the client. As a result, this function allows you to stream object generations from the server to the client when paired with `streamObject`.


#### [Before: Use streamable value to stream object generations](#before-use-streamable-value-to-stream-object-generations)


@/app/actions.ts

```
import{ streamObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ createStreamableValue }from'ai/rsc';import{ notificationsSchema }from'@/utils/schemas';exportasyncfunctiongenerateSampleNotifications(){'use server';const stream =createStreamableValue();(async()=>{const{ partialObjectStream }=streamObject({      model:openai('gpt-4o'),      system:'generate sample ios messages for testing',      prompt:'messages from a family group chat during diwali, max 4',      schema: notificationsSchema,});forawait(const partialObject of partialObjectStream){      stream.update(partialObject);}})();  stream.done();return{ partialNotificationsStream: stream.value};}
```


#### [Before: Read streamable value and update object](#before-read-streamable-value-and-update-object)


@/app/page.tsx

```
'use client';import{ useState }from'react';import{ readStreamableValue }from'ai/rsc';import{ generateSampleNotifications }from'@/app/actions';exportdefaultfunctionPage(){const[notifications, setNotifications]=useState(null);return(<div><button        onClick={async()=>{const{ partialNotificationsStream }=awaitgenerateSampleNotifications();forawait(const partialNotifications ofreadStreamableValue(            partialNotificationsStream,)){if(partialNotifications){setNotifications(partialNotifications.notifications);}}}}>Generate</button></div>);}
```

To migrate to AI SDK UI, you should use the `useObject` hook and implement `streamObject` within your route handler.


#### [After: Replace with route handler and stream text response](#after-replace-with-route-handler-and-stream-text-response)


@/app/api/object/route.ts

```
import{ streamObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ notificationSchema }from'@/utils/schemas';exportasyncfunctionPOST(req:Request){const context =await req.json();const result =streamObject({    model:openai('gpt-4-turbo'),    schema: notificationSchema,    prompt:`Generate 3 notifications for a messages app in this context:`+ context,});return result.toTextStreamResponse();}
```


#### [After: Use object hook to decode stream and update object](#after-use-object-hook-to-decode-stream-and-update-object)


@/app/page.tsx

```
'use client';import{ useObject }from'@ai-sdk/react';import{ notificationSchema }from'@/utils/schemas';exportdefaultfunctionPage(){const{ object, submit }=useObject({    api:'/api/object',    schema: notificationSchema,});return(<div><buttononClick={()=>submit('Messages during finals week.')}>Generate notifications</button>{object?.notifications?.map((notification, index)=>(<divkey={index}><p>{notification?.name}</p><p>{notification?.message}</p></div>))}</div>);}
```
```

### 97. `docs/ai-sdk-rsc/multistep-interfaces.md`

```markdown
# Designing Multistep Interfaces


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/multistep-interfaces
description: Overview of Building Multistep Interfaces with AI SDK RSC
---


# [Designing Multistep Interfaces](#designing-multistep-interfaces)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

Multistep interfaces refer to user interfaces that require multiple independent steps to be executed in order to complete a specific task.

For example, if you wanted to build a Generative UI chatbot capable of booking flights, it could have three steps:

-   Search all flights
-   Pick flight
-   Check availability

To build this kind of application you will leverage two concepts, **tool composition** and **application context**.

**Tool composition** is the process of combining multiple [tools](/docs/ai-sdk-core/tools-and-tool-calling) to create a new tool. This is a powerful concept that allows you to break down complex tasks into smaller, more manageable steps. In the example above, *"search all flights"*, *"pick flight"*, and *"check availability"* come together to create a holistic *"book flight"* tool.

**Application context** refers to the state of the application at any given point in time. This includes the user's input, the output of the language model, and any other relevant information. In the example above, the flight selected in *"pick flight"* would be used as context necessary to complete the *"check availability"* task.


## [Overview](#overview)


In order to build a multistep interface with `ai/rsc`, you will need a few things:

-   A Server Action that calls and returns the result from the `streamUI` function
-   Tool(s) (sub-tasks necessary to complete your overall task)
-   React component(s) that should be rendered when the tool is called
-   A page to render your chatbot

The general flow that you will follow is:

-   User sends a message (calls your Server Action with `useActions`, passing the message as an input)
-   Message is appended to the AI State and then passed to the model alongside a number of tools
-   Model can decide to call a tool, which will render the `<SomeTool />` component
-   Within that component, you can add interactivity by using `useActions` to call the model with your Server Action and `useUIState` to append the model's response (`<SomeOtherTool />`) to the UI State
-   And so on...


## [Implementation](#implementation)


The turn-by-turn implementation is the simplest form of multistep interfaces. In this implementation, the user and the model take turns during the conversation. For every user input, the model generates a response, and the conversation continues in this turn-by-turn fashion.

In the following example, you specify two tools (`searchFlights` and `lookupFlight`) that the model can use to search for flights and lookup details for a specific flight.

app/actions.tsx

```
import{ streamUI }from'ai/rsc';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';constsearchFlights=async(source: string,  destination: string,  date: string,)=>{return[{      id:'1',      flightNumber:'AA123',},{      id:'2',      flightNumber:'AA456',},];};constlookupFlight=async(flightNumber: string)=>{return{    flightNumber: flightNumber,    departureTime:'10:00 AM',    arrivalTime:'12:00 PM',};};exportasyncfunctionsubmitUserMessage(input: string){'use server';const ui =awaitstreamUI({    model:openai('gpt-4o'),    system:'you are a flight booking assistant',    prompt: input,text:async({ content })=><div>{content}</div>,    tools:{      searchFlights:{        description:'search for flights',        parameters: z.object({          source: z.string().describe('The origin of the flight'),          destination: z.string().describe('The destination of the flight'),          date: z.string().describe('The date of the flight'),}),generate:asyncfunction*({ source, destination, date }){yield`Searching for flights from ${source} to ${destination} on ${date}...`;const results =awaitsearchFlights(source, destination, date);return(<div>{results.map(result=>(<divkey={result.id}><div>{result.flightNumber}</div></div>))}</div>);},},      lookupFlight:{        description:'lookup details for a flight',        parameters: z.object({          flightNumber: z.string().describe('The flight number'),}),generate:asyncfunction*({ flightNumber }){yield`Looking up details for flight ${flightNumber}...`;const details =awaitlookupFlight(flightNumber);return(<div><div>FlightNumber:{details.flightNumber}</div><div>DepartureTime:{details.departureTime}</div><div>ArrivalTime:{details.arrivalTime}</div></div>);},},},});return ui.value;}
```

Next, create an AI context that will hold the UI State and AI State.

app/ai.ts

```
import{ createAI }from'ai/rsc';import{ submitUserMessage }from'./actions';exportconstAI=createAI<any[],React.ReactNode[]>({  initialUIState:[],  initialAIState:[],  actions:{    submitUserMessage,},});
```

Next, wrap your application with your newly created context.

app/layout.tsx

```
import{typeReactNode}from'react';import{AI}from'./ai';exportdefaultfunctionRootLayout({  children,}: Readonly<{ children: ReactNode }>){return(<AI><htmllang="en"><body>{children}</body></html></AI>);}
```

To call your Server Action, update your root page with the following:

app/page.tsx

```
'use client';import{ useState }from'react';import{AI}from'./ai';import{ useActions, useUIState }from'ai/rsc';exportdefaultfunctionPage(){const[input, setInput]=useState<string>('');const[conversation, setConversation]=useUIState<typeofAI>();const{ submitUserMessage }=useActions();consthandleSubmit=async(e: React.FormEvent<HTMLFormElement>)=>{    e.preventDefault();setInput('');setConversation(currentConversation=>[...currentConversation,<div>{input}</div>,]);const message =awaitsubmitUserMessage(input);setConversation(currentConversation=>[...currentConversation, message]);};return(<div><div>{conversation.map((message, i)=>(<divkey={i}>{message}</div>))}</div><div><formonSubmit={handleSubmit}><inputtype="text"value={input}onChange={e=>setInput(e.target.value)}/><button>SendMessage</button></form></div></div>);}
```

This page pulls in the current UI State using the `useUIState` hook, which is then mapped over and rendered in the UI. To access the Server Action, you use the `useActions` hook which will return all actions that were passed to the `actions` key of the `createAI` function in your `actions.tsx` file. Finally, you call the `submitUserMessage` function like any other TypeScript function. This function returns a React component (`message`) that is then rendered in the UI by updating the UI State with `setConversation`.

In this example, to call the next tool, the user must respond with plain text. **Given you are streaming a React component, you can add a button to trigger the next step in the conversation**.

To add user interaction, you will have to convert the component into a client component and use the `useAction` hook to trigger the next step in the conversation.

components/flights.tsx

```
'use client';import{ useActions, useUIState }from'ai/rsc';import{ReactNode}from'react';interfaceFlightsProps{  flights:{ id:string; flightNumber:string}[];}exportconstFlights=({ flights }: FlightsProps)=>{const{ submitUserMessage }=useActions();const[_, setMessages]=useUIState();return(<div>{flights.map(result=>(<divkey={result.id}><divonClick={async()=>{const display =awaitsubmitUserMessage(`lookupFlight ${result.flightNumber}`,);setMessages((messages: ReactNode[])=>[...messages, display]);}}>{result.flightNumber}</div></div>))}</div>);};
```

Now, update your `searchFlights` tool to render the new `<Flights />` component.

actions.tsx

```
...searchFlights:{  description:'search for flights',  parameters: z.object({    source: z.string().describe('The origin of the flight'),    destination: z.string().describe('The destination of the flight'),    date: z.string().describe('The date of the flight'),}),generate:asyncfunction*({ source, destination, date }){yield`Searching for flights from ${source} to ${destination} on ${date}...`;const results =awaitsearchFlights(source, destination, date);return(<Flightsflights={results}/>);},}...
```

In the above example, the `Flights` component is used to display the search results. When the user clicks on a flight number, the `lookupFlight` tool is called with the flight number as a parameter. The `submitUserMessage` action is then called to trigger the next step in the conversation.

Learn more about tool calling in Next.js App Router by checking out examples [here](/examples/next-app/tools).
```

### 98. `docs/ai-sdk-rsc/overview.md`

```markdown
# AI SDK RSC


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/overview
description: An overview of AI SDK RSC.
---


# [AI SDK RSC](#ai-sdk-rsc)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

The `ai/rsc` package is compatible with frameworks that support React Server Components.

[React Server Components](https://nextjs.org/docs/app/building-your-application/rendering/server-components) (RSC) allow you to write UI that can be rendered on the server and streamed to the client. RSCs enable [Server Actions](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations#with-client-components) , a new way to call server-side code directly from the client just like any other function with end-to-end type-safety. This combination opens the door to a new way of building AI applications, allowing the large language model (LLM) to generate and stream UI directly from the server to the client.


## [AI SDK RSC Functions](#ai-sdk-rsc-functions)


AI SDK RSC has various functions designed to help you build AI-native applications with React Server Components. These functions:

1.  Provide abstractions for building Generative UI applications.
    -   [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui): calls a model and allows it to respond with React Server Components.
    -   [`useUIState`](/docs/reference/ai-sdk-rsc/use-ui-state): returns the current UI state and a function to update the UI State (like React's `useState`). UI State is the visual representation of the AI state.
    -   [`useAIState`](/docs/reference/ai-sdk-rsc/use-ai-state): returns the current AI state and a function to update the AI State (like React's `useState`). The AI state is intended to contain context and information shared with the AI model, such as system messages, function responses, and other relevant data.
    -   [`useActions`](/docs/reference/ai-sdk-rsc/use-actions): provides access to your Server Actions from the client. This is particularly useful for building interfaces that require user interactions with the server.
    -   [`createAI`](/docs/reference/ai-sdk-rsc/create-ai): creates a client-server context provider that can be used to wrap parts of your application tree to easily manage both UI and AI states of your application.
2.  Make it simple to work with streamable values between the server and client.
    -   [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value): creates a stream that sends values from the server to the client. The value can be any serializable data.
    -   [`readStreamableValue`](/docs/reference/ai-sdk-rsc/read-streamable-value): reads a streamable value from the client that was originally created using `createStreamableValue`.
    -   [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui): creates a stream that sends UI from the server to the client.
    -   [`useStreamableValue`](/docs/reference/ai-sdk-rsc/use-streamable-value): accepts a streamable value created using `createStreamableValue` and returns the current value, error, and pending state.


## [Templates](#templates)


Check out the following templates to see AI SDK RSC in action.

[

Gemini Chatbot

Uses Google Gemini, AI SDK, and Next.js.

](https://vercel.com/templates/next.js/gemini-ai-chatbot)[

Generative UI with RSC (experimental)

Uses Next.js, AI SDK, and streamUI to create generative UIs with React Server Components.

](https://vercel.com/templates/next.js/rsc-genui)


## [API Reference](#api-reference)


Please check out the [AI SDK RSC API Reference](/docs/reference/ai-sdk-rsc) for more details on each function.
```

### 99. `docs/ai-sdk-rsc/saving-and-restoring-states.md`

```markdown
# Saving and Restoring States


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/saving-and-restoring-states
description: Saving and restoring AI and UI states with onGetUIState and onSetAIState
---


# [Saving and Restoring States](#saving-and-restoring-states)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

AI SDK RSC provides convenient methods for saving and restoring AI and UI state. This is useful for saving the state of your application after every model generation, and restoring it when the user revisits the generations.


## [AI State](#ai-state)



### [Saving AI state](#saving-ai-state)


The AI state can be saved using the [`onSetAIState`](/docs/reference/ai-sdk-rsc/create-ai#on-set-ai-state) callback, which gets called whenever the AI state is updated. In the following example, you save the chat history to a database whenever the generation is marked as done.

app/ai.ts

```
exportconstAI=createAI<ServerMessage[],ClientMessage[]>({  actions:{    continueConversation,},onSetAIState:async({ state, done })=>{'use server';if(done){saveChatToDB(state);}},});
```


### [Restoring AI state](#restoring-ai-state)


The AI state can be restored using the [`initialAIState`](/docs/reference/ai-sdk-rsc/create-ai#initial-ai-state) prop passed to the context provider created by the [`createAI`](/docs/reference/ai-sdk-rsc/create-ai) function. In the following example, you restore the chat history from a database when the component is mounted.

```
import{ReactNode}from'react';import{AI}from'./ai';exportdefaultasyncfunctionRootLayout({  children,}: Readonly<{ children: ReactNode }>){const chat =awaitloadChatFromDB();return(<htmllang="en"><body><AIinitialAIState={chat}>{children}</AI></body></html>);}
```


## [UI State](#ui-state)



### [Saving UI state](#saving-ui-state)


The UI state cannot be saved directly, since the contents aren't yet serializable. Instead, you can use the AI state as proxy to store details about the UI state and use it to restore the UI state when needed.


### [Restoring UI state](#restoring-ui-state)


The UI state can be restored using the AI state as a proxy. In the following example, you restore the chat history from the AI state when the component is mounted. You use the [`onGetUIState`](/docs/reference/ai-sdk-rsc/create-ai#on-get-ui-state) callback to listen for SSR events and restore the UI state.

app/ai.ts

```
exportconstAI=createAI<ServerMessage[],ClientMessage[]>({  actions:{    continueConversation,},onGetUIState:async()=>{'use server';const historyFromDB:ServerMessage[]=awaitloadChatFromDB();const historyFromApp:ServerMessage[]=getAIState();// If the history from the database is different from the// history in the app, they're not in sync so return the UIState// based on the history from the databaseif(historyFromDB.length !== historyFromApp.length){return historyFromDB.map(({ role, content })=>({        id:generateId(),        role,        display:          role ==='function'?(<Component{...JSON.parse(content)}/>):(            content),}));}},});
```

To learn more, check out this [example](/examples/next-app/state-management/save-and-restore-states) that persists and restores states in your Next.js application.

Next, you will learn how you can use `ai/rsc` functions like `useActions` and `useUIState` to create interactive, multistep interfaces.
```

### 100. `docs/ai-sdk-rsc/streaming-react-components.md`

```markdown
# Streaming React Components


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/streaming-react-components
description: Overview of streaming RSCs
---


# [Streaming React Components](#streaming-react-components)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

The RSC API allows you to stream React components from the server to the client with the [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function. This is useful when you want to go beyond raw text and stream components to the client in real-time.

Similar to [AI SDK Core](/docs/ai-sdk-core/overview) APIs (like [`streamText`](/docs/reference/ai-sdk-core/stream-text) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object) ), `streamUI` provides a single function to call a model and allow it to respond with React Server Components. It supports the same model interfaces as AI SDK Core APIs.


### [Concepts](#concepts)


To give the model the ability to respond to a user's prompt with a React component, you can leverage [tools](/docs/ai-sdk-core/tools-and-tool-calling).

Remember, tools are like programs you can give to the model, and the model can decide as and when to use based on the context of the conversation.

With the `streamUI` function, **you provide tools that return React components**. With the ability to stream components, the model is akin to a dynamic router that is able to understand the user's intention and display relevant UI.

At a high level, the `streamUI` works like other AI SDK Core functions: you can provide the model with a prompt or some conversation history and, optionally, some tools. If the model decides, based on the context of the conversation, to call a tool, it will generate a tool call. The `streamUI` function will then run the respective tool, returning a React component. If the model doesn't have a relevant tool to use, it will return a text generation, which will be passed to the `text` function, for you to handle (render and return as a React component).

Remember, the `streamUI` function must return a React component.

```
const result =awaitstreamUI({  model:openai('gpt-4o'),  prompt:'Get the weather for San Francisco',text:({ content })=><div>{content}</div>,  tools:{},});
```

This example calls the `streamUI` function using OpenAI's `gpt-4o` model, passes a prompt, specifies how the model's plain text response (`content`) should be rendered, and then provides an empty object for tools. Even though this example does not define any tools, it will stream the model's response as a `div` rather than plain text.


### [Adding A Tool](#adding-a-tool)


Using tools with `streamUI` is similar to how you use tools with `generateText` and `streamText`. A tool is an object that has:

-   `description`: a string telling the model what the tool does and when to use it
-   `parameters`: a Zod schema describing what the tool needs in order to run
-   `generate`: an asynchronous function that will be run if the model calls the tool. This must return a React component

Let's expand the previous example to add a tool.

```
const result =awaitstreamUI({  model:openai('gpt-4o'),  prompt:'Get the weather for San Francisco',text:({ content })=><div>{content}</div>,  tools:{    getWeather:{      description:'Get the weather for a location',      parameters: z.object({location: z.string()}),generate:asyncfunction*({location}){yield<LoadingComponent/>;const weather =awaitgetWeather(location);return<WeatherComponentweather={weather}location={location}/>;},},},});
```

This tool would be run if the user asks for the weather for their location. If the user hasn't specified a location, the model will ask for it before calling the tool. When the model calls the tool, the generate function will initially return a loading component. This component will show until the awaited call to `getWeather` is resolved, at which point, the model will stream the `<WeatherComponent />` to the user.

Note: This example uses a [generator function](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*) (`function*`), which allows you to pause its execution and return a value, then resume from where it left off on the next call. This is useful for handling data streams, as you can fetch and return data from an asynchronous source like an API, then resume the function to fetch the next chunk when needed. By yielding values one at a time, generator functions enable efficient processing of streaming data without blocking the main thread.


## [Using `streamUI` with Next.js](#using-streamui-with-nextjs)


Let's see how you can use the example above in a Next.js application.

To use `streamUI` in a Next.js application, you will need two things:

1.  A Server Action (where you will call `streamUI`)
2.  A page to call the Server Action and render the resulting components


### [Step 1: Create a Server Action](#step-1-create-a-server-action)


Server Actions are server-side functions that you can call directly from the frontend. For more info, see [the documentation](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations#with-client-components).

Create a Server Action at `app/actions.tsx` and add the following code:

app/actions.tsx

```
'use server';import{ streamUI }from'ai/rsc';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';constLoadingComponent=()=>(<divclassName="animate-pulse p-4">getting weather...</div>);constgetWeather=async(location: string)=>{awaitnewPromise(resolve=>setTimeout(resolve,2000));return'82°F️ ☀️';};interfaceWeatherProps{location:string;  weather:string;}constWeatherComponent=(props: WeatherProps)=>(<divclassName="border border-neutral-200 p-4 rounded-lg max-w-fit">The weather in{props.location}is{props.weather}</div>);exportasyncfunctionstreamComponent(){const result =awaitstreamUI({    model:openai('gpt-4o'),    prompt:'Get the weather for San Francisco',text:({ content })=><div>{content}</div>,    tools:{      getWeather:{        description:'Get the weather for a location',        parameters: z.object({location: z.string(),}),generate:asyncfunction*({location}){yield<LoadingComponent/>;const weather =awaitgetWeather(location);return<WeatherComponentweather={weather}location={location}/>;},},},});return result.value;}
```

The `getWeather` tool should look familiar as it is identical to the example in the previous section. In order for this tool to work:

1.  First define a `LoadingComponent`, which renders a pulsing `div` that will show some loading text.
2.  Next, define a `getWeather` function that will timeout for 2 seconds (to simulate fetching the weather externally) before returning the "weather" for a `location`. Note: you could run any asynchronous TypeScript code here.
3.  Finally, define a `WeatherComponent` which takes in `location` and `weather` as props, which are then rendered within a `div`.

Your Server Action is an asynchronous function called `streamComponent` that takes no inputs, and returns a `ReactNode`. Within the action, you call the `streamUI` function, specifying the model (`gpt-4o`), the prompt, the component that should be rendered if the model chooses to return text, and finally, your `getWeather` tool. Last but not least, you return the resulting component generated by the model with `result.value`.

To call this Server Action and display the resulting React Component, you will need a page.


### [Step 2: Create a Page](#step-2-create-a-page)


Create or update your root page (`app/page.tsx`) with the following code:

app/page.tsx

```
'use client';import{ useState }from'react';import{Button}from'@/components/ui/button';import{ streamComponent }from'./actions';exportdefaultfunctionPage(){const[component, setComponent]=useState<React.ReactNode>();return(<div><formonSubmit={asynce=>{          e.preventDefault();setComponent(awaitstreamComponent());}}><Button>StreamComponent</Button></form><div>{component}</div></div>);}
```

This page is first marked as a client component with the `"use client";` directive given it will be using hooks and interactivity. On the page, you render a form. When that form is submitted, you call the `streamComponent` action created in the previous step (just like any other function). The `streamComponent` action returns a `ReactNode` that you can then render on the page using React state (`setComponent`).


## [Going beyond a single prompt](#going-beyond-a-single-prompt)


You can now allow the model to respond to your prompt with a React component. However, this example is limited to a static prompt that is set within your Server Action. You could make this example interactive by turning it into a chatbot.

Learn how to stream React components with the Next.js App Router using `streamUI` with this [example](/examples/next-app/interface/route-components).
```

### 101. `docs/ai-sdk-rsc/streaming-values.md`

```markdown
# Streaming Values


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc/streaming-values
description: Overview of streaming RSCs
---


# [Streaming Values](#streaming-values)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

The RSC API provides several utility functions to allow you to stream values from the server to the client. This is useful when you need more granular control over what you are streaming and how you are streaming it.

These utilities can also be paired with [AI SDK Core](/docs/ai-sdk-core) functions like [`streamText`](/docs/reference/ai-sdk-core/stream-text) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object) to easily stream LLM generations from the server to the client.

There are two functions provided by the RSC API that allow you to create streamable values:

-   [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) - creates a streamable (serializable) value, with full control over how you create, update, and close the stream.
-   [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) - creates a streamable React component, with full control over how you create, update, and close the stream.


## [`createStreamableValue`](#createstreamablevalue)


The RSC API allows you to stream serializable Javascript values from the server to the client using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value), such as strings, numbers, objects, and arrays.

This is useful when you want to stream:

-   Text generations from the language model in real-time.
-   Buffer values of image and audio generations from multi-modal models.
-   Progress updates from multi-step agent runs.


## [Creating a Streamable Value](#creating-a-streamable-value)


You can import `createStreamableValue` from `ai/rsc` and use it to create a streamable value.

```
'use server';import{ createStreamableValue }from'ai/rsc';exportconstrunThread=async()=>{const streamableStatus =createStreamableValue('thread.init');setTimeout(()=>{    streamableStatus.update('thread.run.create');    streamableStatus.update('thread.run.update');    streamableStatus.update('thread.run.end');    streamableStatus.done('thread.end');},1000);return{    status: streamableStatus.value,};};
```


## [Reading a Streamable Value](#reading-a-streamable-value)


You can read streamable values on the client using `readStreamableValue`. It returns an async iterator that yields the value of the streamable as it is updated:

```
import{ readStreamableValue }from'ai/rsc';import{ runThread }from'@/actions';exportdefaultfunctionPage(){return(<buttononClick={async()=>{const{ status }=awaitrunThread();forawait(const value ofreadStreamableValue(status)){console.log(value);}}}>Ask</button>);}
```

Learn how to stream a text generation (with `streamText`) using the Next.js App Router and `createStreamableValue` in this [example](/examples/next-app/basics/streaming-text-generation).


## [`createStreamableUI`](#createstreamableui)


`createStreamableUI` creates a stream that holds a React component. Unlike AI SDK Core APIs, this function does not call a large language model. Instead, it provides a primitive that can be used to have granular control over streaming a React component.


## [Using `createStreamableUI`](#using-createstreamableui)


Let's look at how you can use the `createStreamableUI` function with a Server Action.

app/actions.tsx

```
'use server';import{ createStreamableUI }from'ai/rsc';exportasyncfunctiongetWeather(){const weatherUI =createStreamableUI();  weatherUI.update(<divstyle={{ color:'gray'}}>Loading...</div>);setTimeout(()=>{    weatherUI.done(<div>It&apos;s a sunny day!</div>);},1000);return weatherUI.value;}
```

First, you create a streamable UI with an empty state and then update it with a loading message. After 1 second, you mark the stream as done passing in the actual weather information as its final value. The `.value` property contains the actual UI that can be sent to the client.


## [Reading a Streamable UI](#reading-a-streamable-ui)


On the client side, you can call the `getWeather` Server Action and render the returned UI like any other React component.

app/page.tsx

```
'use client';import{ useState }from'react';import{ readStreamableValue }from'ai/rsc';import{ getWeather }from'@/actions';exportdefaultfunctionPage(){const[weather, setWeather]=useState<React.ReactNode|null>(null);return(<div><buttononClick={async()=>{const weatherUI =awaitgetWeather();setWeather(weatherUI);}}>What&apos;s the weather?</button>{weather}</div>);}
```

When the button is clicked, the `getWeather` function is called, and the returned UI is set to the `weather` state and rendered on the page. Users will see the loading message first and then the actual weather information after 1 second.

Learn more about handling multiple streams in a single request in the [Multiple Streamables](/docs/advanced/multiple-streamables) guide.

Learn more about handling state for more complex use cases with [AI/UI State](/docs/ai-sdk-rsc/generative-ui-state) .
```

### 102. `docs/ai-sdk-rsc.md`

```markdown
# AI SDK RSC


---
url: https://ai-sdk.dev/docs/ai-sdk-rsc
description: Learn about AI SDK RSC.
---


# [AI SDK RSC](#ai-sdk-rsc)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

[

Overview

Learn about AI SDK RSC.

](/docs/ai-sdk-rsc/overview)[

Streaming React Components

Learn how to stream React components.

](/docs/ai-sdk-rsc/streaming-react-components)[

Managing Generative UI State

Learn how to manage generative UI state.

](/docs/ai-sdk-rsc/generative-ui-state)[

Saving and Restoring States

Learn how to save and restore states.

](/docs/ai-sdk-rsc/saving-and-restoring-states)[

Multi-step Interfaces

Learn how to build multi-step interfaces.

](/docs/ai-sdk-rsc/multistep-interfaces)[

Streaming Values

Learn how to stream values with AI SDK RSC.

](/docs/ai-sdk-rsc/streaming-values)[

Error Handling

Learn how to handle errors.

](/docs/ai-sdk-rsc/error-handling)[

Authentication

Learn how to authenticate users.

](/docs/ai-sdk-rsc/authentication)
```

### 103. `docs/ai-sdk-ui/chatbot-message-persistence.md`

```markdown
# Chatbot Message Persistence


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence
description: Learn how to store and load chat messages in a chatbot.
---


# [Chatbot Message Persistence](#chatbot-message-persistence)


Being able to store and load chat messages is crucial for most AI chatbots. In this guide, we'll show how to implement message persistence with `useChat` and `streamText`.

This guide does not cover authorization, error handling, or other real-world considerations. It is intended to be a simple example of how to implement message persistence.


## [Starting a new chat](#starting-a-new-chat)


When the user navigates to the chat page without providing a chat ID, we need to create a new chat and redirect to the chat page with the new chat ID.

app/chat/page.tsx

```
import{ redirect }from'next/navigation';import{ createChat }from'@tools/chat-store';exportdefaultasyncfunctionPage(){const id =awaitcreateChat();// create a new chatredirect(`/chat/${id}`);// redirect to chat page, see below}
```

Our example chat store implementation uses files to store the chat messages. In a real-world application, you would use a database or a cloud storage service, and get the chat ID from the database. That being said, the function interfaces are designed to be easily replaced with other implementations.

tools/chat-store.ts

```
import{ generateId }from'ai';import{ existsSync, mkdirSync }from'fs';import{ writeFile }from'fs/promises';import path from'path';exportasyncfunctioncreateChat():Promise<string>{const id =generateId();// generate a unique chat IDawaitwriteFile(getChatFile(id),'[]');// create an empty chat filereturn id;}functiongetChatFile(id: string):string{const chatDir = path.join(process.cwd(),'.chats');if(!existsSync(chatDir))mkdirSync(chatDir,{ recursive:true});return path.join(chatDir,`${id}.json`);}
```


## [Loading an existing chat](#loading-an-existing-chat)


When the user navigates to the chat page with a chat ID, we need to load the chat messages and display them.

app/chat/\[id\]/page.tsx

```
import{ loadChat }from'@tools/chat-store';importChatfrom'@ui/chat';exportdefaultasyncfunctionPage(props:{ params: Promise<{ id: string }>}){const{ id }=await props.params;// get the chat ID from the URLconst messages =awaitloadChat(id);// load the chat messagesreturn<Chatid={id}initialMessages={messages}/>;// display the chat}
```

The `loadChat` function in our file-based chat store is implemented as follows:

tools/chat-store.ts

```
import{Message}from'ai';import{ readFile }from'fs/promises';exportasyncfunctionloadChat(id: string):Promise<Message[]>{returnJSON.parse(awaitreadFile(getChatFile(id),'utf8'));}// ... rest of the file
```

The display component is a simple chat component that uses the `useChat` hook to send and receive messages:

ui/chat.tsx

```
'use client';import{Message, useChat }from'@ai-sdk/react';exportdefaultfunctionChat({  id,  initialMessages,}:{ id?: string |undefined; initialMessages?: Message[]}={}){const{ input, handleInputChange, handleSubmit, messages }=useChat({    id,// use the provided chat ID    initialMessages,// initial messages if provided    sendExtraMessageFields:true,// send id and createdAt for each message});// simplified rendering code, extend as needed:return(<div>{messages.map(m=>(<divkey={m.id}>{m.role ==='user'?'User: ':'AI: '}{m.content}</div>))}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}/></form></div>);}
```


## [Storing messages](#storing-messages)


`useChat` sends the chat id and the messages to the backend. We have enabled the `sendExtraMessageFields` option to send the id and createdAt fields, meaning that we store messages in the `useChat` message format.

The `useChat` message format is different from the `CoreMessage` format. The `useChat` message format is designed for frontend display, and contains additional fields such as `id` and `createdAt`. We recommend storing the messages in the `useChat` message format.

Storing messages is done in the `onFinish` callback of the `streamText` function. `onFinish` receives the messages from the AI response as a `CoreMessage[]`, and we use the [`appendResponseMessages`](/docs/reference/ai-sdk-ui/append-response-messages) helper to append the AI response messages to the chat messages.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ appendResponseMessages, streamText }from'ai';import{ saveChat }from'@tools/chat-store';exportasyncfunctionPOST(req: Request){const{ messages, id }=await req.json();const result =streamText({    model:openai('gpt-4o-mini'),    messages,asynconFinish({ response }){awaitsaveChat({        id,        messages:appendResponseMessages({          messages,          responseMessages: response.messages,}),});},});return result.toDataStreamResponse();}
```

The actual storage of the messages is done in the `saveChat` function, which in our file-based chat store is implemented as follows:

tools/chat-store.ts

```
import{Message}from'ai';import{ writeFile }from'fs/promises';exportasyncfunctionsaveChat({  id,  messages,}:{  id: string;  messages: Message[];}):Promise<void>{const content =JSON.stringify(messages,null,2);awaitwriteFile(getChatFile(id), content);}// ... rest of the file
```


## [Message IDs](#message-ids)


In addition to a chat ID, each message has an ID. You can use this message ID to e.g. manipulate individual messages.

The IDs for user messages are generated by the `useChat` hook on the client, and the IDs for AI response messages are generated by `streamText`.

You can control the ID format by providing ID generators (see [`createIdGenerator()`](/docs/reference/ai-sdk-core/create-id-generator):

ui/chat.tsx

```
import{ createIdGenerator }from'ai';import{ useChat }from'@ai-sdk/react';const{// ...}=useChat({// ...// id format for client-side messages:  generateId:createIdGenerator({    prefix:'msgc',    size:16,}),});
```

app/api/chat/route.ts

```
import{ createIdGenerator, streamText }from'ai';exportasyncfunctionPOST(req: Request){// ...const result =streamText({// ...// id format for server-side messages:    experimental_generateMessageId:createIdGenerator({      prefix:'msgs',      size:16,}),});// ...}
```


## [Sending only the last message](#sending-only-the-last-message)


Once you have implemented message persistence, you might want to send only the last message to the server. This reduces the amount of data sent to the server on each request and can improve performance.

To achieve this, you can provide an `experimental_prepareRequestBody` function to the `useChat` hook (React only). This function receives the messages and the chat ID, and returns the request body to be sent to the server.

ui/chat.tsx

```
import{ useChat }from'@ai-sdk/react';const{// ...}=useChat({// ...// only send the last message to the server:experimental_prepareRequestBody({ messages, id }){return{ message: messages[messages.length -1], id };},});
```

On the server, you can then load the previous messages and append the new message to the previous messages:

app/api/chat/route.ts

```
import{ appendClientMessage }from'ai';exportasyncfunctionPOST(req: Request){// get the last message from the client:const{ message, id }=await req.json();// load the previous messages from the server:const previousMessages =awaitloadChat(id);// append the new message to the previous messages:const messages =appendClientMessage({    messages: previousMessages,    message,});const result =streamText({// ...    messages,});// ...}
```


## [Handling client disconnects](#handling-client-disconnects)


By default, the AI SDK `streamText` function uses backpressure to the language model provider to prevent the consumption of tokens that are not yet requested.

However, this means that when the client disconnects, e.g. by closing the browser tab or because of a network issue, the stream from the LLM will be aborted and the conversation may end up in a broken state.

Assuming that you have a [storage solution](#storing-messages) in place, you can use the `consumeStream` method to consume the stream on the backend, and then save the result as usual. `consumeStream` effectively removes the backpressure, meaning that the result is stored even when the client has already disconnected.

app/api/chat/route.ts

```
import{ appendResponseMessages, streamText }from'ai';import{ saveChat }from'@tools/chat-store';exportasyncfunctionPOST(req: Request){const{ messages, id }=await req.json();const result =streamText({    model,    messages,asynconFinish({ response }){awaitsaveChat({        id,        messages:appendResponseMessages({          messages,          responseMessages: response.messages,}),});},});// consume the stream to ensure it runs to completion & triggers onFinish// even when the client response is aborted:  result.consumeStream();// no awaitreturn result.toDataStreamResponse();}
```

When the client reloads the page after a disconnect, the chat will be restored from the storage solution.

In production applications, you would also track the state of the request (in progress, complete) in your stored messages and use it on the client to cover the case where the client reloads the page after a disconnection, but the streaming is not yet complete.


## [Resuming ongoing streams](#resuming-ongoing-streams)


This feature is experimental and may change in future versions.

The `useChat` hook has experimental support for resuming an ongoing chat generation stream by any client, either after a network disconnect or by reloading the chat page. This can be useful for building applications that involve long-running conversations or for ensuring that messages are not lost in case of network failures.

The following are the pre-requisities for your chat application to support resumable streams:

-   Installing the [`resumable-stream`](https://www.npmjs.com/package/resumable-stream) package that helps create and manage the publisher/subscriber mechanism of the streams.
-   Creating a [Redis](https://vercel.com/marketplace/redis) instance to store the stream state.
-   Creating a table that tracks the stream IDs associated with a chat.

To resume a chat stream, you will use the `experimental_resume` function returned by the `useChat` hook. You will call this function during the initial mount of the hook inside the main chat component.

app/components/chat.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{Input}from'@/components/input';import{Messages}from'@/components/messages';exportfunctionChat(){const{ experimental_resume }=useChat({ id });useEffect(()=>{experimental_resume();// we use an empty dependency array to// ensure this effect runs only once},[]);return(<div><Messages/><Input/></div>);}
```

For a more resilient implementation that handles race conditions that can occur in-flight during a resume request, you can use the following `useAutoResume` hook. This will automatically process the `append-message` SSE data part streamed by the server.

app/hooks/use-auto-resume.ts

```
'use client';import{ useEffect }from'react';importtype{UIMessage}from'ai';importtype{UseChatHelpers}from'@ai-sdk/react';exporttypeDataPart={type:'append-message'; message:string};exportinterfaceProps{  autoResume:boolean;  initialMessages:UIMessage[];  experimental_resume:UseChatHelpers['experimental_resume'];  data:UseChatHelpers['data'];  setMessages:UseChatHelpers['setMessages'];}exportfunctionuseAutoResume({  autoResume,  initialMessages,  experimental_resume,  data,  setMessages,}: Props){useEffect(()=>{if(!autoResume)return;const mostRecentMessage = initialMessages.at(-1);if(mostRecentMessage?.role ==='user'){experimental_resume();}// we intentionally run this once// eslint-disable-next-line react-hooks/exhaustive-deps},[]);useEffect(()=>{if(!data | data.length ===0)return;const dataPart = data[0]asDataPart;if(dataPart.type==='append-message'){const message =JSON.parse(dataPart.message)asUIMessage;setMessages([...initialMessages, message]);}},[data, initialMessages, setMessages]);}
```

You can then use this hook in your chat component as follows.

app/components/chat.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{Input}from'@/components/input';import{Messages}from'@/components/messages';import{ useAutoResume }from'@/hooks/use-auto-resume';exportfunctionChat(){const{ experimental_resume, data, setMessages }=useChat({ id });useAutoResume({    autoResume:true,    initialMessages:[],    experimental_resume,    data,    setMessages,});return(<div><Messages/><Input/></div>);}
```

The `experimental_resume` function makes a `GET` request to your configured chat endpoint (or `/api/chat` by default) whenever your client calls it. If there’s an active stream, it will pick up where it left off, otherwise it simply finishes without error.

The `GET` request automatically appends the `chatId` query parameter to the URL to help identify the chat the request belongs to. Using the `chatId`, you can look up the most recent stream ID from the database and resume the stream.

```
GET /api/chat?chatId=<your-chat-id>
```

Earlier, you must've implemented the `POST` handler for the `/api/chat` route to create new chat generations. When using `experimental_resume`, you must also implement the `GET` handler for `/api/chat` route to resume streams.


### [1\. Implement the GET handler](#1-implement-the-get-handler)


Add a `GET` method to `/api/chat` that:

1.  Reads `chatId` from the query string
2.  Validates it’s present
3.  Loads any stored stream IDs for that chat
4.  Returns the latest one to `streamContext.resumableStream()`
5.  Falls back to an empty stream if it’s already closed

app/api/chat/route.ts

```
import{ loadStreams }from'@/util/chat-store';import{ createDataStream, getMessagesByChatId }from'ai';import{ after }from'next/server';import{ createResumableStreamContext }from'resumable-stream';const streamContext =createResumableStreamContext({  waitUntil: after,});exportasyncfunctionGET(request:Request){const{ searchParams }=newURL(request.url);const chatId = searchParams.get('chatId');if(!chatId){returnnewResponse('id is required',{ status:400});}const streamIds =awaitloadStreams(chatId);if(!streamIds.length){returnnewResponse('No streams found',{ status:404});}const recentStreamId = streamIds.at(-1);if(!recentStreamId){returnnewResponse('No recent stream found',{ status:404});}const emptyDataStream =createDataStream({execute:()=>{},});const stream =await streamContext.resumableStream(    recentStreamId,()=> emptyDataStream,);if(stream){returnnewResponse(stream,{ status:200});}/*   * For when the generation is "active" during SSR but the   * resumable stream has concluded after reaching this point.   */const messages =awaitgetMessagesByChatId({ id: chatId });const mostRecentMessage = messages.at(-1);if(!mostRecentMessage | mostRecentMessage.role!=='assistant'){returnnewResponse(emptyDataStream,{ status:200});}const messageCreatedAt =newDate(mostRecentMessage.createdAt);const streamWithMessage =createDataStream({execute: buffer =>{      buffer.writeData({type:'append-message',        message:JSON.stringify(mostRecentMessage),});},});returnnewResponse(streamWithMessage,{ status:200});}
```

After you've implemented the `GET` handler, you can update the `POST` handler to handle the creation of resumable streams.


### [2\. Update the POST handler](#2-update-the-post-handler)


When you create a brand-new chat completion, you must:

1.  Generate a fresh `streamId`
2.  Persist it alongside your `chatId`
3.  Kick off a `createDataStream` that pipes tokens as they arrive
4.  Hand that new stream to `streamContext.resumableStream()`

app/api/chat/route.ts

```
import{  appendResponseMessages,  createDataStream,  generateId,  streamText,}from'ai';import{ appendStreamId, saveChat }from'@/util/chat-store';import{ createResumableStreamContext }from'resumable-stream';const streamContext =createResumableStreamContext({  waitUntil: after,});asyncfunctionPOST(request:Request){const{ id, messages }=await req.json();const streamId =generateId();// Record this new stream so we can resume laterawaitappendStreamId({ chatId: id, streamId });// Build the data stream that will emit tokensconst stream =createDataStream({execute: dataStream =>{const result =streamText({        model:openai('gpt-4o'),        messages,onFinish:async({ response })=>{awaitsaveChat({            id,            messages:appendResponseMessages({              messages,              responseMessages: response.messages,}),});},});// Return a resumable stream to the client      result.mergeIntoDataStream(dataStream);},});returnnewResponse(await streamContext.resumableStream(streamId,()=> stream),);}
```

With both handlers, your clients can now gracefully resume ongoing streams.
```

### 104. `docs/ai-sdk-ui/chatbot-tool-usage.md`

```markdown
# Chatbot Tool Usage


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage
description: Learn how to use tools with the useChat hook.
---


# [Chatbot Tool Usage](#chatbot-tool-usage)


With [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`streamText`](/docs/reference/ai-sdk-core/stream-text), you can use tools in your chatbot application. The AI SDK supports three types of tools in this context:

1.  Automatically executed server-side tools
2.  Automatically executed client-side tools
3.  Tools that require user interaction, such as confirmation dialogs

The flow is as follows:

1.  The user enters a message in the chat UI.
2.  The message is sent to the API route.
3.  In your server side route, the language model generates tool calls during the `streamText` call.
4.  All tool calls are forwarded to the client.
5.  Server-side tools are executed using their `execute` method and their results are forwarded to the client.
6.  Client-side tools that should be automatically executed are handled with the `onToolCall` callback. You can return the tool result from the callback.
7.  Client-side tool that require user interactions can be displayed in the UI. The tool calls and results are available as tool invocation parts in the `parts` property of the last assistant message.
8.  When the user interaction is done, `addToolResult` can be used to add the tool result to the chat.
9.  When there are tool calls in the last assistant message and all tool results are available, the client sends the updated messages back to the server. This triggers another iteration of this flow.

The tool call and tool executions are integrated into the assistant message as tool invocation parts. A tool invocation is at first a tool call, and then it becomes a tool result when the tool is executed. The tool result contains all information about the tool call as well as the result of the tool execution.

In order to automatically send another request to the server when all tool calls are server-side, you need to set [`maxSteps`](/docs/reference/ai-sdk-ui/use-chat#max-steps) to a value greater than 1 in the `useChat` options. It is disabled by default for backward compatibility.


## [Example](#example)


In this example, we'll use three tools:

-   `getWeatherInformation`: An automatically executed server-side tool that returns the weather in a given city.
-   `askForConfirmation`: A user-interaction client-side tool that asks the user for confirmation.
-   `getLocation`: An automatically executed client-side tool that returns a random city.


### [API route](#api-route)


app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';import{ z }from'zod';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{// server-side tool with execute function:      getWeatherInformation:{        description:'show the weather in a given city to the user',        parameters: z.object({ city: z.string()}),execute:async({}:{ city: string })=>{const weatherOptions =['sunny','cloudy','rainy','snowy','windy'];return weatherOptions[Math.floor(Math.random()* weatherOptions.length)];},},// client-side tool that starts user interaction:      askForConfirmation:{        description:'Ask the user for confirmation.',        parameters: z.object({          message: z.string().describe('The message to ask for confirmation.'),}),},// client-side tool that is automatically executed on the client:      getLocation:{        description:'Get the user location. Always ask for confirmation before using this tool.',        parameters: z.object({}),},},});return result.toDataStreamResponse();}
```


### [Client-side page](#client-side-page)


The client-side page uses the `useChat` hook to create a chatbot application with real-time message streaming. Tool invocations are displayed in the chat UI as tool invocation parts. Please make sure to render the messages using the `parts` property of the message.

There are three things worth mentioning:

1.  The [`onToolCall`](/docs/reference/ai-sdk-ui/use-chat#on-tool-call) callback is used to handle client-side tools that should be automatically executed. In this example, the `getLocation` tool is a client-side tool that returns a random city.

2.  The `toolInvocations` property of the last assistant message contains all tool calls and results. The client-side tool `askForConfirmation` is displayed in the UI. It asks the user for confirmation and displays the result once the user confirms or denies the execution. The result is added to the chat using `addToolResult`.

3.  The [`maxSteps`](/docs/reference/ai-sdk-ui/use-chat#max-steps) option is set to 5. This enables several tool use iterations between the client and the server.


app/page.tsx

```
'use client';import{ToolInvocation}from'ai';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit, addToolResult }=useChat({      maxSteps:5,// run client-side tools that are automatically executed:asynconToolCall({ toolCall }){if(toolCall.toolName ==='getLocation'){const cities =['New York','Los Angeles','Chicago','San Francisco',];return cities[Math.floor(Math.random()* cities.length)];}},});return(<>{messages?.map(message=>(<divkey={message.id}><strong>{`${message.role}: `}</strong>{message.parts.map(part=>{switch(part.type){// render text parts as simple text:case'text':return part.text;// for tool invocations, distinguish between the tools and the state:case'tool-invocation':{const callId = part.toolInvocation.toolCallId;switch(part.toolInvocation.toolName){case'askForConfirmation':{switch(part.toolInvocation.state){case'call':return(<divkey={callId}>{part.toolInvocation.args.message}<div><buttononClick={()=>addToolResult({                                    toolCallId: callId,                                    result:'Yes, confirmed.',})}>Yes</button><buttononClick={()=>addToolResult({                                    toolCallId: callId,                                    result:'No, denied',})}>No</button></div></div>);case'result':return(<divkey={callId}>Location access allowed:{' '}{part.toolInvocation.result}</div>);}break;}case'getLocation':{switch(part.toolInvocation.state){case'call':return<divkey={callId}>Gettinglocation...</div>;case'result':return(<divkey={callId}>Location:{part.toolInvocation.result}</div>);}break;}case'getWeatherInformation':{switch(part.toolInvocation.state){// example of pre-rendering streaming tool calls:case'partial-call':return(<prekey={callId}>{JSON.stringify(part.toolInvocation,null,2)}</pre>);case'call':return(<divkey={callId}>Getting weather information for{' '}{part.toolInvocation.args.city}...</div>);case'result':return(<divkey={callId}>Weatherin{part.toolInvocation.args.city}:{' '}{part.toolInvocation.result}</div>);}break;}}}}})}<br/></div>))}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}/></form></>);}
```


## [Tool call streaming](#tool-call-streaming)


You can stream tool calls while they are being generated by enabling the `toolCallStreaming` option in `streamText`.

app/api/chat/route.ts

```
exportasyncfunctionPOST(req: Request){// ...const result =streamText({    toolCallStreaming:true,// ...});return result.toDataStreamResponse();}
```

When the flag is enabled, partial tool calls will be streamed as part of the data stream. They are available through the `useChat` hook. The tool invocation parts of assistant messages will also contain partial tool calls. You can use the `state` property of the tool invocation to render the correct UI.

app/page.tsx

```
exportdefaultfunctionChat(){// ...return(<>{messages?.map(message=>(<divkey={message.id}>{message.parts.map(part=>{if(part.type==='tool-invocation'){switch(part.toolInvocation.state){case'partial-call':return<>render partial tool call</>;case'call':return<>render full tool call</>;case'result':return<>render tool result</>;}}})}</div>))}</>);}
```


## [Step start parts](#step-start-parts)


When you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages. If you want to display boundaries between tool invocations, you can use the `step-start` parts as follows:

app/page.tsx

```
// ...// where you render the message parts:message.parts.map((part, index)=>{switch(part.type){case'step-start':// show step boundaries as horizontal lines:return index >0?(<divkey={index}className="text-gray-500"><hrclassName="my-2 border-gray-300"/></div>):null;case'text':// ...case'tool-invocation':// ...}});// ...
```


## [Server-side Multi-Step Calls](#server-side-multi-step-calls)


You can also use multi-step calls on the server-side with `streamText`. This works when all invoked tools have an `execute` function on the server side.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';import{ z }from'zod';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      getWeatherInformation:{        description:'show the weather in a given city to the user',        parameters: z.object({ city: z.string()}),// tool has execute function:execute:async({}:{ city: string })=>{const weatherOptions =['sunny','cloudy','rainy','snowy','windy'];return weatherOptions[Math.floor(Math.random()* weatherOptions.length)];},},},    maxSteps:5,});return result.toDataStreamResponse();}
```


## [Errors](#errors)


Language models can make errors when calling tools. By default, these errors are masked for security reasons, and show up as "An error occurred" in the UI.

To surface the errors, you can use the `getErrorMessage` function when calling `toDataStreamResponse`.

```
exportfunctionerrorHandler(error: unknown){if(error ==null){return'unknown error';}if(typeof error ==='string'){return error;}if(error instanceofError){return error.message;}returnJSON.stringify(error);}
```

```
const result =streamText({// ...});return result.toDataStreamResponse({  getErrorMessage: errorHandler,});
```

In case you are using `createDataStreamResponse`, you can use the `onError` function when calling `toDataStreamResponse`:

```
const response =createDataStreamResponse({// ...asyncexecute(dataStream){// ...},onError:error=>`Custom error: ${error.message}`,});
```
```

### 105. `docs/ai-sdk-ui/chatbot.md`

```markdown
# Chatbot


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/chatbot
description: Learn how to use the useChat hook.
---


# [Chatbot](#chatbot)


The `useChat` hook makes it effortless to create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages arrive.

To summarize, the `useChat` hook provides the following features:

-   **Message Streaming**: All the messages from the AI provider are streamed to the chat UI in real-time.
-   **Managed States**: The hook manages the states for input, messages, status, error and more for you.
-   **Seamless Integration**: Easily integrate your chat AI into any design or layout with minimal effort.

In this guide, you will learn how to use the `useChat` hook to create a chatbot application with real-time message streaming. Check out our [chatbot with tools guide](/docs/ai-sdk-ui/chatbot-with-tool-calling) to learn how to use tools in your chatbot. Let's start with the following example first.


## [Example](#example)


app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit }=useChat({});return(<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.content}</div>))}<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button></form></>);}
```

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4-turbo'),    system:'You are a helpful assistant.',    messages,});return result.toDataStreamResponse();}
```

The UI messages have a new `parts` property that contains the message parts. We recommend rendering the messages using the `parts` property instead of the `content` property. The parts property supports different message types, including text, tool invocation, and tool result, and allows for more flexible and complex chat UIs.

In the `Page` component, the `useChat` hook will request to your AI provider endpoint whenever the user submits a message. The messages are then streamed back in real-time and displayed in the chat UI.

This enables a seamless chat experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.


## [Customized UI](#customized-ui)


`useChat` also provides ways to manage the chat message and input states via code, show status, and update messages without being triggered by user interactions.


### [Status](#status)


The `useChat` hook returns a `status`. It has the following possible values:

-   `submitted`: The message has been sent to the API and we're awaiting the start of the response stream.
-   `streaming`: The response is actively streaming in from the API, receiving chunks of data.
-   `ready`: The full response has been received and processed; a new user message can be submitted.
-   `error`: An error occurred during the API request, preventing successful completion.

You can use `status` for e.g. the following purposes:

-   To show a loading spinner while the chatbot is processing the user's message.
-   To show a "Stop" button to abort the current message.
-   To disable the submit button.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit, status, stop }=useChat({});return(<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.content}</div>))}{(status ==='submitted'| status ==='streaming')&&(<div>{status ==='submitted'&&<Spinner/>}<buttontype="button"onClick={()=>stop()}>Stop</button></div>)}<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}disabled={status !=='ready'}/><buttontype="submit">Submit</button></form></>);}
```


### [Error State](#error-state)


Similarly, the `error` state reflects the error object thrown during the fetch request. It can be used to display an error message, disable the submit button, or show a retry button:

We recommend showing a generic error message to the user, such as "Something went wrong." This is a good practice to avoid leaking information from the server.

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit, error, reload }=useChat({});return(<div>{messages.map(m=>(<divkey={m.id}>{m.role}:{m.content}</div>))}{error &&(<><div>An error occurred.</div><buttontype="button"onClick={()=>reload()}>Retry</button></>)}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}disabled={error !=null}/></form></div>);}
```

Please also see the [error handling](/docs/ai-sdk-ui/error-handling) guide for more information.


### [Modify messages](#modify-messages)


Sometimes, you may want to directly modify some existing messages. For example, a delete button can be added to each message to allow users to remove them from the chat history.

The `setMessages` function can help you achieve these tasks:

```
const{ messages, setMessages,...}=useChat()consthandleDelete=(id)=>{setMessages(messages.filter(message=> message.id !== id))}return<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.content}<buttononClick={()=>handleDelete(message.id)}>Delete</button></div>))}...
```

You can think of `messages` and `setMessages` as a pair of `state` and `setState` in React.


### [Controlled input](#controlled-input)


In the initial example, we have `handleSubmit` and `handleInputChange` callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.

The following example demonstrates how to use more granular APIs like `setInput` and `append` with your custom input and submit button components:

```
const{ input, setInput, append }=useChat()return<><MyCustomInputvalue={input}onChange={value=>setInput(value)}/><MySubmitButtononClick={()=>{// Send a new message to the AI providerappend({      role:'user',      content: input,})}}/>...
```


### [Cancellation and regeneration](#cancellation-and-regeneration)


It's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useChat` hook.

```
const{ stop, status,...}=useChat()return<><buttononClick={stop}disabled={!(status ==='streaming'| status ==='submitted')}>Stop</button>...
```

When the user clicks the "Stop" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your chatbot application.

Similarly, you can also request the AI provider to reprocess the last message by calling the `reload` function returned by the `useChat` hook:

```
const{ reload, status,...}=useChat()return<><buttononClick={reload}disabled={!(status ==='ready'| status ==='error')}>Regenerate</button>...</>
```

When the user clicks the "Regenerate" button, the AI provider will regenerate the last message and replace the current one correspondingly.


### [Throttling UI Updates](#throttling-ui-updates)


This feature is currently only available for React.

By default, the `useChat` hook will trigger a render every time a new chunk is received. You can throttle the UI updates with the `experimental_throttle` option.

page.tsx

```
const{ messages,...}=useChat({// Throttle the messages and data updates to 50ms:  experimental_throttle:50})
```


## [Event Callbacks](#event-callbacks)


`useChat` provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle:

-   `onFinish`: Called when the assistant message is completed
-   `onError`: Called when an error occurs during the fetch request.
-   `onResponse`: Called when the response from the API is received.

These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.

```
import{Message}from'@ai-sdk/react';const{/* ... */}=useChat({onFinish:(message,{ usage, finishReason })=>{console.log('Finished streaming message:', message);console.log('Token usage:', usage);console.log('Finish reason:', finishReason);},onError:error=>{console.error('An error occurred:', error);},onResponse:response=>{console.log('Received HTTP response from server:', response);},});
```

It's worth noting that you can abort the processing by throwing an error in the `onResponse` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.


## [Request Configuration](#request-configuration)



### [Custom headers, body, and credentials](#custom-headers-body-and-credentials)


By default, the `useChat` hook sends a HTTP POST request to the `/api/chat` endpoint with the message list as the request body. You can customize the request by passing additional options to the `useChat` hook:

```
const{ messages, input, handleInputChange, handleSubmit }=useChat({  api:'/api/custom-chat',  headers:{Authorization:'your_token',},  body:{    user_id:'123',},  credentials:'same-origin',});
```

In this example, the `useChat` hook sends a POST request to the `/api/custom-chat` endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.


### [Setting custom body fields per request](#setting-custom-body-fields-per-request)


You can configure custom `body` fields on a per-request basis using the `body` option of the `handleSubmit` function. This is useful if you want to pass in additional information to your backend that is not part of the message list.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<div>{messages.map(m=>(<divkey={m.id}>{m.role}:{m.content}</div>))}<form        onSubmit={event=>{handleSubmit(event,{            body:{              customKey:'customValue',},});}}><inputvalue={input}onChange={handleInputChange}/></form></div>);}
```

You can retrieve these custom fields on your server side by destructuring the request body:

app/api/chat/route.ts

```
exportasyncfunctionPOST(req:Request){// Extract addition information ("customKey") from the body of the request:const{ messages, customKey }=await req.json();//...}
```


## [Controlling the response stream](#controlling-the-response-stream)


With `streamText`, you can control how error messages and usage information are sent back to the client.


### [Error Messages](#error-messages)


By default, the error message is masked for security reasons. The default error message is "An error occurred." You can forward error messages or send your own error message by providing a `getErrorMessage` function:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse({getErrorMessage: error =>{if(error ==null){return'unknown error';}if(typeof error ==='string'){return error;}if(error instanceofError){return error.message;}returnJSON.stringify(error);},});}
```


### [Usage Information](#usage-information)


By default, the usage information is sent back to the client. You can disable it by setting the `sendUsage` option to `false`:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse({    sendUsage:false,});}
```


### [Text Streams](#text-streams)


`useChat` can handle plain text streams by setting the `streamProtocol` option to `text`:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages }=useChat({    streamProtocol:'text',});return<>...</>;}
```

This configuration also works with other backend servers that stream plain text. Check out the [stream protocol guide](/docs/ai-sdk-ui/stream-protocol) for more information.

When using `streamProtocol: 'text'`, tool calls, usage information and finish reasons are not available.


## [Empty Submissions](#empty-submissions)


You can configure the `useChat` hook to allow empty submissions by setting the `allowEmptySubmit` option to `true`.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<div>{messages.map(m=>(<divkey={m.id}>{m.role}:{m.content}</div>))}<formonSubmit={event=>{handleSubmit(event,{            allowEmptySubmit:true,});}}><inputvalue={input}onChange={handleInputChange}/></form></div>);}
```


## [Reasoning](#reasoning)


Some models such as as DeepSeek `deepseek-reasoner` and Anthropic `claude-3-7-sonnet-20250219` support reasoning tokens. These tokens are typically sent before the message content. You can forward them to the client with the `sendReasoning` option:

app/api/chat/route.ts

```
import{ deepseek }from'@ai-sdk/deepseek';import{ streamText }from'ai';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();const result =streamText({    model:deepseek('deepseek-reasoner'),    messages,});return result.toDataStreamResponse({    sendReasoning:true,});}
```

On the client side, you can access the reasoning parts of the message object.

They have a `details` property that contains the reasoning and redacted reasoning parts. You can also use `reasoning` to access just the reasoning as a string.

app/page.tsx

```
messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.parts.map((part, index)=>{// text parts:if(part.type==='text'){return<divkey={index}>{part.text}</div>;}// reasoning parts:if(part.type==='reasoning'){return(<prekey={index}>{part.details.map(detail=>              detail.type==='text'? detail.text :'<redacted>',)}</pre>);}})}</div>));
```


## [Sources](#sources)


Some providers such as [Perplexity](/providers/ai-sdk-providers/perplexity#sources) and [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.

Currently sources are limited to web pages that ground the response. You can forward them to the client with the `sendSources` option:

app/api/chat/route.ts

```
import{ perplexity }from'@ai-sdk/perplexity';import{ streamText }from'ai';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();const result =streamText({    model:perplexity('sonar-pro'),    messages,});return result.toDataStreamResponse({    sendSources:true,});}
```

On the client side, you can access source parts of the message object. Here is an example that renders the sources as links at the bottom of the message:

app/page.tsx

```
messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.parts.filter(part=> part.type!=='source').map((part, index)=>{if(part.type==='text'){return<divkey={index}>{part.text}</div>;}})}{message.parts.filter(part=> part.type==='source').map(part=>(<spankey={`source-${part.source.id}`}>[<ahref={part.source.url}target="_blank">{part.source.title ??newURL(part.source.url).hostname}</a>]</span>))}</div>));
```


## [Image Generation](#image-generation)


Some models such as Google `gemini-2.0-flash-exp` support image generation. When images are generated, they are exposed as files to the client. On the client side, you can access file parts of the message object and render them as images.

app/page.tsx

```
messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.parts.map((part, index)=>{if(part.type==='text'){return<divkey={index}>{part.text}</div>;}elseif(part.type==='file'&& part.mimeType.startsWith('image/')){return(<imgkey={index}src={`data:${part.mimeType};base64,${part.data}`}/>);}})}</div>));
```


## [Attachments (Experimental)](#attachments-experimental)


The `useChat` hook supports sending attachments along with a message as well as rendering them on the client. This can be useful for building applications that involve sending images, files, or other media content to the AI provider.

There are two ways to send attachments with a message, either by providing a `FileList` object or a list of URLs to the `handleSubmit` function:


### [FileList](#filelist)


By using `FileList`, you can send multiple files as attachments along with a message using the file input element. The `useChat` hook will automatically convert them into data URLs and send them to the AI provider.

Currently, only `image/*` and `text/*` content types get automatically converted into [multi-modal content parts](/docs/foundations/prompts#multi-modal-messages). You will need to handle other content types manually.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{ useRef, useState }from'react';exportdefaultfunctionPage(){const{ messages, input, handleSubmit, handleInputChange, status }=useChat();const[files, setFiles]=useState<FileList|undefined>(undefined);const fileInputRef =useRef<HTMLInputElement>(null);return(<div><div>{messages.map(message=>(<divkey={message.id}><div>{`${message.role}: `}</div><div>{message.content}<div>{message.experimental_attachments?.filter(attachment=>                    attachment.contentType.startsWith('image/'),).map((attachment, index)=>(<imgkey={`${message.id}-${index}`}src={attachment.url}alt={attachment.name}/>))}</div></div></div>))}</div><formonSubmit={event=>{handleSubmit(event,{            experimental_attachments: files,});setFiles(undefined);if(fileInputRef.current){            fileInputRef.current.value ='';}}}><inputtype="file"onChange={event=>{if(event.target.files){setFiles(event.target.files);}}}multipleref={fileInputRef}/><inputvalue={input}placeholder="Send message..."onChange={handleInputChange}disabled={status !=='ready'}/></form></div>);}
```


### [URLs](#urls)


You can also send URLs as attachments along with a message. This can be useful for sending links to external resources or media content.

> **Note:** The URL can also be a data URL, which is a base64-encoded string that represents the content of a file. Currently, only `image/*` content types get automatically converted into [multi-modal content parts](/docs/foundations/prompts#multi-modal-messages). You will need to handle other content types manually.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{ useState }from'react';import{Attachment}from'@ai-sdk/ui-utils';exportdefaultfunctionPage(){const{ messages, input, handleSubmit, handleInputChange, status }=useChat();const[attachments]=useState<Attachment[]>([{      name:'earth.png',      contentType:'image/png',      url:'https://example.com/earth.png',},{      name:'moon.png',      contentType:'image/png',      url:'data:image/png;base64,iVBORw0KGgo...',},]);return(<div><div>{messages.map(message=>(<divkey={message.id}><div>{`${message.role}: `}</div><div>{message.content}<div>{message.experimental_attachments?.filter(attachment=>                    attachment.contentType?.startsWith('image/'),).map((attachment, index)=>(<imgkey={`${message.id}-${index}`}src={attachment.url}alt={attachment.name}/>))}</div></div></div>))}</div><formonSubmit={event=>{handleSubmit(event,{            experimental_attachments: attachments,});}}><inputvalue={input}placeholder="Send message..."onChange={handleInputChange}disabled={status !=='ready'}/></form></div>);}
```
```

### 106. `docs/ai-sdk-ui/completion.md`

```markdown
# Completion


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/completion
description: Learn how to use the useCompletion hook.
---


# [Completion](#completion)


The `useCompletion` hook allows you to create a user interface to handle text completions in your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.

In this guide, you will learn how to use the `useCompletion` hook in your application to generate text completions and stream them in real-time to your users.


## [Example](#example)


app/page.tsx

```
'use client';import{ useCompletion }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ completion, input, handleInputChange, handleSubmit }=useCompletion({    api:'/api/completion',});return(<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}id="input"/><buttontype="submit">Submit</button><div>{completion}</div></form>);}
```

app/api/completion/route.ts

```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const{ prompt }:{ prompt:string}=await req.json();const result =streamText({    model:openai('gpt-3.5-turbo'),    prompt,});return result.toDataStreamResponse();}
```

In the `Page` component, the `useCompletion` hook will request to your AI provider endpoint whenever the user submits a message. The completion is then streamed back in real-time and displayed in the UI.

This enables a seamless text completion experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.


## [Customized UI](#customized-ui)


`useCompletion` also provides ways to manage the prompt via code, show loading and error states, and update messages without being triggered by user interactions.


### [Loading and error states](#loading-and-error-states)


To show a loading spinner while the chatbot is processing the user's message, you can use the `isLoading` state returned by the `useCompletion` hook:

```
const{ isLoading,...}=useCompletion()return(<>{isLoading ?<Spinner/>:null}</>)
```

Similarly, the `error` state reflects the error object thrown during the fetch request. It can be used to display an error message, or show a toast notification:

```
const{ error,...}=useCompletion()useEffect(()=>{if(error){    toast.error(error.message)}},[error])// Or display the error message in the UI:return(<>{error ?<div>{error.message}</div>:null}</>)
```


### [Controlled input](#controlled-input)


In the initial example, we have `handleSubmit` and `handleInputChange` callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.

The following example demonstrates how to use more granular APIs like `setInput` with your custom input and submit button components:

```
const{ input, setInput }=useCompletion();return(<><MyCustomInputvalue={input}onChange={value=>setInput(value)}/></>);
```


### [Cancelation](#cancelation)


It's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useCompletion` hook.

```
const{ stop, isLoading,...}=useCompletion()return(<><buttononClick={stop}disabled={!isLoading}>Stop</button></>)
```

When the user clicks the "Stop" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your application.


### [Throttling UI Updates](#throttling-ui-updates)


This feature is currently only available for React.

By default, the `useCompletion` hook will trigger a render every time a new chunk is received. You can throttle the UI updates with the `experimental_throttle` option.

page.tsx

```
const{ completion,...}=useCompletion({// Throttle the completion and data updates to 50ms:  experimental_throttle:50})
```


## [Event Callbacks](#event-callbacks)


`useCompletion` also provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle. These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.

```
const{...}=useCompletion({onResponse:(response: Response)=>{console.log('Received response from server:', response)},onFinish:(message: Message)=>{console.log('Finished streaming message:', message)},onError:(error: Error)=>{console.error('An error occurred:', error)},})
```

It's worth noting that you can abort the processing by throwing an error in the `onResponse` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.


## [Configure Request Options](#configure-request-options)


By default, the `useCompletion` hook sends a HTTP POST request to the `/api/completion` endpoint with the prompt as part of the request body. You can customize the request by passing additional options to the `useCompletion` hook:

```
const{ messages, input, handleInputChange, handleSubmit }=useCompletion({  api:'/api/custom-completion',  headers:{Authorization:'your_token',},  body:{    user_id:'123',},  credentials:'same-origin',});
```

In this example, the `useCompletion` hook sends a POST request to the `/api/completion` endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.
```

### 107. `docs/ai-sdk-ui/error-handling.md`

```markdown
# Error Handling


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/error-handling
description: Learn how to handle errors in the AI SDK UI
---


# [Error Handling](#error-handling)



### [Error Helper Object](#error-helper-object)


Each AI SDK UI hook also returns an [error](/docs/reference/ai-sdk-ui/use-chat#error) object that you can use to render the error in your UI. You can use the error object to show an error message, disable the submit button, or show a retry button.

We recommend showing a generic error message to the user, such as "Something went wrong." This is a good practice to avoid leaking information from the server.

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit, error, reload }=useChat({});return(<div>{messages.map(m=>(<divkey={m.id}>{m.role}:{m.content}</div>))}{error &&(<><div>An error occurred.</div><buttontype="button"onClick={()=>reload()}>Retry</button></>)}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}disabled={error !=null}/></form></div>);}
```


#### [Alternative: replace last message](#alternative-replace-last-message)


Alternatively you can write a custom submit handler that replaces the last message when an error is present.

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{    handleInputChange,    handleSubmit,    error,    input,    messages,    setMessages,}=useChat({});functioncustomSubmit(event: React.FormEvent<HTMLFormElement>){if(error !=null){setMessages(messages.slice(0,-1));// remove last message}handleSubmit(event);}return(<div>{messages.map(m=>(<divkey={m.id}>{m.role}:{m.content}</div>))}{error &&<div>An error occurred.</div>}<formonSubmit={customSubmit}><inputvalue={input}onChange={handleInputChange}/></form></div>);}
```


### [Error Handling Callback](#error-handling-callback)


Errors can be processed by passing an [`onError`](/docs/reference/ai-sdk-ui/use-chat#on-error) callback function as an option to the [`useChat`](/docs/reference/ai-sdk-ui/use-chat), [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) or [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) hooks. The callback function receives an error object as an argument.

```
import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{/* ... */}=useChat({// handle error:onError:error=>{console.error(error);},});}
```


### [Injecting Errors for Testing](#injecting-errors-for-testing)


You might want to create errors for testing. You can easily do so by throwing an error in your route handler:

```
exportasyncfunctionPOST(req:Request){thrownewError('This is a test error');}
```
```

### 108. `docs/ai-sdk-ui/generative-user-interfaces.md`

```markdown
# Generative User Interfaces


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces
description: Learn how to build Generative UI with AI SDK UI.
---


# [Generative User Interfaces](#generative-user-interfaces)


Generative user interfaces (generative UI) is the process of allowing a large language model (LLM) to go beyond text and "generate UI". This creates a more engaging and AI-native experience for users.

What is the weather in SF?

getWeather("San Francisco")

Thursday, March 7

47°

sunny

7am

48°

8am

50°

9am

52°

10am

54°

11am

56°

12pm

58°

1pm

60°

Thanks!

At the core of generative UI are [tools](/docs/ai-sdk-core/tools-and-tool-calling) , which are functions you provide to the model to perform specialized tasks like getting the weather in a location. The model can decide when and how to use these tools based on the context of the conversation.

Generative UI is the process of connecting the results of a tool call to a React component. Here's how it works:

1.  You provide the model with a prompt or conversation history, along with a set of tools.
2.  Based on the context, the model may decide to call a tool.
3.  If a tool is called, it will execute and return data.
4.  This data can then be passed to a React component for rendering.

By passing the tool results to React components, you can create a generative UI experience that's more engaging and adaptive to your needs.


## [Build a Generative UI Chat Interface](#build-a-generative-ui-chat-interface)


Let's create a chat interface that handles text-based conversations and incorporates dynamic UI elements based on model responses.


### [Basic Chat Implementation](#basic-chat-implementation)


Start with a basic chat implementation using the `useChat` hook:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<div>{messages.map(message=>(<divkey={message.id}><div>{message.role ==='user'?'User: ':'AI: '}</div><div>{message.content}</div></div>))}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}placeholder="Type a message..."/><buttontype="submit">Send</button></form></div>);}
```

To handle the chat requests and model responses, set up an API route:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';exportasyncfunctionPOST(request:Request){const{ messages }=await request.json();const result =streamText({    model:openai('gpt-4o'),    system:'You are a friendly assistant!',    messages,    maxSteps:5,});return result.toDataStreamResponse();}
```

This API route uses the `streamText` function to process chat messages and stream the model's responses back to the client.


### [Create a Tool](#create-a-tool)


Before enhancing your chat interface with dynamic UI elements, you need to create a tool and corresponding React component. A tool will allow the model to perform a specific action, such as fetching weather information.

Create a new file called `ai/tools.ts` with the following content:

ai/tools.ts

```
import{ tool as createTool }from'ai';import{ z }from'zod';exportconst weatherTool =createTool({  description:'Display the weather for a location',  parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:asyncfunction({location}){awaitnewPromise(resolve =>setTimeout(resolve,2000));return{ weather:'Sunny', temperature:75,location};},});exportconst tools ={  displayWeather: weatherTool,};
```

In this file, you've created a tool called `weatherTool`. This tool simulates fetching weather information for a given location. This tool will return simulated data after a 2-second delay. In a real-world application, you would replace this simulation with an actual API call to a weather service.


### [Update the API Route](#update-the-api-route)


Update the API route to include the tool you've defined:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';import{ tools }from'@/ai/tools';exportasyncfunctionPOST(request:Request){const{ messages }=await request.json();const result =streamText({    model:openai('gpt-4o'),    system:'You are a friendly assistant!',    messages,    maxSteps:5,    tools,});return result.toDataStreamResponse();}
```

Now that you've defined the tool and added it to your `streamText` call, let's build a React component to display the weather information it returns.


### [Create UI Components](#create-ui-components)


Create a new file called `components/weather.tsx`:

components/weather.tsx

```
typeWeatherProps={  temperature:number;  weather:string;location:string;};exportconstWeather=({ temperature, weather, location }: WeatherProps)=>{return(<div><h2>CurrentWeatherfor{location}</h2><p>Condition:{weather}</p><p>Temperature:{temperature}°C</p></div>);};
```

This component will display the weather information for a given location. It takes three props: `temperature`, `weather`, and `location` (exactly what the `weatherTool` returns).


### [Render the Weather Component](#render-the-weather-component)


Now that you have your tool and corresponding React component, let's integrate them into your chat interface. You'll render the Weather component when the model calls the weather tool.

To check if the model has called a tool, you can use the `toolInvocations` property of the message object. This property contains information about any tools that were invoked in that generation including `toolCallId`, `toolName`, `args`, `toolState`, and `result`.

Update your `page.tsx` file:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{Weather}from'@/components/weather';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<div>{messages.map(message=>(<divkey={message.id}><div>{message.role ==='user'?'User: ':'AI: '}</div><div>{message.content}</div><div>{message.toolInvocations?.map(toolInvocation=>{const{ toolName, toolCallId, state }= toolInvocation;if(state ==='result'){if(toolName ==='displayWeather'){const{ result }= toolInvocation;return(<divkey={toolCallId}><Weather{...result}/></div>);}}else{return(<divkey={toolCallId}>{toolName ==='displayWeather'?(<div>Loading weather...</div>):null}</div>);}})}</div></div>))}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}placeholder="Type a message..."/><buttontype="submit">Send</button></form></div>);}
```

In this updated code snippet, you:

1.  Check if the message has `toolInvocations`.
2.  Check if the tool invocation state is 'result'.
3.  If it's a result and the tool name is 'displayWeather', render the Weather component.
4.  If the tool invocation state is not 'result', show a loading message.

This approach allows you to dynamically render UI components based on the model's responses, creating a more interactive and context-aware chat experience.


## [Expanding Your Generative UI Application](#expanding-your-generative-ui-application)


You can enhance your chat application by adding more tools and components, creating a richer and more versatile user experience. Here's how you can expand your application:


### [Adding More Tools](#adding-more-tools)


To add more tools, simply define them in your `ai/tools.ts` file:

```
// Add a new stock toolexportconst stockTool =createTool({  description:'Get price for a stock',  parameters: z.object({symbol: z.string().describe('The stock symbol to get the price for'),}),execute:asyncfunction({symbol}){// Simulated API callawaitnewPromise(resolve =>setTimeout(resolve,2000));return{symbol, price:100};},});// Update the tools objectexportconst tools ={  displayWeather: weatherTool,  getStockPrice: stockTool,};
```

Now, create a new file called `components/stock.tsx`:

```
typeStockProps={  price:number;symbol:string;};exportconstStock=({ price, symbol }: StockProps)=>{return(<div><h2>StockInformation</h2><p>Symbol:{symbol}</p><p>Price: ${price}</p></div>);};
```

Finally, update your `page.tsx` file to include the new Stock component:

```
'use client';import{ useChat }from'@ai-sdk/react';import{Weather}from'@/components/weather';import{Stock}from'@/components/stock';exportdefaultfunctionPage(){const{ messages, input, setInput, handleSubmit }=useChat();return(<div>{messages.map(message=>(<divkey={message.id}><div>{message.role}</div><div>{message.content}</div><div>{message.toolInvocations?.map(toolInvocation=>{const{ toolName, toolCallId, state }= toolInvocation;if(state ==='result'){if(toolName ==='displayWeather'){const{ result }= toolInvocation;return(<divkey={toolCallId}><Weather{...result}/></div>);}elseif(toolName ==='getStockPrice'){const{ result }= toolInvocation;return<Stockkey={toolCallId}{...result}/>;}}else{return(<divkey={toolCallId}>{toolName ==='displayWeather'?(<div>Loading weather...</div>): toolName ==='getStockPrice'?(<div>Loading stock price...</div>):(<div>Loading...</div>)}</div>);}})}</div></div>))}<formonSubmit={handleSubmit}><inputtype="text"value={input}onChange={event=>{setInput(event.target.value);}}/><buttontype="submit">Send</button></form></div>);}
```

By following this pattern, you can continue to add more tools and components, expanding the capabilities of your Generative UI application.
```

### 109. `docs/ai-sdk-ui/object-generation.md`

```markdown
# Object Generation


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/object-generation
description: Learn how to use the useObject hook.
---


# [Object Generation](#object-generation)


`useObject` is an experimental feature and only available in React.

The [`useObject`](/docs/reference/ai-sdk-ui/use-object) hook allows you to create interfaces that represent a structured JSON object that is being streamed.

In this guide, you will learn how to use the `useObject` hook in your application to generate UIs for structured data on the fly.


## [Example](#example)


The example shows a small notifications demo app that generates fake notifications in real-time.


### [Schema](#schema)


It is helpful to set up the schema in a separate file that is imported on both the client and server.

app/api/notifications/schema.ts

```
import{ z }from'zod';// define a schema for the notificationsexportconst notificationSchema = z.object({  notifications: z.array(    z.object({      name: z.string().describe('Name of a fictional person.'),      message: z.string().describe('Message. Do not use emojis or links.'),}),),});
```


### [Client](#client)


The client uses [`useObject`](/docs/reference/ai-sdk-ui/use-object) to stream the object generation process.

The results are partial and are displayed as they are received. Please note the code for handling `undefined` values in the JSX.

app/page.tsx

```
'use client';import{ experimental_useObject as useObject }from'@ai-sdk/react';import{ notificationSchema }from'./api/notifications/schema';exportdefaultfunctionPage(){const{ object, submit }=useObject({    api:'/api/notifications',    schema: notificationSchema,});return(<><buttononClick={()=>submit('Messages during finals week.')}>Generate notifications</button>{object?.notifications?.map((notification, index)=>(<divkey={index}><p>{notification?.name}</p><p>{notification?.message}</p></div>))}</>);}
```


### [Server](#server)


On the server, we use [`streamObject`](/docs/reference/ai-sdk-core/stream-object) to stream the object generation process.

app/api/notifications/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ notificationSchema }from'./schema';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const context =await req.json();const result =streamObject({    model:openai('gpt-4-turbo'),    schema: notificationSchema,    prompt:`Generate 3 notifications for a messages app in this context:`+ context,});return result.toTextStreamResponse();}
```


## [Customized UI](#customized-ui)


`useObject` also provides ways to show loading and error states:


### [Loading State](#loading-state)


The `isLoading` state returned by the `useObject` hook can be used for several purposes:

-   To show a loading spinner while the object is generated.
-   To disable the submit button.

app/page.tsx

```
'use client';import{ useObject }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ isLoading, object, submit }=useObject({    api:'/api/notifications',    schema: notificationSchema,});return(<>{isLoading &&<Spinner/>}<buttononClick={()=>submit('Messages during finals week.')}disabled={isLoading}>Generate notifications</button>{object?.notifications?.map((notification, index)=>(<divkey={index}><p>{notification?.name}</p><p>{notification?.message}</p></div>))}</>);}
```


### [Stop Handler](#stop-handler)


The `stop` function can be used to stop the object generation process. This can be useful if the user wants to cancel the request or if the server is taking too long to respond.

app/page.tsx

```
'use client';import{ useObject }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ isLoading, stop, object, submit }=useObject({    api:'/api/notifications',    schema: notificationSchema,});return(<>{isLoading &&(<buttontype="button"onClick={()=>stop()}>Stop</button>)}<buttononClick={()=>submit('Messages during finals week.')}>Generate notifications</button>{object?.notifications?.map((notification, index)=>(<divkey={index}><p>{notification?.name}</p><p>{notification?.message}</p></div>))}</>);}
```


### [Error State](#error-state)


Similarly, the `error` state reflects the error object thrown during the fetch request. It can be used to display an error message, or to disable the submit button:

We recommend showing a generic error message to the user, such as "Something went wrong." This is a good practice to avoid leaking information from the server.

```
'use client';import{ useObject }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ error, object, submit }=useObject({    api:'/api/notifications',    schema: notificationSchema,});return(<>{error &&<div>An error occurred.</div>}<buttononClick={()=>submit('Messages during finals week.')}>Generate notifications</button>{object?.notifications?.map((notification, index)=>(<divkey={index}><p>{notification?.name}</p><p>{notification?.message}</p></div>))}</>);}
```


## [Event Callbacks](#event-callbacks)


`useObject` provides optional event callbacks that you can use to handle life-cycle events.

-   `onFinish`: Called when the object generation is completed.
-   `onError`: Called when an error occurs during the fetch request.

These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.

app/page.tsx

```
'use client';import{ experimental_useObject as useObject }from'@ai-sdk/react';import{ notificationSchema }from'./api/notifications/schema';exportdefaultfunctionPage(){const{ object, submit }=useObject({    api:'/api/notifications',    schema: notificationSchema,onFinish({ object, error }){// typed object, undefined if schema validation fails:console.log('Object generation completed:', object);// error, undefined if schema validation succeeds:console.log('Schema validation error:', error);},onError(error){// error during fetch request:console.error('An error occurred:', error);},});return(<div><buttononClick={()=>submit('Messages during finals week.')}>Generate notifications</button>{object?.notifications?.map((notification, index)=>(<divkey={index}><p>{notification?.name}</p><p>{notification?.message}</p></div>))}</div>);}
```


## [Configure Request Options](#configure-request-options)


You can configure the API endpoint, optional headers and credentials using the `api`, `headers` and `credentials` settings.

```
const{ submit, object }=useObject({  api:'/api/use-object',  headers:{'X-Custom-Header':'CustomValue',},  credentials:'include',  schema: yourSchema,});
```
```

### 110. `docs/ai-sdk-ui/openai-assistants.md`

```markdown
# OpenAI Assistants


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/openai-assistants
description: Learn how to use the useAssistant hook.
---


# [OpenAI Assistants](#openai-assistants)


The `useAssistant` hook allows you to handle the client state when interacting with an OpenAI compatible assistant API. This hook is useful when you want to integrate assistant capabilities into your application, with the UI updated automatically as the assistant is streaming its execution.

The `useAssistant` hook is supported in `@ai-sdk/react`, `ai/svelte`, and `ai/vue`.


## [Example](#example)


app/page.tsx

```
'use client';import{Message, useAssistant }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ status, messages, input, submitMessage, handleInputChange }=useAssistant({ api:'/api/assistant'});return(<div>{messages.map((m: Message)=>(<divkey={m.id}><strong>{`${m.role}: `}</strong>{m.role !=='data'&& m.content}{m.role ==='data'&&(<>{(m.data asany).description}<br/><preclassName={'bg-gray-200'}>{JSON.stringify(m.data,null,2)}</pre></>)}</div>))}{status ==='in_progress'&&<div/>}<formonSubmit={submitMessage}><inputdisabled={status !=='awaiting_message'}value={input}placeholder="What is the temperature in the living room?"onChange={handleInputChange}/></form></div>);}
```

app/api/assistant/route.ts

```
import{AssistantResponse}from'ai';importOpenAIfrom'openai';const openai =newOpenAI({  apiKey: process.env.OPENAI_API_KEY|'',});// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){// Parse the request bodyconst input:{    threadId:string|null;    message:string;}=await req.json();// Create a thread if neededconst threadId = input.threadId ??(await openai.beta.threads.create({})).id;// Add a message to the threadconst createdMessage =await openai.beta.threads.messages.create(threadId,{    role:'user',    content: input.message,});returnAssistantResponse({ threadId, messageId: createdMessage.id },async({ forwardStream, sendDataMessage })=>{// Run the assistant on the threadconst runStream = openai.beta.threads.runs.stream(threadId,{        assistant_id:          process.env.ASSISTANT_ID??(()=>{thrownewError('ASSISTANT_ID is not set');})(),});// forward run status would stream message deltaslet runResult =awaitforwardStream(runStream);// status can be: queued, in_progress, requires_action, cancelling, cancelled, failed, completed, or expiredwhile(        runResult?.status ==='requires_action'&&        runResult.required_action?.type==='submit_tool_outputs'){const tool_outputs =          runResult.required_action.submit_tool_outputs.tool_calls.map((toolCall: any)=>{const parameters =JSON.parse(toolCall.function.arguments);switch(toolCall.function.name){// configure your tool calls heredefault:thrownewError(`Unknown tool call function: ${toolCall.function.name}`,);}},);        runResult =awaitforwardStream(          openai.beta.threads.runs.submitToolOutputsStream(            threadId,            runResult.id,{ tool_outputs },),);}},);}
```


## [Customized UI](#customized-ui)


`useAssistant` also provides ways to manage the chat message and input states via code and show loading and error states.


### [Loading and error states](#loading-and-error-states)


To show a loading spinner while the assistant is running the thread, you can use the `status` state returned by the `useAssistant` hook:

```
const{ status,...}=useAssistant()return(<>{status ==="in_progress"?<Spinner/>:null}</>)
```

Similarly, the `error` state reflects the error object thrown during the fetch request. It can be used to display an error message, or show a toast notification:

```
const{ error,...}=useAssistant()useEffect(()=>{if(error){    toast.error(error.message)}},[error])// Or display the error message in the UI:return(<>{error ?<div>{error.message}</div>:null}</>)
```


### [Controlled input](#controlled-input)


In the initial example, we have `handleSubmit` and `handleInputChange` callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.

The following example demonstrates how to use more granular APIs like `append` with your custom input and submit button components:

```
const{ append }=useAssistant();return(<><MySubmitButtononClick={()=>{// Send a new message to the AI providerappend({          role:'user',          content: input,});}}/></>);
```


## [Configure Request Options](#configure-request-options)


By default, the `useAssistant` hook sends a HTTP POST request to the `/api/assistant` endpoint with the prompt as part of the request body. You can customize the request by passing additional options to the `useAssistant` hook:

```
const{ messages, input, handleInputChange, handleSubmit }=useAssistant({  api:'/api/custom-completion',  headers:{Authorization:'your_token',},  body:{    user_id:'123',},  credentials:'same-origin',});
```

In this example, the `useAssistant` hook sends a POST request to the `/api/custom-completion` endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.
```

### 111. `docs/ai-sdk-ui/overview.md`

```markdown
# AI SDK UI


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/overview
description: An overview of AI SDK UI.
---


# [AI SDK UI](#ai-sdk-ui)


AI SDK UI is designed to help you build interactive chat, completion, and assistant applications with ease. It is a **framework-agnostic toolkit**, streamlining the integration of advanced AI functionalities into your applications.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently. With four main hooks — **`useChat`**, **`useCompletion`**, **`useObject`**, and **`useAssistant`** — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

-   **[`useChat`](/docs/ai-sdk-ui/chatbot)** offers real-time streaming of chat messages, abstracting state management for inputs, messages, loading, and errors, allowing for seamless integration into any UI design.
-   **[`useCompletion`](/docs/ai-sdk-ui/completion)** enables you to handle text completions in your applications, managing the prompt input and automatically updating the UI as new completions are streamed.
-   **[`useObject`](/docs/ai-sdk-ui/object-generation)** is a hook that allows you to consume streamed JSON objects, providing a simple way to handle and display structured data in your application.
-   **[`useAssistant`](/docs/ai-sdk-ui/openai-assistants)** is designed to facilitate interaction with OpenAI-compatible assistant APIs, managing UI state and updating it automatically as responses are streamed.

These hooks are designed to reduce the complexity and time required to implement AI interactions, letting you focus on creating exceptional user experiences.


## [UI Framework Support](#ui-framework-support)


AI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), [Vue.js](https://vuejs.org/), and [SolidJS](https://www.solidjs.com/) (deprecated). Here is a comparison of the supported functions across these frameworks:

Function

React

Svelte

Vue.js

SolidJS (deprecated)

[useChat](/docs/reference/ai-sdk-ui/use-chat)

Chat

[useCompletion](/docs/reference/ai-sdk-ui/use-completion)

Completion

[useObject](/docs/reference/ai-sdk-ui/use-object)

StructuredObject

[useAssistant](/docs/reference/ai-sdk-ui/use-assistant)

[Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are welcome to implement missing features for non-React frameworks.


## [API Reference](#api-reference)


Please check out the [AI SDK UI API Reference](/docs/reference/ai-sdk-ui) for more details on each function.
```

### 112. `docs/ai-sdk-ui/smooth-stream-chinese.md`

```markdown
# Smooth streaming chinese text


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/smooth-stream-chinese
description: Learn how to stream smooth stream chinese text
---


# [Smooth streaming chinese text](#smooth-streaming-chinese-text)


You can smooth stream chinese text by using the `smoothStream` function, and the following regex that splits either on words of chinese characters:

page.tsx

```
import{ smoothStream }from'ai';import{ useChat }from'@ai-sdk/react';const{ data }=useChat({  experimental_transform:smoothStream({    chunking:/[\u4E00-\u9FFF]|\S+\s+/,}),});
```
```

### 113. `docs/ai-sdk-ui/smooth-stream-japanese.md`

```markdown
# Smooth streaming japanese text


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/smooth-stream-japanese
description: Learn how to stream smooth stream japanese text
---


# [Smooth streaming japanese text](#smooth-streaming-japanese-text)


You can smooth stream japanese text by using the `smoothStream` function, and the following regex that splits either on words of japanese characters:

page.tsx

```
import{ smoothStream }from'ai';import{ useChat }from'@ai-sdk/react';const{ data }=useChat({  experimental_transform:smoothStream({    chunking:/[\u3040-\u309F\u30A0-\u30FF]|\S+\s+/,}),});
```
```

### 114. `docs/ai-sdk-ui/stream-protocol.md`

```markdown
# Stream Protocols


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol
description: Learn more about the supported stream protocols in the AI SDK.
---


# [Stream Protocols](#stream-protocols)


AI SDK UI functions such as `useChat` and `useCompletion` support both text streams and data streams. The stream protocol defines how the data is streamed to the frontend on top of the HTTP protocol.

This page describes both protocols and how to use them in the backend and frontend.

You can use this information to develop custom backends and frontends for your use case, e.g., to provide compatible API endpoints that are implemented in a different language such as Python.

For instance, here's an example using [FastAPI](https://github.com/vercel/ai/tree/main/examples/next-fastapi) as a backend.


## [Text Stream Protocol](#text-stream-protocol)


A text stream contains chunks in plain text, that are streamed to the frontend. Each chunk is then appended together to form a full text response.

Text streams are supported by `useChat`, `useCompletion`, and `useObject`. When you use `useChat` or `useCompletion`, you need to enable text streaming by setting the `streamProtocol` options to `text`.

You can generate text streams with `streamText` in the backend. When you call `toTextStreamResponse()` on the result object, a streaming HTTP response is returned.

Text streams only support basic text data. If you need to stream other types of data such as tool calls, use data streams.


### [Text Stream Example](#text-stream-example)


Here is a Next.js example that uses the text stream protocol:

app/page.tsx

```
'use client';import{ useCompletion }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ completion, input, handleInputChange, handleSubmit }=useCompletion({    streamProtocol:'text',});return(<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button><div>{completion}</div></form>);}
```

app/api/completion/route.ts

```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const{ prompt }:{ prompt:string}=await req.json();const result =streamText({    model:openai('gpt-4o'),    prompt,});return result.toTextStreamResponse();}
```


## [Data Stream Protocol](#data-stream-protocol)


A data stream follows a special protocol that the AI SDK provides to send information to the frontend.

Each stream part has the format `TYPE_ID:CONTENT_JSON\n`.

When you provide data streams from a custom backend, you need to set the `x-vercel-ai-data-stream` header to `v1`.

The following stream parts are currently supported:


### [Text Part](#text-part)


The text parts are appended to the message as they are received.

Format: `0:string\n`

Example: `0:"example"\n`


### [Reasoning Part](#reasoning-part)


The reasoning parts are appended to the message as they are received. The reasoning part is available through `reasoning`.

Format: `g:string\n`

Example: `g:"I will open the conversation with witty banter."\n`


### [Redacted Reasoning Part](#redacted-reasoning-part)


The redacted reasoning parts contain reasoning data that has been redacted. This is available through `redacted_reasoning`.

Format: `i:{"data": string}\n`

Example: `i:{"data": "This reasoning has been redacted for security purposes."}\n`


### [Reasoning Signature Part](#reasoning-signature-part)


The reasoning signature parts contain a signature associated with the reasoning. This is available through `reasoning_signature`.

Format: `j:{"signature": string}\n`

Example: `j:{"signature": "abc123xyz"}\n`


### [Source Part](#source-part)


The source parts are appended to the message as they are received. The source part is available through `source`.

Format: `h:Source\n`

Example: `h:{"sourceType":"url","id":"source-id","url":"https://example.com","title":"Example"}\n`


### [File Part](#file-part)


The file parts contain binary data encoded as base64 strings along with their MIME type. The file part is available through `file`.

Format: `k:{data:string; mimeType:string}\n`

Example: `k:{"data":"base64EncodedData","mimeType":"image/png"}\n`


### [Data Part](#data-part)


The data parts are parsed as JSON and appended to the data array as they are received. The data is available through `data`.

Format: `2:Array<JSONValue>\n`

Example: `2:[{"key":"object1"},{"anotherKey":"object2"}]\n`


### [Message Annotation Part](#message-annotation-part)


The message annotation parts are appended to the message as they are received. The annotation part is available through `annotations`.

Format: `8:Array<JSONValue>\n`

Example: `8:[{"id":"message-123","other":"annotation"}]\n`


### [Error Part](#error-part)


The error parts are appended to the message as they are received.

Format: `3:string\n`

Example: `3:"error message"\n`


### [Tool Call Streaming Start Part](#tool-call-streaming-start-part)


A part indicating the start of a streaming tool call. This part needs to be sent before any tool call delta for that tool call. Tool call streaming is optional, you can use tool call and tool result parts without it.

Format: `b:{toolCallId:string; toolName:string}\n`

Example: `b:{"toolCallId":"call-456","toolName":"streaming-tool"}\n`


### [Tool Call Delta Part](#tool-call-delta-part)


A part representing a delta update for a streaming tool call.

Format: `c:{toolCallId:string; argsTextDelta:string}\n`

Example: `c:{"toolCallId":"call-456","argsTextDelta":"partial arg"}\n`


### [Tool Call Part](#tool-call-part)


A part representing a tool call. When there are streamed tool calls, the tool call part needs to come after the tool call streaming is finished.

Format: `9:{toolCallId:string; toolName:string; args:object}\n`

Example: `9:{"toolCallId":"call-123","toolName":"my-tool","args":{"some":"argument"}}\n`


### [Tool Result Part](#tool-result-part)


A part representing a tool result. The result part needs to be sent after the tool call part for that tool call.

Format: `a:{toolCallId:string; result:object}\n`

Example: `a:{"toolCallId":"call-123","result":"tool output"}\n`


### [Start Step Part](#start-step-part)


A part indicating the start of a step.

It includes the following metadata:

-   `messageId` to indicate the id of the message that this step belongs to.

Format: `f:{messageId:string}\n`

Example: `f:{"messageId":"step_123"}\n`


### [Finish Step Part](#finish-step-part)


A part indicating that a step (i.e., one LLM API call in the backend) has been completed.

This part is necessary to correctly process multiple stitched assistant calls, e.g. when calling tools in the backend, and using steps in `useChat` at the same time.

It includes the following metadata:

-   [`FinishReason`](/docs/reference/ai-sdk-ui/use-chat#on-finish-finish-reason)
-   [`Usage`](/docs/reference/ai-sdk-ui/use-chat#on-finish-usage) for that step.
-   `isContinued` to indicate if the step text will be continued in the next step.

The finish step part needs to come at the end of a step.

Format: `e:{finishReason:'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown';usage:{promptTokens:number; completionTokens:number;},isContinued:boolean}\n`

Example: `e:{"finishReason":"stop","usage":{"promptTokens":10,"completionTokens":20},"isContinued":false}\n`


### [Finish Message Part](#finish-message-part)


A part indicating the completion of a message with additional metadata, such as [`FinishReason`](/docs/reference/ai-sdk-ui/use-chat#on-finish-finish-reason) and [`Usage`](/docs/reference/ai-sdk-ui/use-chat#on-finish-usage). This part needs to be the last part in the stream.

Format: `d:{finishReason:'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown';usage:{promptTokens:number; completionTokens:number;}}\n`

Example: `d:{"finishReason":"stop","usage":{"promptTokens":10,"completionTokens":20}}\n`

The data stream protocol is supported by `useChat` and `useCompletion` on the frontend and used by default. `useCompletion` only supports the `text` and `data` stream parts.

On the backend, you can use `toDataStreamResponse()` from the `streamText` result object to return a streaming HTTP response.


### [Data Stream Example](#data-stream-example)


Here is a Next.js example that uses the data stream protocol:

app/page.tsx

```
'use client';import{ useCompletion }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ completion, input, handleInputChange, handleSubmit }=useCompletion({    streamProtocol:'data',// optional, this is the default});return(<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button><div>{completion}</div></form>);}
```

app/api/completion/route.ts

```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const{ prompt }:{ prompt:string}=await req.json();const result =streamText({    model:openai('gpt-4o'),    prompt,});return result.toDataStreamResponse();}
```
```

### 115. `docs/ai-sdk-ui/streaming-data.md`

```markdown
# Streaming Custom Data


---
url: https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data
description: Learn how to stream custom data to the client.
---


# [Streaming Custom Data](#streaming-custom-data)


It is often useful to send additional data alongside the model's response. For example, you may want to send status information, the message ids after storing them, or references to content that the language model is referring to.

The AI SDK provides several helpers that allows you to stream additional data to the client and attach it either to the `Message` or to the `data` object of the `useChat` hook:

-   `createDataStream`: creates a data stream
-   `createDataStreamResponse`: creates a response object that streams data
-   `pipeDataStreamToResponse`: pipes a data stream to a server response object

The data is streamed as part of the response stream.


## [Sending Custom Data from the Server](#sending-custom-data-from-the-server)


In your server-side route handler, you can use `createDataStreamResponse` and `pipeDataStreamToResponse` in combination with `streamText`. You need to:

1.  Call `createDataStreamResponse` or `pipeDataStreamToResponse` to get a callback function with a `DataStreamWriter`.
2.  Write to the `DataStreamWriter` to stream additional data.
3.  Merge the `streamText` result into the `DataStreamWriter`.
4.  Return the response from `createDataStreamResponse` (if that method is used)

Here is an example:

route.ts

```
import{ openai }from'@ai-sdk/openai';import{ generateId, createDataStreamResponse, streamText }from'ai';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();// immediately start streaming (solves RAG issues with status, etc.)returncreateDataStreamResponse({execute:dataStream=>{      dataStream.writeData('initialized call');const result =streamText({        model:openai('gpt-4o'),        messages,onChunk(){          dataStream.writeMessageAnnotation({ chunk:'123'});},onFinish(){// message annotation:          dataStream.writeMessageAnnotation({            id:generateId(),// e.g. id from saved DB record            other:'information',});// call annotation:          dataStream.writeData('call completed');},});      result.mergeIntoDataStream(dataStream);},onError:error=>{// Error messages are masked by default for security reasons.// If you want to expose the error message to the client, you can do so here:return error instanceofError? error.message :String(error);},});}
```

You can also send stream data from custom backends, e.g. Python / FastAPI, using the [Data Stream Protocol](/docs/ai-sdk-ui/stream-protocol#data-stream-protocol).


## [Sending Custom Sources](#sending-custom-sources)


You can send custom sources to the client using the `writeSource` method on the `DataStreamWriter`:

route.ts

```
import{ openai }from'@ai-sdk/openai';import{ createDataStreamResponse, streamText }from'ai';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();returncreateDataStreamResponse({execute:dataStream=>{// write a custom url source to the stream:      dataStream.writeSource({        sourceType:'url',        id:'source-1',        url:'https://example.com',        title:'Example Source',});const result =streamText({        model:openai('gpt-4o'),        messages,});      result.mergeIntoDataStream(dataStream);},});}
```


## [Processing Custom Data in `useChat`](#processing-custom-data-in-usechat)


The `useChat` hook automatically processes the streamed data and makes it available to you.


### [Accessing Data](#accessing-data)


On the client, you can destructure `data` from the `useChat` hook which stores all `StreamData` as a `JSONValue[]`.

page.tsx

```
import{ useChat }from'@ai-sdk/react';const{ data }=useChat();
```


### [Accessing Message Annotations](#accessing-message-annotations)


Each message from the `useChat` hook has an optional `annotations` property that contains the message annotations sent from the server.

Since the shape of the annotations depends on what you send from the server, you have to destructure them in a type-safe way on the client side.

Here we just show the annotations as a JSON string:

page.tsx

```
import{Message, useChat }from'@ai-sdk/react';const{ messages }=useChat();const result =(<>{messages?.map((m: Message)=>(<divkey={m.id}>{m.annotations &&<>{JSON.stringify(m.annotations)}</>}</div>))}</>);
```


### [Updating and Clearing Data](#updating-and-clearing-data)


You can update and clear the `data` object of the `useChat` hook using the `setData` function.

page.tsx

```
const{ setData }=useChat();// clear existing datasetData(undefined);// set new datasetData([{ test:'value'}]);// transform existing data, e.g. adding additional values:setData(currentData=>[...currentData,{ test:'value'}]);
```


#### [Example: Clear on Submit](#example-clear-on-submit)


page.tsx

```
'use client';import{Message, useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit, data, setData }=useChat();return(<>{data &&<pre>{JSON.stringify(data,null,2)}</pre>}{messages?.map((m: Message)=>(<divkey={m.id}>{`${m.role}: ${m.content}`}</div>))}<formonSubmit={e=>{setData(undefined);// clear stream datahandleSubmit(e);}}><inputvalue={input}onChange={handleInputChange}/></form></>);}
```
```

### 116. `docs/ai-sdk-ui.md`

```markdown
# AI SDK UI


---
url: https://ai-sdk.dev/docs/ai-sdk-ui
description: Learn about the AI SDK UI.
---


# [AI SDK UI](#ai-sdk-ui)


[

Overview

Get an overview about the AI SDK UI.

](/docs/ai-sdk-ui/overview)[

Chatbot

Learn how to integrate an interface for a chatbot.

](/docs/ai-sdk-ui/chatbot)[

Chatbot Message Persistence

Learn how to store and load chat messages in a chatbot.

](/docs/ai-sdk-ui/chatbot-message-persistence)[

Chatbot Tool Usage

Learn how to integrate an interface for a chatbot with tool calling.

](/docs/ai-sdk-ui/chatbot-tool-usage)[

Completion

Learn how to integrate an interface for text completion.

](/docs/ai-sdk-ui/completion)[

Object Generation

Learn how to integrate an interface for object generation.

](/docs/ai-sdk-ui/object-generation)[

OpenAI Assistants

Learn how to integrate an interface for OpenAI Assistants.

](/docs/ai-sdk-ui/openai-assistants)[

Streaming Data

Learn how to stream data.

](/docs/ai-sdk-ui/streaming-data)[

Error Handling

Learn how to handle errors.

](/docs/ai-sdk-ui/error-handling)[

Stream Protocol

The stream protocol defines how data is sent from the backend to the AI SDK UI frontend.

](/docs/ai-sdk-ui/stream-protocol)
```

### 117. `docs/announcing-ai-sdk-5-alpha.md`

```markdown
# Announcing AI SDK 5 Alpha


---
url: https://ai-sdk.dev/docs/announcing-ai-sdk-5-alpha
description: Get started with the Alpha version of AI SDK 5.
---


# [Announcing AI SDK 5 Alpha](#announcing-ai-sdk-5-alpha)


This is an early preview — AI SDK 5 is under active development. APIs may change without notice. Pin to specific versions as breaking changes may occur even in patch releases.


## [Alpha Version Guidance](#alpha-version-guidance)


The AI SDK 5 Alpha is intended for:

-   Exploration and early prototypes
-   Green-field projects where you can experiment freely
-   Development environments where you can tolerate breaking changes

This Alpha release is **not recommended** for:

-   Production applications
-   Projects that require stable APIs
-   Existing applications that would need migration paths

During this Alpha phase, we expect to make significant, potentially breaking changes to the API surface. We're sharing early to gather feedback and improve the SDK before stabilization. Your input is invaluable—please share your experiences through GitHub issues or discussions to help shape the final release.

We expect bugs in this Alpha release. To help us improve the SDK, please [file bug reports on GitHub](https://github.com/vercel/ai/issues/new/choose). Your reports directly contribute to making the final release more stable and reliable.


## [Installation](#installation)


To install the AI SDK 5 - Alpha, run the following command:

```

# replace with your provider and frameworknpminstall ai@alpha @ai-sdk/[your-provider]@alpha @ai-sdk/[your-framework]@alpha

```

APIs may change without notice. Pin to specific versions as breaking changes may occur even in patch releases.


## [What's new in AI SDK 5?](#whats-new-in-ai-sdk-5)


AI SDK 5 is a complete redesign of the AI SDK's protocol and architecture based on everything we've learned over the last two years of real-world usage. We've also modernized the UI and protocols that have remained largely unchanged since AI SDK v2/3, creating a strong foundation for the future.


### [Why AI SDK 5?](#why-ai-sdk-5)


When we originally designed the v1 protocol over a year ago, the standard interaction pattern with language models was simple: text in, text or tool call out. But today's LLMs go way beyond text and tool calls, generating reasoning, sources, images and more. Additionally, new use-cases like computer using agents introduce a fundamentally new approach to interacting with language models that made it near-impossible to support in a unified approach with our original architecture.

We needed a protocol designed for this new reality. While this is a breaking change that we don't take lightly, it's provided an opportunity to rebuild the foundation and add powerful new features.

While we've designed AI SDK 5 to be a substantial improvement over previous versions, we're still in active development. You might encounter bugs or unexpected behavior. We'd greatly appreciate your feedback and bug reports—they're essential to making this release better. Please share your experiences and suggestions with us through [GitHub issues](https://github.com/vercel/ai/issues/new/choose) or [GitHub discussions](https://github.com/vercel/ai/discussions).


## [New Features](#new-features)


-   [**LanguageModelV2**](#languagemodelv2) - new redesigned architecture
-   [**Message Overhaul**](#message-overhaul) - new `UIMessage` and `ModelMessage` types
-   [**ChatStore**](#chatstore) - new `useChat` architecture
-   [**Server-Sent Events (SSE)**](#server-sent-events-sse) - new standardised protocol for sending UI messages to the client
-   [**Agentic Control**](#agentic-control) - new primitives for building agentic systems


## [LanguageModelV2](#languagemodelv2)


LanguageModelV2 represents a complete redesign of how the AI SDK communicates with language models, adapting to the increasingly complex outputs modern AI systems generate. The new LanguageModelV2 treats all LLM outputs as content parts, enabling more consistent handling of text, images, reasoning, sources, and other response types. It now has:

-   **Content-First Design** - Rather than separating text, reasoning, and tool calls, everything is now represented as ordered content parts in a unified array
-   **Improved Type Safety** - The new LanguageModelV2 provides better TypeScript type guarantees, making it easier to work with different content types
-   **Simplified Extensibility** - Adding support for new model capabilities no longer requires changes to the core structure


## [Message Overhaul](#message-overhaul)


AI SDK 5 introduces a completely redesigned message system with two message types that address the dual needs of what you render in your UI and what you send to the model. Context is crucial for effective language model generations, and these two message types serve distinct purposes:

-   **UIMessage** represents the complete conversation history for your interface, preserving all message parts (text, images, data), metadata (creation timestamps, generation times), and UI state—regardless of length.

-   **ModelMessage** is optimized for sending to language models, considering token input constraints. It strips away UI-specific metadata and irrelevant content.


With this change, you will be required to explicitly convert your `UIMessage`s to `ModelMessage`s before sending them to the model.

```
import{ openai }from'@ai-sdk/openai';import{ convertToModelMessages, streamText,UIMessage}from'ai';exportasyncfunctionPOST(req:Request){const{ messages }:{ messages:UIMessage[]}=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages:convertToModelMessages(messages),});return result.toUIMessageStreamResponse();}
```

This separation is essential as you cannot use a single message format for both purposes. The state you save should always be the UIMessage format to prevent information loss, with explicit conversion to ModelMessage when communicating with language models.

The new message system has made possible several highly requested features:

-   **Type-safe Message Metadata** - add structured information per message
-   **New Stream Writer** - stream any part type (reasoning, sources, etc.) retaining proper order
-   **Data Parts** - stream type-safe arbitrary data parts for dynamic UI components


### [Message metadata](#message-metadata)


Metadata allows you to attach structured information to individual messages, making it easier to track important details like response time, token usage, or model specifications. This information can enhance your UI with contextual data without embedding it in the message content itself.

To add metadata to a message, first define the metadata schema:

app/api/chat/example-metadata-schema.ts

```
exportconst exampleMetadataSchema = z.object({  duration: z.number().optional(),  model: z.string().optional(),  totalTokens: z.number().optional(),});exporttypeExampleMetadata= z.infer<typeof exampleMetadataSchema>;
```

Then add the metadata using the `message.metadata` property on the `toUIMessageStreamResponse()` utility:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ convertToModelMessages, streamText,UIMessage}from'ai';import{ExampleMetadata}from'./example-metadata-schema';exportasyncfunctionPOST(req:Request){const{ messages }:{ messages:UIMessage[]}=await req.json();const startTime =Date.now();const result =streamText({    model:openai('gpt-4o'),    prompt:convertToModelMessages(messages),});return result.toUIMessageStreamResponse({    messageMetadata:({ part }):ExampleMetadata|undefined=>{// send custom information to the client on start:if(part.type==='start'){return{          model:'gpt-4o',// initial model id};}// send additional model information on finish-step:if(part.type==='finish-step'){return{          model: part.response.modelId,// update with the actual model id          duration:Date.now()- startTime,};}// when the message is finished, send additional information:if(part.type==='finish'){return{          totalTokens: part.totalUsage.totalTokens,};}},});}
```

Finally, specify the message metadata schema on the client and then render the (type-safe) metadata in your UI:

app/page.tsx

```
import{ zodSchema }from'@ai-sdk/provider-utils';import{ useChat }from'@ai-sdk/react';import{ defaultChatStore }from'ai';import{ exampleMetadataSchema }from'@/api/chat/example-metadata-schema';exportdefaultfunctionChat(){const{ messages }=useChat({    chatStore:defaultChatStore({      api:'/api/use-chat',      messageMetadataSchema:zodSchema(exampleMetadataSchema),}),});return(<div>{messages.map(message=>{const{ metadata }= message;return(<divkey={message.id}className="whitespace-pre-wrap">{metadata?.duration &&<div>Duration:{metadata.duration}ms</div>}{metadata?.model &&<div>Model:{metadata.model}</div>}{metadata?.totalTokens &&(<div>Total tokens:{metadata.totalTokens}</div>)}</div>);})}</div>);}
```


### [UI Message Stream](#ui-message-stream)


The UI Message Stream enables streaming any content parts from the server to the client. With this stream, you can send structured data like custom sources from your RAG pipeline directly to your UI. The stream writer is simply a utility that makes it easy to write to this message stream.

```
const stream =createUIMessageStream({execute: writer =>{// stream custom sources    writer.write({type:'source',      value:{type:'source',        sourceType:'url',        id:'source-1',        url:'https://example.com',        title:'Example Source',},});},});
```

On the client, these will be added to the ordered `message.parts` array.


### [Data Parts](#data-parts)


The new stream writer also enables a type-safe way to stream arbitrary data from the server to the client and display it in your UI.

You can create and stream custom data parts on the server:

```
// On the serverconst stream =createUIMessageStream({execute:writer=>{// Initial update    writer.write({type:'data-weather',// Custom typeid: toolCallId,// ID for updates      data:{ city, status:'loading'},// Your data});// Later, update the same part    writer.write({type:'data-weather',      id: toolCallId,      data:{ city, weather, status:'success'},});},});
```

On the client, you can render these parts with full type safety:

```
{  message.parts.filter(part=> part.type==='data-weather')// type-safe.map((part, index)=>(<Weather        key={index}        city={part.data.city}// type-safe        weather={part.data.weather}// type-safe        status={part.data.status}// type-safe/>));}
```

Data parts appear in the `message.parts` array along with other content, maintaining the proper ordering of the conversation. You can update parts by referencing the same ID, enabling dynamic experiences like collaborative artifacts.


## [ChatStore](#chatstore)


AI SDK 5 introduces a new `useChat` architecture with ChatStore and ChatTransport components. These two core building blocks make state management and API integration more flexible, allowing you to compose reactive UI bindings, share chat state across multiple instances, and swap out your backend protocol without rewriting application logic.

The `ChatStore` is responsible for:

-   **Managing multiple chats** – access and switch between conversations seamlessly.
-   **Processing response streams** – handle streams from the server and synchronize state (e.g. when there are concurrent client-side tool results).
-   **Caching and synchronizing** – share state (messages, status, errors) between `useChat` hooks.

You can create a basic ChatStore with the helper function:

```
import{ defaultChatStore }from'ai';const chatStore =defaultChatStore({  api:'/api/chat',// your chat endpoint  maxSteps:5,// optional: limit LLM calls in tool chains  chats:{},// optional: preload previous chat sessions});import{ useChat }from'@ai-sdk/react';const{ messages, input, handleSubmit }=useChat({ chatStore });
```


## [Server-Sent Events (SSE)](#server-sent-events-sse)


AI SDK 5 now uses Server-Sent Events (SSE) instead of a custom streaming protocol. SSE is a common web standard for sending data from servers to browsers. This switch has several advantages:

-   **Works everywhere** - Uses technology that works in all major browsers and platforms
-   **Easier to troubleshoot** - You can see the data stream in browser developer tools
-   **Simple to build upon** - Adding new features is more straightforward
-   **More stable** - Built on proven technology that many developers already use


## [Agentic Control](#agentic-control)


AI SDK 5 introduces new features for building agents that help you control model behavior more precisely.


### [prepareStep](#preparestep)


The `prepareStep` function gives you fine-grained control over each step in a multi-step agent. It's called before a step starts and allows you to:

-   Dynamically change the model used for specific steps
-   Force specific tool selections for particular steps
-   Limit which tools are available during specific steps
-   Examine the context of previous steps before proceeding

```
const result =awaitgenerateText({// ...experimental_prepareStep:async({ model, stepNumber, maxSteps, steps })=>{if(stepNumber ===0){return{// use a different model for this step:        model: modelForThisParticularStep,// force a tool choice for this step:        toolChoice:{type:'tool', toolName:'tool1'},// limit the tools that are available for this step:        experimental_activeTools:['tool1'],};}// when nothing is returned, the default settings are used},});
```

This makes it easier to build AI systems that adapt their capabilities based on the current context and task requirements.


### [continueUntil](#continueuntil)


The `continueUntil` parameter lets you define stopping conditions for your agent. Instead of running indefinitely, you can specify exactly when the agent should terminate based on various conditions:

-   Reaching a maximum number of steps
-   Calling a specific tool
-   Satisfying any custom condition you define

```
const result =generateText({// ...// stop loop at 5 steps  continueUntil:maxSteps(5),});const result =generateText({// ...// stop loop when weather tool called  continueUntil:hasToolCall('weather'),});const result =generateText({// ...// stop loop at your own custom condition  continueUntil:maxTotalTokens(20000),});
```

These agentic controls form the foundation for building more reliable, controllable AI systems that can tackle complex problems while remaining within well-defined constraints.
```

### 118. `docs/foundations/agents.md`

```markdown
# Agents


---
url: https://ai-sdk.dev/docs/foundations/agents
description: Learn how to build agents with AI SDK Core.
---


# [Agents](#agents)


When building AI applications, you often need **systems that can understand context and take meaningful actions**. When building these systems, the key consideration is finding the right balance between flexibility and control. Let's explore different approaches and patterns for building these systems, with a focus on helping you match capabilities to your needs.


## [Building Blocks](#building-blocks)


When building AI systems, you can combine these fundamental components:


### [Single-Step LLM Generation](#single-step-llm-generation)


The basic building block - one call to an LLM to get a response. Useful for straightforward tasks like classification or text generation.


### [Tool Usage](#tool-usage)


Enhanced capabilities through tools (like calculators, APIs, or databases) that the LLM can use to accomplish tasks. Tools provide a controlled way to extend what the LLM can do.

When solving complex problems, **an LLM can make multiple tool calls across multiple steps without you explicity specifying the order** - for example, looking up information in a database, using that to make calculations, and then storing results. The AI SDK makes this [multi-step tool usage](#multi-step-tool-usage) straightforward through the `maxSteps` parameter.


### [Multi-Agent Systems](#multi-agent-systems)


Multiple LLMs working together, each specialized for different aspects of a complex task. This enables sophisticated behaviors while keeping individual components focused.


## [Patterns](#patterns)


These building blocks can be combined with workflow patterns that help manage complexity:

-   [Sequential Processing](#sequential-processing-chains) - Steps executed in order
-   [Parallel Processing](#parallel-processing) - Independent tasks run simultaneously
-   [Evaluation/Feedback Loops](#evaluator-optimizer) - Results checked and improved iteratively
-   [Orchestration](#orchestrator-worker) - Coordinating multiple components
-   [Routing](#routing) - Directing work based on context


## [Choosing Your Approach](#choosing-your-approach)


The key factors to consider:

-   **Flexibility vs Control** - How much freedom does the LLM need vs how tightly must you constrain its actions?
-   **Error Tolerance** - What are the consequences of mistakes in your use case?
-   **Cost Considerations** - More complex systems typically mean more LLM calls and higher costs
-   **Maintenance** - Simpler architectures are easier to debug and modify

**Start with the simplest approach that meets your needs**. Add complexity only when required by:

1.  Breaking down tasks into clear steps
2.  Adding tools for specific capabilities
3.  Implementing feedback loops for quality control
4.  Introducing multiple agents for complex workflows

Let's look at examples of these patterns in action.


## [Patterns with Examples](#patterns-with-examples)


The following patterns, adapted from [Anthropic's guide on building effective agents](https://www.anthropic.com/research/building-effective-agents), serve as building blocks that can be combined to create comprehensive workflows. Each pattern addresses specific aspects of task execution, and by combining them thoughtfully, you can build reliable solutions for complex problems.


### [Sequential Processing (Chains)](#sequential-processing-chains)


The simplest workflow pattern executes steps in a predefined order. Each step's output becomes input for the next step, creating a clear chain of operations. This pattern is ideal for tasks with well-defined sequences, like content generation pipelines or data transformation processes.

```
import{ openai }from'@ai-sdk/openai';import{ generateText, generateObject }from'ai';import{ z }from'zod';asyncfunctiongenerateMarketingCopy(input:string){const model =openai('gpt-4o');// First step: Generate marketing copyconst{ text: copy }=awaitgenerateText({    model,    prompt:`Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,});// Perform quality check on copyconst{ object: qualityMetrics }=awaitgenerateObject({    model,    schema: z.object({      hasCallToAction: z.boolean(),      emotionalAppeal: z.number().min(1).max(10),      clarity: z.number().min(1).max(10),}),    prompt:`Evaluate this marketing copy for:    1. Presence of call to action (true/false)    2. Emotional appeal (1-10)    3. Clarity (1-10)    Copy to evaluate: ${copy}`,});// If quality check fails, regenerate with more specific instructionsif(!qualityMetrics.hasCallToAction|    qualityMetrics.emotionalAppeal<7|    qualityMetrics.clarity<7){const{ text: improvedCopy }=awaitgenerateText({      model,      prompt:`Rewrite this marketing copy with:${!qualityMetrics.hasCallToAction ?'- A clear call to action':''}${qualityMetrics.emotionalAppeal <7?'- Stronger emotional appeal':''}${qualityMetrics.clarity <7?'- Improved clarity and directness':''}      Original copy: ${copy}`,});return{ copy: improvedCopy, qualityMetrics };}return{ copy, qualityMetrics };}
```


### [Routing](#routing)


This pattern allows the model to make decisions about which path to take through a workflow based on context and intermediate results. The model acts as an intelligent router, directing the flow of execution between different branches of your workflow. This is particularly useful when handling varied inputs that require different processing approaches. In the example below, the results of the first LLM call change the properties of the second LLM call like model size and system prompt.

```
import{ openai }from'@ai-sdk/openai';import{ generateObject, generateText }from'ai';import{ z }from'zod';asyncfunctionhandleCustomerQuery(query:string){const model =openai('gpt-4o');// First step: Classify the query typeconst{ object: classification }=awaitgenerateObject({    model,    schema: z.object({      reasoning: z.string(),type: z.enum(['general','refund','technical']),      complexity: z.enum(['simple','complex']),}),    prompt:`Classify this customer query:${query}    Determine:    1. Query type (general, refund, or technical)    2. Complexity (simple or complex)    3. Brief reasoning for classification`,});// Route based on classification// Set model and system prompt based on query type and complexityconst{ text: response }=awaitgenerateText({    model:      classification.complexity==='simple'?openai('gpt-4o-mini'):openai('o3-mini'),    system:{      general:'You are an expert customer service agent handling general inquiries.',      refund:'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.',      technical:'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.',}[classification.type],    prompt: query,});return{ response, classification };}
```


### [Parallel Processing](#parallel-processing)


Some tasks can be broken down into independent subtasks that can be executed simultaneously. This pattern takes advantage of parallel execution to improve efficiency while maintaining the benefits of structured workflows. For example, analyzing multiple documents or processing different aspects of a single input concurrently (like code review).

```
import{ openai }from'@ai-sdk/openai';import{ generateText, generateObject }from'ai';import{ z }from'zod';// Example: Parallel code review with multiple specialized reviewersasyncfunctionparallelCodeReview(code:string){const model =openai('gpt-4o');// Run parallel reviewsconst[securityReview, performanceReview, maintainabilityReview]=awaitPromise.all([generateObject({        model,        system:'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.',        schema: z.object({          vulnerabilities: z.array(z.string()),          riskLevel: z.enum(['low','medium','high']),          suggestions: z.array(z.string()),}),        prompt:`Review this code:${code}`,}),generateObject({        model,        system:'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.',        schema: z.object({          issues: z.array(z.string()),          impact: z.enum(['low','medium','high']),          optimizations: z.array(z.string()),}),        prompt:`Review this code:${code}`,}),generateObject({        model,        system:'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.',        schema: z.object({          concerns: z.array(z.string()),          qualityScore: z.number().min(1).max(10),          recommendations: z.array(z.string()),}),        prompt:`Review this code:${code}`,}),]);const reviews =[{...securityReview.object,type:'security'},{...performanceReview.object,type:'performance'},{...maintainabilityReview.object,type:'maintainability'},];// Aggregate results using another model instanceconst{ text: summary }=awaitgenerateText({    model,    system:'You are a technical lead summarizing multiple code reviews.',    prompt:`Synthesize these code review results into a concise summary with key actions:${JSON.stringify(reviews,null,2)}`,});return{ reviews, summary };}
```


### [Orchestrator-Worker](#orchestrator-worker)


In this pattern, a primary model (orchestrator) coordinates the execution of specialized workers. Each worker is optimized for a specific subtask, while the orchestrator maintains overall context and ensures coherent results. This pattern excels at complex tasks requiring different types of expertise or processing.

```
import{ openai }from'@ai-sdk/openai';import{ generateObject }from'ai';import{ z }from'zod';asyncfunctionimplementFeature(featureRequest:string){// Orchestrator: Plan the implementationconst{ object: implementationPlan }=awaitgenerateObject({    model:openai('o3-mini'),    schema: z.object({      files: z.array(        z.object({          purpose: z.string(),          filePath: z.string(),          changeType: z.enum(['create','modify','delete']),}),),      estimatedComplexity: z.enum(['low','medium','high']),}),    system:'You are a senior software architect planning feature implementations.',    prompt:`Analyze this feature request and create an implementation plan:${featureRequest}`,});// Workers: Execute the planned changesconst fileChanges =awaitPromise.all(    implementationPlan.files.map(async file =>{// Each worker is specialized for the type of changeconst workerSystemPrompt ={        create:'You are an expert at implementing new files following best practices and project patterns.',        modify:'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.',delete:'You are an expert at safely removing code while ensuring no breaking changes.',}[file.changeType];const{ object: change }=awaitgenerateObject({        model:openai('gpt-4o'),        schema: z.object({          explanation: z.string(),          code: z.string(),}),        system: workerSystemPrompt,        prompt:`Implement the changes for ${file.filePath} to support:${file.purpose}        Consider the overall feature context:${featureRequest}`,});return{        file,        implementation: change,};}),);return{    plan: implementationPlan,    changes: fileChanges,};}
```


### [Evaluator-Optimizer](#evaluator-optimizer)


This pattern introduces quality control into workflows by having dedicated evaluation steps that assess intermediate results. Based on the evaluation, the workflow can either proceed, retry with adjusted parameters, or take corrective action. This creates more robust workflows capable of self-improvement and error recovery.

```
import{ openai }from'@ai-sdk/openai';import{ generateText, generateObject }from'ai';import{ z }from'zod';asyncfunctiontranslateWithFeedback(text:string, targetLanguage:string){let currentTranslation ='';let iterations =0;constMAX_ITERATIONS=3;// Initial translationconst{ text: translation }=awaitgenerateText({    model:openai('gpt-4o-mini'),// use small model for first attempt    system:'You are an expert literary translator.',    prompt:`Translate this text to ${targetLanguage}, preserving tone and cultural nuances:${text}`,});  currentTranslation = translation;// Evaluation-optimization loopwhile(iterations <MAX_ITERATIONS){// Evaluate current translationconst{ object: evaluation }=awaitgenerateObject({      model:openai('gpt-4o'),// use a larger model to evaluate      schema: z.object({        qualityScore: z.number().min(1).max(10),        preservesTone: z.boolean(),        preservesNuance: z.boolean(),        culturallyAccurate: z.boolean(),        specificIssues: z.array(z.string()),        improvementSuggestions: z.array(z.string()),}),      system:'You are an expert in evaluating literary translations.',      prompt:`Evaluate this translation:      Original: ${text}      Translation: ${currentTranslation}      Consider:      1. Overall quality      2. Preservation of tone      3. Preservation of nuance      4. Cultural accuracy`,});// Check if quality meets thresholdif(      evaluation.qualityScore>=8&&      evaluation.preservesTone&&      evaluation.preservesNuance&&      evaluation.culturallyAccurate){break;}// Generate improved translation based on feedbackconst{ text: improvedTranslation }=awaitgenerateText({      model:openai('gpt-4o'),// use a larger model      system:'You are an expert literary translator.',      prompt:`Improve this translation based on the following feedback:${evaluation.specificIssues.join('\n')}${evaluation.improvementSuggestions.join('\n')}      Original: ${text}      Current Translation: ${currentTranslation}`,});    currentTranslation = improvedTranslation;    iterations++;}return{    finalTranslation: currentTranslation,    iterationsRequired: iterations,};}
```


## [Multi-Step Tool Usage](#multi-step-tool-usage)


If your use case involves solving problems where the solution path is poorly defined or too complex to map out as a workflow in advance, you may want to provide the LLM with a set of lower-level tools and allow it to break down the task into small pieces that it can solve on its own iteratively, without discrete instructions. To implement this kind of agentic pattern, you need to call an LLM in a loop until a task is complete. The AI SDK makes this simple with the `maxSteps` parameter.

With `maxSteps`, the AI SDK automatically triggers an additional request to the model after every tool result (each request is considered a "step"). If the model does not generate a tool call or the `maxSteps` threshold has been met, the generation is complete.

`maxSteps` can be used with both `generateText` and `streamText`


### [Using maxSteps](#using-maxsteps)


This example demonstrates how to create an agent that solves math problems. It has a calculator tool (using [math.js](https://mathjs.org/)) that it can call to evaluate mathematical expressions.

```
import{ openai }from'@ai-sdk/openai';import{ generateText, tool }from'ai';import*as mathjsfrom'mathjs';import{ z }from'zod';const{ text: answer }=awaitgenerateText({  model:openai('gpt-4o-2024-08-06',{ structuredOutputs:true}),  tools:{    calculate:tool({      description:'A tool for evaluating mathematical expressions. '+'Example expressions: '+"'1.2 * (2 + 4.5)', '12.7 cm to inch', 'sin(45 deg) ^ 2'.",      parameters: z.object({ expression: z.string()}),execute:async({ expression })=> mathjs.evaluate(expression),}),},  maxSteps:10,  system:'You are solving math problems. '+'Reason step by step. '+'Use the calculator when necessary. '+'When you give the final answer, '+'provide an explanation for how you arrived at it.',  prompt:'A taxi driver earns $9461 per 1-hour of work. '+'If he works 12 hours a day and in 1 hour '+'he uses 12 liters of petrol with a price  of $134 for 1 liter. '+'How much money does he earn in one day?',});console.log(`ANSWER: ${answer}`);
```


### [Structured Answers](#structured-answers)


When building an agent for tasks like mathematical analysis or report generation, it's often useful to have the agent's final output structured in a consistent format that your application can process. You can use an **answer tool** and the `toolChoice: 'required'` setting to force the LLM to answer with a structured output that matches the schema of the answer tool. The answer tool has no `execute` function, so invoking it will terminate the agent.

```
import{ openai }from'@ai-sdk/openai';import{ generateText, tool }from'ai';import'dotenv/config';import{ z }from'zod';const{ toolCalls }=awaitgenerateText({  model:openai('gpt-4o-2024-08-06',{ structuredOutputs:true}),  tools:{    calculate:tool({      description:'A tool for evaluating mathematical expressions. Example expressions: '+"'1.2 * (2 + 4.5)', '12.7 cm to inch', 'sin(45 deg) ^ 2'.",      parameters: z.object({ expression: z.string()}),execute:async({ expression })=> mathjs.evaluate(expression),}),// answer tool: the LLM will provide a structured answer    answer:tool({      description:'A tool for providing the final answer.',      parameters: z.object({        steps: z.array(          z.object({            calculation: z.string(),            reasoning: z.string(),}),),        answer: z.string(),}),// no execute function - invoking it will terminate the agent}),},  toolChoice:'required',  maxSteps:10,  system:'You are solving math problems. '+'Reason step by step. '+'Use the calculator when necessary. '+'The calculator can only do simple additions, subtractions, multiplications, and divisions. '+'When you give the final answer, provide an explanation for how you got it.',  prompt:'A taxi driver earns $9461 per 1-hour work. '+'If he works 12 hours a day and in 1 hour he uses 14-liters petrol with price $134 for 1-liter. '+'How much money does he earn in one day?',});console.log(`FINAL TOOL CALLS: ${JSON.stringify(toolCalls,null,2)}`);
```

You can also use the [`experimental_output`](/docs/ai-sdk-core/generating-structured-data#structured-output-with-generatetext) setting for `generateText` to generate structured outputs.


### [Accessing all steps](#accessing-all-steps)


Calling `generateText` with `maxSteps` can result in several calls to the LLM (steps). You can access information from all steps by using the `steps` property of the response.

```
import{ generateText }from'ai';const{ steps }=awaitgenerateText({  model:openai('gpt-4o'),  maxSteps:10,// ...});// extract all tool calls from the steps:const allToolCalls = steps.flatMap(step => step.toolCalls);
```


### [Getting notified on each completed step](#getting-notified-on-each-completed-step)


You can use the `onStepFinish` callback to get notified on each completed step. It is triggered when a step is finished, i.e. all text deltas, tool calls, and tool results for the step are available.

```
import{ generateText }from'ai';const result =awaitgenerateText({  model: yourModel,  maxSteps:10,onStepFinish({ text, toolCalls, toolResults, finishReason, usage }){// your own logic, e.g. for saving the chat history or recording usage},// ...});
```
```

### 119. `docs/foundations/overview.md`

```markdown
# Overview


---
url: https://ai-sdk.dev/docs/foundations/overview
description: An overview of foundational concepts critical to understanding the AI SDK
---


# [Overview](#overview)


This page is a beginner-friendly introduction to high-level artificial intelligence (AI) concepts. To dive right into implementing the AI SDK, feel free to skip ahead to our [quickstarts](/docs/getting-started) or learn about our [supported models and providers](/docs/foundations/providers-and-models).

The AI SDK standardizes integrating artificial intelligence (AI) models across [supported providers](/docs/foundations/providers-and-models). This enables developers to focus on building great AI applications, not waste time on technical details.

For example, here’s how you can generate text with various models using the AI SDK:

xAI

OpenAI

Anthropic

Google

Custom

import { generateText } from "ai"

import { xai } from "@ai-sdk/xai"

const { text } = await generateText({

model: xai("grok-3-beta"),

prompt: "What is love?"

})

Love is a universal emotion that is characterized by feelings of affection, attachment, and warmth towards someone or something. It is a complex and multifaceted experience that can take many different forms, including romantic love, familial love, platonic love, and self-love.

To effectively leverage the AI SDK, it helps to familiarize yourself with the following concepts:


## [Generative Artificial Intelligence](#generative-artificial-intelligence)


**Generative artificial intelligence** refers to models that predict and generate various types of outputs (such as text, images, or audio) based on what’s statistically likely, pulling from patterns they’ve learned from their training data. For example:

-   Given a photo, a generative model can generate a caption.
-   Given an audio file, a generative model can generate a transcription.
-   Given a text description, a generative model can generate an image.


## [Large Language Models](#large-language-models)


A **large language model (LLM)** is a subset of generative models focused primarily on **text**. An LLM takes a sequence of words as input and aims to predict the most likely sequence to follow. It assigns probabilities to potential next sequences and then selects one. The model continues to generate sequences until it meets a specified stopping criterion.

LLMs learn by training on massive collections of written text, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well.

However, it's crucial to understand LLMs' limitations. When asked about less known or absent information, like the birthday of a personal relative, LLMs might "hallucinate" or make up information. It's essential to consider how well-represented the information you need is in the model.


## [Embedding Models](#embedding-models)


An **embedding model** is used to convert complex data (like words or images) into a dense vector (a list of numbers) representation, known as an embedding. Unlike generative models, embedding models do not generate new text or data. Instead, they provide representations of semantic and syntactic relationships between entities that can be used as input for other models or other natural language processing tasks.

In the next section, you will learn about the difference between models providers and models, and which ones are available in the AI SDK.
```

### 120. `docs/foundations/prompts.md`

```markdown
# Prompts


---
url: https://ai-sdk.dev/docs/foundations/prompts
description: Learn about the Prompt structure used in the AI SDK.
---


# [Prompts](#prompts)


Prompts are instructions that you give a [large language model (LLM)](/docs/foundations/overview#large-language-models) to tell it what to do. It's like when you ask someone for directions; the clearer your question, the better the directions you'll get.

Many LLM providers offer complex interfaces for specifying prompts. They involve different roles and message types. While these interfaces are powerful, they can be hard to use and understand.

In order to simplify prompting, the AI SDK supports text, message, and system prompts.


## [Text Prompts](#text-prompts)


Text prompts are strings. They are ideal for simple generation use cases, e.g. repeatedly generating content for variants of the same prompt text.

You can set text prompts using the `prompt` property made available by AI SDK functions like [`streamText`](/docs/reference/ai-sdk-core/stream-text) or [`generateObject`](/docs/reference/ai-sdk-core/generate-object). You can structure the text in any way and inject variables, e.g. using a template literal.

```
const result =awaitgenerateText({  model: yourModel,  prompt:'Invent a new holiday and describe its traditions.',});
```

You can also use template literals to provide dynamic data to your prompt.

```
const result =awaitgenerateText({  model: yourModel,  prompt:`I am planning a trip to ${destination} for ${lengthOfStay} days. `+`Please suggest the best tourist activities for me to do.`,});
```


## [System Prompts](#system-prompts)


System prompts are the initial set of instructions given to models that help guide and constrain the models' behaviors and responses. You can set system prompts using the `system` property. System prompts work with both the `prompt` and the `messages` properties.

```
const result =awaitgenerateText({  model: yourModel,  system:`You help planning travel itineraries. `+`Respond to the users' request with a list `+`of the best stops to make in their destination.`,  prompt:`I am planning a trip to ${destination} for ${lengthOfStay} days. `+`Please suggest the best tourist activities for me to do.`,});
```

When you use a message prompt, you can also use system messages instead of a system prompt.


## [Message Prompts](#message-prompts)


A message prompt is an array of user, assistant, and tool messages. They are great for chat interfaces and more complex, multi-modal prompts. You can use the `messages` property to set message prompts.

Each message has a `role` and a `content` property. The content can either be text (for user and assistant messages), or an array of relevant parts (data) for that message type.

```
const result =awaitstreamUI({  model: yourModel,  messages:[{ role:'user', content:'Hi!'},{ role:'assistant', content:'Hello, how can I help?'},{ role:'user', content:'Where can I buy the best Currywurst in Berlin?'},],});
```

Instead of sending a text in the `content` property, you can send an array of parts that includes a mix of text and other content parts.

Not all language models support all message and content types. For example, some models might not be capable of handling multi-modal inputs or tool messages. [Learn more about the capabilities of select models](./providers-and-models#model-capabilities).


### [Provider Options](#provider-options)


You can pass through additional provider-specific metadata to enable provider-specific functionality at 3 levels.


#### [Function Call Level](#function-call-level)


Functions like [`streamText`](/docs/reference/ai-sdk-core/stream-text#provider-options) or [`generateText`](/docs/reference/ai-sdk-core/generate-text#provider-options) accept a `providerOptions` property.

Adding provider options at the function call level should be used when you do not need granular control over where the provider options are applied.

```
const{ text }=awaitgenerateText({  model:azure('your-deployment-name'),  providerOptions:{    openai:{      reasoningEffort:'low',},},});
```


#### [Message Level](#message-level)


For granular control over applying provider options at the message level, you can pass `providerOptions` to the message object:

```
const messages =[{    role:'system',    content:'Cached system message',    providerOptions:{// Sets a cache control breakpoint on the system message      anthropic:{ cacheControl:{type:'ephemeral'}},},},];
```


#### [Message Part Level](#message-part-level)


Certain provider-specific options require configuration at the message part level:

```
const messages =[{    role:'user',    content:[{type:'text',        text:'Describe the image in detail.',        providerOptions:{          openai:{ imageDetail:'low'},},},{type:'image',        image:'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',// Sets image detail configuration for image part:        providerOptions:{          openai:{ imageDetail:'low'},},},],},];
```

AI SDK UI hooks like [`useChat`](/docs/reference/ai-sdk-ui/use-chat) return arrays of `UIMessage` objects, which do not support provider options. We recommend using the [`convertToCoreMessages`](/docs/reference/ai-sdk-ui/convert-to-core-messages) function to convert `UIMessage` objects to [`CoreMessage`](/docs/reference/ai-sdk-core/core-message) objects before applying or appending message(s) or message parts with `providerOptions`.


### [User Messages](#user-messages)



#### [Text Parts](#text-parts)


Text content is the most common type of content. It is a string that is passed to the model.

If you only need to send text content in a message, the `content` property can be a string, but you can also use it to send multiple content parts.

```
const result =awaitgenerateText({  model: yourModel,  messages:[{      role:'user',      content:[{type:'text',          text:'Where can I buy the best Currywurst in Berlin?',},],},],});
```


#### [Image Parts](#image-parts)


User messages can include image parts. An image can be one of the following:

-   base64-encoded image:
    -   `string` with base-64 encoded content
    -   data URL `string`, e.g. `data:image/png;base64,...`
-   binary image:
    -   `ArrayBuffer`
    -   `Uint8Array`
    -   `Buffer`
-   URL:
    -   http(s) URL `string`, e.g. `https://example.com/image.png`
    -   `URL` object, e.g. `new URL('https://example.com/image.png')`


##### [Example: Binary image (Buffer)](#example-binary-image-buffer)


```
const result =awaitgenerateText({  model,  messages:[{      role:'user',      content:[{type:'text', text:'Describe the image in detail.'},{type:'image',          image: fs.readFileSync('./data/comic-cat.png'),},],},],});
```


##### [Example: Base-64 encoded image (string)](#example-base-64-encoded-image-string)


```
const result =awaitgenerateText({  model: yourModel,  messages:[{      role:'user',      content:[{type:'text', text:'Describe the image in detail.'},{type:'image',          image: fs.readFileSync('./data/comic-cat.png').toString('base64'),},],},],});
```


##### [Example: Image URL (string)](#example-image-url-string)


```
const result =awaitgenerateText({  model: yourModel,  messages:[{      role:'user',      content:[{type:'text', text:'Describe the image in detail.'},{type:'image',          image:'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',},],},],});
```


#### [File Parts](#file-parts)


Only a few providers and models currently support file parts: [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai), [Google Vertex AI](/providers/ai-sdk-providers/google-vertex), [OpenAI](/providers/ai-sdk-providers/openai) (for `wav` and `mp3` audio with `gpt-4o-audio-preview`), [Anthropic](/providers/ai-sdk-providers/anthropic), [OpenAI](/providers/ai-sdk-providers/openai) (for `pdf`).

User messages can include file parts. A file can be one of the following:

-   base64-encoded file:
    -   `string` with base-64 encoded content
    -   data URL `string`, e.g. `data:image/png;base64,...`
-   binary data:
    -   `ArrayBuffer`
    -   `Uint8Array`
    -   `Buffer`
-   URL:
    -   http(s) URL `string`, e.g. `https://example.com/some.pdf`
    -   `URL` object, e.g. `new URL('https://example.com/some.pdf')`

You need to specify the MIME type of the file you are sending.


##### [Example: PDF file from Buffer](#example-pdf-file-from-buffer)


```
import{ google }from'@ai-sdk/google';import{ generateText }from'ai';const result =awaitgenerateText({  model:google('gemini-1.5-flash'),  messages:[{      role:'user',      content:[{type:'text', text:'What is the file about?'},{type:'file',          mimeType:'application/pdf',          data: fs.readFileSync('./data/example.pdf'),          filename:'example.pdf',// optional, not used by all providers},],},],});
```


##### [Example: mp3 audio file from Buffer](#example-mp3-audio-file-from-buffer)


```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const result =awaitgenerateText({  model:openai('gpt-4o-audio-preview'),  messages:[{      role:'user',      content:[{type:'text', text:'What is the audio saying?'},{type:'file',          mimeType:'audio/mpeg',          data: fs.readFileSync('./data/galileo.mp3'),},],},],});
```


### [Assistant Messages](#assistant-messages)


Assistant messages are messages that have a role of `assistant`. They are typically previous responses from the assistant and can contain text, reasoning, and tool call parts.


#### [Example: Assistant message with text content](#example-assistant-message-with-text-content)


```
const result =awaitgenerateText({  model: yourModel,  messages:[{ role:'user', content:'Hi!'},{ role:'assistant', content:'Hello, how can I help?'},],});
```


#### [Example: Assistant message with text content in array](#example-assistant-message-with-text-content-in-array)


```
const result =awaitgenerateText({  model: yourModel,  messages:[{ role:'user', content:'Hi!'},{      role:'assistant',      content:[{type:'text', text:'Hello, how can I help?'}],},],});
```


#### [Example: Assistant message with tool call content](#example-assistant-message-with-tool-call-content)


```
const result =awaitgenerateText({  model: yourModel,  messages:[{ role:'user', content:'How many calories are in this block of cheese?'},{      role:'assistant',      content:[{type:'tool-call',          toolCallId:'12345',          toolName:'get-nutrition-data',          args:{ cheese:'Roquefort'},},],},],});
```


#### [Example: Assistant message with file content](#example-assistant-message-with-file-content)


This content part is for model-generated files. Only a few models support this, and only for file types that they can generate.

```
const result =awaitgenerateText({  model: yourModel,  messages:[{ role:'user', content:'Generate an image of a roquefort cheese!'},{      role:'assistant',      content:[{type:'file',          mimeType:'image/png',          data: fs.readFileSync('./data/roquefort.jpg'),},],},],});
```


### [Tool messages](#tool-messages)


[Tools](/docs/foundations/tools) (also known as function calling) are programs that you can provide an LLM to extend its built-in functionality. This can be anything from calling an external API to calling functions within your UI. Learn more about Tools in [the next section](/docs/foundations/tools).

For models that support [tool](/docs/foundations/tools) calls, assistant messages can contain tool call parts, and tool messages can contain tool result parts. A single assistant message can call multiple tools, and a single tool message can contain multiple tool results.

```
const result =awaitgenerateText({  model: yourModel,  messages:[{      role:'user',      content:[{type:'text',          text:'How many calories are in this block of cheese?',},{type:'image', image: fs.readFileSync('./data/roquefort.jpg')},],},{      role:'assistant',      content:[{type:'tool-call',          toolCallId:'12345',          toolName:'get-nutrition-data',          args:{ cheese:'Roquefort'},},// there could be more tool calls here (parallel calling)],},{      role:'tool',      content:[{type:'tool-result',          toolCallId:'12345',// needs to match the tool call id          toolName:'get-nutrition-data',          result:{            name:'Cheese, roquefort',            calories:369,            fat:31,            protein:22,},},// there could be more tool results here (parallel calling)],},],});
```


#### [Multi-modal Tool Results](#multi-modal-tool-results)


Multi-part tool results are experimental and only supported by Anthropic.

Tool results can be multi-part and multi-modal, e.g. a text and an image. You can use the `experimental_content` property on tool parts to specify multi-part tool results.

```
const result =awaitgenerateText({  model: yourModel,  messages:[// ...{      role:'tool',      content:[{type:'tool-result',          toolCallId:'12345',// needs to match the tool call id          toolName:'get-nutrition-data',// for models that do not support multi-part tool results,// you can include a regular result part:          result:{            name:'Cheese, roquefort',            calories:369,            fat:31,            protein:22,},// for models that support multi-part tool results,// you can include a multi-part content part:          content:[{type:'text',              text:'Here is an image of the nutrition data for the cheese:',},{type:'image',              data: fs.readFileSync('./data/roquefort-nutrition-data.png'),              mimeType:'image/png',},],},],},],});
```


### [System Messages](#system-messages)


System messages are messages that are sent to the model before the user messages to guide the assistant's behavior. You can alternatively use the `system` property.

```
const result =awaitgenerateText({  model: yourModel,  messages:[{ role:'system', content:'You help planning travel itineraries.'},{      role:'user',      content:'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.',},],});
```
```

### 121. `docs/foundations/providers-and-models.md`

```markdown
# Providers and Models


---
url: https://ai-sdk.dev/docs/foundations/providers-and-models
description: Learn about the providers and models available in the AI SDK.
---


# [Providers and Models](#providers-and-models)


Companies such as OpenAI and Anthropic (providers) offer access to a range of large language models (LLMs) with differing strengths and capabilities through their own APIs.

Each provider typically has its own unique method for interfacing with their models, complicating the process of switching providers and increasing the risk of vendor lock-in.

To solve these challenges, AI SDK Core offers a standardized approach to interacting with LLMs through a [language model specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v1) that abstracts differences between providers. This unified interface allows you to switch between providers with ease while using the same API for all providers.

Here is an overview of the AI SDK Provider Architecture:


## [AI SDK Providers](#ai-sdk-providers)


The AI SDK comes with a wide range of providers that you can use to interact with different language models:

-   [xAI Grok Provider](/providers/ai-sdk-providers/xai) (`@ai-sdk/xai`)
-   [OpenAI Provider](/providers/ai-sdk-providers/openai) (`@ai-sdk/openai`)
-   [Azure OpenAI Provider](/providers/ai-sdk-providers/azure) (`@ai-sdk/azure`)
-   [Anthropic Provider](/providers/ai-sdk-providers/anthropic) (`@ai-sdk/anthropic`)
-   [Amazon Bedrock Provider](/providers/ai-sdk-providers/amazon-bedrock) (`@ai-sdk/amazon-bedrock`)
-   [Google Generative AI Provider](/providers/ai-sdk-providers/google-generative-ai) (`@ai-sdk/google`)
-   [Google Vertex Provider](/providers/ai-sdk-providers/google-vertex) (`@ai-sdk/google-vertex`)
-   [Mistral Provider](/providers/ai-sdk-providers/mistral) (`@ai-sdk/mistral`)
-   [Together.ai Provider](/providers/ai-sdk-providers/togetherai) (`@ai-sdk/togetherai`)
-   [Cohere Provider](/providers/ai-sdk-providers/cohere) (`@ai-sdk/cohere`)
-   [Fireworks Provider](/providers/ai-sdk-providers/fireworks) (`@ai-sdk/fireworks`)
-   [DeepInfra Provider](/providers/ai-sdk-providers/deepinfra) (`@ai-sdk/deepinfra`)
-   [DeepSeek Provider](/providers/ai-sdk-providers/deepseek) (`@ai-sdk/deepseek`)
-   [Cerebras Provider](/providers/ai-sdk-providers/cerebras) (`@ai-sdk/cerebras`)
-   [Groq Provider](/providers/ai-sdk-providers/groq) (`@ai-sdk/groq`)
-   [Perplexity Provider](/providers/ai-sdk-providers/perplexity) (`@ai-sdk/perplexity`)
-   [ElevenLabs Provider](/providers/ai-sdk-providers/elevenlabs) (`@ai-sdk/elevenlabs`)
-   [LMNT Provider](/providers/ai-sdk-providers/lmnt) (`@ai-sdk/lmnt`)
-   [Hume Provider](/providers/ai-sdk-providers/hume) (`@ai-sdk/hume`)
-   [Rev.ai Provider](/providers/ai-sdk-providers/revai) (`@ai-sdk/revai`)
-   [Deepgram Provider](/providers/ai-sdk-providers/deepgram) (`@ai-sdk/deepgram`)
-   [Gladia Provider](/providers/ai-sdk-providers/gladia) (`@ai-sdk/gladia`)
-   [AssemblyAI Provider](/providers/ai-sdk-providers/assemblyai) (`@ai-sdk/assemblyai`)

You can also use the [OpenAI Compatible provider](/providers/openai-compatible-providers) with OpenAI-compatible APIs:

-   [LM Studio](/providers/openai-compatible-providers/lmstudio)
-   [Baseten](/providers/openai-compatible-providers/baseten)

Our [language model specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v1) is published as an open-source package, which you can use to create [custom providers](/providers/community-providers/custom-providers).

The open-source community has created the following providers:

-   [Ollama Provider](/providers/community-providers/ollama) (`ollama-ai-provider`)
-   [ChromeAI Provider](/providers/community-providers/chrome-ai) (`chrome-ai`)
-   [FriendliAI Provider](/providers/community-providers/friendliai) (`@friendliai/ai-provider`)
-   [Portkey Provider](/providers/community-providers/portkey) (`@portkey-ai/vercel-provider`)
-   [Cloudflare Workers AI Provider](/providers/community-providers/cloudflare-workers-ai) (`workers-ai-provider`)
-   [OpenRouter Provider](/providers/community-providers/openrouter) (`@openrouter/ai-sdk-provider`)
-   [Crosshatch Provider](/providers/community-providers/crosshatch) (`@crosshatch/ai-provider`)
-   [Mixedbread Provider](/providers/community-providers/mixedbread) (`mixedbread-ai-provider`)
-   [Voyage AI Provider](/providers/community-providers/voyage-ai) (`voyage-ai-provider`)
-   [Mem0 Provider](/providers/community-providers/mem0)(`@mem0/vercel-ai-provider`)
-   [Letta Provider](/providers/community-providers/letta)(`@letta-ai/vercel-ai-sdk-provider`)
-   [Spark Provider](/providers/community-providers/spark) (`spark-ai-provider`)
-   [AnthropicVertex Provider](/providers/community-providers/anthropic-vertex-ai) (`anthropic-vertex-ai`)
-   [LangDB Provider](/providers/community-providers/langdb) (`@langdb/vercel-provider`)
-   [Dify Provider](/providers/community-providers/dify) (`dify-ai-provider`)
-   [Sarvam Provider](/providers/community-providers/sarvam) (`sarvam-ai-provider`)


## [Self-Hosted Models](#self-hosted-models)


You can access self-hosted models with the following providers:

-   [Ollama Provider](/providers/community-providers/ollama)
-   [LM Studio](/providers/openai-compatible-providers/lmstudio)
-   [Baseten](/providers/openai-compatible-providers/baseten)

Additionally, any self-hosted provider that supports the OpenAI specification can be used with the [OpenAI Compatible Provider](/providers/openai-compatible-providers) .


## [Model Capabilities](#model-capabilities)


The AI providers support different language models with various capabilities. Here are the capabilities of popular models:

Provider

Model

Image Input

Object Generation

Tool Usage

Tool Streaming

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-3`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-3-fast`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-3-mini`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-3-mini-fast`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-2-1212`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-2-vision-1212`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-beta`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-vision-beta`

[Vercel](/providers/ai-sdk-providers/vercel)

`v0-1.0-md`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4.1`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4.1-mini`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4.1-nano`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4o`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4o-mini`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4-turbo`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4`

[OpenAI](/providers/ai-sdk-providers/openai)

`o3-mini`

[OpenAI](/providers/ai-sdk-providers/openai)

`o3`

[OpenAI](/providers/ai-sdk-providers/openai)

`o4-mini`

[OpenAI](/providers/ai-sdk-providers/openai)

`o1`

[OpenAI](/providers/ai-sdk-providers/openai)

`o1-mini`

[OpenAI](/providers/ai-sdk-providers/openai)

`o1-preview`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-4-opus-20250514`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-4-sonnet-20250514`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-3-7-sonnet-20250219`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-3-5-sonnet-20241022`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-3-5-sonnet-20240620`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-3-5-haiku-20241022`

[Mistral](/providers/ai-sdk-providers/mistral)

`pixtral-large-latest`

[Mistral](/providers/ai-sdk-providers/mistral)

`mistral-large-latest`

[Mistral](/providers/ai-sdk-providers/mistral)

`mistral-small-latest`

[Mistral](/providers/ai-sdk-providers/mistral)

`pixtral-12b-2409`

[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai)

`gemini-2.0-flash-exp`

[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai)

`gemini-1.5-flash`

[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai)

`gemini-1.5-pro`

[Google Vertex](/providers/ai-sdk-providers/google-vertex)

`gemini-2.0-flash-exp`

[Google Vertex](/providers/ai-sdk-providers/google-vertex)

`gemini-1.5-flash`

[Google Vertex](/providers/ai-sdk-providers/google-vertex)

`gemini-1.5-pro`

[DeepSeek](/providers/ai-sdk-providers/deepseek)

`deepseek-chat`

[DeepSeek](/providers/ai-sdk-providers/deepseek)

`deepseek-reasoner`

[Cerebras](/providers/ai-sdk-providers/cerebras)

`llama3.1-8b`

[Cerebras](/providers/ai-sdk-providers/cerebras)

`llama3.1-70b`

[Cerebras](/providers/ai-sdk-providers/cerebras)

`llama3.3-70b`

[Groq](/providers/ai-sdk-providers/groq)

`meta-llama/llama-4-scout-17b-16e-instruct`

[Groq](/providers/ai-sdk-providers/groq)

`llama-3.3-70b-versatile`

[Groq](/providers/ai-sdk-providers/groq)

`llama-3.1-8b-instant`

[Groq](/providers/ai-sdk-providers/groq)

`mixtral-8x7b-32768`

[Groq](/providers/ai-sdk-providers/groq)

`gemma2-9b-it`

This table is not exhaustive. Additional models can be found in the provider documentation pages and on the provider websites.
```

### 122. `docs/foundations/streaming.md`

```markdown
# Streaming


---
url: https://ai-sdk.dev/docs/foundations/streaming
description: Why use streaming for AI applications?
---


# [Streaming](#streaming)


Streaming conversational text UIs (like ChatGPT) have gained massive popularity over the past few months. This section explores the benefits and drawbacks of streaming and blocking interfaces.

[Large language models (LLMs)](/docs/foundations/overview#large-language-models) are extremely powerful. However, when generating long outputs, they can be very slow compared to the latency you're likely used to. If you try to build a traditional blocking UI, your users might easily find themselves staring at loading spinners for 5, 10, even up to 40s waiting for the entire LLM response to be generated. This can lead to a poor user experience, especially in conversational applications like chatbots. Streaming UIs can help mitigate this issue by **displaying parts of the response as they become available**.

Blocking UI

Blocking responses wait until the full response is available before displaying it.

Streaming UI

Streaming responses can transmit parts of the response as they become available.


## [Real-world Examples](#real-world-examples)


Here are 2 examples that illustrate how streaming UIs can improve user experiences in a real-world setting – the first uses a blocking UI, while the second uses a streaming UI.


### [Blocking UI](#blocking-ui)


Come up with the first 200 characters of the first book in the Harry Potter series.

Generate

...


### [Streaming UI](#streaming-ui)


Come up with the first 200 characters of the first book in the Harry Potter series.

Generate

...

As you can see, the streaming UI is able to start displaying the response much faster than the blocking UI. This is because the blocking UI has to wait for the entire response to be generated before it can display anything, while the streaming UI can display parts of the response as they become available.

While streaming interfaces can greatly enhance user experiences, especially with larger language models, they aren't always necessary or beneficial. If you can achieve your desired functionality using a smaller, faster model without resorting to streaming, this route can often lead to simpler and more manageable development processes.

However, regardless of the speed of your model, the AI SDK is designed to make implementing streaming UIs as simple as possible. In the example below, we stream text generation from OpenAI's `gpt-4-turbo` in under 10 lines of code using the SDK's [`streamText`](/docs/reference/ai-sdk-core/stream-text) function:

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';const{ textStream }=streamText({  model:openai('gpt-4-turbo'),  prompt:'Write a poem about embedding models.',});forawait(const textPart of textStream){console.log(textPart);}
```

For an introduction to streaming UIs and the AI SDK, check out our [Getting Started guides](/docs/getting-started).
```

### 123. `docs/foundations/tools.md`

```markdown
# Tools


---
url: https://ai-sdk.dev/docs/foundations/tools
description: Learn about tools with the AI SDK.
---


# [Tools](#tools)


While [large language models (LLMs)](/docs/foundations/overview#large-language-models) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather).

Tools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.

For example, when you ask an LLM for the "weather in London", and there is a weather tool available, it could call a tool with London as the argument. The tool would then fetch the weather data and return it to the LLM. The LLM can then use this information in its response.


## [What is a tool?](#what-is-a-tool)


A tool is an object that can be called by the model to perform a specific task. You can use tools with [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`streamText`](/docs/reference/ai-sdk-core/stream-text) by passing one or more tools to the `tools` parameter.

A tool consists of three properties:

-   **`description`**: An optional description of the tool that can influence when the tool is picked.
-   **`parameters`**: A [Zod schema](/docs/foundations/tools#schema-specification-and-validation-with-zod) or a [JSON schema](/docs/reference/ai-sdk-core/json-schema) that defines the parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.
-   **`execute`**: An optional async function that is called with the arguments from the tool call.

`streamUI` uses UI generator tools with a `generate` function that can return React components.

If the LLM decides to use a tool, it will generate a tool call. Tools with an `execute` function are run automatically when these calls are generated. The results of the tool calls are returned using tool result objects.

You can automatically pass tool results back to the LLM using [multi-step calls](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls) with `streamText` and `generateText`.


## [Schemas](#schemas)


Schemas are used to define the parameters for tools and to validate the [tool calls](/docs/ai-sdk-core/tools-and-tool-calling).

The AI SDK supports both raw JSON schemas (using the [`jsonSchema` function](/docs/reference/ai-sdk-core/json-schema)) and [Zod](https://zod.dev/) schemas (either directly or using the [`zodSchema` function](/docs/reference/ai-sdk-core/zod-schema)).

[Zod](https://zod.dev/) is a popular TypeScript schema validation library. You can install it with:

pnpm

npm

yarn

pnpm add zod

You can then specify a Zod schema, for example:

```
importzfrom'zod';const recipeSchema = z.object({  recipe: z.object({    name: z.string(),    ingredients: z.array(      z.object({        name: z.string(),        amount: z.string(),}),),    steps: z.array(z.string()),}),});
```

You can also use schemas for structured output generation with [`generateObject`](/docs/reference/ai-sdk-core/generate-object) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object).


## [Toolkits](#toolkits)


When you work with tools, you typically need a mix of application specific tools and general purpose tools. There are several providers that offer pre-built tools as **toolkits** that you can use out of the box:

-   **[agentic](https://github.com/transitive-bullshit/agentic)** - A collection of 20+ tools. Most tools connect to access external APIs such as [Exa](https://exa.ai/) or [E2B](https://e2b.dev/).
-   **[browserbase](https://docs.browserbase.com/integrations/vercel-ai/introduction)** - Browser tool that runs a headless browser
-   **[browserless](https://docs.browserless.io/ai-integrations/vercel-ai-sdk)** - Browser automation service with AI integration - self hosted or cloud based
-   **[Stripe agent tools](https://docs.stripe.com/agents)** - Tools for interacting with Stripe.
-   **[StackOne ToolSet](https://docs.stackone.com/agents)** - Agentic integrations for hundreds of [enterprise SaaS](https://www.stackone.com/integrations)
-   **[Toolhouse](https://docs.toolhouse.ai/toolhouse/using-vercel-ai)** - AI function-calling in 3 lines of code for over 25 different actions.
-   **[Agent Tools](https://ai-sdk-agents.vercel.app/?item=introduction)** - A collection of tools for agents.
-   **[AI Tool Maker](https://github.com/nihaocami/ai-tool-maker)** - A CLI utility to generate AI SDK tools from OpenAPI specs.
-   **[Composio](https://docs.composio.dev/javascript/vercel)** - Composio provides 250+ tools like GitHub, Gmail, Salesforce and [more](https://composio.dev/tools).
-   **[Interlify](https://www.interlify.com/docs/integrate-with-vercel-ai)** - Convert APIs into tools so that AI can connect to your backend in minutes.
-   **[Freestyle](https://docs.freestyle.sh/integrations/vercel)** — Tool for your AI to execute JavaScript or TypeScript with arbitrary node modules.

Do you have open source tools or tool libraries that are compatible with the AI SDK? Please [file a pull request](https://github.com/vercel/ai/pulls) to add them to this list.


## [Learn more](#learn-more)


The AI SDK Core [Tool Calling](/docs/ai-sdk-core/tools-and-tool-calling) and [Agents](/docs/foundations/agents) documentation has more information about tools and tool calling.
```

### 124. `docs/foundations.md`

```markdown
# Foundations


---
url: https://ai-sdk.dev/docs/foundations
description: A section that covers foundational knowledge around LLMs and concepts crucial to the AI SDK
---


# [Foundations](#foundations)


[

Overview

Learn about foundational concepts around AI and LLMs.

](/docs/foundations/overview)[

Providers and Models

Learn about the providers and models that you can use with the AI SDK.

](/docs/foundations/providers-and-models)[

Prompts

Learn about how Prompts are used and defined in the AI SDK.

](/docs/foundations/prompts)[

Tools

Learn about tools in the AI SDK.

](/docs/foundations/tools)[

Streaming

Learn why streaming is used for AI applications.

](/docs/foundations/streaming)[

Agents

Learn how to build agents with the AI SDK.

](/docs/foundations/agents)
```

### 125. `docs/getting-started/expo.md`

```markdown
# Expo Quickstart


---
url: https://ai-sdk.dev/docs/getting-started/expo
description: Welcome to the AI SDK quickstart guide for Expo!
---


# [Expo Quickstart](#expo-quickstart)


In this quick start tutorial, you'll build a simple AI-chatbot with a streaming user interface with [Expo](https://expo.dev/). Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.

Check out [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming) if you haven't heard of them.


## [Prerequisites](#prerequisites)


To follow this quickstart, you'll need:

-   Node.js 18+ and pnpm installed on your local development machine.
-   An OpenAI API key.

If you haven't obtained your OpenAI API key, you can do so by [signing up](https://platform.openai.com/signup/) on the OpenAI website.


## [Create Your Application](#create-your-application)


Start by creating a new Expo application. This command will create a new directory named `my-ai-app` and set up a basic Expo application inside it.

pnpm create expo-app@latest my-ai-app

Navigate to the newly created directory:

cd my-ai-app

This guide requires Expo 52 or higher.


### [Install dependencies](#install-dependencies)


Install `ai`, `@ai-sdk/react` and `@ai-sdk/openai`, the AI package, the AI React package and AI SDK's [OpenAI provider](/providers/ai-sdk-providers/openai) respectively.

The AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about [available providers](/providers) and [building custom providers](/providers/community-providers/custom-providers) in the [providers](/providers) section.

pnpm

npm

yarn

pnpm add ai @ai-sdk/openai @ai-sdk/react zod

Make sure you are using `ai` version 3.1 or higher.


### [Configure OpenAI API key](#configure-openai-api-key)


Create a `.env.local` file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.

touch .env.local

Edit the `.env.local` file:

.env.local

```
OPENAI_API_KEY=xxxxxxxxx
```

Replace `xxxxxxxxx` with your actual OpenAI API key.

The AI SDK's OpenAI Provider will default to using the `OPENAI_API_KEY` environment variable.


## [Create an API Route](#create-an-api-route)


Create a route handler, `app/api/chat+api.ts` and add the following code:

app/api/chat+api.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse({    headers:{'Content-Type':'application/octet-stream','Content-Encoding':'none',},});}
```

Let's take a look at what is happening in this code:

1.  Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation.
2.  Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (imported from `@ai-sdk/openai`) and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model's behaviour.
3.  The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toDataStreamResponse`](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.
4.  Finally, return the result to the client to stream the response.

This API route creates a POST request endpoint at `/api/chat`.


## [Wire up the UI](#wire-up-the-ui)


Now that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).

Update your root page (`app/(tabs)/index.tsx`) with the following code to show a list of chat messages and provide a user message input:

app/(tabs)/index.tsx

```
import{ generateAPIUrl }from'@/utils';import{ useChat }from'@ai-sdk/react';import{ fetch as expoFetch }from'expo/fetch';import{View,TextInput,ScrollView,Text,SafeAreaView}from'react-native';exportdefaultfunctionApp(){const{ messages, error, handleInputChange, input, handleSubmit }=useChat({    fetch: expoFetch asunknownastypeof globalThis.fetch,    api:generateAPIUrl('/api/chat'),onError:error=>console.error(error,'ERROR'),});if(error)return<Text>{error.message}</Text>;return(<SafeAreaViewstyle={{ height:'100%'}}><Viewstyle={{          height:'95%',          display:'flex',          flexDirection:'column',          paddingHorizontal:8,}}><ScrollViewstyle={{ flex:1}}>{messages.map(m=>(<Viewkey={m.id}style={{ marginVertical:8}}><View><Textstyle={{ fontWeight:700}}>{m.role}</Text><Text>{m.content}</Text></View></View>))}</ScrollView><Viewstyle={{ marginTop:8}}><TextInputstyle={{ backgroundColor:'white', padding:8}}placeholder="Say something..."value={input}onChange={e=>handleInputChange({...e,                target:{...e.target,                  value: e.nativeEvent.text,},}asunknownasReact.ChangeEvent<HTMLInputElement>)}onSubmitEditing={e=>{handleSubmit(e);              e.preventDefault();}}autoFocus={true}/></View></View></SafeAreaView>);}
```

This page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:

-   `messages` - the current chat messages (an array of objects with `id`, `role`, and `content` properties).
-   `input` - the current value of the user's input field.
-   `handleInputChange` and `handleSubmit` - functions to handle user interactions (typing into the input field and submitting the form, respectively).

You use the expo/fetch function instead of the native node fetch to enable streaming of chat responses. This requires Expo 52 or higher.


### [Create the API URL Generator](#create-the-api-url-generator)


Because you're using expo/fetch for streaming responses instead of the native fetch function, you'll need an API URL generator to ensure you are using the correct base url and format depending on the client environment (e.g. web or mobile). Create a new file called `utils.ts` in the root of your project and add the following code:

utils.ts

```
importConstantsfrom'expo-constants';exportconstgenerateAPIUrl=(relativePath:string)=>{const origin =Constants.experienceUrl.replace('exp://','http://');const path = relativePath.startsWith('/')? relativePath :`/${relativePath}`;if(process.env.NODE_ENV==='development'){return origin.concat(path);}if(!process.env.EXPO_PUBLIC_API_BASE_URL){thrownewError('EXPO_PUBLIC_API_BASE_URL environment variable is not defined',);}return process.env.EXPO_PUBLIC_API_BASE_URL.concat(path);};
```

This utility function handles URL generation for both development and production environments, ensuring your API calls work correctly across different devices and configurations.

Before deploying to production, you must set the `EXPO_PUBLIC_API_BASE_URL` environment variable in your production environment. This variable should point to the base URL of your API server.


## [Running Your Application](#running-your-application)


With that, you have built everything you need for your chatbot! To start your application, use the command:

pnpm expo

Head to your browser and open [http://localhost:8081](http://localhost:8081). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Expo.

If you experience "Property `structuredClone` doesn't exist" errors on mobile, add the [polyfills described below](#polyfills).


## [Enhance Your Chatbot with Tools](#enhance-your-chatbot-with-tools)


While large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.

Tools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.

For example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.

Let's enhance your chatbot by adding a simple weather tool.


### [Update Your API route](#update-your-api-route)


Modify your `app/api/chat+api.ts` file to include the new weather tool:

app/api/chat+api.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      weather:tool({        description:'Get the weather in a location (fahrenheit)',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,            temperature,};},}),},});return result.toDataStreamResponse({    headers:{'Content-Type':'application/octet-stream','Content-Encoding':'none',},});}
```

In this updated code:

1.  You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.

2.  You define a `tools` object with a `weather` tool. This tool:

    -   Has a description that helps the model understand when to use it.
    -   Defines parameters using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this parameter from the context of the conversation. If it can't, it will ask the user for the missing information.
    -   Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.

    Now your chatbot can "fetch" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and you can access the results via `toolInvocations` that is available on the message object.


You may need to restart your development server for the changes to take effect.

Try asking something like "What's the weather in New York?" and see how the model uses the new tool.

Notice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result in the `toolInvocations` key of the message object.


### [Update the UI](#update-the-ui)


To display the tool invocations in your UI, update your `app/(tabs)/index.tsx` file:

app/(tabs)/index.tsx

```
import{ generateAPIUrl }from'@/utils';import{ useChat }from'@ai-sdk/react';import{ fetch as expoFetch }from'expo/fetch';import{View,TextInput,ScrollView,Text,SafeAreaView}from'react-native';exportdefaultfunctionApp(){const{ messages, error, handleInputChange, input, handleSubmit }=useChat({    fetch: expoFetch asunknownastypeof globalThis.fetch,    api:generateAPIUrl('/api/chat'),onError:error=>console.error(error,'ERROR'),});if(error)return<Text>{error.message}</Text>;return(<SafeAreaViewstyle={{ height:'100vh'}}><Viewstyle={{          height:'95%',          display:'flex',          flexDirection:'column',          paddingHorizontal:8,}}><ScrollViewstyle={{ flex:1}}>{messages.map(m=>(<Viewkey={m.id}style={{ marginVertical:8}}><View><Textstyle={{ fontWeight:700}}>{m.role}</Text>{m.toolInvocations ?(<Text>{JSON.stringify(m.toolInvocations,null,2)}</Text>):(<Text>{m.content}</Text>)}</View></View>))}</ScrollView><Viewstyle={{ marginTop:8}}><TextInputstyle={{ backgroundColor:'white', padding:8}}placeholder="Say something..."value={input}onChange={e=>handleInputChange({...e,                target:{...e.target,                  value: e.nativeEvent.text,},}asunknownasReact.ChangeEvent<HTMLInputElement>)}onSubmitEditing={e=>{handleSubmit(e);              e.preventDefault();}}autoFocus={true}/></View></View></SafeAreaView>);}
```

You may need to restart your development server for the changes to take effect.

With this change, you check each message for any tool calls (`toolInvocations`). These tool calls will be displayed as stringified JSON. Otherwise, you show the message content as before.

Now, when you ask about the weather, you'll see the tool invocation and its result displayed in your chat interface.


## [Enabling Multi-Step Tool Calls](#enabling-multi-step-tool-calls)


You may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.

To solve this, you can enable multi-step tool calls using the `maxSteps` option in your `useChat` hook. This feature will automatically send tool results back to the model to trigger an additional generation. In this case, you want the model to answer your question using the results from the weather tool.


### [Update Your Client-Side Code](#update-your-client-side-code)


Modify your `app/(tabs)/index.tsx` file to include the `maxSteps` option:

app/(tabs)/index.tsx

```
import{ useChat }from'@ai-sdk/react';// ... rest of your importsexportdefaultfunctionApp(){const{ messages, error, handleInputChange, input, handleSubmit }=useChat({    fetch: expoFetch asunknownastypeof globalThis.fetch,    api:generateAPIUrl('/api/chat'),onError:error=>console.error(error,'ERROR'),    maxSteps:5,});// ... rest of your component code}
```

You may need to restart your development server for the changes to take effect.

Head back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.

By setting `maxSteps` to 5, you're allowing the model to use up to 5 "steps" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.


### [Update Your API Route](#update-your-api-route-1)


Update your `app/api/chat+api.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:

app/api/chat+api.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      weather:tool({        description:'Get the weather in a location (fahrenheit)',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,            temperature,};},}),      convertFahrenheitToCelsius:tool({        description:'Convert a temperature in fahrenheit to celsius',        parameters: z.object({          temperature: z.number().describe('The temperature in fahrenheit to convert'),}),execute:async({ temperature })=>{const celsius =Math.round((temperature -32)*(5/9));return{            celsius,};},}),},});return result.toDataStreamResponse({    headers:{'Content-Type':'application/octet-stream','Content-Encoding':'none',},});}
```

You may need to restart your development server for the changes to take effect.

Now, when you ask "What's the weather in New York in celsius?", you should see a more complete interaction:

1.  The model will call the weather tool for New York.
2.  You'll see the tool result displayed.
3.  It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.
4.  The model will then use that information to provide a natural language response about the weather in New York.

This multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.

This simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.


## [Polyfills](#polyfills)


Several functions that are internally used by the AI SDK might not available in the Expo runtime depending on your configuration and the target platform.

First, install the following packages:

pnpm

npm

yarn

pnpm add @ungap/structured-clone @stardazed/streams-text-encoding

Then create a new file in the root of your project with the following polyfills:

polyfills.js

```
import{Platform}from'react-native';importstructuredClonefrom'@ungap/structured-clone';if(Platform.OS!=='web'){constsetupPolyfills=async()=>{const{ polyfillGlobal }=awaitimport('react-native/Libraries/Utilities/PolyfillFunctions');const{TextEncoderStream,TextDecoderStream}=awaitimport('@stardazed/streams-text-encoding');if(!('structuredClone'in global)){polyfillGlobal('structuredClone',()=> structuredClone);}polyfillGlobal('TextEncoderStream',()=>TextEncoderStream);polyfillGlobal('TextDecoderStream',()=>TextDecoderStream);};setupPolyfills();}export{};
```

Finally, import the polyfills in your root `_layout.tsx`:

\_layout.tsx

```
import'@/polyfills';
```


## [Where to Next?](#where-to-next)


You've built an AI chatbot using the AI SDK! From here, you have several paths to explore:

-   To learn more about the AI SDK, read through the [documentation](/docs).
-   If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.
-   To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).
```

### 126. `docs/getting-started/navigating-the-library.md`

```markdown
# Navigating the Library


---
url: https://ai-sdk.dev/docs/getting-started/navigating-the-library
description: Learn how to navigate the AI SDK.
---


# [Navigating the Library](#navigating-the-library)


the AI SDK is a powerful toolkit for building AI applications. This page will help you pick the right tools for your requirements.

Let’s start with a quick overview of the AI SDK, which is comprised of three parts:

-   **[AI SDK Core](/docs/ai-sdk-core/overview):** A unified, provider agnostic API for generating text, structured objects, and tool calls with LLMs.
-   **[AI SDK UI](/docs/ai-sdk-ui/overview):** A set of framework-agnostic hooks for building chat and generative user interfaces.
-   [AI SDK RSC](/docs/ai-sdk-rsc/overview): Stream generative user interfaces with React Server Components (RSC). Development is currently experimental and we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview).


## [Choosing the Right Tool for Your Environment](#choosing-the-right-tool-for-your-environment)


When deciding which part of the AI SDK to use, your first consideration should be the environment and existing stack you are working with. Different components of the SDK are tailored to specific frameworks and environments.

Library

Purpose

Environment Compatibility

[AI SDK Core](/docs/ai-sdk-core/overview)

Call any LLM with unified API (e.g. [generateText](/docs/reference/ai-sdk-core/generate-text) and [generateObject](/docs/reference/ai-sdk-core/generate-object))

Any JS environment (e.g. Node.js, Deno, Browser)

[AI SDK UI](/docs/ai-sdk-ui/overview)

Build streaming chat and generative UIs (e.g. [useChat](/docs/reference/ai-sdk-ui/use-chat))

React & Next.js, Vue & Nuxt, Svelte & SvelteKit, Solid.js & SolidStart

[AI SDK RSC](/docs/ai-sdk-rsc/overview)

Stream generative UIs from Server to Client (e.g. [streamUI](/docs/reference/ai-sdk-rsc/stream-ui)). Development is currently experimental and we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview).

Any framework that supports React Server Components (e.g. Next.js)


## [Environment Compatibility](#environment-compatibility)


These tools have been designed to work seamlessly with each other and it's likely that you will be using them together. Let's look at how you could decide which libraries to use based on your application environment, existing stack, and requirements.

The following table outlines AI SDK compatibility based on environment:

Environment

[AI SDK Core](/docs/ai-sdk-core/overview)

[AI SDK UI](/docs/ai-sdk-ui/overview)

[AI SDK RSC](/docs/ai-sdk-rsc/overview)

None / Node.js / Deno

Vue / Nuxt

Svelte / SvelteKit

Solid.js / SolidStart

Next.js Pages Router

Next.js App Router


## [When to use AI SDK UI](#when-to-use-ai-sdk-ui)


AI SDK UI provides a set of framework-agnostic hooks for quickly building **production-ready AI-native applications**. It offers:

-   Full support for streaming chat and client-side generative UI
-   Utilities for handling common AI interaction patterns (i.e. chat, completion, assistant)
-   Production-tested reliability and performance
-   Compatibility across popular frameworks


## [AI SDK UI Framework Compatibility](#ai-sdk-ui-framework-compatibility)


AI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), [Vue.js](https://vuejs.org/), and [SolidJS](https://www.solidjs.com/). Here is a comparison of the supported functions across these frameworks:

Function

React

Svelte

Vue.js

SolidJS

[useChat](/docs/reference/ai-sdk-ui/use-chat)

[useChat](/docs/reference/ai-sdk-ui/use-chat) tool calling

[useCompletion](/docs/reference/ai-sdk-ui/use-completion)

[useObject](/docs/reference/ai-sdk-ui/use-object)

[useAssistant](/docs/reference/ai-sdk-ui/use-assistant)

[Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are welcome to implement missing features for non-React frameworks.


## [When to use AI SDK RSC](#when-to-use-ai-sdk-rsc)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

[React Server Components](https://nextjs.org/docs/app/building-your-application/rendering/server-components) (RSCs) provide a new approach to building React applications that allow components to render on the server, fetch data directly, and stream the results to the client, reducing bundle size and improving performance. They also introduce a new way to call server-side functions from anywhere in your application called [Server Actions](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations).

AI SDK RSC provides a number of utilities that allow you to stream values and UI directly from the server to the client. However, **it's important to be aware of current limitations**:

-   **Cancellation**: currently, it is not possible to abort a stream using Server Actions. This will be improved in future releases of React and Next.js.
-   **Increased Data Transfer**: using [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) can lead to quadratic data transfer (quadratic to the length of generated text). You can avoid this using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) instead, and rendering the component client-side.
-   **Re-mounting Issue During Streaming**: when using `createStreamableUI`, components re-mount on `.done()`, causing [flickering](https://github.com/vercel/ai/issues/2232).

Given these limitations, **we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production applications**.
```

### 127. `docs/getting-started/nextjs-app-router.md`

```markdown
# Next.js App Router Quickstart


---
url: https://ai-sdk.dev/docs/getting-started/nextjs-app-router
description: Welcome to the AI SDK quickstart guide for Next.js App Router!
---


# [Next.js App Router Quickstart](#nextjs-app-router-quickstart)


In this quick start tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.

Check out [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming) if you haven't heard of them.


## [Prerequisites](#prerequisites)


To follow this quickstart, you'll need:

-   Node.js 18+ and pnpm installed on your local development machine.
-   An OpenAI API key.

If you haven't obtained your OpenAI API key, you can do so by [signing up](https://platform.openai.com/signup/) on the OpenAI website.


## [Create Your Application](#create-your-application)


Start by creating a new Next.js application. This command will create a new directory named `my-ai-app` and set up a basic Next.js application inside it.

Be sure to select yes when prompted to use the App Router and Tailwind CSS. If you are looking for the Next.js Pages Router quickstart guide, you can find it [here](/docs/getting-started/nextjs-pages-router).

pnpm create next-app@latest my-ai-app

Navigate to the newly created directory:

cd my-ai-app


### [Install dependencies](#install-dependencies)


Install `ai`, `@ai-sdk/react`, and `@ai-sdk/openai`, the AI package, AI SDK's React hooks, and AI SDK's [OpenAI provider](/providers/ai-sdk-providers/openai) respectively.

The AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about [available providers](/providers) and [building custom providers](/providers/community-providers/custom-providers) in the [providers](/providers) section.

pnpm

npm

yarn

pnpm add ai @ai-sdk/react @ai-sdk/openai zod

Make sure you are using `ai` version 3.1 or higher.


### [Configure OpenAI API key](#configure-openai-api-key)


Create a `.env.local` file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.

touch .env.local

Edit the `.env.local` file:

.env.local

```
OPENAI_API_KEY=xxxxxxxxx
```

Replace `xxxxxxxxx` with your actual OpenAI API key.

The AI SDK's OpenAI Provider will default to using the `OPENAI_API_KEY` environment variable.


## [Create a Route Handler](#create-a-route-handler)


Create a route handler, `app/api/chat/route.ts` and add the following code:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse();}
```

Let's take a look at what is happening in this code:

1.  Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation.
2.  Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (imported from `@ai-sdk/openai`) and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model's behaviour.
3.  The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toDataStreamResponse`](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.
4.  Finally, return the result to the client to stream the response.

This Route Handler creates a POST request endpoint at `/api/chat`.


## [Wire up the UI](#wire-up-the-ui)


Now that you have a Route Handler that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).

Update your root page (`app/page.tsx`) with the following code to show a list of chat messages and provide a user message input:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages.map(message=>(<divkey={message.id}className="whitespace-pre-wrap">{message.role ==='user'?'User: ':'AI: '}{message.parts.map((part, i)=>{switch(part.type){case'text':return<divkey={`${message.id}-${i}`}>{part.text}</div>;}})}</div>))}<formonSubmit={handleSubmit}><inputclassName="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

Make sure you add the `"use client"` directive to the top of your file. This allows you to add interactivity with Javascript.

This page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:

-   `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).
-   `input` - the current value of the user's input field.
-   `handleInputChange` and `handleSubmit` - functions to handle user interactions (typing into the input field and submitting the form, respectively).

The LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.


## [Running Your Application](#running-your-application)


With that, you have built everything you need for your chatbot! To start your application, use the command:

pnpm run dev

Head to your browser and open [http://localhost:3000](http://localhost:3000). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.


## [Enhance Your Chatbot with Tools](#enhance-your-chatbot-with-tools)


While large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.

Tools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.

For example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.

Let's enhance your chatbot by adding a simple weather tool.


### [Update Your Route Handler](#update-your-route-handler)


Modify your `app/api/chat/route.ts` file to include the new weather tool:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';exportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      weather:tool({        description:'Get the weather in a location (fahrenheit)',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,            temperature,};},}),},});return result.toDataStreamResponse();}
```

In this updated code:

1.  You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.

2.  You define a `tools` object with a `weather` tool. This tool:

    -   Has a description that helps the model understand when to use it.
    -   Defines parameters using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this parameter from the context of the conversation. If it can't, it will ask the user for the missing information.
    -   Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.

Now your chatbot can "fetch" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and you can access the results via `tool-invocations` part that is available on the `message.parts` array.

Try asking something like "What's the weather in New York?" and see how the model uses the new tool.

Notice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result via the `tool-invocation` part of the `message.parts` array.


### [Update the UI](#update-the-ui)


To display the tool invocations in your UI, update your `app/page.tsx` file:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages.map(message=>(<divkey={message.id}className="whitespace-pre-wrap">{message.role ==='user'?'User: ':'AI: '}{message.parts.map((part, i)=>{switch(part.type){case'text':return<divkey={`${message.id}-${i}`}>{part.text}</div>;case'tool-invocation':return(<prekey={`${message.id}-${i}`}>{JSON.stringify(part.toolInvocation,null,2)}</pre>);}})}</div>))}<formonSubmit={handleSubmit}><inputclassName="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

With this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For tool invocations, you display a JSON representation of the tool call and its result.

Now, when you ask about the weather, you'll see the tool invocation and its result displayed in your chat interface.


## [Enabling Multi-Step Tool Calls](#enabling-multi-step-tool-calls)


You may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.

To solve this, you can enable multi-step tool calls using the `maxSteps` option in your `useChat` hook. This feature will automatically send tool results back to the model to trigger an additional generation. In this case, you want the model to answer your question using the results from the weather tool.


### [Update Your Client-Side Code](#update-your-client-side-code)


Modify your `app/page.tsx` file to include the `maxSteps` option:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat({    maxSteps:5,});// ... rest of your component code}
```

Head back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.

By setting `maxSteps` to 5, you're allowing the model to use up to 5 "steps" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.


### [Update Your Route Handler](#update-your-route-handler-1)


Update your `app/api/chat/route.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';exportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      weather:tool({        description:'Get the weather in a location (fahrenheit)',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,            temperature,};},}),      convertFahrenheitToCelsius:tool({        description:'Convert a temperature in fahrenheit to celsius',        parameters: z.object({          temperature: z.number().describe('The temperature in fahrenheit to convert'),}),execute:async({ temperature })=>{const celsius =Math.round((temperature -32)*(5/9));return{            celsius,};},}),},});return result.toDataStreamResponse();}
```

Now, when you ask "What's the weather in New York in celsius?", you should see a more complete interaction:

1.  The model will call the weather tool for New York.
2.  You'll see the tool result displayed.
3.  It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.
4.  The model will then use that information to provide a natural language response about the weather in New York.

This multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.

This simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.


## [Where to Next?](#where-to-next)


You've built an AI chatbot using the AI SDK! From here, you have several paths to explore:

-   To learn more about the AI SDK, read through the [documentation](/docs).
-   If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.
-   To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).
```

### 128. `docs/getting-started/nextjs-pages-router.md`

```markdown
# Next.js Pages Router Quickstart


---
url: https://ai-sdk.dev/docs/getting-started/nextjs-pages-router
description: Welcome to the AI SDK quickstart guide for Next.js Pages Router!
---


# [Next.js Pages Router Quickstart](#nextjs-pages-router-quickstart)


The AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.

In this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.

If you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.


## [Prerequisites](#prerequisites)


To follow this quickstart, you'll need:

-   Node.js 18+ and pnpm installed on your local development machine.
-   An OpenAI API key.

If you haven't obtained your OpenAI API key, you can do so by [signing up](https://platform.openai.com/signup/) on the OpenAI website.


## [Setup Your Application](#setup-your-application)


Start by creating a new Next.js application. This command will create a new directory named `my-ai-app` and set up a basic Next.js application inside it.

Be sure to select no when prompted to use the App Router. If you are looking for the Next.js App Router quickstart guide, you can find it [here](/docs/getting-started/nextjs-app-router).

pnpm create next-app@latest my-ai-app

Navigate to the newly created directory:

cd my-ai-app


### [Install dependencies](#install-dependencies)


Install `ai`, `@ai-sdk/react`, and `@ai-sdk/openai`, the AI package, AI SDK's React hooks, and AI SDK's [OpenAI provider](/providers/ai-sdk-providers/openai) respectively.

The AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about [available providers](/providers) and [building custom providers](/providers/community-providers/custom-providers) in the [providers](/providers) section.

pnpm

npm

yarn

pnpm add ai @ai-sdk/openai zod

Make sure you are using `ai` version 3.1 or higher.


### [Configure OpenAI API Key](#configure-openai-api-key)


Create a `.env.local` file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.

touch .env.local

Edit the `.env.local` file:

.env.local

```
OPENAI_API_KEY=xxxxxxxxx
```

Replace `xxxxxxxxx` with your actual OpenAI API key.

The AI SDK's OpenAI Provider will default to using the `OPENAI_API_KEY` environment variable.


## [Create a Route Handler](#create-a-route-handler)


As long as you are on Next.js 13+, you can use Route Handlers (using the App Router) alongside the Pages Router. This is recommended to enable you to use the Web APIs interface/signature and to better support streaming.

Create a Route Handler (`app/api/chat/route.ts`) and add the following code:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse();}
```

Let's take a look at what is happening in this code:

1.  Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation.
2.  Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (imported from `@ai-sdk/openai`) and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model's behaviour.
3.  The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toDataStreamResponse`](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.
4.  Finally, return the result to the client to stream the response.

This Route Handler creates a POST request endpoint at `/api/chat`.


## [Wire up the UI](#wire-up-the-ui)


Now that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](/docs/ai-sdk-ui) package abstract the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).

Update your root page (`pages/index.tsx`) with the following code to show a list of chat messages and provide a user message input:

pages/index.tsx

```
import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages.map(message=>(<divkey={message.id}className="whitespace-pre-wrap">{message.role ==='user'?'User: ':'AI: '}{message.parts.map((part, i)=>{switch(part.type){case'text':return<divkey={`${message.id}-${i}`}>{part.text}</div>;}})}</div>))}<formonSubmit={handleSubmit}><inputclassName="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

This page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:

-   `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).
-   `input` - the current value of the user's input field.
-   `handleInputChange` and `handleSubmit` - functions to handle user interactions (typing into the input field and submitting the form, respectively).

The LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.


## [Running Your Application](#running-your-application)


With that, you have built everything you need for your chatbot! To start your application, use the command:

pnpm run dev

Head to your browser and open [http://localhost:3000](http://localhost:3000). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.


## [Enhance Your Chatbot with Tools](#enhance-your-chatbot-with-tools)


While large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.

Tools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.

For example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.

Let's enhance your chatbot by adding a simple weather tool.


### [Update Your Route Handler](#update-your-route-handler)


Modify your `app/api/chat/route.ts` file to include the new weather tool:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';exportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      weather:tool({        description:'Get the weather in a location (fahrenheit)',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,            temperature,};},}),},});return result.toDataStreamResponse();}
```

In this updated code:

1.  You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.

2.  You define a `tools` object with a `weather` tool. This tool:

    -   Has a description that helps the model understand when to use it.
    -   Defines parameters using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this parameter from the context of the conversation. If it can't, it will ask the user for the missing information.
    -   Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.

    Now your chatbot can "fetch" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and you can access the results via `toolInvocations` that is available on the message object.


Try asking something like "What's the weather in New York?" and see how the model uses the new tool.

Notice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result in the `toolInvocations` key of the message object.


### [Update the UI](#update-the-ui)


To display the tool invocations in your UI, update your `pages/index.tsx` file:

pages/index.tsx

```
import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages.map(message=>(<divkey={message.id}className="whitespace-pre-wrap">{message.role ==='user'?'User: ':'AI: '}{message.parts.map((part, i)=>{switch(part.type){case'text':return<divkey={`${message.id}-${i}`}>{part.text}</div>;case'tool-invocation':return(<prekey={`${message.id}-${i}`}>{JSON.stringify(part.toolInvocation,null,2)}</pre>);}})}</div>))}<formonSubmit={handleSubmit}><inputclassName="fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

With this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For tool invocations, you display a JSON representation of the tool call and its result.

Now, when you ask about the weather, you'll see the tool invocation and its result displayed in your chat interface.


## [Enabling Multi-Step Tool Calls](#enabling-multi-step-tool-calls)


You may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.

To solve this, you can enable multi-step tool calls using the `maxSteps` option in your `useChat` hook. This feature will automatically send tool results back to the model to trigger an additional generation. In this case, you want the model to answer your question using the results from the weather tool.


### [Update Your Client-Side Code](#update-your-client-side-code)


Modify your `pages/index.tsx` file to include the `maxSteps` option:

pages/index.tsx

```
import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat({    maxSteps:5,});// ... rest of your component code}
```

Head back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.

By setting `maxSteps` to 5, you're allowing the model to use up to 5 "steps" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.


### [Update Your Route Handler](#update-your-route-handler-1)


Update your `app/api/chat/route.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';exportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      weather:tool({        description:'Get the weather in a location (fahrenheit)',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,            temperature,};},}),      convertFahrenheitToCelsius:tool({        description:'Convert a temperature in fahrenheit to celsius',        parameters: z.object({          temperature: z.number().describe('The temperature in fahrenheit to convert'),}),execute:async({ temperature })=>{const celsius =Math.round((temperature -32)*(5/9));return{            celsius,};},}),},});return result.toDataStreamResponse();}
```

Now, when you ask "What's the weather in New York in celsius?", you should see a more complete interaction:

1.  The model will call the weather tool for New York.
2.  You'll see the tool result displayed.
3.  It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.
4.  The model will then use that information to provide a natural language response about the weather in New York.

This multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.

This simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.


## [Where to Next?](#where-to-next)


You've built an AI chatbot using the AI SDK! From here, you have several paths to explore:

-   To learn more about the AI SDK, read through the [documentation](/docs).
-   If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.
-   To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).
```

### 129. `docs/getting-started/nodejs.md`

```markdown
# Node.js Quickstart


---
url: https://ai-sdk.dev/docs/getting-started/nodejs
description: Welcome to the AI SDK quickstart guide for Node.js!
---


# [Node.js Quickstart](#nodejs-quickstart)


In this quickstart tutorial, you'll build a simple AI chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.

If you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.


## [Prerequisites](#prerequisites)


To follow this quickstart, you'll need:

-   Node.js 18+ and pnpm installed on your local development machine.
-   An OpenAI API key.

If you haven't obtained your OpenAI API key, you can do so by [signing up](https://platform.openai.com/signup/) on the OpenAI website.


## [Setup Your Application](#setup-your-application)


Start by creating a new directory using the `mkdir` command. Change into your new directory and then run the `pnpm init` command. This will create a `package.json` in your new directory.

```
mkdir my-ai-appcd my-ai-apppnpm init
```


### [Install Dependencies](#install-dependencies)


Install `ai` and `@ai-sdk/openai`, the AI SDK's OpenAI provider, along with other necessary dependencies.

The AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about [available providers](/providers) and [building custom providers](/providers/community-providers/custom-providers) in the [providers](/providers) section.

```
pnpmadd ai @ai-sdk/openai zod dotenvpnpmadd -D @types/node tsx typescript
```

Make sure you are using `ai` version 3.1 or higher.

The `ai` and `@ai-sdk/openai` packages contain the AI SDK and the [AI SDK OpenAI provider](/providers/ai-sdk-providers/openai), respectively. You will use `zod` to define type-safe schemas that you will pass to the large language model (LLM). You will use `dotenv` to access environment variables (your OpenAI key) within your application. There are also three development dependencies, installed with the `-D` flag, that are necessary to run your Typescript code.


### [Configure OpenAI API key](#configure-openai-api-key)


Create a `.env` file in your project's root directory and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.

touch .env

Edit the `.env` file:

.env

```
OPENAI_API_KEY=xxxxxxxxx
```

Replace `xxxxxxxxx` with your actual OpenAI API key.

The AI SDK's OpenAI Provider will default to using the `OPENAI_API_KEY` environment variable.


## [Create Your Application](#create-your-application)


Create an `index.ts` file in the root of your project and add the following code:

index.ts

```
import{ openai }from'@ai-sdk/openai';import{CoreMessage, streamText }from'ai';importdotenvfrom'dotenv';import*as readlinefrom'node:readline/promises';dotenv.config();const terminal = readline.createInterface({  input: process.stdin,  output: process.stdout,});const messages:CoreMessage[]=[];asyncfunctionmain(){while(true){const userInput =await terminal.question('You: ');    messages.push({ role:'user', content: userInput });const result =streamText({      model:openai('gpt-4o'),      messages,});let fullResponse ='';    process.stdout.write('\nAssistant: ');forawait(const delta of result.textStream){      fullResponse += delta;      process.stdout.write(delta);}    process.stdout.write('\n\n');    messages.push({ role:'assistant', content: fullResponse });}}main().catch(console.error);
```

Let's take a look at what is happening in this code:

1.  Set up a readline interface to take input from the terminal, enabling interactive sessions directly from the command line.
2.  Initialize an array called `messages` to store the history of your conversation. This history allows the model to maintain context in ongoing dialogues.
3.  In the `main` function:

-   Prompt for and capture user input, storing it in `userInput`.
-   Add user input to the `messages` array as a user message.
-   Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages`.
-   Iterate over the text stream returned by the `streamText` function (`result.textStream`) and print the contents of the stream to the terminal.
-   Add the assistant's response to the `messages` array.


## [Running Your Application](#running-your-application)


With that, you have built everything you need for your chatbot! To start your application, use the command:

pnpm tsx index.ts

You should see a prompt in your terminal. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Node.js.


## [Enhance Your Chatbot with Tools](#enhance-your-chatbot-with-tools)


While large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.

Tools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered for the next response.

For example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.

Let's enhance your chatbot by adding a simple weather tool.


### [Update Your Application](#update-your-application)


Modify your `index.ts` file to include the new weather tool:

index.ts

```
import{ openai }from'@ai-sdk/openai';import{CoreMessage, streamText, tool }from'ai';importdotenvfrom'dotenv';import{ z }from'zod';import*as readlinefrom'node:readline/promises';dotenv.config();const terminal = readline.createInterface({  input: process.stdin,  output: process.stdout,});const messages:CoreMessage[]=[];asyncfunctionmain(){while(true){const userInput =await terminal.question('You: ');    messages.push({ role:'user', content: userInput });const result =streamText({      model:openai('gpt-4o'),      messages,      tools:{        weather:tool({          description:'Get the weather in a location (in Celsius)',          parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,            temperature:Math.round((Math.random()*30+5)*10)/10,// Random temp between 5°C and 35°C}),}),},});let fullResponse ='';    process.stdout.write('\nAssistant: ');forawait(const delta of result.textStream){      fullResponse += delta;      process.stdout.write(delta);}    process.stdout.write('\n\n');    messages.push({ role:'assistant', content: fullResponse });}}main().catch(console.error);
```

In this updated code:

1.  You import the `tool` function from the `ai` package.
2.  You define a `tools` object with a `weather` tool. This tool:
    -   Has a description that helps the model understand when to use it.
    -   Defines parameters using a Zod schema, specifying that it requires a `location` string to execute this tool.
    -   Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function, so you could fetch real data from an external API.

Now your chatbot can "fetch" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and the results will be used by the model to generate its response.

Try asking something like "What's the weather in New York?" and see how the model uses the new tool.

Notice the blank "assistant" response? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result in the `toolCall` and `toolResult` keys of the result object.

```
import{ openai }from'@ai-sdk/openai';import{CoreMessage, streamText, tool }from'ai';importdotenvfrom'dotenv';import{ z }from'zod';import*as readlinefrom'node:readline/promises';dotenv.config();const terminal = readline.createInterface({  input: process.stdin,  output: process.stdout,});const messages:CoreMessage[]=[];asyncfunctionmain(){while(true){const userInput =await terminal.question('You: ');    messages.push({ role:'user', content: userInput });const result =streamText({      model:openai('gpt-4o'),      messages,      tools:{        weather:tool({          description:'Get the weather in a location (in Celsius)',          parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,            temperature:Math.round((Math.random()*30+5)*10)/10,// Random temp between 5°C and 35°C}),}),},});let fullResponse ='';    process.stdout.write('\nAssistant: ');forawait(const delta of result.textStream){      fullResponse += delta;      process.stdout.write(delta);}    process.stdout.write('\n\n');console.log(await result.toolCalls);console.log(await result.toolResults);    messages.push({ role:'assistant', content: fullResponse });}}main().catch(console.error);
```

Now, when you ask about the weather, you'll see the tool call and its result displayed in your chat interface.


## [Enabling Multi-Step Tool Calls](#enabling-multi-step-tool-calls)


You may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.

To solve this, you can enable multi-step tool calls using `maxSteps`. This feature will automatically send tool results back to the model to trigger an additional generation. In this case, you want the model to answer your question using the results from the weather tool.


### [Update Your Application](#update-your-application-1)


Modify your `index.ts` file to include the `maxSteps` option:

index.ts

```
import{ openai }from'@ai-sdk/openai';import{CoreMessage, streamText, tool }from'ai';importdotenvfrom'dotenv';import{ z }from'zod';import*as readlinefrom'node:readline/promises';dotenv.config();const terminal = readline.createInterface({  input: process.stdin,  output: process.stdout,});const messages:CoreMessage[]=[];asyncfunctionmain(){while(true){const userInput =await terminal.question('You: ');    messages.push({ role:'user', content: userInput });const result =streamText({      model:openai('gpt-4o'),      messages,      tools:{        weather:tool({          description:'Get the weather in a location (in Celsius)',          parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,            temperature:Math.round((Math.random()*30+5)*10)/10,// Random temp between 5°C and 35°C}),}),},      maxSteps:5,onStepFinish: step =>{console.log(JSON.stringify(step,null,2));},});let fullResponse ='';    process.stdout.write('\nAssistant: ');forawait(const delta of result.textStream){      fullResponse += delta;      process.stdout.write(delta);}    process.stdout.write('\n\n');    messages.push({ role:'assistant', content: fullResponse });}}main().catch(console.error);
```

In this updated code:

1.  You set `maxSteps` to 5, allowing the model to use up to 5 "steps" for any given generation.
2.  You add an `onStepFinish` callback to log each step of the interaction, helping you understand the model's tool usage. This means we can also delete the `toolCall` and `toolResult` `console.log` statements from the previous example.

Now, when you ask about the weather in a location, you should see the model using the weather tool results to answer your question.

By setting `maxSteps` to 5, you're allowing the model to use up to 5 "steps" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.


### [Adding a second tool](#adding-a-second-tool)


Update your `index.ts` file to add a new tool to convert the temperature from Celsius to Fahrenheit:

index.ts

```
import{ openai }from'@ai-sdk/openai';import{CoreMessage, streamText, tool }from'ai';importdotenvfrom'dotenv';import{ z }from'zod';import*as readlinefrom'node:readline/promises';dotenv.config();const terminal = readline.createInterface({  input: process.stdin,  output: process.stdout,});const messages:CoreMessage[]=[];asyncfunctionmain(){while(true){const userInput =await terminal.question('You: ');    messages.push({ role:'user', content: userInput });const result =streamText({      model:openai('gpt-4o'),      messages,      tools:{        weather:tool({          description:'Get the weather in a location (in Celsius)',          parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,            temperature:Math.round((Math.random()*30+5)*10)/10,// Random temp between 5°C and 35°C}),}),        convertCelsiusToFahrenheit:tool({          description:'Convert a temperature from Celsius to Fahrenheit',          parameters: z.object({            celsius: z.number().describe('The temperature in Celsius to convert'),}),execute:async({ celsius })=>{const fahrenheit =(celsius *9)/5+32;return{ fahrenheit:Math.round(fahrenheit *100)/100};},}),},      maxSteps:5,onStepFinish: step =>{console.log(JSON.stringify(step,null,2));},});let fullResponse ='';    process.stdout.write('\nAssistant: ');forawait(const delta of result.textStream){      fullResponse += delta;      process.stdout.write(delta);}    process.stdout.write('\n\n');    messages.push({ role:'assistant', content: fullResponse });}}main().catch(console.error);
```

Now, when you ask "What's the weather in New York in Celsius?", you should see a more complete interaction:

1.  The model will call the weather tool for New York.
2.  You'll see the tool result logged.
3.  It will then call the temperature conversion tool to convert the temperature from Celsius to Fahrenheit.
4.  The model will then use that information to provide a natural language response about the weather in New York.

This multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.

This example shows how tools can expand the model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.


## [Where to Next?](#where-to-next)


You've built an AI chatbot using the AI SDK! From here, you have several paths to explore:

-   To learn more about the AI SDK, read through the [documentation](/docs).
-   If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.
-   To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).
```

### 130. `docs/getting-started/nuxt.md`

```markdown
# Vue.js (Nuxt) Quickstart


---
url: https://ai-sdk.dev/docs/getting-started/nuxt
description: Welcome to the AI SDK quickstart guide for Vue.js (Nuxt)!
---


# [Vue.js (Nuxt) Quickstart](#vuejs-nuxt-quickstart)


The AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.

In this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.

If you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.


## [Prerequisites](#prerequisites)


To follow this quickstart, you'll need:

-   Node.js 18+ and pnpm installed on your local development machine.
-   An OpenAI API key.

If you haven't obtained your OpenAI API key, you can do so by [signing up](https://platform.openai.com/signup/) on the OpenAI website.


## [Setup Your Application](#setup-your-application)


Start by creating a new Nuxt application. This command will create a new directory named `my-ai-app` and set up a basic Nuxt application inside it.

pnpm create nuxt my-ai-app

Navigate to the newly created directory:

cd my-ai-app


### [Install dependencies](#install-dependencies)


Install `ai` and `@ai-sdk/openai`, the AI SDK's OpenAI provider.

The AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about [available providers](/providers) and [building custom providers](/providers/community-providers/custom-providers) in the [providers](/providers) section.

pnpm

npm

yarn

pnpm add ai @ai-sdk/openai @ai-sdk/vue zod

Make sure you are using `ai` version 3.1 or higher.


### [Configure OpenAI API key](#configure-openai-api-key)


Create a `.env` file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.

touch .env

Edit the `.env` file:

.env

```
OPENAI_API_KEY=xxxxxxxxx
```

Replace `xxxxxxxxx` with your actual OpenAI API key and configure the environment variable in `nuxt.config.ts`:

nuxt.config.ts

```
exportdefaultdefineNuxtConfig({// rest of your nuxt config  runtimeConfig:{    openaiApiKey: process.env.OPENAI_API_KEY,},});
```

The AI SDK's OpenAI Provider will default to using the `OPENAI_API_KEY` environment variable.


## [Create an API route](#create-an-api-route)


Create an API route, `server/api/chat.ts` and add the following code:

server/api/chat.ts

```
import{ streamText }from'ai';import{ createOpenAI }from'@ai-sdk/openai';exportdefaultdefineLazyEventHandler(async()=>{const apiKey =useRuntimeConfig().openaiApiKey;if(!apiKey)thrownewError('Missing OpenAI API key');const openai =createOpenAI({    apiKey: apiKey,});returndefineEventHandler(async(event:any)=>{const{ messages }=awaitreadBody(event);const result =streamText({      model:openai('gpt-4o'),      messages,});return result.toDataStreamResponse();});});
```

Let's take a look at what is happening in this code:

1.  Create an OpenAI provider instance with the `createOpenAI` function from the `@ai-sdk/openai` package.
2.  Define an Event Handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation with you and the chatbot and will provide the chatbot with the necessary context to make the next generation.
3.  Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (defined in step 1) and `messages` (defined in step 2). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model's behaviour.
4.  The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result). This result object contains the [`toDataStreamResponse`](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.
5.  Return the result to the client to stream the response.


## [Wire up the UI](#wire-up-the-ui)


Now that you have an API route that can query an LLM, it's time to setup your frontend. The AI SDK's [UI](/docs/ai-sdk-ui/overview) package abstract the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).

Update your root page (`pages/index.vue`) with the following code to show a list of chat messages and provide a user message input:

pages/index.vue

```
<script setup lang="ts">import{ useChat }from'@ai-sdk/vue';const{ messages, input, handleSubmit }=useChat();</script><template><div><div      v-for="m in messages":key="m.id ? m.id : index">{{ m.role==='user'?'User: ':'AI: '}}<div v-for="part in m.parts":key="part.id"><div v-if="part.type === 'text'">{{ part.text}}</div></div></div><form @submit="handleSubmit"><input        v-model="input"        placeholder="Say something..."/></form></div></template>
```

If your project has `app.vue` instead of `pages/index.vue`, delete the `app.vue` file and create a new `pages/index.vue` file with the code above.

This page utilizes the `useChat` hook, which will, by default, use the API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:

-   `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).
-   `input` - the current value of the user's input field.
-   `handleSubmit` - function to handle form submission.

The LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.


## [Running Your Application](#running-your-application)


With that, you have built everything you need for your chatbot! To start your application, use the command:

pnpm run dev

Head to your browser and open [http://localhost:3000](http://localhost:3000). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Nuxt.


## [Enhance Your Chatbot with Tools](#enhance-your-chatbot-with-tools)


While large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.

Tools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.

For example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.

Let's enhance your chatbot by adding a simple weather tool.


### [Update Your API Route](#update-your-api-route)


Modify your `server/api/chat.ts` file to include the new weather tool:

server/api/chat.ts

```
import{ streamText, tool }from'ai';import{ createOpenAI }from'@ai-sdk/openai';import{ z }from'zod';exportdefaultdefineLazyEventHandler(async()=>{const apiKey =useRuntimeConfig().openaiApiKey;if(!apiKey)thrownewError('Missing OpenAI API key');const openai =createOpenAI({    apiKey: apiKey,});returndefineEventHandler(async(event:any)=>{const{ messages }=awaitreadBody(event);const result =streamText({      model:openai('gpt-4o'),      messages,      tools:{        weather:tool({          description:'Get the weather in a location (fahrenheit)',          parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,              temperature,};},}),},});return result.toDataStreamResponse();});});
```

In this updated code:

1.  You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.

2.  You define a `tools` object with a `weather` tool. This tool:

    -   Has a description that helps the model understand when to use it.
    -   Defines parameters using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this parameter from the context of the conversation. If it can't, it will ask the user for the missing information.
    -   Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.

Now your chatbot can "fetch" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and you can access the results via `tool-invocations` part that is available on the `message.parts` array.

Try asking something like "What's the weather in New York?" and see how the model uses the new tool.

Notice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result via the `tool-invocation` part of the `message.parts` array.


### [Update the UI](#update-the-ui)


To display the tool invocations in your UI, update your `pages/index.vue` file:

pages/index.vue

```
<script setup lang="ts">import{ useChat }from'@ai-sdk/vue';const{ messages, input, handleSubmit }=useChat();</script><template><div><div      v-for="m in messages":key="m.id ? m.id : index">{{ m.role==='user'?'User: ':'AI: '}}<div v-for="part in m.parts":key="part.id"><div v-if="part.type === 'text'">{{ part.text}}</div><div v-if="part.type === 'tool-invocation'">{{ part.toolInvocation}}</div></div></div><form @submit="handleSubmit"><input        v-model="input"        placeholder="Say something..."/></form></div></template>
```

With this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For tool invocations, you display a JSON representation of the tool call and its result.

Now, when you ask about the weather, you'll see the tool invocation and its result displayed in your chat interface.


## [Enabling Multi-Step Tool Calls](#enabling-multi-step-tool-calls)


You may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.

To solve this, you can enable multi-step tool calls using the `maxSteps` option in your `useChat` hook. This feature will automatically send tool results back to the model to trigger an additional generation. In this case, you want the model to answer your question using the results from the weather tool.


### [Update Your Client-Side Code](#update-your-client-side-code)


Modify your `pages/index.vue` file to include the `maxSteps` option:

pages/index.vue

```
<script setup lang="ts">import{ useChat }from'@ai-sdk/vue';const{ messages, input, handleSubmit }=useChat({ maxSteps:5});</script><!--... rest of your component code -->
```

Head back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.

By setting `maxSteps` to 5, you're allowing the model to use up to 5 "steps" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.


### [Update Your API Route](#update-your-api-route-1)


Update your `server/api/chat.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:

server/api/chat.ts

```
import{ streamText, tool }from'ai';import{ createOpenAI }from'@ai-sdk/openai';import{ z }from'zod';exportdefaultdefineLazyEventHandler(async()=>{const apiKey =useRuntimeConfig().openaiApiKey;if(!apiKey)thrownewError('Missing OpenAI API key');const openai =createOpenAI({    apiKey: apiKey,});returndefineEventHandler(async(event:any)=>{const{ messages }=awaitreadBody(event);const result =streamText({      model:openai('gpt-4o-preview'),      messages,      tools:{        weather:tool({          description:'Get the weather in a location (fahrenheit)',          parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,              temperature,};},}),        convertFahrenheitToCelsius:tool({          description:'Convert a temperature in fahrenheit to celsius',          parameters: z.object({            temperature: z.number().describe('The temperature in fahrenheit to convert'),}),execute:async({ temperature })=>{const celsius =Math.round((temperature -32)*(5/9));return{              celsius,};},}),},});return result.toDataStreamResponse();});});
```

Now, when you ask "What's the weather in New York in celsius?", you should see a more complete interaction:

1.  The model will call the weather tool for New York.
2.  You'll see the tool result displayed.
3.  It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.
4.  The model will then use that information to provide a natural language response about the weather in New York.

This multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.

This simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.


## [Where to Next?](#where-to-next)


You've built an AI chatbot using the AI SDK! From here, you have several paths to explore:

-   To learn more about the AI SDK, read through the [documentation](/docs).
-   If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.
-   To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).
```

### 131. `docs/getting-started/svelte.md`

```markdown
# Svelte Quickstart


---
url: https://ai-sdk.dev/docs/getting-started/svelte
description: Welcome to the AI SDK quickstart guide for Svelte!
---


# [Svelte Quickstart](#svelte-quickstart)


The AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.

In this quickstart tutorial, you'll build a simple AI-chatbot with a streaming user interface. Along the way, you'll learn key concepts and techniques that are fundamental to using the SDK in your own projects.

If you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.


## [Prerequisites](#prerequisites)


To follow this quickstart, you'll need:

-   Node.js 18+ and pnpm installed on your local development machine.
-   An OpenAI API key.

If you haven't obtained your OpenAI API key, you can do so by [signing up](https://platform.openai.com/signup/) on the OpenAI website.


## [Set Up Your Application](#set-up-your-application)


This guide applies to SvelteKit versions 4 and below.

Start by creating a new SvelteKit application. This command will create a new directory named `my-ai-app` and set up a basic SvelteKit application inside it.

npx sv create my-ai-app

Navigate to the newly created directory:

cd my-ai-app


### [Install Dependencies](#install-dependencies)


Install `ai` and `@ai-sdk/openai`, the AI SDK's OpenAI provider.

The AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about [available providers](/providers) and [building custom providers](/providers/community-providers/custom-providers) in the [providers](/providers) section.

pnpm

npm

yarn

pnpm add -D ai @ai-sdk/openai @ai-sdk/svelte zod

Make sure you are using `ai` version 3.1 or higher.


### [Configure OpenAI API Key](#configure-openai-api-key)


Create a `.env.local` file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.

touch .env.local

Edit the `.env.local` file:

.env.local

```
OPENAI_API_KEY=xxxxxxxxx
```

Replace `xxxxxxxxx` with your actual OpenAI API key.

Vite does not automatically load environment variables onto `process.env`, so you'll need to import `OPENAI_API_KEY` from `$env/static/private` in your code (see below).


## [Create an API route](#create-an-api-route)


Create a SvelteKit Endpoint, `src/routes/api/chat/+server.ts` and add the following code:

src/routes/api/chat/+server.ts

```
import{ createOpenAI }from'@ai-sdk/openai';import{ streamText }from'ai';import{OPENAI_API_KEY}from'$env/static/private';const openai =createOpenAI({  apiKey:OPENAI_API_KEY,});exportasyncfunctionPOST({ request }){const{ messages }=await request.json();const result =streamText({    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse();}
```

If you see type errors with `OPENAI_API_KEY` or your `POST` function, run the dev server.

Let's take a look at what is happening in this code:

1.  Create an OpenAI provider instance with the `createOpenAI` function from the `@ai-sdk/openai` package.
2.  Define a `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation with you and the chatbot and will provide the chatbot with the necessary context to make the next generation.
3.  Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (defined in step 1) and `messages` (defined in step 2). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model's behaviour.
4.  The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toDataStreamResponse`](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.
5.  Return the result to the client to stream the response.


## [Wire up the UI](#wire-up-the-ui)


Now that you have an API route that can query an LLM, it's time to set up your frontend. The AI SDK's [UI](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one class, `Chat`. Its properties and API are largely the same as React's [`useChat`](/docs/reference/ai-sdk-ui/use-chat).

Update your root page (`src/routes/+page.svelte`) with the following code to show a list of chat messages and provide a user message input:

src/routes/+page.svelte

```
<script>import{Chat}from'@ai-sdk/svelte';const chat =newChat();</script><main><ul>    {#each chat.messages as message, messageIndex (messageIndex)}<li><div>{message.role}</div><div>          {#each message.parts as part, partIndex (partIndex)}            {#if part.type === 'text'}<div>{part.text}</div>            {/if}          {/each}</div></li>    {/each}</ul><formonsubmit={chat.handleSubmit}><inputbind:value={chat.input}/><buttontype="submit">Send</button></form></main>
```

This page utilizes the `Chat` class, which will, by default, use the `POST` route handler you created earlier. The hook provides functions and state for handling user input and form submission. The `Chat` class provides multiple utility functions and state variables:

-   `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).
-   `input` - the current value of the user's input field.
-   `handleSubmit` - function to handle form submission.

The LLM's response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model's outputs, allowing you to display or process each component in the order it was generated.


## [Running Your Application](#running-your-application)


With that, you have built everything you need for your chatbot! To start your application, use the command:

pnpm run dev

Head to your browser and open [http://localhost:5173](http://localhost:5173). You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Svelte.


## [Enhance Your Chatbot with Tools](#enhance-your-chatbot-with-tools)


While large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.

Tools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.

For example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.

Let's enhance your chatbot by adding a simple weather tool.


### [Update Your API Route](#update-your-api-route)


Modify your `src/routes/api/chat/+server.ts` file to include the new weather tool:

src/routes/api/chat/+server.ts

```
import{ createOpenAI }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';import{OPENAI_API_KEY}from'$env/static/private';const openai =createOpenAI({  apiKey:OPENAI_API_KEY,});exportasyncfunctionPOST({ request }){const{ messages }=await request.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      weather:tool({        description:'Get the weather in a location (fahrenheit)',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,            temperature,};},}),},});return result.toDataStreamResponse();}
```

In this updated code:

1.  You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.

2.  You define a `tools` object with a `weather` tool. This tool:

    -   Has a description that helps the model understand when to use it.
    -   Defines parameters using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this parameter from the context of the conversation. If it can't, it will ask the user for the missing information.
    -   Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.

Now your chatbot can "fetch" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and you can access the results via `tool-invocations` part that is available on the `message.parts` array.

Try asking something like "What's the weather in New York?" and see how the model uses the new tool.

Notice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result via the `tool-invocation` part of the `message.parts` array.


### [Update the UI](#update-the-ui)


To display the tool invocations in your UI, update your `src/routes/+page.svelte` file:

src/routes/+page.svelte

```
<script>import{Chat}from'@ai-sdk/svelte';const chat =newChat();</script><main><ul>    {#each chat.messages as message, messageIndex (messageIndex)}<li><div>{message.role}</div><div>          {#each message.parts as part, partIndex (partIndex)}            {#if part.type === 'text'}<div>{part.text}</div>            {:else if part.type === 'tool-invocation'}<pre>{JSON.stringify(part.toolInvocation, null, 2)}</pre>            {/if}          {/each}</div></li>    {/each}</ul><formonsubmit={chat.handleSubmit}><inputbind:value={chat.input}/><buttontype="submit">Send</button></form></main>
```

With this change, you're updating the UI to handle different message parts. For text parts, you display the text content as before. For tool invocations, you display a JSON representation of the tool call and its result.

Now, when you ask about the weather, you'll see the tool invocation and its result displayed in your chat interface.


## [Enabling Multi-Step Tool Calls](#enabling-multi-step-tool-calls)


You may have noticed that while the tool results are visible in the chat interface, the model isn't using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.

To solve this, you can enable multi-step tool calls using the `maxSteps` option in your `Chat` class. This feature will automatically send tool results back to the model to trigger an additional generation. In this case, you want the model to answer your question using the results from the weather tool.


### [Update Your UI](#update-your-ui)


Modify your `src/routes/+page.svelte` file to include the `maxSteps` option:

src/routes/+page.svelte

```
<script>import{Chat}from'@ai-sdk/svelte';const chat =newChat({ maxSteps:5});</script><!-- ... rest of your component code -->
```

Head back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.

By setting `maxSteps` to 5, you're allowing the model to use up to 5 "steps" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.


### [Update Your API Route](#update-your-api-route-1)


Update your `src/routes/api/chat/+server.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:

src/routes/api/chat/+server.ts

```
import{ createOpenAI }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';import{OPENAI_API_KEY}from'$env/static/private';const openai =createOpenAI({  apiKey:OPENAI_API_KEY,});exportasyncfunctionPOST({ request }){const{ messages }=await request.json();const result =streamText({    model:openai('gpt-4o'),    messages,    tools:{      weather:tool({        description:'Get the weather in a location (fahrenheit)',        parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>{const temperature =Math.round(Math.random()*(90-32)+32);return{location,            temperature,};},}),      convertFahrenheitToCelsius:tool({        description:'Convert a temperature in fahrenheit to celsius',        parameters: z.object({          temperature: z.number().describe('The temperature in fahrenheit to convert'),}),execute:async({ temperature })=>{const celsius =Math.round((temperature -32)*(5/9));return{            celsius,};},}),},});return result.toDataStreamResponse();}
```

Now, when you ask "What's the weather in New York in celsius?", you should see a more complete interaction:

1.  The model will call the weather tool for New York.
2.  You'll see the tool result displayed.
3.  It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.
4.  The model will then use that information to provide a natural language response about the weather in New York.

This multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.

This simple example demonstrates how tools can expand your model's capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model's knowledge cutoff and current information.


## [How does `@ai-sdk/svelte` differ from `@ai-sdk/react`?](#how-does-ai-sdksvelte-differ-from-ai-sdkreact)


The surface-level difference is that Svelte uses classes to manage state, whereas React uses hooks, so `useChat` in React is `Chat` in Svelte. Other than that, there are a few things to keep in mind:


### [1\. Arguments to classes aren't reactive by default](#1-arguments-to-classes-arent-reactive-by-default)


Unlike in React, where hooks are rerun any time their containing component is invalidated, code in the `script` block of a Svelte component is only run once when the component is created. This means that, if you want arguments to your class to be reactive, you need to make sure you pass a *reference* into the class, rather than a value:

```
<script>import{Chat}from'@ai-sdk/svelte';let{ id }=$props();// won't work; the class instance will be created once, `id` will be copied by value, and won't update when $props.id changeslet chat =newChat({ id });// will work; passes `id` by reference, so `Chat` always has the latest valuelet chat =newChat({getid(){return id;},});</script>
```

Keep in mind that this normally doesn't matter; most parameters you'll pass into the Chat class are static (for example, you typically wouldn't expect your `onError` handler to change).


### [2\. You can't destructure class properties](#2-you-cant-destructure-class-properties)


In vanilla JavaScript, destructuring class properties copies them by value and "disconnects" them from their class instance:

```
const classInstance =newWhatever();classInstance.foo='bar';const{ foo }= classInstance;classInstance.foo='baz';console.log(foo);// 'bar'
```

The same is true of classes in Svelte:

```
<script>import{Chat}from'@ai-sdk/svelte';const chat =newChat();let{ messages }= chat;  chat.append({ content:'Hello, world!', role:'user'}).then(()=>{console.log(messages);// []console.log(chat.messages);// [{ content: 'Hello, world!', role: 'user' }] (plus some other stuff)});</script>
```


### [3\. Instance synchronization requires context](#3-instance-synchronization-requires-context)


In React, hook instances with the same `id` are synchronized -- so two instances of `useChat` will have the same `messages`, `status`, etc. if they have the same `id`. For most use cases, you probably don't need this behavior -- but if you do, you can create a context in your root layout file using `createAIContext`:

```
<script>import{ createAIContext }from'@ai-sdk/svelte';let{ children }=$props();createAIContext();// all hooks created after this or in components that are children of this component// will have synchronized state</script>{@render children()}
```


## [Where to Next?](#where-to-next)


You've built an AI chatbot using the AI SDK! From here, you have several paths to explore:

-   To learn more about the AI SDK, read through the [documentation](/docs).
-   If you're interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.
-   To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).
-   To learn more about Svelte, check out the [official documentation](https://svelte.dev/docs/svelte).
```

### 132. `docs/getting-started.md`

```markdown
# Getting Started


---
url: https://ai-sdk.dev/docs/getting-started
description: Welcome to the AI SDK documentation!
---


# [Getting Started](#getting-started)


The following guides are intended to provide you with an introduction to some of the core features provided by the AI SDK.

[

Next.js App Router

](/docs/getting-started/nextjs-app-router)[

Next.js Pages Router

](/docs/getting-started/nextjs-pages-router)[

SvelteKit

](/docs/getting-started/svelte)[

Nuxt

](/docs/getting-started/nuxt)[

Node.js

](/docs/getting-started/nodejs)[

Expo

](/docs/getting-started/expo)


## [Backend Framework Examples](#backend-framework-examples)


You can also use [AI SDK Core](/docs/ai-sdk-core/overview) and [AI SDK UI](/docs/ai-sdk-ui/overview) with the following backend frameworks:

[

Node.js HTTP Server

Send AI responses from a Node.js HTTP server.

](/examples/api-servers/node-js-http-server)[

Express

Send AI responses from an Express server.

](/examples/api-servers/express)[

Hono

Send AI responses from a Hono server.

](/examples/api-servers/hono)[

Fastify

Send AI responses from a Fastify server.

](/examples/api-servers/fastify)[

Nest.js

Send AI responses from a Nest.js server.

](/examples/api-servers/nest)
```

### 133. `docs/guides/claude-4.md`

```markdown
# Get started with Claude 4


---
url: https://ai-sdk.dev/docs/guides/claude-4
description: Get started with Claude 4 using the AI SDK.
---


# [Get started with Claude 4](#get-started-with-claude-4)


With the release of Claude 4, there has never been a better time to start building AI applications, particularly those that require complex reasoning capabilities and advanced intelligence.

The [AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like Claude 4 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.


## [Claude 4](#claude-4)


Claude 4 is Anthropic's most advanced model family to date, offering exceptional capabilities across reasoning, instruction following, coding, and knowledge tasks. Available in two variants—Sonnet and Opus—Claude 4 delivers state-of-the-art performance with enhanced reliability and control. Claude 4 builds on the extended thinking capabilities introduced in Claude 3.7, allowing for even more sophisticated problem-solving through careful, step-by-step reasoning.

Claude 4 excels at complex reasoning, code generation and analysis, detailed content creation, and agentic capabilities, making it ideal for powering sophisticated AI workflows, customer-facing agents, and applications requiring nuanced understanding and responses. Claude Opus 4 is an excellent coding model, leading on SWE-bench (72.5%) and Terminal-bench (43.2%), with the ability to sustain performance on long-running tasks that require focused effort and thousands of steps. Claude Sonnet 4 significantly improves on Sonnet 3.7, excelling in coding with 72.7% on SWE-bench while balancing performance and efficiency.


### [Prompt Engineering for Claude 4 Models](#prompt-engineering-for-claude-4-models)


Claude 4 models respond well to clear, explicit instructions. The following best practices can help achieve optimal performance:

1.  **Provide explicit instructions**: Clearly state what you want the model to do, including specific steps or formats for the response.
2.  **Include context and motivation**: Explain why a task is being performed to help the model better understand the underlying goals.
3.  **Avoid negative examples**: When providing examples, only demonstrate the behavior you want to see, not what you want to avoid.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is [AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call Claude 3.7 Sonnet with the AI SDK:

```
import{ anthropic }from'@ai-sdk/anthropic';import{ generateText }from'ai';const{ text, reasoning, reasoningDetails }=awaitgenerateText({  model:anthropic('claude-4-sonnet-20250514'),  prompt:'How will quantum computing impact cryptography by 2050?',});console.log(text);
```


### [Reasoning Ability](#reasoning-ability)


Claude 4 enhances the extended thinking capabilities first introduced in Claude 3.7 Sonnet—the ability to solve complex problems with careful, step-by-step reasoning. Additionally, both Opus 4 and Sonnet 4 can now use tools during extended thinking, allowing Claude to alternate between reasoning and tool use to improve responses. You can enable extended thinking using the `thinking` provider option and specifying a thinking budget in tokens. For interleaved thinking (where Claude can think in between tool calls) you'll need to enable a beta feature using the `anthropic-beta` header:

```
import{ anthropic,AnthropicProviderOptions}from'@ai-sdk/anthropic';import{ generateText }from'ai';const{ text, reasoning, reasoningDetails }=awaitgenerateText({  model:anthropic('claude-4-sonnet-20250514'),  prompt:'How will quantum computing impact cryptography by 2050?',  providerOptions:{    anthropic:{      thinking:{type:'enabled', budgetTokens:15000},} satisfies AnthropicProviderOptions,},  headers:{'anthropic-beta':'interleaved-thinking-2025-05-14',},});console.log(text);// text responseconsole.log(reasoning);// reasoning textconsole.log(reasoningDetails);// reasoning details including redacted reasoning
```


### [Building Interactive Interfaces](#building-interactive-interfaces)


AI SDK Core can be paired with [AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — [`useChat`](/docs/reference/ai-sdk-ui/use-chat), [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), [`useObject`](/docs/reference/ai-sdk-ui/use-object), and [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with [Next.js](https://nextjs.org), the AI SDK, and Claude Sonnet 4:

In a new Next.js application, first install the AI SDK and the Anthropic provider:

pnpm install ai @ai-sdk/anthropic

Then, create a route handler for the chat endpoint:

app/api/chat/route.ts

```
import{ anthropic,AnthropicProviderOptions}from'@ai-sdk/anthropic';import{ streamText }from'ai';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:anthropic('claude-4-sonnet-20250514'),    messages,    headers:{'anthropic-beta':'interleaved-thinking-2025-05-14',},    providerOptions:{      anthropic:{        thinking:{type:'enabled', budgetTokens:15000},} satisfies AnthropicProviderOptions,},});return result.toDataStreamResponse({    sendReasoning:true,});}
```

You can forward the model's reasoning tokens to the client with `sendReasoning: true` in the `toDataStreamResponse` method.

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit, error }=useChat();return(<divclassName="flex flex-col h-screen max-w-2xl mx-auto p-4"><divclassName="flex-1 overflow-y-auto space-y-4 mb-4">{messages.map(message=>(<divkey={message.id}className={`p-3 rounded-lg ${              message.role ==='user'?'bg-blue-50 ml-auto':'bg-gray-50'}`}><pclassName="font-semibold">{message.role ==='user'?'You':'Claude 4'}</p>{message.parts.map((part, index)=>{if(part.type==='text'){return(<divkey={index}className="mt-1">{part.text}</div>);}if(part.type==='reasoning'){return(<prekey={index}className="bg-gray-100 p-2 rounded mt-2 text-xs overflow-x-auto"><details><summaryclassName="cursor-pointer">View reasoning</summary>{part.details.map(detail=>                        detail.type==='text'? detail.text :'<redacted>',)}</details></pre>);}})}</div>))}</div><formonSubmit={handleSubmit}className="flex gap-2"><inputname="prompt"value={input}onChange={handleInputChange}className="flex-1 p-2 border rounded focus:outline-none focus:ring-2 focus:ring-blue-500"placeholder="Ask Claude 4 something..."/><buttontype="submit"className="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600">Send</button></form></div>);}
```

You can access the model's reasoning tokens with the `reasoning` part on the message `parts`.

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.


### [Claude 4 Model Variants](#claude-4-model-variants)


Claude 4 is available in two variants, each optimized for different use cases:

-   **Claude Sonnet 4**: Balanced performance suitable for most enterprise applications, with significant improvements over Sonnet 3.7.
-   **Claude Opus 4**: Anthropic's most powerful model and the best coding model available. Excels at sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours.


## [Get Started](#get-started)


Ready to dive in? Here's how you can begin:

1.  Explore the documentation at [ai-sdk.dev/docs](/docs) to understand the capabilities of the AI SDK.
2.  Check out practical examples at [ai-sdk.dev/examples](/examples) to see the SDK in action.
3.  Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) at [ai-sdk.dev/docs/guides](/docs/guides).
4.  Use ready-to-deploy AI templates at [vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).
```

### 134. `docs/guides/computer-use.md`

```markdown
# Get started with Computer Use


---
url: https://ai-sdk.dev/docs/guides/computer-use
description: Get started with Claude's Computer Use capabilities with the AI SDK
---


# [Get started with Computer Use](#get-started-with-computer-use)


With the [release of Computer Use in Claude 3.5 Sonnet](https://www.anthropic.com/news/3-5-models-and-computer-use), you can now direct AI models to interact with computers like humans do - moving cursors, clicking buttons, and typing text. This capability enables automation of complex tasks while leveraging Claude's advanced reasoning abilities.

The AI SDK is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like Anthropic's Claude alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more. In this guide, you will learn how to integrate Computer Use into your AI SDK applications.

Computer Use is currently in beta with some [limitations](https://docs.anthropic.com/en/docs/build-with-claude/computer-use#understand-computer-use-limitations) . The feature may be error-prone at times. Anthropic recommends starting with low-risk tasks and implementing appropriate safety measures.


## [Computer Use](#computer-use)


Anthropic recently released a new version of the Claude 3.5 Sonnet model which is capable of 'Computer Use'. This allows the model to interact with computer interfaces through basic actions like:

-   Moving the cursor
-   Clicking buttons
-   Typing text
-   Taking screenshots
-   Reading screen content


## [How It Works](#how-it-works)


Computer Use enables the model to read and interact with on-screen content through a series of coordinated steps. Here's how the process works:

1.  **Start with a prompt and tools**

    Add Anthropic-defined Computer Use tools to your request and provide a task (prompt) for the model. For example: "save an image to your downloads folder."

2.  **Select the right tool**

    The model evaluates which computer tools can help accomplish the task. It then sends a formatted `tool_call` to use the appropriate tool.

3.  **Execute the action and return results**

    The AI SDK processes Claude's request by running the selected tool. The results can then be sent back to Claude through a `tool_result` message.

4.  **Complete the task through iterations**

    Claude analyzes each result to determine if more actions are needed. It continues requesting tool use and processing results until it completes your task or requires additional input.



### [Available Tools](#available-tools)


There are three main tools available in the Computer Use API:

1.  **Computer Tool**: Enables basic computer control like mouse movement, clicking, and keyboard input
2.  **Text Editor Tool**: Provides functionality for viewing and editing text files
3.  **Bash Tool**: Allows execution of bash commands


### [Implementation Considerations](#implementation-considerations)


Computer Use tools in the AI SDK are predefined interfaces that require your own implementation of the execution layer. While the SDK provides the type definitions and structure for these tools, you need to:

1.  Set up a controlled environment for Computer Use execution
2.  Implement core functionality like mouse control and keyboard input
3.  Handle screenshot capture and processing
4.  Set up rules and limits for how Claude can interact with your system

The recommended approach is to start with [Anthropic's reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo) , which provides:

-   A containerized environment configured for safe Computer Use
-   Ready-to-use (Python) implementations of Computer Use tools
-   An agent loop for API interaction and tool execution
-   A web interface for monitoring and control

This reference implementation serves as a foundation to understand the requirements before building your own custom solution.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


If you have never used the AI SDK before, start by following the [Getting Started guide](/docs/getting-started).

First, ensure you have the AI SDK and [Anthropic AI SDK provider](/providers/ai-sdk-providers/anthropic) installed:

pnpm add ai @ai-sdk/anthropic

You can add Computer Use to your AI SDK applications using provider-defined tools. These tools accept various input parameters (like display height and width in the case of the computer tool) and then require that you define an execute function.

Here's how you could set up the Computer Tool with the AI SDK:

```
import{ anthropic }from'@ai-sdk/anthropic';import{ getScreenshot, executeComputerAction }from'@/utils/computer-use';const computerTool = anthropic.tools.computer_20241022({  displayWidthPx:1920,  displayHeightPx:1080,execute:async({ action, coordinate, text })=>{switch(action){case'screenshot':{return{type:'image',          data:getScreenshot(),};}default:{returnexecuteComputerAction(action, coordinate, text);}}},experimental_toToolResultContent(result){returntypeof result ==='string'?[{type:'text', text: result }]:[{type:'image', data: result.data, mimeType:'image/png'}];},});
```

The `computerTool` handles two main actions: taking screenshots via `getScreenshot()` and executing computer actions like mouse movements and clicks through `executeComputerAction()`. Remember, you have to implement this execution logic (eg. the `getScreenshot` and `executeComputerAction` functions) to handle the actual computer interactions. The `execute` function should handle all low-level interactions with the operating system.

Finally, to send tool results back to the model, use the [`experimental_toToolResultContent()`](/docs/foundations/prompts#multi-modal-tool-results) function to convert text and image responses into a format the model can process. The AI SDK includes experimental support for these multi-modal tool results when using Anthropic's models.

Computer Use requires appropriate safety measures like using virtual machines, limiting access to sensitive data, and implementing human oversight for critical actions.


### [Using Computer Tools with Text Generation](#using-computer-tools-with-text-generation)


Once your tool is defined, you can use it with both the [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`streamText`](/docs/reference/ai-sdk-core/stream-text) functions.

For one-shot text generation, use `generateText`:

```
const result =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-20241022'),  prompt:'Move the cursor to the center of the screen and take a screenshot',  tools:{ computer: computerTool },});console.log(response.text);
```

For streaming responses, use `streamText` to receive updates in real-time:

```
const result =streamText({  model:anthropic('claude-3-5-sonnet-20241022'),  prompt:'Open the browser and navigate to vercel.com',  tools:{ computer: computerTool },});forawait(const chunk of result.textStream){console.log(chunk);}
```


### [Configure Multi-Step (Agentic) Generations](#configure-multi-step-agentic-generations)


To allow the model to perform multiple steps without user intervention, specify a `maxSteps` value. This will automatically send any tool results back to the model to trigger a subsequent generation:

```
const stream =streamText({  model:anthropic('claude-3-5-sonnet-20241022'),  prompt:'Open the browser and navigate to vercel.com',  tools:{ computer: computerTool },  maxSteps:10,// experiment with this value based on your use case});
```


### [Combine Multiple Tools](#combine-multiple-tools)


You can combine multiple tools in a single request to enable more complex workflows. The AI SDK supports all three of Claude's Computer Use tools:

```
const computerTool = anthropic.tools.computer_20241022({...});const bashTool = anthropic.tools.bash_20241022({execute:async({ command, restart })=>execSync(command).toString()});const textEditorTool = anthropic.tools.textEditor_20241022({execute:async({    command,    path,    file_text,    insert_line,    new_str,    old_str,    view_range})=>{// Handle file operations based on commandswitch(command){returnexecuteTextEditorFunction({        command,        path,        fileText: file_text,        insertLine: insert_line,        newStr: new_str,        oldStr: old_str,        viewRange: view_range});}}});const response =awaitgenerateText({  model:anthropic("claude-3-5-sonnet-20241022"),  prompt:"Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal",  tools:{    computer: computerTool,    bash: bashTool    str_replace_editor: textEditorTool,},});
```

Always implement appropriate [security measures](#security-measures) and obtain user consent before enabling Computer Use in production applications.


### [Best Practices for Computer Use](#best-practices-for-computer-use)


To get the best results when using Computer Use:

1.  Specify simple, well-defined tasks with explicit instructions for each step
2.  Prompt Claude to verify outcomes through screenshots
3.  Use keyboard shortcuts when UI elements are difficult to manipulate
4.  Include example screenshots for repeatable tasks
5.  Provide explicit tips in system prompts for known tasks


## [Security Measures](#security-measures)


Remember, Computer Use is a beta feature. Please be aware that it poses unique risks that are distinct from standard API features or chat interfaces. These risks are heightened when using Computer Use to interact with the internet. To minimize risks, consider taking precautions such as:

1.  Use a dedicated virtual machine or container with minimal privileges to prevent direct system attacks or accidents.
2.  Avoid giving the model access to sensitive data, such as account login information, to prevent information theft.
3.  Limit internet access to an allowlist of domains to reduce exposure to malicious content.
4.  Ask a human to confirm decisions that may result in meaningful real-world consequences as well as any tasks requiring affirmative consent, such as accepting cookies, executing financial transactions, or agreeing to terms of service.
```

### 135. `docs/guides/gpt-4-5.md`

```markdown
# Get started with OpenAI GPT-4.5


---
url: https://ai-sdk.dev/docs/guides/gpt-4-5
description: Get started with OpenAI GPT-4.5 using the AI SDK.
---


# [Get started with OpenAI GPT-4.5](#get-started-with-openai-gpt-45)


With the [release of OpenAI's GPT-4.5 model](https://openai.com/index/introducing-gpt-4-5), there has never been a better time to start building AI applications, particularly those that require a deeper understanding of the world.

The [AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like OpenAI GPT-4.5 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.


## [OpenAI GPT-4.5](#openai-gpt-45)


OpenAI recently released GPT-4.5, their largest and best model for chat yet. GPT‑4.5 is a step forward in scaling up pretraining and post-training. By scaling unsupervised learning, GPT‑4.5 improves its ability to recognize patterns, draw connections, and generate creative insights without reasoning.

Based on early testing, developers may find GPT‑4.5 particularly useful for applications that benefit from its higher emotional intelligence and creativity such as writing help, communication, learning, coaching, and brainstorming. It also shows strong capabilities in agentic planning and execution, including multi-step coding workflows and complex task automation.


### [Benchmarks](#benchmarks)


GPT-4.5 demonstrates impressive performance across various benchmarks:

-   **SimpleQA Accuracy**: 62.5% (higher is better)
-   **SimpleQA Hallucination Rate**: 37.1% (lower is better)

[Source](https://openai.com/index/introducing-gpt-4-5)


### [Prompt Engineering for GPT-4.5](#prompt-engineering-for-gpt-45)


GPT-4.5 performs best with the following approach:

1.  **Be clear and specific**: GPT-4.5 responds well to direct, well-structured prompts.
2.  **Use delimiters for clarity**: Use delimiters like triple quotation marks, XML tags, or section titles to clearly indicate distinct parts of the input.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is [AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call OpenAI GPT-4.5 with the AI SDK:

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model:openai('gpt-4.5-preview'),  prompt:'Explain the concept of quantum entanglement.',});
```


### [Generating Structured Data](#generating-structured-data)


While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions ([`generateObject`](/docs/reference/ai-sdk-core/generate-object) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```
import{ generateObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model:openai('gpt-4.5-preview'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.object({ name: z.string(), amount: z.string()})),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});
```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.


### [Using Tools with the AI SDK](#using-tools-with-the-ai-sdk)


GPT-4.5 supports tool calling out of the box, allowing it to interact with external systems and perform discrete tasks. Here's an example of using tool calling with the AI SDK:

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model:openai('gpt-4.5-preview'),  prompt:'What is the weather like today in San Francisco?',  tools:{    getWeather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},});
```

In this example, the `getWeather` tool allows the model to fetch real-time weather data (simulated for simplicity), enhancing its ability to provide accurate and up-to-date information.


### [Building Interactive Interfaces](#building-interactive-interfaces)


AI SDK Core can be paired with [AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — [`useChat`](/docs/reference/ai-sdk-ui/use-chat), [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), [`useObject`](/docs/reference/ai-sdk-ui/use-object), and [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with [Next.js](https://nextjs.org), the AI SDK, and OpenAI GPT-4.5:

In a new Next.js application, first install the AI SDK and the OpenAI provider:

npm install ai @ai-sdk/openai

Then, create a route handler for the chat endpoint:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';// Allow responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4.5-preview'),    messages,});return result.toDataStreamResponse();}
```

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit, error }=useChat();return(<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.content}</div>))}<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button></form></>);}
```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.


## [Get Started](#get-started)


Ready to get started? Here's how you can dive in:

1.  Explore the documentation at [ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.
2.  Check out practical examples at [ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.
3.  Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at [ai-sdk.dev/docs/guides](/docs/guides).
4.  Check out ready-to-deploy AI templates at [vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).
```

### 136. `docs/guides/llama-3_1.md`

```markdown
# Get started with Llama 3.1


---
url: https://ai-sdk.dev/docs/guides/llama-3_1
description: Get started with Llama 3.1 using the AI SDK.
---


# [Get started with Llama 3.1](#get-started-with-llama-31)


The current generation of Llama models is 3.3. Please note that while this guide focuses on Llama 3.1, the newer Llama 3.3 models are now available and may offer improved capabilities. The concepts and integration techniques described here remain applicable, though you may want to use the latest generation models for optimal performance.

With the [release of Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/), there has never been a better time to start building AI applications.

The [AI SDK](/) is a powerful TypeScript toolkit for building AI application with large language models (LLMs) like Llama 3.1 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more


## [Llama 3.1](#llama-31)


The release of Meta's Llama 3.1 is an important moment in AI development. As the first state-of-the-art open weight AI model, Llama 3.1 is helping accelerate developers building AI apps. Available in 8B, 70B, and 405B sizes, these instruction-tuned models work well for tasks like dialogue generation, translation, reasoning, and code generation.


## [Benchmarks](#benchmarks)


Llama 3.1 surpasses most available open-source chat models on common industry benchmarks and even outperforms some closed-source models, offering superior performance in language nuances, contextual understanding, and complex multi-step tasks. The models' refined post-training processes significantly improve response alignment, reduce false refusal rates, and enhance answer diversity, making Llama 3.1 a powerful and accessible tool for building generative AI applications.

Source: [Meta AI - Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)


## [Choosing Model Size](#choosing-model-size)


Llama 3.1 includes a new 405B parameter model, becoming the largest open-source model available today. This model is designed to handle the most complex and demanding tasks.

When choosing between the different sizes of Llama 3.1 models (405B, 70B, 8B), consider the trade-off between performance and computational requirements. The 405B model offers the highest accuracy and capability for complex tasks but requires significant computational resources. The 70B model provides a good balance of performance and efficiency for most applications, while the 8B model is suitable for simpler tasks or resource-constrained environments where speed and lower computational overhead are priorities.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is [AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call Llama 3.1 (using [DeepInfra](https://deepinfra.com)) with the AI SDK:

```
import{ generateText }from'ai';import{ deepinfra }from'@ai-sdk/deepinfra';const{ text }=awaitgenerateText({  model:deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),  prompt:'What is love?',});
```

Llama 3.1 is available to use with many AI SDK providers including [DeepInfra](/providers/ai-sdk-providers/deepinfra), [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock), [Baseten](/providers/openai-compatible-providers/baseten) [Fireworks](/providers/ai-sdk-providers/fireworks), and more.

AI SDK Core abstracts away the differences between model providers, allowing you to focus on building great applications. Prefer to use [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock)? The unified interface also means that you can easily switch between models by changing just two lines of code.

```
import{ generateText }from'ai';import{ bedrock }from'@ai-sdk/amazon-bedrock';const{ text }=awaitgenerateText({  model:bedrock('meta.llama3-1-405b-instruct-v1'),  prompt:'What is love?',});
```


### [Streaming the Response](#streaming-the-response)


To stream the model's response as it's being generated, update your code snippet to use the [`streamText`](/docs/reference/ai-sdk-core/stream-text) function.

```
import{ streamText }from'ai';import{ deepinfra }from'@ai-sdk/deepinfra';const{ textStream }=streamText({  model:deepinfra('meta-llama/Meta-Llama-3.1-405B-Instruct'),  prompt:'What is love?',});
```


### [Generating Structured Data](#generating-structured-data)


While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions ([`generateObject`](/docs/reference/ai-sdk-core/generate-object) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```
import{ generateObject }from'ai';import{ deepinfra }from'@ai-sdk/deepinfra';import{ z }from'zod';const{ object }=awaitgenerateObject({  model:deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.object({ name: z.string(), amount: z.string()})),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});
```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.


### [Tools](#tools)


While LLMs have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). The solution: tools, which are like programs that you provide to the model, which it can choose to call as necessary.


### [Using Tools with the AI SDK](#using-tools-with-the-ai-sdk)


The AI SDK supports tool usage across several of its functions, including [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui). By passing one or more tools to the `tools` parameter, you can extend the capabilities of LLMs, allowing them to perform discrete tasks and interact with external systems.

Here's an example of how you can use a tool with the AI SDK and Llama 3.1:

```
import{ generateText, tool }from'ai';import{ deepinfra }from'@ai-sdk/deepinfra';import{ getWeather }from'./weatherTool';const{ text }=awaitgenerateText({  model:deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),  prompt:'What is the weather like today?',  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},});
```

In this example, the `getWeather` tool allows the model to fetch real-time weather data, enhancing its ability to provide accurate and up-to-date information.


### [Agents](#agents)


Agents take your AI applications a step further by allowing models to execute multiple steps (i.e. tools) in a non-deterministic way, making decisions based on context and user input.

Agents use LLMs to choose the next step in a problem-solving process. They can reason at each step and make decisions based on the evolving context.


### [Implementing Agents with the AI SDK](#implementing-agents-with-the-ai-sdk)


The AI SDK supports agent implementation through the `maxSteps` parameter. This allows the model to make multiple decisions and tool calls in a single interaction.

Here's an example of an agent that solves math problems:

```
import{ generateText, tool }from'ai';import{ deepinfra }from'@ai-sdk/deepinfra';import*as mathjs from'mathjs';import{ z }from'zod';const problem ='Calculate the profit for a day if revenue is $5000 and expenses are $3500.';const{ text: answer }=awaitgenerateText({  model:deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),  system:'You are solving math problems. Reason step by step. Use the calculator when necessary.',  prompt: problem,  tools:{    calculate:tool({      description:'A tool for evaluating mathematical expressions.',      parameters: z.object({ expression: z.string()}),execute:async({ expression })=> mathjs.evaluate(expression),}),},  maxSteps:5,});
```

In this example, the agent can use the calculator tool multiple times if needed, reasoning through the problem step by step.


### [Building Interactive Interfaces](#building-interactive-interfaces)


AI SDK Core can be paired with [AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — [`useChat`](/docs/reference/ai-sdk-ui/use-chat), [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), [`useObject`](/docs/reference/ai-sdk-ui/use-object), and [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with [Next.js](https://nextjs.org), the AI SDK, and Llama 3.1 (via [DeepInfra](https://deepinfra.com)):

app/api/chat/route.ts

```
import{ streamText }from'ai';import{ deepinfra }from'@ai-sdk/deepinfra';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),    system:'You are a helpful assistant.',    messages,});return result.toDataStreamResponse();}
```

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.content}</div>))}<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button></form></>);}
```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then streamed back in real-time and displayed in the chat UI.

This enables a seamless chat experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.


### [Going Beyond Text](#going-beyond-text)


The AI SDK's React Server Components (RSC) API enables you to create rich, interactive interfaces that go beyond simple text generation. With the [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function, you can dynamically stream React components from the server to the client.

Let's dive into how you can leverage tools with [AI SDK RSC](/docs/ai-sdk-rsc/overview) to build a generative user interface with Next.js (App Router).

First, create a Server Action.

app/actions.tsx

```
'use server';import{ streamUI }from'ai/rsc';import{ deepinfra }from'@ai-sdk/deepinfra';import{ z }from'zod';exportasyncfunctionstreamComponent(){const result =awaitstreamUI({    model:deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),    prompt:'Get the weather for San Francisco',text:({ content })=><div>{content}</div>,    tools:{      getWeather:{        description:'Get the weather for a location',        parameters: z.object({location: z.string()}),generate:asyncfunction*({location}){yield<div>loading...</div>;const weather ='25c';// await getWeather(location);return(<div>              the weather in{location}is{weather}.</div>);},},},});return result.value;}
```

In this example, if the model decides to use the `getWeather` tool, it will first yield a `div` while fetching the weather data, then return a weather component with the fetched data (note: static data in this example). This allows for a more dynamic and responsive UI that can adapt based on the AI's decisions and external data.

On the frontend, you can call this Server Action like any other asynchronous function in your application. In this case, the function returns a regular React component.

app/page.tsx

```
'use client';import{ useState }from'react';import{ streamComponent }from'./actions';exportdefaultfunctionPage(){const[component, setComponent]=useState<React.ReactNode>();return(<div><formonSubmit={asynce=>{          e.preventDefault();setComponent(awaitstreamComponent());}}><button>StreamComponent</button></form><div>{component}</div></div>);}
```

To see AI SDK RSC in action, check out our open-source [Next.js Gemini Chatbot](https://gemini.vercel.ai/).


## [Migrate from OpenAI](#migrate-from-openai)


One of the key advantages of the AI SDK is its unified API, which makes it incredibly easy to switch between different AI models and providers. This flexibility is particularly useful when you want to migrate from one model to another, such as moving from OpenAI's GPT models to Meta's Llama models hosted on DeepInfra.

Here's how simple the migration process can be:

**OpenAI Example:**

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model:openai('gpt-4-turbo'),  prompt:'What is love?',});
```

**Llama on DeepInfra Example:**

```
import{ generateText }from'ai';import{ deepinfra }from'@ai-sdk/deepinfra';const{ text }=awaitgenerateText({  model:deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),  prompt:'What is love?',});
```

Thanks to the unified API, the core structure of the code remains the same. The main differences are:

1.  Creating a DeepInfra client
2.  Changing the model name from `openai("gpt-4-turbo")` to `deepinfra("meta-llama/Meta-Llama-3.1-70B-Instruct")`.

With just these few changes, you've migrated from using OpenAI's GPT-4-Turbo to Meta's Llama 3.1 hosted on DeepInfra. The `generateText` function and its usage remain identical, showcasing the power of the AI SDK's unified API.

This feature allows you to easily experiment with different models, compare their performance, and choose the best one for your specific use case without having to rewrite large portions of your codebase.


## [Prompt Engineering and Fine-tuning](#prompt-engineering-and-fine-tuning)


While the Llama 3.1 family of models are powerful out-of-the-box, their performance can be enhanced through effective prompt engineering and fine-tuning techniques.


### [Prompt Engineering](#prompt-engineering)


Prompt engineering is the practice of crafting input prompts to elicit desired outputs from language models. It involves structuring and phrasing prompts in ways that guide the model towards producing more accurate, relevant, and coherent responses.

For more information on prompt engineering techniques (specific to Llama models), check out these resources:

-   [Official Llama 3.1 Prompt Guide](https://llama.meta.com/docs/how-to-guides/prompting)
-   [Prompt Engineering with Llama 3](https://github.com/amitsangani/Llama/blob/main/Llama_3_Prompt_Engineering.ipynb)
-   [How to prompt Llama 3](https://huggingface.co/blog/llama3#how-to-prompt-llama-3)


### [Fine-tuning](#fine-tuning)


Fine-tuning involves further training a pre-trained model on a specific dataset or task to customize its performance for particular use cases. This process allows you to adapt Llama 3.1 to your specific domain or application, potentially improving its accuracy and relevance for your needs.

To learn more about fine-tuning Llama models, check out these resources:

-   [Official Fine-tuning Llama Guide](https://llama.meta.com/docs/how-to-guides/fine-tuning)
-   [Fine-tuning and Inference with Llama 3](https://docs.inferless.com/how-to-guides/how-to-finetune--and-inference-llama3)
-   [Fine-tuning Models with Fireworks AI](https://docs.fireworks.ai/fine-tuning/fine-tuning-models)
-   [Fine-tuning Llama with Modal](https://modal.com/docs/examples/llm-finetuning)


## [Conclusion](#conclusion)


The AI SDK offers a powerful and flexible way to integrate cutting-edge AI models like Llama 3.1 into your applications. With AI SDK Core, you can seamlessly switch between different AI models and providers by changing just two lines of code. This flexibility allows for quick experimentation and adaptation, reducing the time required to change models from days to minutes.

The AI SDK ensures that your application remains clean and modular, accelerating development and future-proofing against the rapidly evolving landscape.

Ready to get started? Here's how you can dive in:

1.  Explore the documentation at [ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.
2.  Check out practical examples at [ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.
3.  Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at [ai-sdk.dev/docs/guides](/docs/guides).
4.  Check out ready-to-deploy AI templates at [vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).
```

### 137. `docs/guides/multi-modal-chatbot.md`

```markdown
# Multi-Modal Chatbot


---
url: https://ai-sdk.dev/docs/guides/multi-modal-chatbot
description: Learn how to build a multi-modal chatbot that can process images and PDFs with the AI SDK.
---


# [Multi-Modal Chatbot](#multi-modal-chatbot)


In this guide, you will build a multi-modal AI-chatbot capable of understanding both images and PDFs.

Multi-modal refers to the ability of the chatbot to understand and generate responses in multiple formats, such as text, images, PDFs, and videos. In this example, we will focus on sending images and PDFs and generating text-based responses.

Different AI providers have varying levels of multi-modal support, for example:

-   OpenAI (GPT-4o): Supports image input
-   Anthropic (Sonnet 3.5): Supports image and PDF input
-   Google (Gemini 2.0): Supports image and PDF input

For a complete list of providers that support both image and PDF inputs, visit the [providers documentation](/providers/ai-sdk-providers).

We'll first build a chatbot capable of generating responses based on an image input using OpenAI, then show how to switch providers to handle PDFs.


## [Prerequisites](#prerequisites)


To follow this quickstart, you'll need:

-   Node.js 18+ and pnpm installed on your local development machine.
-   An OpenAI API key.
-   An Anthropic API Key.

If you haven't obtained your OpenAI API key, you can do so by [signing up](https://platform.openai.com/signup/) on the OpenAI website.

If you haven't obtained your Anthropic API key, you can do so by [signing up](https://console.anthropic.com/) on Anthropic's website.


## [Create Your Application](#create-your-application)


Start by creating a new Next.js application. This command will create a new directory named `multi-modal-chatbot` and set up a basic Next.js application inside it.

Be sure to select yes when prompted to use the App Router. If you are looking for the Next.js Pages Router quickstart guide, you can find it [here](/docs/getting-started/nextjs-pages-router).

pnpm create next-app@latest multi-modal-chatbot

Navigate to the newly created directory:

cd multi-modal-chatbot


### [Install dependencies](#install-dependencies)


Install `ai` and `@ai-sdk/openai`, the Vercel AI package and the AI SDK's [OpenAI provider](/providers/ai-sdk-providers/openai) respectively.

The AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about [available providers](/providers) and [building custom providers](/providers/community-providers/custom-providers) in the [providers](/providers) section.

pnpm

npm

yarn

pnpm add ai @ai-sdk/react @ai-sdk/openai

Make sure you are using `ai` version 3.2.27 or higher.


### [Configure OpenAI API key](#configure-openai-api-key)


Create a `.env.local` file in your project root and add your OpenAI API Key. This key is used to authenticate your application with the OpenAI service.

touch .env.local

Edit the `.env.local` file:

.env.local

```
OPENAI_API_KEY=xxxxxxxxx
```

Replace `xxxxxxxxx` with your actual OpenAI API key.

The AI SDK's OpenAI Provider will default to using the `OPENAI_API_KEY` environment variable.


## [Implementation Plan](#implementation-plan)


To build a multi-modal chatbot, you will need to:

-   Create a Route Handler to handle incoming chat messages and generate responses.
-   Wire up the UI to display chat messages, provide a user input, and handle submitting new messages.
-   Add the ability to upload images and attach them alongside the chat messages.


## [Create a Route Handler](#create-a-route-handler)


Create a route handler, `app/api/chat/route.ts` and add the following code:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText,Message}from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }:{ messages:Message[]}=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse();}
```

Let's take a look at what is happening in this code:

1.  Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation.
2.  Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (imported from `@ai-sdk/openai`) and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model's behaviour.
3.  The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [`toDataStreamResponse`](/docs/reference/ai-sdk-core/stream-text#to-ai-stream-response) function which converts the result to a streamed response object.
4.  Finally, return the result to the client to stream the response.

This Route Handler creates a POST request endpoint at `/api/chat`.


## [Wire up the UI](#wire-up-the-ui)


Now that you have a Route Handler that can query a large language model (LLM), it's time to setup your frontend. [AI SDK UI](/docs/ai-sdk-ui) abstracts the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).

Update your root page (`app/page.tsx`) with the following code to show a list of chat messages and provide a user message input:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages.map(m=>(<divkey={m.id}className="whitespace-pre-wrap">{m.role ==='user'?'User: ':'AI: '}{m.content}</div>))}<formonSubmit={handleSubmit}className="fixed bottom-0 w-full max-w-md mb-8 border border-gray-300 rounded shadow-xl"><inputclassName="w-full p-2"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

Make sure you add the `"use client"` directive to the top of your file. This allows you to add interactivity with Javascript.

This page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:

-   `messages` - the current chat messages (an array of objects with `id`, `role`, and `content` properties).
-   `input` - the current value of the user's input field.
-   `handleInputChange` and `handleSubmit` - functions to handle user interactions (typing into the input field and submitting the form, respectively).
-   `status` - the status of the API request.


## [Add Image Upload](#add-image-upload)


To make your chatbot multi-modal, let's add the ability to upload and send images to the model. There are two ways to send attachments alongside a message with the `useChat` hook: by [providing a `FileList` object](/docs/ai-sdk-ui/chatbot#filelist) or a [list of URLs](/docs/ai-sdk-ui/chatbot#urls) to the `handleSubmit` function. In this guide, you will be using the `FileList` approach as it does not require any additional setup.

Update your root page (`app/page.tsx`) with the following code:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{ useRef, useState }from'react';importImagefrom'next/image';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();const[files, setFiles]=useState<FileList|undefined>(undefined);const fileInputRef =useRef<HTMLInputElement>(null);return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages.map(m=>(<divkey={m.id}className="whitespace-pre-wrap">{m.role ==='user'?'User: ':'AI: '}{m.content}<div>{m?.experimental_attachments?.filter(attachment=>                attachment?.contentType?.startsWith('image/'),).map((attachment, index)=>(<Imagekey={`${m.id}-${index}`}src={attachment.url}width={500}height={500}alt={attachment.name ??`attachment-${index}`}/>))}</div></div>))}<formclassName="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2"onSubmit={event=>{handleSubmit(event,{            experimental_attachments: files,});setFiles(undefined);if(fileInputRef.current){            fileInputRef.current.value ='';}}}><inputtype="file"className=""onChange={event=>{if(event.target.files){setFiles(event.target.files);}}}multipleref={fileInputRef}/><inputclassName="w-full p-2"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

In this code, you:

1.  Create state to hold the files and create a ref to the file input field.
2.  Display the "uploaded" files in the UI.
3.  Update the `onSubmit` function, to call the `handleSubmit` function manually, passing the files as an option using the `experimental_attachments` key.
4.  Add a file input field to the form, including an `onChange` handler to handle updating the files state.


## [Running Your Application](#running-your-application)


With that, you have built everything you need for your multi-modal chatbot! To start your application, use the command:

pnpm run dev

Head to your browser and open [http://localhost:3000](http://localhost:3000). You should see an input field and a button to upload an image.

Upload an image and ask the model to describe what it sees. Watch as the model's response is streamed back to you!


## [Working with PDFs](#working-with-pdfs)


To enable PDF support, you can switch to a provider that handles PDFs like Google's Gemini or Anthropic's Claude. Here's how to modify the code to use Anthropic:

1.  First install the Anthropic provider:

pnpm add @ai-sdk/anthropic

2.  Update your environment variables:

.env.local

```
OPENAI_API_KEY=xxxxxxxxxANTHROPIC_API_KEY=xxxxxxxxx
```

3.  Modify your route handler:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ anthropic }from'@ai-sdk/anthropic';import{ streamText,typeMessage}from'ai';exportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }:{ messages:Message[]}=await req.json();// check if user has sent a PDFconst messagesHavePDF = messages.some(message=>    message.experimental_attachments?.some(a=> a.contentType ==='application/pdf',),);const result =streamText({    model: messagesHavePDF?anthropic('claude-3-5-sonnet-latest'):openai('gpt-4o'),    messages,});return result.toDataStreamResponse();}
```

Now your chatbot can process both images and PDFs! You automatically route PDF requests to Claude Sonnet 3.5 and image requests to OpenAI's gpt-4o model.

Finally, to display PDFs in your chat interface, update the message rendering code in your frontend to show PDF attachments in an `<iframe>`:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';import{ useRef, useState }from'react';importImagefrom'next/image';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();const[files, setFiles]=useState<FileList|undefined>(undefined);const fileInputRef =useRef<HTMLInputElement>(null);return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch">{messages.map(m=>(<divkey={m.id}className="whitespace-pre-wrap">{m.role ==='user'?'User: ':'AI: '}{m.content}<div>{m?.experimental_attachments?.filter(attachment=>                  attachment?.contentType?.startsWith('image/')|                  attachment?.contentType?.startsWith('application/pdf'),).map((attachment, index)=>                attachment.contentType?.startsWith('image/')?(<Imagekey={`${m.id}-${index}`}src={attachment.url}width={500}height={500}alt={attachment.name ??`attachment-${index}`}/>): attachment.contentType?.startsWith('application/pdf')?(<iframekey={`${m.id}-${index}`}src={attachment.url}width={500}height={600}title={attachment.name ??`attachment-${index}`}/>):null,)}</div></div>))}<formclassName="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl space-y-2"onSubmit={event=>{handleSubmit(event,{            experimental_attachments: files,});setFiles(undefined);if(fileInputRef.current){            fileInputRef.current.value ='';}}}><inputtype="file"className=""onChange={event=>{if(event.target.files){setFiles(event.target.files);}}}multipleref={fileInputRef}/><inputclassName="w-full p-2"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

Try uploading a PDF and asking questions about its contents.

When switching providers, be sure to check the [provider documentation](/providers/ai-sdk-providers) for specific file size limits and supported file types.


## [Where to Next?](#where-to-next)


You've built a multi-modal AI chatbot using the AI SDK! Experiment and extend the functionality of this application further by exploring [tool calling](/docs/ai-sdk-core/tools-and-tool-calling) or introducing more granular control over [AI and UI states](/docs/ai-sdk-rsc/generative-ui-state).

If you are looking to leverage the broader capabilities of LLMs, Vercel [AI SDK Core](/docs/ai-sdk-core) provides a comprehensive set of lower-level tools and APIs that will help you unlock a wider range of AI functionalities beyond the chatbot paradigm.
```

### 138. `docs/guides/natural-language-postgres.md`

```markdown
# Natural Language Postgres Guide


---
url: https://ai-sdk.dev/docs/guides/natural-language-postgres
description: Learn how to build a Next.js app that lets you talk to a PostgreSQL database in natural language.
---


# [Natural Language Postgres Guide](#natural-language-postgres-guide)


In this guide, you will learn how to build an app that uses AI to interact with a PostgreSQL database using natural language.

The application will:

-   Generate SQL queries from a natural language input
-   Explain query components in plain English
-   Create a chart to visualise query results

You can find a completed version of this project at [natural-language-postgres.vercel.app](https://natural-language-postgres.vercel.app).


## [Project setup](#project-setup)


This project uses the following stack:

-   [Next.js](https://nextjs.org) (App Router)
-   [AI SDK](/docs)
-   [OpenAI](https://openai.com)
-   [Zod](https://zod.dev)
-   [Postgres](https://www.postgresql.org/) with [Vercel Postgres](https://vercel.com/postgres)
-   [shadcn-ui](https://ui.shadcn.com) and [TailwindCSS](https://tailwindcss.com) for styling
-   [Recharts](https://recharts.org) for data visualization


### [Clone repo](#clone-repo)


To focus on the AI-powered functionality rather than project setup and configuration we've prepared a starter repository which includes a database schema and a few components.

Clone the starter repository and check out the `starter` branch:

git clone https://github.com/vercel-labs/natural-language-postgres

cd natural-language-postgres

git checkout starter


### [Project setup and data](#project-setup-and-data)


Let's set up the project and seed the database with the dataset:

1.  Install dependencies:

pnpm install

2.  Copy the example environment variables file:

cp .env.example .env

3.  Add your environment variables to `.env`:

.env

```
OPENAI_API_KEY="your_api_key_here"POSTGRES_URL="..."POSTGRES_PRISMA_URL="..."POSTGRES_URL_NO_SSL="..."POSTGRES_URL_NON_POOLING="..."POSTGRES_USER="..."POSTGRES_HOST="..."POSTGRES_PASSWORD="..."POSTGRES_DATABASE="..."
```

This project uses Vercel Postgres. You can learn more about how to set up at the [Vercel Postgres documentation](https://vercel.com/postgres).

4.  This project uses CB Insights' Unicorn Companies dataset. You can download the dataset by following these instructions:
    -   Navigate to [CB Insights Unicorn Companies](https://www.cbinsights.com/research-unicorn-companies)
    -   Enter in your email. You will receive a link to download the dataset.
    -   Save it as `unicorns.csv` in your project root


### [About the dataset](#about-the-dataset)


The Unicorn List dataset contains the following information about unicorn startups (companies with a valuation above $1bn):

-   Company name
-   Valuation
-   Date joined (unicorn status)
-   Country
-   City
-   Industry
-   Select investors

This dataset contains over 1000 rows of data over 7 columns, giving us plenty of structured data to analyze. This makes it perfect for exploring various SQL queries that can reveal interesting insights about the unicorn startup ecosystem.

5.  Now that you have the dataset downloaded and added to your project, you can initialize the database with the following command:

pnpm run seed

Note: this step can take a little while. You should see a message indicating the Unicorns table has been created and then that the database has been seeded successfully.

Remember, the dataset should be named `unicorns.csv` and located in root of your project.

6.  Start the development server:

pnpm run dev

Your application should now be running at [http://localhost:3000](http://localhost:3000).


## [Project structure](#project-structure)


The starter repository already includes everything that you will need, including:

-   Database seed script (`lib/seed.ts`)
-   Basic components built with shadcn/ui (`components/`)
-   Function to run SQL queries (`app/actions.ts`)
-   Type definitions for the database schema (`lib/types.ts`)


### [Existing components](#existing-components)


The application contains a single page in `app/page.tsx` that serves as the main interface.

At the top, you'll find a header (`header.tsx`) displaying the application title and description. Below that is an input field and search button (`search.tsx`) where you can enter natural language queries.

Initially, the page shows a collection of suggested example queries (`suggested-queries.tsx`) that you can click to quickly try out the functionality.

When you submit a query:

-   The suggested queries section disappears and a loading state appears
-   Once complete, a card appears with "TODO - IMPLEMENT ABOVE" (`query-viewer.tsx`) which will eventually show your generated SQL
-   Below that is an empty results area with "No results found" (`results.tsx`)

After you implement the core functionality:

-   The results section will display data in a table format
-   A toggle button will allow switching between table and chart views
-   The chart view will visualize your query results

Let's implement the AI-powered functionality to bring it all together.


## [Building the application](#building-the-application)


As a reminder, this application will have three main features:

1.  Generate SQL queries from natural language
2.  Create a chart from the query results
3.  Explain SQL queries in plain English

For each of these features, you'll use the AI SDK via [Server Actions](https://react.dev/reference/rsc/server-actions) to interact with OpenAI's GPT-4o and GPT-4o-mini models. Server Actions are a powerful React Server Component feature that allows you to call server-side functions directly from your frontend code.

Let's start with generating a SQL query from natural language.


## [Generate SQL queries](#generate-sql-queries)



### [Providing context](#providing-context)


For the model to generate accurate SQL queries, it needs context about your database schema, tables, and relationships. You will communicate this information through a prompt that should include:

1.  Schema information
2.  Example data formats
3.  Available SQL operations
4.  Best practices for query structure
5.  Nuanced advice for specific fields

Let's write a prompt that includes all of this information:

```
You are a SQL (postgres) and data visualization expert. Your job is to help the user write a SQL query to retrieve the data they need. The table schema is as follows:unicorns (  id SERIAL PRIMARY KEY,  company VARCHAR(255) NOT NULL UNIQUE,  valuation DECIMAL(10, 2) NOT NULL,  date_joined DATE,  country VARCHAR(255) NOT NULL,  city VARCHAR(255) NOT NULL,  industry VARCHAR(255) NOT NULL,  select_investors TEXT NOT NULL);Only retrieval queries are allowed.For things like industry, company names and other string fields, use the ILIKE operator and convert both the search term and the field to lowercase using LOWER() function. For example: LOWER(industry) ILIKE LOWER('%search_term%').Note: select_investors is a comma-separated list of investors. Trim whitespace to ensure you're grouping properly. Note, some fields may be null or have only one value.When answering questions about a specific field, ensure you are selecting the identifying column (ie. what is Vercel's valuation would select company and valuation').The industries available are:- healthcare & life sciences- consumer & retail- financial services- enterprise tech- insurance- media & entertainment- industrials- healthIf the user asks for a category that is not in the list, infer based on the list above.Note: valuation is in billions of dollars so 10b would be 10.0.Note: if the user asks for a rate, return it as a decimal. For example, 0.1 would be 10%.If the user asks for 'over time' data, return by year.When searching for UK or USA, write out United Kingdom or United States respectively.EVERY QUERY SHOULD RETURN QUANTITATIVE DATA THAT CAN BE PLOTTED ON A CHART! There should always be at least two columns. If the user asks for a single column, return the column and the count of the column. If the user asks for a rate, return the rate as a decimal. For example, 0.1 would be 10%.
```

There are several important elements of this prompt:

-   Schema description helps the model understand exactly what data fields to work with
-   Includes rules for handling queries based on common SQL patterns - for example, always using ILIKE for case-insensitive string matching
-   Explains how to handle edge cases in the dataset, like dealing with the comma-separated investors field and ensuring whitespace is properly handled
-   Instead of having the model guess at industry categories, it provides the exact list that exists in the data, helping avoid mismatches
-   The prompt helps standardize data transformations - like knowing to interpret "10b" as "10.0" billion dollars, or that rates should be decimal values
-   Clear rules ensure the query output will be chart-friendly by always including at least two columns of data that can be plotted

This prompt structure provides a strong foundation for query generation, but you should experiment and iterate based on your specific needs and the model you're using.


### [Create a Server Action](#create-a-server-action)


With the prompt done, let's create a Server Action.

Open `app/actions.ts`. You should see one action already defined (`runGeneratedSQLQuery`).

Add a new action. This action should be asynchronous and take in one parameter - the natural language query.

app/actions.ts

```
/* ...rest of the file... */exportconstgenerateQuery=async(input:string)=>{};
```

In this action, you'll use the `generateObject` function from the AI SDK which allows you to constrain the model's output to a pre-defined schema. This process, sometimes called structured output, ensures the model returns only the SQL query without any additional prefixes, explanations, or formatting that would require manual parsing.

app/actions.ts

```
/* ...other imports... */import{ generateObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';/* ...rest of the file... */exportconstgenerateQuery=async(input:string)=>{'use server';try{const result =awaitgenerateObject({      model:openai('gpt-4o'),      system:`You are a SQL (postgres) ...`,// SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY      prompt:`Generate the query necessary to retrieve the data the user wants: ${input}`,      schema: z.object({        query: z.string(),}),});return result.object.query;}catch(e){console.error(e);thrownewError('Failed to generate query');}};
```

Note, you are constraining the output to a single string field called `query` using `zod`, a TypeScript schema validation library. This will ensure the model only returns the SQL query itself. The resulting generated query will then be returned.


### [Update the frontend](#update-the-frontend)


With the Server Action in place, you can now update the frontend to call this action when the user submits a natural language query. In the root page (`app/page.tsx`), you should see a `handleSubmit` function that is called when the user submits a query.

Import the `generateQuery` function and call it with the user's input.

app/page.tsx

```
/* ...other imports... */import{ runGeneratedSQLQuery, generateQuery }from'./actions';/* ...rest of the file... */consthandleSubmit=async(suggestion?:string)=>{clearExistingData();const question = suggestion ?? inputValue;if(inputValue.length===0&&!suggestion)return;if(question.trim()){setSubmitted(true);}setLoading(true);setLoadingStep(1);setActiveQuery('');try{const query =awaitgenerateQuery(question);if(query ===undefined){      toast.error('An error occurred. Please try again.');setLoading(false);return;}setActiveQuery(query);setLoadingStep(2);const companies =awaitrunGeneratedSQLQuery(query);const columns = companies.length>0?Object.keys(companies[0]):[];setResults(companies);setColumns(columns);setLoading(false);}catch(e){    toast.error('An error occurred. Please try again.');setLoading(false);}};/* ...rest of the file... */
```

Now, when the user submits a natural language query (ie. "how many unicorns are from San Francisco?"), that question will be sent to your newly created Server Action. The Server Action will call the model, passing in your system prompt and the users query, and return the generated SQL query in a structured format. This query is then passed to the `runGeneratedSQLQuery` action to run the query against your database. The results are then saved in local state and displayed to the user.

Save the file, make sure the dev server is running, and then head to `localhost:3000` in your browser. Try submitting a natural language query and see the generated SQL query and results. You should see a SQL query generated and displayed under the input field. You should also see the results of the query displayed in a table below the input field.

Try clicking the SQL query to see the full query if it's too long to display in the input field. You should see a button on the right side of the input field with a question mark icon. Clicking this button currently does nothing, but you'll add the "explain query" functionality to it in the next step.


## [Explain SQL Queries](#explain-sql-queries)


Next, let's add the ability to explain SQL queries in plain English. This feature helps users understand how the generated SQL query works by breaking it down into logical sections. As with the SQL query generation, you'll need a prompt to guide the model when explaining queries.

Let's craft a prompt for the explain query functionality:

```
You are a SQL (postgres) expert. Your job is to explain to the user write a SQL query you wrote to retrieve the data they asked for. The table schema is as follows:unicorns (  id SERIAL PRIMARY KEY,  company VARCHAR(255) NOT NULL UNIQUE,  valuation DECIMAL(10, 2) NOT NULL,  date_joined DATE,  country VARCHAR(255) NOT NULL,  city VARCHAR(255) NOT NULL,  industry VARCHAR(255) NOT NULL,  select_investors TEXT NOT NULL);When you explain you must take a section of the query, and then explain it. Each "section" should be unique. So in a query like: "SELECT * FROM unicorns limit 20", the sections could be "SELECT *", "FROM UNICORNS", "LIMIT 20".If a section doesn't have any explanation, include it, but leave the explanation empty.
```

Like the prompt for generating SQL queries, you provide the model with the schema of the database. Additionally, you provide an example of what each section of the query might look like. This helps the model understand the structure of the query and how to break it down into logical sections.


### [Create a Server Action](#create-a-server-action-1)


Add a new Server Action to generate explanations for SQL queries.

This action takes two parameters - the original natural language input and the generated SQL query.

app/actions.ts

```
/* ...rest of the file... */exportconstexplainQuery=async(input:string, sqlQuery:string)=>{'use server';try{const result =awaitgenerateObject({      model:openai('gpt-4o'),      system:`You are a SQL (postgres) expert. ...`,// SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY      prompt:`Explain the SQL query you generated to retrieve the data the user wanted. Assume the user is not an expert in SQL. Break down the query into steps. Be concise.      User Query:${input}      Generated SQL Query:${sqlQuery}`,});return result.object;}catch(e){console.error(e);thrownewError('Failed to generate query');}};
```

This action uses the `generateObject` function again. However, you haven't defined the schema yet. Let's define it in another file so it can also be used as a type in your components.

Update your `lib/types.ts` file to include the schema for the explanations:

lib/types.ts

```
import{ z }from'zod';/* ...rest of the file... */exportconst explanationSchema = z.object({  section: z.string(),  explanation: z.string(),});exporttypeQueryExplanation= z.infer<typeof explanationSchema>;
```

This schema defines the structure of the explanation that the model will generate. Each explanation will have a `section` and an `explanation`. The `section` is the part of the query being explained, and the `explanation` is the plain English explanation of that section. Go back to your `actions.ts` file and import and use the `explanationSchema`:

app/actions.ts

```
// other importsimport{ explanationSchema }from'@/lib/types';/* ...rest of the file... */exportconstexplainQuery=async(input:string, sqlQuery:string)=>{'use server';try{const result =awaitgenerateObject({      model:openai('gpt-4o'),      system:`You are a SQL (postgres) expert. ...`,// SYSTEM PROMPT AS ABOVE - OMITTED FOR BREVITY      prompt:`Explain the SQL query you generated to retrieve the data the user wanted. Assume the user is not an expert in SQL. Break down the query into steps. Be concise.      User Query:${input}      Generated SQL Query:${sqlQuery}`,      schema: explanationSchema,      output:'array',});return result.object;}catch(e){console.error(e);thrownewError('Failed to generate query');}};
```

You can use `output: "array"` to indicate to the model that you expect an array of objects matching the schema to be returned.


### [Update query viewer](#update-query-viewer)


Next, update the `query-viewer.tsx` component to display these explanations. The `handleExplainQuery` function is called every time the user clicks the question icon button on the right side of the query. Let's update this function to use the new `explainQuery` action:

components/query-viewer.tsx

```
/* ...other imports... */import{ explainQuery }from'@/app/actions';/* ...rest of the component... */consthandleExplainQuery=async()=>{setQueryExpanded(true);setLoadingExplanation(true);const explanations =awaitexplainQuery(inputValue, activeQuery);setQueryExplanations(explanations);setLoadingExplanation(false);};/* ...rest of the component... */
```

Now when users click the explanation button (the question mark icon), the component will:

1.  Show a loading state
2.  Send the active SQL query and the users natural language query to your Server Action
3.  The model will generate an array of explanations
4.  The explanations will be set in the component state and rendered in the UI

Submit a new query and then click the explanation button. Hover over different elements of the query. You should see the explanations for each section!


## [Visualizing query results](#visualizing-query-results)


Finally, let's render the query results visually in a chart. There are two approaches you could take:

1.  Send both the query and data to the model and ask it to return the data in a visualization-ready format. While this provides complete control over the visualization, it requires the model to send back all of the data, which significantly increases latency and costs.

2.  Send the query and data to the model and ask it to generate a chart configuration (fixed-size and not many tokens) that maps your data appropriately. This configuration specifies how to visualize the information while delivering the insights from your natural language query. Importnatly, this is done without requiring the model return the full dataset.


Since you don't know the SQL query or data shape beforehand, let's use the second approach to dynamically generate chart configurations based on the query results and user intent.


### [Generate the chart configuration](#generate-the-chart-configuration)


For this feature, you'll create a Server Action that takes the query results and the user's original natural language query to determine the best visualization approach. Your application is already set up to use `shadcn` charts (which uses [`Recharts`](https://recharts.org/en-US/) under the hood) so the model will need to generate:

-   Chart type (bar, line, area, or pie)
-   Axis mappings
-   Visual styling

Let's start by defining the schema for the chart configuration in `lib/types.ts`:

lib/types.ts

```
/* ...rest of the file... */exportconst configSchema = z.object({    description: z.string().describe('Describe the chart. What is it showing? What is interesting about the way the data is displayed?',),    takeaway: z.string().describe('What is the main takeaway from the chart?'),type: z.enum(['bar','line','area','pie']).describe('Type of chart'),    title: z.string(),    xKey: z.string().describe('Key for x-axis or category'),    yKeys: z.array(z.string()).describe('Key(s) for y-axis values this is typically the quantitative column',),    multipleLines: z.boolean().describe('For line charts only: whether the chart is comparing groups of data.',).optional(),    measurementColumn: z.string().describe('For line charts only: key for quantitative y-axis column to measure against (eg. values, counts etc.)',).optional(),    lineCategories: z.array(z.string()).describe('For line charts only: Categories used to compare different lines or data series. Each category represents a distinct line in the chart.',).optional(),    colors: z.record(        z.string().describe('Any of the yKeys'),        z.string().describe('Color value in CSS format (e.g., hex, rgb, hsl)'),).describe('Mapping of data keys to color values for chart elements').optional(),    legend: z.boolean().describe('Whether to show legend'),}).describe('Chart configuration object');exporttypeConfig= z.infer<typeof configSchema>;
```

Replace the existing `export type Config = any;` type with the new one.

This schema makes extensive use of Zod's `.describe()` function to give the model extra context about each of the key's you are expecting in the chart configuration. This will help the model understand the purpose of each key and generate more accurate results.

Another important technique to note here is that you are defining `description` and `takeaway` fields. Not only are these useful for the user to quickly understand what the chart means and what they should take away from it, but they also force the model to generate a description of the data first, before it attempts to generate configuration attributes like axis and columns. This will help the model generate more accurate and relevant chart configurations.


### [Create the Server Action](#create-the-server-action)


Create a new action in `app/actions.ts`:

```
/* ...other imports... */import{Config, configSchema, explanationsSchema,Result}from'@/lib/types';/* ...rest of the file... */exportconstgenerateChartConfig=async(  results:Result[],  userQuery:string,)=>{'use server';try{const{ object: config }=awaitgenerateObject({      model:openai('gpt-4o'),      system:'You are a data visualization expert.',      prompt:`Given the following data from a SQL query result, generate the chart config that best visualises the data and answers the users query.      For multiple groups use multi-lines.      Here is an example complete config:      export const chartConfig = {        type: "pie",        xKey: "month",        yKeys: ["sales", "profit", "expenses"],        colors: {          sales: "#4CAF50",    // Green for sales          profit: "#2196F3",   // Blue for profit          expenses: "#F44336"  // Red for expenses        },        legend: true      }      User Query:${userQuery}      Data:${JSON.stringify(results,null,2)}`,      schema: configSchema,});// Override with shadcn theme colorsconst colors:Record<string,string>={};    config.yKeys.forEach((key, index)=>{      colors[key]=`hsl(var(--chart-${index +1}))`;});const updatedConfig ={...config, colors };return{ config: updatedConfig };}catch(e){console.error(e);thrownewError('Failed to generate chart suggestion');}};
```


### [Update the chart component](#update-the-chart-component)


With the action in place, you'll want to trigger it automatically after receiving query results. This ensures the visualization appears almost immediately after data loads.

Update the `handleSubmit` function in your root page (`app/page.tsx`) to generate and set the chart configuration after running the query:

app/page.tsx

```
/* ...other imports... */import{ getCompanies, generateQuery, generateChartConfig }from'./actions';/* ...rest of the file... */consthandleSubmit=async(suggestion?:string)=>{clearExistingData();const question = suggestion ?? inputValue;if(inputValue.length===0&&!suggestion)return;if(question.trim()){setSubmitted(true);}setLoading(true);setLoadingStep(1);setActiveQuery('');try{const query =awaitgenerateQuery(question);if(query ===undefined){      toast.error('An error occurred. Please try again.');setLoading(false);return;}setActiveQuery(query);setLoadingStep(2);const companies =awaitrunGeneratedSQLQuery(query);const columns = companies.length>0?Object.keys(companies[0]):[];setResults(companies);setColumns(columns);setLoading(false);const{ config }=awaitgenerateChartConfig(companies, question);setChartConfig(config);}catch(e){    toast.error('An error occurred. Please try again.');setLoading(false);}};/* ...rest of the file... */
```

Now when users submit queries, the application will:

1.  Generate and run the SQL query
2.  Display the table results
3.  Generate a chart configuration for the results
4.  Allow toggling between table and chart views

Head back to the browser and test the application with a few queries. You should see the chart visualization appear after the table results.


## [Next steps](#next-steps)


You've built an AI-powered SQL analysis tool that can convert natural language to SQL queries, visualize query results, and explain SQL queries in plain English.

You could, for example, extend the application to use your own data sources or add more advanced features like customizing the chart configuration schema to support more chart types and options. You could also add more complex SQL query generation capabilities.
```

### 139. `docs/guides/o1.md`

```markdown
# Get started with OpenAI o1


---
url: https://ai-sdk.dev/docs/guides/o1
description: Get started with OpenAI o1 using the AI SDK.
---


# [Get started with OpenAI o1](#get-started-with-openai-o1)


With the [release of OpenAI's o1 series models](https://openai.com/index/introducing-openai-o1-preview/), there has never been a better time to start building AI applications, particularly those that require complex reasoning capabilities.

The [AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like OpenAI o1 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.


## [OpenAI o1](#openai-o1)


OpenAI released a series of AI models designed to spend more time thinking before responding. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math. These models, named the o1 series, are trained with reinforcement learning and can "think before they answer". As a result, they are able to produce a long internal chain of thought before responding to a prompt.

There are three reasoning models available in the API:

1.  [**o1**](https://platform.openai.com/docs/models#o1): Designed to reason about hard problems using broad general knowledge about the world.
2.  [**o1-preview**](https://platform.openai.com/docs/models#o1): The original preview version of o1 - slower than o1 but supports streaming.
3.  [**o1-mini**](https://platform.openai.com/docs/models#o1): A faster and cheaper version of o1, particularly adept at coding, math, and science tasks where extensive general knowledge isn't required. o1-mini supports streaming.

Model

Streaming

Tools

Object Generation

Reasoning Effort

o1

o1-preview

o1-mini


### [Benchmarks](#benchmarks)


OpenAI o1 models excel in scientific reasoning, with impressive performance across various domains:

-   Ranking in the 89th percentile on competitive programming questions (Codeforces)
-   Placing among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME)
-   Exceeding human PhD-level accuracy on a benchmark of physics, biology, and chemistry problems (GPQA)

[Source](https://openai.com/index/learning-to-reason-with-llms/)


### [Prompt Engineering for o1 Models](#prompt-engineering-for-o1-models)


The o1 models perform best with straightforward prompts. Some prompt engineering techniques, like few-shot prompting or instructing the model to "think step by step," may not enhance performance and can sometimes hinder it. Here are some best practices:

1.  Keep prompts simple and direct: The models excel at understanding and responding to brief, clear instructions without the need for extensive guidance.
2.  Avoid chain-of-thought prompts: Since these models perform reasoning internally, prompting them to "think step by step" or "explain your reasoning" is unnecessary.
3.  Use delimiters for clarity: Use delimiters like triple quotation marks, XML tags, or section titles to clearly indicate distinct parts of the input, helping the model interpret different sections appropriately.
4.  Limit additional context in retrieval-augmented generation (RAG): When providing additional context or documents, include only the most relevant information to prevent the model from overcomplicating its response.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is [AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call OpenAI o1-mini with the AI SDK:

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model:openai('o1-mini'),  prompt:'Explain the concept of quantum entanglement.',});
```

To use the o1 series of models, you must either be using @ai-sdk/openai version 0.0.59 or greater, or set `temperature: 1`.

AI SDK Core abstracts away the differences between model providers, allowing you to focus on building great applications. The unified interface also means that you can easily switch between models by changing just one line of code.

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model:openai('o1'),  prompt:'Explain the concept of quantum entanglement.',});
```

System messages are automatically converted to OpenAI developer messages.


### [Refining Reasoning Effort](#refining-reasoning-effort)


You can control the amount of reasoning effort expended by o1 through the `reasoningEffort` parameter. This parameter can be set to `'low'`, `'medium'`, or `'high'` to adjust how much time and computation the model spends on internal reasoning before producing a response.

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';// Reduce reasoning effort for faster responsesconst{ text }=awaitgenerateText({  model:openai('o1'),  prompt:'Explain quantum entanglement briefly.',  providerOptions:{    openai:{ reasoningEffort:'low'},},});
```

The `reasoningEffort` parameter is only supported by o1 and has no effect on other models.


### [Generating Structured Data](#generating-structured-data)


While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions ([`generateObject`](/docs/reference/ai-sdk-core/generate-object) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```
import{ generateObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model:openai('o1'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.object({ name: z.string(), amount: z.string()})),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});
```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.

Structured object generation is only supported with o1, not o1-preview or o1-mini.


### [Tools](#tools)


While LLMs have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). The solution: [tools](/docs/foundations/tools), which are like programs that you provide to the model, which it can choose to call as necessary.


### [Using Tools with the AI SDK](#using-tools-with-the-ai-sdk)


The AI SDK supports tool usage across several of its functions, like [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`streamText`](/docs/reference/ai-sdk-core/stream-text). By passing one or more tools to the `tools` parameter, you can extend the capabilities of LLMs, allowing them to perform discrete tasks and interact with external systems.

Here's an example of how you can use a tool with the AI SDK and o1:

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model:openai('o1'),  prompt:'What is the weather like today?',  tools:{    getWeather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},});
```

In this example, the `getWeather` tool allows the model to fetch real-time weather data (simulated for simplicity), enhancing its ability to provide accurate and up-to-date information.

Tools are only compatible with o1, not o1-preview or o1-mini.


### [Building Interactive Interfaces](#building-interactive-interfaces)


AI SDK Core can be paired with [AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — [`useChat`](/docs/reference/ai-sdk-ui/use-chat), [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), [`useObject`](/docs/reference/ai-sdk-ui/use-object), and [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with [Next.js](https://nextjs.org), the AI SDK, and OpenAI o1:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';// Allow responses up to 5 minutesexportconst maxDuration =300;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('o1-mini'),    messages,});return result.toDataStreamResponse();}
```

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit, error }=useChat();return(<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.content}</div>))}<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button></form></>);}
```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.


## [Get Started](#get-started)


Ready to get started? Here's how you can dive in:

1.  Explore the documentation at [ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.
2.  Check out our support for the o1 series of reasoning models in the [OpenAI Provider](/providers/ai-sdk-providers/openai#reasoning-models).
3.  Check out practical examples at [ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.
4.  Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at [ai-sdk.dev/docs/guides](/docs/guides).
5.  Check out ready-to-deploy AI templates at [vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).
```

### 140. `docs/guides/o3.md`

```markdown
# Get started with OpenAI o3-mini


---
url: https://ai-sdk.dev/docs/guides/o3
description: Get started with OpenAI o3-mini using the AI SDK.
---


# [Get started with OpenAI o3-mini](#get-started-with-openai-o3-mini)


With the [release of OpenAI's o3-mini model](https://openai.com/index/openai-o3-mini/), there has never been a better time to start building AI applications, particularly those that require complex STEM reasoning capabilities.

The [AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like OpenAI o3-mini alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.


## [OpenAI o3-mini](#openai-o3-mini)


OpenAI recently released a new AI model optimized for STEM reasoning that excels in science, math, and coding tasks. o3-mini matches o1's performance in these domains while delivering faster responses and lower costs. The model supports tool calling, structured outputs, and system messages, making it a great option for a wide range of applications.

o3-mini offers three reasoning effort levels:

1.  \[**Low**\]: Optimized for speed while maintaining solid reasoning capabilities
2.  \[**Medium**\]: Balanced approach matching o1's performance levels
3.  \[**High**\]: Enhanced reasoning power exceeding o1 in many STEM domains

Model

Streaming

Tool Calling

Structured Output

Reasoning Effort

Image Input

o3-mini


### [Benchmarks](#benchmarks)


OpenAI o3-mini demonstrates impressive performance across technical domains:

-   87.3% accuracy on AIME competition math questions
-   79.7% accuracy on PhD-level science questions (GPQA Diamond)
-   2130 Elo rating on competitive programming (Codeforces)
-   49.3% accuracy on verified software engineering tasks (SWE-bench)

These benchmark results are using high reasoning effort setting.

[Source](https://openai.com/index/openai-o3-mini/)


### [Prompt Engineering for o3-mini](#prompt-engineering-for-o3-mini)


The o3-mini model performs best with straightforward prompts. Some prompt engineering techniques, like few-shot prompting or instructing the model to "think step by step," may not enhance performance and can sometimes hinder it. Here are some best practices:

1.  Keep prompts simple and direct: The model excels at understanding and responding to brief, clear instructions without the need for extensive guidance.
2.  Avoid chain-of-thought prompts: Since the model performs reasoning internally, prompting it to "think step by step" or "explain your reasoning" is unnecessary.
3.  Use delimiters for clarity: Use delimiters like triple quotation marks, XML tags, or section titles to clearly indicate distinct parts of the input.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is [AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call OpenAI o3-mini with the AI SDK:

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model:openai('o3-mini'),  prompt:'Explain the concept of quantum entanglement.',});
```

To use o3-mini, you must be using @ai-sdk/openai version 1.1.9 or greater.

System messages are automatically converted to OpenAI developer messages.


### [Refining Reasoning Effort](#refining-reasoning-effort)


You can control the amount of reasoning effort expended by o3-mini through the `reasoningEffort` parameter. This parameter can be set to `low`, `medium`, or `high` to adjust how much time and computation the model spends on internal reasoning before producing a response.

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';// Reduce reasoning effort for faster responsesconst{ text }=awaitgenerateText({  model:openai('o3-mini'),  prompt:'Explain quantum entanglement briefly.',  providerOptions:{    openai:{ reasoningEffort:'low'},},});
```


### [Generating Structured Data](#generating-structured-data)


While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions ([`generateObject`](/docs/reference/ai-sdk-core/generate-object) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```
import{ generateObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model:openai('o3-mini'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.object({ name: z.string(), amount: z.string()})),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});
```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.


### [Using Tools with the AI SDK](#using-tools-with-the-ai-sdk)


o3-mini supports tool calling out of the box, allowing it to interact with external systems and perform discrete tasks. Here's an example of using tool calling with the AI SDK:

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model:openai('o3-mini'),  prompt:'What is the weather like today in San Francisco?',  tools:{    getWeather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({ location })=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},});
```

In this example, the `getWeather` tool allows the model to fetch real-time weather data (simulated for simplicity), enhancing its ability to provide accurate and up-to-date information.


### [Building Interactive Interfaces](#building-interactive-interfaces)


AI SDK Core can be paired with [AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — [`useChat`](/docs/reference/ai-sdk-ui/use-chat), [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), [`useObject`](/docs/reference/ai-sdk-ui/use-object), and [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with [Next.js](https://nextjs.org), the AI SDK, and OpenAI o3-mini:

In a new Next.js application, first install the AI SDK and the DeepSeek provider:

npm install ai @ai-sdk/openai

Then, create a route handler for the chat endpoint:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';// Allow responses up to 5 minutesexportconst maxDuration =300;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('o3-mini'),    messages,});return result.toDataStreamResponse();}
```

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit, error }=useChat();return(<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.content}</div>))}<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button></form></>);}
```

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.


## [Get Started](#get-started)


Ready to get started? Here's how you can dive in:

1.  Explore the documentation at [ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.
2.  Check out our support for o3-mini in the [OpenAI Provider](/providers/ai-sdk-providers/openai#reasoning-models).
3.  Check out practical examples at [ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.
4.  Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at [ai-sdk.dev/docs/guides](/docs/guides).
5.  Check out ready-to-deploy AI templates at [vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).
```

### 141. `docs/guides/openai-responses.md`

```markdown
# Get started with OpenAI Responses API


---
url: https://ai-sdk.dev/docs/guides/openai-responses
description: Get started with the OpenAI Responses API using the AI SDK.
---


# [Get started with OpenAI Responses API](#get-started-with-openai-responses-api)


With the [release of OpenAI's responses API](https://openai.com/index/new-tools-for-building-agents/), there has never been a better time to start building AI applications, particularly those that require a deeper understanding of the world.

The [AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.


## [OpenAI Responses API](#openai-responses-api)


OpenAI recently released the Responses API, a brand new way to build applications on OpenAI's platform. The new API offers a way to persist chat history, a web search tool for grounding LLM responses, file search tool for finding relevant files, and a computer use tool for building agents that can interact with and operate computers. Let's explore how to use the Responses API with the AI SDK.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is [AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call GPT-4o with the new Responses API using the AI SDK:

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model: openai.responses('gpt-4o'),  prompt:'Explain the concept of quantum entanglement.',});
```


### [Generating Structured Data](#generating-structured-data)


While text generation can be useful, you might want to generate structured JSON data. For example, you might want to extract information from text, classify data, or generate synthetic data. AI SDK Core provides two functions ([`generateObject`](/docs/reference/ai-sdk-core/generate-object) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object)) to generate structured data, allowing you to constrain model outputs to a specific schema.

```
import{ generateObject }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model: openai.responses('gpt-4o'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.object({ name: z.string(), amount: z.string()})),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});
```

This code snippet will generate a type-safe recipe that conforms to the specified zod schema.


### [Using Tools with the AI SDK](#using-tools-with-the-ai-sdk)


The Responses API supports tool calling out of the box, allowing it to interact with external systems and perform discrete tasks. Here's an example of using tool calling with the AI SDK:

```
import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';const{ text }=awaitgenerateText({  model: openai.responses('gpt-4o'),  prompt:'What is the weather like today in San Francisco?',  tools:{    getWeather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},});
```

In this example, the `getWeather` tool allows the model to fetch real-time weather data (simulated for simplicity), enhancing its ability to provide accurate and up-to-date information.


### [Web Search Tool](#web-search-tool)


The Responses API introduces a built-in tool for grounding responses called `webSearch`. With this tool, the model can access the internet to find relevant information for its responses.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const result =awaitgenerateText({  model: openai.responses('gpt-4o-mini'),  prompt:'What happened in San Francisco last week?',  tools:{    web_search_preview: openai.tools.webSearchPreview(),},});console.log(result.text);console.log(result.sources);
```

The `webSearch` tool also allows you to specify query-specific metadata that can be used to improve the quality of the search results.

```
import{ generateText }from'ai';const result =awaitgenerateText({  model: openai.responses('gpt-4o-mini'),  prompt:'What happened in San Francisco last week?',  tools:{    web_search_preview: openai.tools.webSearchPreview({      searchContextSize:'high',      userLocation:{type:'approximate',        city:'San Francisco',        region:'California',},}),},});console.log(result.text);console.log(result.sources);
```


## [Using Persistence](#using-persistence)


With the Responses API, you can persist chat history with OpenAI across requests. This allows you to send just the user's last message and OpenAI can access the entire chat history:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const result1 =awaitgenerateText({  model: openai.responses('gpt-4o-mini'),  prompt:'Invent a new holiday and describe its traditions.',});const result2 =awaitgenerateText({  model: openai.responses('gpt-4o-mini'),  prompt:'Summarize in 2 sentences',  providerOptions:{    openai:{      previousResponseId: result1.providerMetadata?.openai.responseId asstring,},},});
```


## [Migrating from Completions API](#migrating-from-completions-api)


Migrating from the OpenAI Completions API (via the AI SDK) to the new Responses API is simple. To migrate, simply change your provider instance from `openai(modelId)` to `openai.responses(modelId)`:

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';// Completions APIconst{ text }=awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Explain the concept of quantum entanglement.',});// Responses APIconst{ text }=awaitgenerateText({  model: openai.responses('gpt-4o'),  prompt:'Explain the concept of quantum entanglement.',});
```

When using the Responses API, provider specific options that were previously specified on the model provider instance have now moved to the `providerOptions` object:

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';// Completions APIconst{ text }=awaitgenerateText({  model:openai('gpt-4o',{ parallelToolCalls:false}),  prompt:'Explain the concept of quantum entanglement.',});// Responses APIconst{ text }=awaitgenerateText({  model: openai.responses('gpt-4o'),  prompt:'Explain the concept of quantum entanglement.',  providerOptions:{    openai:{      parallelToolCalls:false,},},});
```


## [Get Started](#get-started)


Ready to get started? Here's how you can dive in:

1.  Explore the documentation at [ai-sdk.dev/docs](/docs) to understand the full capabilities of the AI SDK.
2.  Check out practical examples at [ai-sdk.dev/examples](/examples) to see the SDK in action and get inspired for your own projects.
3.  Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) and multi-modal chat at [ai-sdk.dev/docs/guides](/docs/guides).
4.  Check out ready-to-deploy AI templates at [vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).
```

### 142. `docs/guides/r1.md`

```markdown
# Get started with DeepSeek R1


---
url: https://ai-sdk.dev/docs/guides/r1
description: Get started with DeepSeek R1 using the AI SDK.
---


# [Get started with DeepSeek R1](#get-started-with-deepseek-r1)


With the [release of DeepSeek R1](https://deepseek.ai/deepseek-r1), there has never been a better time to start building AI applications, particularly those that require complex reasoning capabilities.

The [AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like DeepSeek R1 alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.


## [DeepSeek R1](#deepseek-r1)


DeepSeek R1 is a series of advanced AI models designed to tackle complex reasoning tasks in science, coding, and mathematics. These models are optimized to "think before they answer," producing detailed internal chains of thought that aid in solving challenging problems.

The series includes two primary variants:

-   **DeepSeek R1-Zero**: Trained exclusively with reinforcement learning (RL) without any supervised fine-tuning. It exhibits advanced reasoning capabilities but may struggle with readability and formatting.
-   **DeepSeek R1**: Combines reinforcement learning with cold-start data and supervised fine-tuning to improve both reasoning performance and the readability of outputs.


### [Benchmarks](#benchmarks)


DeepSeek R1 models excel in reasoning tasks, delivering competitive performance across key benchmarks:

-   **AIME 2024 (Pass@1)**: 79.8%
-   **MATH-500 (Pass@1)**: 97.3%
-   **Codeforces (Percentile)**: Top 4% (96.3%)
-   **GPQA Diamond (Pass@1)**: 71.5%

[Source](https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#4-evaluation-results)


### [Prompt Engineering for DeepSeek R1 Models](#prompt-engineering-for-deepseek-r1-models)


DeepSeek R1 models excel with structured and straightforward prompts. The following best practices can help achieve optimal performance:

1.  **Use a structured format**: Leverage the model’s preferred output structure with `<think>` tags for reasoning and `<answer>` tags for the final result.
2.  **Prefer zero-shot prompts**: Avoid few-shot prompting as it can degrade performance; instead, directly state the problem clearly.
3.  **Specify output expectations**: Guide the model by defining desired formats, such as markdown for readability or XML-like tags for clarity.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is [AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call DeepSeek R1 with the AI SDK:

```
import{ deepseek }from'@ai-sdk/deepseek';import{ generateText }from'ai';const{ reasoning, text }=awaitgenerateText({  model:deepseek('deepseek-reasoner'),  prompt:'Explain quantum entanglement.',});
```

The unified interface also means that you can easily switch between providers by changing just two lines of code. For example, to use DeepSeek R1 via Fireworks:

```
import{ fireworks }from'@ai-sdk/fireworks';import{  generateText,  wrapLanguageModel,  extractReasoningMiddleware,}from'ai';// middleware to extract reasoning tokensconst enhancedModel =wrapLanguageModel({  model:fireworks('accounts/fireworks/models/deepseek-r1'),  middleware:extractReasoningMiddleware({ tagName:'think'}),});const{ reasoning, text }=awaitgenerateText({  model: enhancedModel,  prompt:'Explain quantum entanglement.',});
```

Or to use Groq's `deepseek-r1-distill-llama-70b` model:

```
import{ groq }from'@ai-sdk/groq';import{  generateText,  wrapLanguageModel,  extractReasoningMiddleware,}from'ai';// middleware to extract reasoning tokensconst enhancedModel =wrapLanguageModel({  model:groq('deepseek-r1-distill-llama-70b'),  middleware:extractReasoningMiddleware({ tagName:'think'}),});const{ reasoning, text }=awaitgenerateText({  model: enhancedModel,  prompt:'Explain quantum entanglement.',});
```

The AI SDK provides a [middleware](/docs/ai-sdk-core/middleware) (`extractReasoningMiddleware`) that can be used to extract the reasoning tokens from the model's output.

When using DeepSeek-R1 series models with third-party providers like Together AI, we recommend using the `startWithReasoning` option in the `extractReasoningMiddleware` function, as they tend to bypass thinking patterns.


### [Model Provider Comparison](#model-provider-comparison)


You can use DeepSeek R1 with the AI SDK through various providers. Here's a comparison of the providers that support DeepSeek R1:

Provider

Model ID

Reasoning Tokens

[DeepSeek](/providers/ai-sdk-providers/deepseek)

[`deepseek-reasoner`](https://api-docs.deepseek.com/guides/reasoning_model)

[Fireworks](/providers/ai-sdk-providers/fireworks)

[`accounts/fireworks/models/deepseek-r1`](https://fireworks.ai/models/fireworks/deepseek-r1)

Requires Middleware

[Groq](/providers/ai-sdk-providers/groq)

[`deepseek-r1-distill-llama-70b`](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)

Requires Middleware

[Azure](/providers/ai-sdk-providers/azure)

[`DeepSeek-R1`](https://ai.azure.com/explore/models/DeepSeek-R1/version/1/registry/azureml-deepseek#code-samples)

Requires Middleware

[Together AI](/providers/ai-sdk-providers/togetherai)

[`deepseek-ai/DeepSeek-R1`](https://www.together.ai/models/deepseek-r1)

Requires Middleware

[FriendliAI](/providers/community-providers/friendliai)

[`deepseek-r1`](https://huggingface.co/deepseek-ai/DeepSeek-R1)

Requires Middleware

[LangDB](/providers/community-providers/langdb)

[`deepseek/deepseek-reasoner`](https://docs.langdb.ai/guides/deepseek)

Requires Middleware


### [Building Interactive Interfaces](#building-interactive-interfaces)


AI SDK Core can be paired with [AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — [`useChat`](/docs/reference/ai-sdk-ui/use-chat), [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), [`useObject`](/docs/reference/ai-sdk-ui/use-object), and [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with [Next.js](https://nextjs.org), the AI SDK, and DeepSeek R1:

In a new Next.js application, first install the AI SDK and the DeepSeek provider:

npm install ai @ai-sdk/deepseek

Then, create a route handler for the chat endpoint:

app/api/chat/route.ts

```
import{ deepseek }from'@ai-sdk/deepseek';import{ streamText }from'ai';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:deepseek('deepseek-reasoner'),    messages,});return result.toDataStreamResponse({    sendReasoning:true,});}
```

You can forward the model's reasoning tokens to the client with `sendReasoning: true` in the `toDataStreamResponse` method.

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit, error }=useChat();return(<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.reasoning &&<pre>{message.reasoning}</pre>}{message.content}</div>))}<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button></form></>);}
```

You can access the model's reasoning tokens with the `reasoning` property on the `message` object.

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.


## [Limitations](#limitations)


While DeepSeek R1 models are powerful, they have certain limitations:

-   No tool-calling support: DeepSeek R1 cannot directly interact with APIs or external tools.
-   No object generation support: DeepSeek R1 does not support structured object generation. However, you can combine it with models that support structured object generation (like gpt-4o-mini) to generate objects. See the [structured object generation with a reasoning model recipe](/cookbook/node/generate-object-reasoning) for more information.


## [Get Started](#get-started)


Ready to dive in? Here's how you can begin:

1.  Explore the documentation at [ai-sdk.dev/docs](/docs) to understand the capabilities of the AI SDK.
2.  Check out practical examples at [ai-sdk.dev/examples](/examples) to see the SDK in action.
3.  Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) at [ai-sdk.dev/docs/guides](/docs/guides).
4.  Use ready-to-deploy AI templates at [vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

DeepSeek R1 opens new opportunities for reasoning-intensive AI applications. Start building today and leverage the power of advanced reasoning in your AI projects.
```

### 143. `docs/guides/rag-chatbot.md`

```markdown
# RAG Chatbot Guide


---
url: https://ai-sdk.dev/docs/guides/rag-chatbot
description: Learn how to build a RAG Chatbot with the AI SDK and Next.js
---


# [RAG Chatbot Guide](#rag-chatbot-guide)


In this guide, you will learn how to build a retrieval-augmented generation (RAG) chatbot application.

Before we dive in, let's look at what RAG is, and why we would want to use it.


### [What is RAG?](#what-is-rag)


RAG stands for retrieval augmented generation. In simple terms, RAG is the process of providing a Large Language Model (LLM) with specific information relevant to the prompt.


### [Why is RAG important?](#why-is-rag-important)


While LLMs are powerful, the information they can reason on is restricted to the data they were trained on. This problem becomes apparent when asking an LLM for information outside of their training data, like proprietary data or common knowledge that has occurred after the model’s training cutoff. RAG solves this problem by fetching information relevant to the prompt and then passing that to the model as context.

To illustrate with a basic example, imagine asking the model for your favorite food:

```
**input**What is my favorite food?**generation**I don't have access to personal information about individuals, including theirfavorite foods.
```

Not surprisingly, the model doesn’t know. But imagine, alongside your prompt, the model received some extra context:

```
**input**Respond to the user's prompt using only the provided context.user prompt: 'What is my favorite food?'context: user loves chicken nuggets**generation**Your favorite food is chicken nuggets!
```

Just like that, you have augmented the model’s generation by providing relevant information to the query. Assuming the model has the appropriate information, it is now highly likely to return an accurate response to the users query. But how does it retrieve the relevant information? The answer relies on a concept called embedding.

You could fetch any context for your RAG application (eg. Google search). Embeddings and Vector Databases are just a specific retrieval approach to achieve semantic search.


### [Embedding](#embedding)


[Embeddings](/docs/ai-sdk-core/embeddings) are a way to represent words, phrases, or images as vectors in a high-dimensional space. In this space, similar words are close to each other, and the distance between words can be used to measure their similarity.

In practice, this means that if you embedded the words `cat` and `dog`, you would expect them to be plotted close to each other in vector space. The process of calculating the similarity between two vectors is called ‘cosine similarity’ where a value of 1 would indicate high similarity and a value of -1 would indicate high opposition.

Don’t worry if this seems complicated. a high level understanding is all you need to get started! For a more in-depth introduction to embeddings, check out [this guide](https://jalammar.github.io/illustrated-word2vec/).

As mentioned above, embeddings are a way to represent the semantic meaning of **words and phrases**. The implication here is that the larger the input to your embedding, the lower quality the embedding will be. So how would you approach embedding content longer than a simple phrase?


### [Chunking](#chunking)


Chunking refers to the process of breaking down a particular source material into smaller pieces. There are many different approaches to chunking and it’s worth experimenting as the most effective approach can differ by use case. A simple and common approach to chunking (and what you will be using in this guide) is separating written content by sentences.

Once your source material is appropriately chunked, you can embed each one and then store the embedding and the chunk together in a database. Embeddings can be stored in any database that supports vectors. For this tutorial, you will be using [Postgres](https://www.postgresql.org/) alongside the [pgvector](https://github.com/pgvector/pgvector) plugin.


### [All Together Now](#all-together-now)


Combining all of this together, RAG is the process of enabling the model to respond with information outside of it’s training data by embedding a users query, retrieving the relevant source material (chunks) with the highest semantic similarity, and then passing them alongside the initial query as context. Going back to the example where you ask the model for your favorite food, the prompt preparation process would look like this.

By passing the appropriate context and refining the model’s objective, you are able to fully leverage its strengths as a reasoning machine.

Onto the project!


## [Project Setup](#project-setup)


In this project, you will build a chatbot that will only respond with information that it has within its knowledge base. The chatbot will be able to both store and retrieve information. This project has many interesting use cases from customer support through to building your own second brain!

This project will use the following stack:

-   [Next.js](https://nextjs.org) 14 (App Router)
-   [AI SDK](/docs)
-   [OpenAI](https://openai.com)
-   [Drizzle ORM](https://orm.drizzle.team)
-   [Postgres](https://www.postgresql.org/) with [pgvector](https://github.com/pgvector/pgvector)
-   [shadcn-ui](https://ui.shadcn.com) and [TailwindCSS](https://tailwindcss.com) for styling


### [Clone Repo](#clone-repo)


To reduce the scope of this guide, you will be starting with a [repository](https://github.com/vercel/ai-sdk-rag-starter) that already has a few things set up for you:

-   Drizzle ORM (`lib/db`) including an initial migration and a script to migrate (`db:migrate`)
-   a basic schema for the `resources` table (this will be for source material)
-   a Server Action for creating a `resource`

To get started, clone the starter repository with the following command:

git clone https://github.com/vercel/ai-sdk-rag-starter

cd ai-sdk-rag-starter

First things first, run the following command to install the project’s dependencies:

pnpm install


### [Create Database](#create-database)


You will need a Postgres database to complete this tutorial. If you don’t have Postgres setup on your local machine you can:

-   Create a free Postgres database with [Vercel Postgres](https://vercel.com/docs/storage/vercel-postgres); or
-   Follow [this guide](https://www.prisma.io/dataguide/postgresql/setting-up-a-local-postgresql-database) to set it up locally


### [Migrate Database](#migrate-database)


Once you have a Postgres database, you need to add the connection string as an environment secret.

Make a copy of the `.env.example` file and rename it to `.env`.

cp .env.example .env

Open the new `.env` file. You should see an item called `DATABASE_URL`. Copy in your database connection string after the equals sign.

With that set up, you can now run your first database migration. Run the following command:

pnpm db:migrate

This will first add the `pgvector` extension to your database. Then it will create a new table for your `resources` schema that is defined in `lib/db/schema/resources.ts`. This schema has four columns: `id`, `content`, `createdAt`, and `updatedAt`.

If you experience an error with the migration, open your migration file (`lib/db/migrations/0000_yielding_bloodaxe.sql`), cut (copy and remove) the first line, and run it directly on your postgres instance. You should now be able to run the updated migration. [More info](https://github.com/vercel/ai-sdk-rag-starter/issues/1).


### [OpenAI API Key](#openai-api-key)


For this guide, you will need an OpenAI API key. To generate an API key, go to [platform.openai.com](http://platform.openai.com/).

Once you have your API key, paste it into your `.env` file (`OPENAI_API_KEY`).


## [Build](#build)


Let’s build a quick task list of what needs to be done:

1.  Create a table in your database to store embeddings
2.  Add logic to chunk and create embeddings when creating resources
3.  Create a chatbot
4.  Give the chatbot tools to query / create resources for it’s knowledge base


### [Create Embeddings Table](#create-embeddings-table)


Currently, your application has one table (`resources`) which has a column (`content`) for storing content. Remember, each `resource` (source material) will have to be chunked, embedded, and then stored. Let’s create a table called `embeddings` to store these chunks.

Create a new file (`lib/db/schema/embeddings.ts`) and add the following code:

lib/db/schema/embeddings.ts

```
import{ nanoid }from'@/lib/utils';import{ index, pgTable, text, varchar, vector }from'drizzle-orm/pg-core';import{ resources }from'./resources';exportconst embeddings =pgTable('embeddings',{    id:varchar('id',{ length:191}).primaryKey().$defaultFn(()=>nanoid()),    resourceId:varchar('resource_id',{ length:191}).references(()=> resources.id,{ onDelete:'cascade'},),    content:text('content').notNull(),    embedding:vector('embedding',{ dimensions:1536}).notNull(),},table=>({    embeddingIndex:index('embeddingIndex').using('hnsw',      table.embedding.op('vector_cosine_ops'),),}),);
```

This table has four columns:

-   `id` - unique identifier
-   `resourceId` - a foreign key relation to the full source material
-   `content` - the plain text chunk
-   `embedding` - the vector representation of the plain text chunk

To perform similarity search, you also need to include an index ([HNSW](https://github.com/pgvector/pgvector?tab=readme-ov-file#hnsw) or [IVFFlat](https://github.com/pgvector/pgvector?tab=readme-ov-file#ivfflat)) on this column for better performance.

To push this change to the database, run the following command:

pnpm db:push


### [Add Embedding Logic](#add-embedding-logic)


Now that you have a table to store embeddings, it’s time to write the logic to create the embeddings.

Create a file with the following command:

mkdir lib/ai && touch lib/ai/embedding.ts


### [Generate Chunks](#generate-chunks)


Remember, to create an embedding, you will start with a piece of source material (unknown length), break it down into smaller chunks, embed each chunk, and then save the chunk to the database. Let’s start by creating a function to break the source material into small chunks.

lib/ai/embedding.ts

```
const generateChunks =(input:string):string[]=>{return input.trim().split('.').filter(i=> i !=='');};
```

This function will take an input string and split it by periods, filtering out any empty items. This will return an array of strings. It is worth experimenting with different chunking techniques in your projects as the best technique will vary.


### [Install AI SDK](#install-ai-sdk)


You will use the AI SDK to create embeddings. This will require two more dependencies, which you can install by running the following command:

pnpm add ai @ai-sdk/react @ai-sdk/openai

This will install the [AI SDK](/docs), AI SDK's React hooks, and AI SDK's [OpenAI provider](/providers/ai-sdk-providers/openai).

The AI SDK is designed to be a unified interface to interact with any large language model. This means that you can change model and providers with just one line of code! Learn more about [available providers](/providers) and [building custom providers](/providers/community-providers/custom-providers) in the [providers](/providers) section.


### [Generate Embeddings](#generate-embeddings)


Let’s add a function to generate embeddings. Copy the following code into your `lib/ai/embedding.ts` file.

lib/ai/embedding.ts

```
import{ embedMany }from'ai';import{ openai }from'@ai-sdk/openai';const embeddingModel = openai.embedding('text-embedding-ada-002');const generateChunks =(input:string):string[]=>{return input.trim().split('.').filter(i=> i !=='');};exportconst generateEmbeddings =async(  value:string,):Promise<Array<{ embedding:number[]; content:string}>>=>{const chunks =generateChunks(value);const{ embeddings }=awaitembedMany({    model: embeddingModel,    values: chunks,});return embeddings.map((e, i)=>({ content: chunks[i], embedding: e }));};
```

In this code, you first define the model you want to use for the embeddings. In this example, you are using OpenAI’s `text-embedding-ada-002` embedding model.

Next, you create an asynchronous function called `generateEmbeddings`. This function will take in the source material (`value`) as an input and return a promise of an array of objects, each containing an embedding and content. Within the function, you first generate chunks for the input. Then, you pass those chunks to the [`embedMany`](/docs/reference/ai-sdk-core/embed-many) function imported from the AI SDK which will return embeddings of the chunks you passed in. Finally, you map over and return the embeddings in a format that is ready to save in the database.


### [Update Server Action](#update-server-action)


Open the file at `lib/actions/resources.ts`. This file has one function, `createResource`, which, as the name implies, allows you to create a resource.

lib/actions/resources.ts

```
'use server';import{NewResourceParams,  insertResourceSchema,  resources,}from'@/lib/db/schema/resources';import{ db }from'../db';exportconstcreateResource=async(input: NewResourceParams)=>{try{const{ content }= insertResourceSchema.parse(input);const[resource]=await db.insert(resources).values({ content }).returning();return'Resource successfully created.';}catch(e){if(e instanceofError)return e.message.length >0? e.message :'Error, please try again.';}};
```

This function is a [Server Action](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations#with-client-components), as denoted by the `“use server”;` directive at the top of the file. This means that it can be called anywhere in your Next.js application. This function will take an input, run it through a [Zod](https://zod.dev) schema to ensure it adheres to the correct schema, and then creates a new resource in the database. This is the ideal location to generate and store embeddings of the newly created resources.

Update the file with the following code:

lib/actions/resources.ts

```
'use server';import{NewResourceParams,  insertResourceSchema,  resources,}from'@/lib/db/schema/resources';import{ db }from'../db';import{ generateEmbeddings }from'../ai/embedding';import{ embeddings as embeddingsTable }from'../db/schema/embeddings';exportconstcreateResource=async(input: NewResourceParams)=>{try{const{ content }= insertResourceSchema.parse(input);const[resource]=await db.insert(resources).values({ content }).returning();const embeddings =awaitgenerateEmbeddings(content);await db.insert(embeddingsTable).values(      embeddings.map(embedding=>({        resourceId: resource.id,...embedding,})),);return'Resource successfully created and embedded.';}catch(error){return error instanceofError&& error.message.length >0? error.message:'Error, please try again.';}};
```

First, you call the `generateEmbeddings` function created in the previous step, passing in the source material (`content`). Once you have your embeddings (`e`) of the source material, you can save them to the database, passing the `resourceId` alongside each embedding.


### [Create Root Page](#create-root-page)


Great! Let's build the frontend. The AI SDK’s [`useChat`](/docs/reference/ai-sdk-ui/use-chat) hook allows you to easily create a conversational user interface for your chatbot application.

Replace your root page (`app/page.tsx`) with the following code.

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch"><divclassName="space-y-4">{messages.map(m=>(<divkey={m.id}className="whitespace-pre-wrap"><div><divclassName="font-bold">{m.role}</div><p>{m.content}</p></div></div>))}</div><formonSubmit={handleSubmit}><inputclassName="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

The `useChat` hook enables the streaming of chat messages from your AI provider (you will be using OpenAI), manages the state for chat input, and updates the UI automatically as new messages are received.

Run the following command to start the Next.js dev server:

pnpm run dev

Head to [http://localhost:3000](http://localhost:3000/). You should see an empty screen with an input bar floating at the bottom. Try to send a message. The message shows up in the UI for a fraction of a second and then disappears. This is because you haven’t set up the corresponding API route to call the model! By default, `useChat` will send a POST request to the `/api/chat` endpoint with the `messages` as the request body.

You can customize the endpoint in the useChat configuration object


### [Create API Route](#create-api-route)


In Next.js, you can create custom request handlers for a given route using [Route Handlers](https://nextjs.org/docs/app/building-your-application/routing/route-handlers). Route Handlers are defined in a `route.ts` file and can export HTTP methods like `GET`, `POST`, `PUT`, `PATCH` etc.

Create a file at `app/api/chat/route.ts` by running the following command:

mkdir -p app/api/chat && touch app/api/chat/route.ts

Open the file and add the following code:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,});return result.toDataStreamResponse();}
```

In this code, you declare and export an asynchronous function called POST. You retrieve the `messages` from the request body and then pass them to the [`streamText`](/docs/reference/ai-sdk-core/stream-text) function imported from the AI SDK, alongside the model you would like to use. Finally, you return the model’s response in `AIStreamResponse` format.

Head back to the browser and try to send a message again. You should see a response from the model streamed directly in!


### [Refining your prompt](#refining-your-prompt)


While you now have a working chatbot, it isn't doing anything special.

Let’s add system instructions to refine and restrict the model’s behavior. In this case, you want the model to only use information it has retrieved to generate responses. Update your route handler with the following code:

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    system:`You are a helpful assistant. Check your knowledge base before answering any questions.    Only respond to questions using information from tool calls.    if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,    messages,});return result.toDataStreamResponse();}
```

Head back to the browser and try to ask the model what your favorite food is. The model should now respond exactly as you instructed above (“Sorry, I don’t know”) given it doesn’t have any relevant information.

In its current form, your chatbot is now, well, useless. How do you give the model the ability to add and query information?


### [Using Tools](#using-tools)


A [tool](/docs/foundations/tools) is a function that can be called by the model to perform a specific task. You can think of a tool like a program you give to the model that it can run as and when it deems necessary.

Let’s see how you can create a tool to give the model the ability to create, embed and save a resource to your chatbots’ knowledge base.


### [Add Resource Tool](#add-resource-tool)


Update your route handler with the following code:

app/api/chat/route.ts

```
import{ createResource }from'@/lib/actions/resources';import{ openai }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    system:`You are a helpful assistant. Check your knowledge base before answering any questions.    Only respond to questions using information from tool calls.    if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,    messages,    tools:{      addResource:tool({        description:`add a resource to your knowledge base.          If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,        parameters: z.object({          content: z.string().describe('the content or resource to add to the knowledge base'),}),execute:async({ content })=>createResource({ content }),}),},});return result.toDataStreamResponse();}
```

In this code, you define a tool called `addResource`. This tool has three elements:

-   **description**: description of the tool that will influence when the tool is picked.
-   **parameters**: [Zod schema](/docs/foundations/tools#schema-specification-and-validation-with-zod) that defines the parameters necessary for the tool to run.
-   **execute**: An asynchronous function that is called with the arguments from the tool call.

In simple terms, on each generation, the model will decide whether it should call the tool. If it deems it should call the tool, it will extract the parameters from the input and then append a new `message` to the `messages` array of type `tool-call`. The AI SDK will then run the `execute` function with the parameters provided by the `tool-call` message.

Head back to the browser and tell the model your favorite food. You should see an empty response in the UI. Did anything happen? Let’s see. Run the following command in a new terminal window.

pnpm db:studio

This will start Drizzle Studio where we can view the rows in our database. You should see a new row in both the `embeddings` and `resources` table with your favorite food!

Let’s make a few changes in the UI to communicate to the user when a tool has been called. Head back to your root page (`app/page.tsx`) and add the following code:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ messages, input, handleInputChange, handleSubmit }=useChat();return(<divclassName="flex flex-col w-full max-w-md py-24 mx-auto stretch"><divclassName="space-y-4">{messages.map(m=>(<divkey={m.id}className="whitespace-pre-wrap"><div><divclassName="font-bold">{m.role}</div><p>{m.content.length >0?(                  m.content):(<spanclassName="italic font-light">{'calling tool: '+ m?.toolInvocations?.[0].toolName}</span>)}</p></div></div>))}</div><formonSubmit={handleSubmit}><inputclassName="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"value={input}placeholder="Say something..."onChange={handleInputChange}/></form></div>);}
```

With this change, you now conditionally render the tool that has been called directly in the UI. Save the file and head back to browser. Tell the model your favorite movie. You should see which tool is called in place of the model’s typical text response.


### [Improving UX with Multi-Step Calls](#improving-ux-with-multi-step-calls)


It would be nice if the model could summarize the action too. However, technically, once the model calls a tool, it has completed its generation as it ‘generated’ a tool call. How could you achieve this desired behaviour?

The AI SDK has a feature called [`maxSteps`](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls) which will automatically send tool call results back to the model!

Open your root page (`app/page.tsx`) and add the following key to the `useChat` configuration object:

app/page.tsx

```
// ... Rest of your codeconst{ messages, input, handleInputChange, handleSubmit }=useChat({  maxSteps:3,});// ... Rest of your code
```

Head back to the browser and tell the model your favorite pizza topping (note: pineapple is not an option). You should see a follow-up response from the model confirming the action.


### [Retrieve Resource Tool](#retrieve-resource-tool)


The model can now add and embed arbitrary information to your knowledge base. However, it still isn’t able to query it. Let’s create a new tool to allow the model to answer questions by finding relevant information in your knowledge base.

To find similar content, you will need to embed the users query, search the database for semantic similarities, then pass those items to the model as context alongside the query. To achieve this, let’s update your embedding logic file (`lib/ai/embedding.ts`):

lib/ai/embedding.ts

```
import{ embed, embedMany }from'ai';import{ openai }from'@ai-sdk/openai';import{ db }from'../db';import{ cosineDistance, desc, gt, sql }from'drizzle-orm';import{ embeddings }from'../db/schema/embeddings';const embeddingModel = openai.embedding('text-embedding-ada-002');const generateChunks =(input:string):string[]=>{return input.trim().split('.').filter(i=> i !=='');};exportconst generateEmbeddings =async(  value:string,):Promise<Array<{ embedding:number[]; content:string}>>=>{const chunks =generateChunks(value);const{ embeddings }=awaitembedMany({    model: embeddingModel,    values: chunks,});return embeddings.map((e, i)=>({ content: chunks[i], embedding: e }));};exportconst generateEmbedding =async(value:string):Promise<number[]>=>{const input = value.replaceAll('\\n',' ');const{ embedding }=awaitembed({    model: embeddingModel,    value: input,});return embedding;};exportconstfindRelevantContent=async(userQuery: string)=>{const userQueryEmbedded =awaitgenerateEmbedding(userQuery);const similarity = sql<number>`1 - (${cosineDistance(    embeddings.embedding,    userQueryEmbedded,)})`;const similarGuides =await db.select({ name: embeddings.content, similarity }).from(embeddings).where(gt(similarity,0.5)).orderBy(t=>desc(t.similarity)).limit(4);return similarGuides;};
```

In this code, you add two functions:

-   `generateEmbedding`: generate a single embedding from an input string
-   `findRelevantContent`: embeds the user’s query, searches the database for similar items, then returns relevant items

With that done, it’s onto the final step: creating the tool.

Go back to your route handler (`api/chat/route.ts`) and add a new tool called `getInformation`:

api/chat/route.ts

```
import{ createResource }from'@/lib/actions/resources';import{ openai }from'@ai-sdk/openai';import{ streamText, tool }from'ai';import{ z }from'zod';import{ findRelevantContent }from'@/lib/ai/embedding';// Allow streaming responses up to 30 secondsexportconst maxDuration =30;exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages,    system:`You are a helpful assistant. Check your knowledge base before answering any questions.    Only respond to questions using information from tool calls.    if no relevant information is found in the tool calls, respond, "Sorry, I don't know."`,    tools:{      addResource:tool({        description:`add a resource to your knowledge base.          If the user provides a random piece of knowledge unprompted, use this tool without asking for confirmation.`,        parameters: z.object({          content: z.string().describe('the content or resource to add to the knowledge base'),}),execute:async({ content })=>createResource({ content }),}),      getInformation:tool({        description:`get information from your knowledge base to answer questions.`,        parameters: z.object({          question: z.string().describe('the users question'),}),execute:async({ question })=>findRelevantContent(question),}),},});return result.toDataStreamResponse();}
```

Head back to the browser, refresh the page, and ask for your favorite food. You should see the model call the `getInformation` tool, and then use the relevant information to formulate a response!


## [Conclusion](#conclusion)


Congratulations, you have successfully built an AI chatbot that can dynamically add and retrieve information to and from a knowledge base. Throughout this guide, you learned how to create and store embeddings, set up server actions to manage resources, and use tools to extend the capabilities of your chatbot.
```

### 144. `docs/guides/slackbot.md`

```markdown
# Building a Slack AI Chatbot with the AI SDK


---
url: https://ai-sdk.dev/docs/guides/slackbot
description: Learn how to use the AI SDK to build an AI Slackbot.
---


# [Building a Slack AI Chatbot with the AI SDK](#building-a-slack-ai-chatbot-with-the-ai-sdk)


In this guide, you will learn how to build a Slackbot powered by the AI SDK. The bot will be able to respond to direct messages and mentions in channels using the full context of the thread.


## [Slack App Setup](#slack-app-setup)


Before we start building, you'll need to create and configure a Slack app:

1.  Go to [api.slack.com/apps](https://api.slack.com/apps)
2.  Click "Create New App" and choose "From scratch"
3.  Give your app a name and select your workspace
4.  Under "OAuth & Permissions", add the following bot token scopes:
    -   `app_mentions:read`
    -   `chat:write`
    -   `im:history`
    -   `im:write`
    -   `assistant:write`
5.  Install the app to your workspace (button under "OAuth Tokens" subsection)
6.  Copy the Bot User OAuth Token and Signing Secret for the next step
7.  Under App Home -> Show Tabs -> Chat Tab, check "Allow users to send Slash commands and messages from the chat tab"


## [Project Setup](#project-setup)


This project uses the following stack:

-   [AI SDK by Vercel](/docs)
-   [Slack Web API](https://api.slack.com/web)
-   [Vercel](https://vercel.com)
-   [OpenAI](https://openai.com)


## [Getting Started](#getting-started)


1.  Clone [the repository](https://github.com/vercel-labs/ai-sdk-slackbot) and check out the `starter` branch

git clone https://github.com/vercel-labs/ai-sdk-slackbot.git

cd ai-sdk-slackbot

git checkout starter

2.  Install dependencies

pnpm install


## [Project Structure](#project-structure)


The starter repository already includes:

-   Slack utilities (`lib/slack-utils.ts`) including functions for validating incoming requests, converting Slack threads to AI SDK compatible message formats, and getting the Slackbot's user ID
-   General utility functions (`lib/utils.ts`) including initial Exa setup
-   Files to handle the different types of Slack events (`lib/handle-messages.ts` and `lib/handle-app-mention.ts`)
-   An API endpoint (`POST`) for Slack events (`api/events.ts`)


## [Event Handler](#event-handler)


First, let's take a look at our API route (`api/events.ts`):

```
importtype{SlackEvent}from'@slack/web-api';import{  assistantThreadMessage,  handleNewAssistantMessage,}from'../lib/handle-messages';import{ waitUntil }from'@vercel/functions';import{ handleNewAppMention }from'../lib/handle-app-mention';import{ verifyRequest, getBotId }from'../lib/slack-utils';exportasyncfunctionPOST(request:Request){const rawBody =await request.text();const payload =JSON.parse(rawBody);const requestType = payload.typeas'url_verification'|'event_callback';// See https://api.slack.com/events/url_verificationif(requestType ==='url_verification'){returnnewResponse(payload.challenge,{ status:200});}awaitverifyRequest({ requestType, request, rawBody });try{const botUserId =awaitgetBotId();const event = payload.eventasSlackEvent;if(event.type==='app_mention'){waitUntil(handleNewAppMention(event, botUserId));}if(event.type==='assistant_thread_started'){waitUntil(assistantThreadMessage(event));}if(      event.type==='message'&&!event.subtype&&      event.channel_type==='im'&&!event.bot_id&&!event.bot_profile&&      event.bot_id!== botUserId){waitUntil(handleNewAssistantMessage(event, botUserId));}returnnewResponse('Success!',{ status:200});}catch(error){console.error('Error generating response', error);returnnewResponse('Error generating response',{ status:500});}}
```

This file defines a `POST` function that handles incoming requests from Slack. First, you check the request type to see if it's a URL verification request. If it is, you respond with the challenge string provided by Slack. If it's an event callback, you verify the request and then have access to the event data. This is where you can implement your event handling logic.

You then handle three types of events: `app_mention`, `assistant_thread_started`, and `message`:

-   For `app_mention`, you call `handleNewAppMention` with the event and the bot user ID.
-   For `assistant_thread_started`, you call `assistantThreadMessage` with the event.
-   For `message`, you call `handleNewAssistantMessage` with the event and the bot user ID.

Finally, you respond with a success message to Slack. Note, each handler function is wrapped in a `waitUntil` function. Let's take a look at what this means and why it's important.


### [The waitUntil Function](#the-waituntil-function)


Slack expects a response within 3 seconds to confirm the request is being handled. However, generating AI responses can take longer. If you don't respond to the Slack request within 3 seconds, Slack will send another request, leading to another invocation of your API route, another call to the LLM, and ultimately another response to the user. To solve this, you can use the `waitUntil` function, which allows you to run your AI logic after the response is sent, without blocking the response itself.

This means, your API endpoint will:

1.  Immediately respond to Slack (within 3 seconds)
2.  Continue processing the message asynchronously
3.  Send the AI response when it's ready


## [Event Handlers](#event-handlers)


Let's look at how each event type is currently handled.


### [App Mentions](#app-mentions)


When a user mentions your bot in a channel, the `app_mention` event is triggered. The `handleNewAppMention` function in `handle-app-mention.ts` processes these mentions:

1.  Checks if the message is from a bot to avoid infinite response loops
2.  Creates a status updater to show the bot is "thinking"
3.  If the mention is in a thread, it retrieves the thread history
4.  Calls the LLM with the message content (using the `generateResponse` function which you will implement in the next section)
5.  Updates the initial "thinking" message with the AI response

Here's the code for the `handleNewAppMention` function:

lib/handle-app-mention.ts

```
import{AppMentionEvent}from'@slack/web-api';import{ client, getThread }from'./slack-utils';import{ generateResponse }from'./ai';constupdateStatusUtil=async(  initialStatus:string,  event:AppMentionEvent,)=>{const initialMessage =await client.chat.postMessage({    channel: event.channel,    thread_ts: event.thread_ts?? event.ts,    text: initialStatus,});if(!initialMessage |!initialMessage.ts)thrownewError('Failed to post initial message');constupdateMessage=async(status:string)=>{await client.chat.update({      channel: event.channel,      ts: initialMessage.tsasstring,      text: status,});};return updateMessage;};exportasyncfunctionhandleNewAppMention(  event:AppMentionEvent,  botUserId:string,){console.log('Handling app mention');if(event.bot_id| event.bot_id=== botUserId | event.bot_profile){console.log('Skipping app mention');return;}const{ thread_ts, channel }= event;const updateMessage =awaitupdateStatusUtil('is thinking...', event);if(thread_ts){const messages =awaitgetThread(channel, thread_ts, botUserId);const result =awaitgenerateResponse(messages, updateMessage);updateMessage(result);}else{const result =awaitgenerateResponse([{ role:'user', content: event.text}],      updateMessage,);updateMessage(result);}}
```

Now let's see how new assistant threads and messages are handled.


### [Assistant Thread Messages](#assistant-thread-messages)


When a user starts a thread with your assistant, the `assistant_thread_started` event is triggered. The `assistantThreadMessage` function in `handle-messages.ts` handles this:

1.  Posts a welcome message to the thread
2.  Sets up suggested prompts to help users get started

Here's the code for the `assistantThreadMessage` function:

lib/handle-messages.ts

```
importtype{AssistantThreadStartedEvent}from'@slack/web-api';import{ client }from'./slack-utils';exportasyncfunctionassistantThreadMessage(  event:AssistantThreadStartedEvent,){const{ channel_id, thread_ts }= event.assistant_thread;console.log(`Thread started: ${channel_id}${thread_ts}`);console.log(JSON.stringify(event));await client.chat.postMessage({    channel: channel_id,    thread_ts: thread_ts,    text:"Hello, I'm an AI assistant built with the AI SDK by Vercel!",});await client.assistant.threads.setSuggestedPrompts({    channel_id: channel_id,    thread_ts: thread_ts,    prompts:[{        title:'Get the weather',        message:'What is the current weather in London?',},{        title:'Get the news',        message:'What is the latest Premier League news from the BBC?',},],});}
```


### [Direct Messages](#direct-messages)


For direct messages to your bot, the `message` event is triggered and the event is handled by the `handleNewAssistantMessage` function in `handle-messages.ts`:

1.  Verifies the message isn't from a bot
2.  Updates the status to show the response is being generated
3.  Retrieves the conversation history
4.  Calls the LLM with the conversation context
5.  Posts the LLM's response to the thread

Here's the code for the `handleNewAssistantMessage` function:

lib/handle-messages.ts

```
importtype{GenericMessageEvent}from'@slack/web-api';import{ client, getThread }from'./slack-utils';import{ generateResponse }from'./ai';exportasyncfunctionhandleNewAssistantMessage(  event:GenericMessageEvent,  botUserId:string,){if(    event.bot_id|    event.bot_id=== botUserId |    event.bot_profile|!event.thread_ts)return;const{ thread_ts, channel }= event;const updateStatus =updateStatusUtil(channel, thread_ts);updateStatus('is thinking...');const messages =awaitgetThread(channel, thread_ts, botUserId);const result =awaitgenerateResponse(messages, updateStatus);await client.chat.postMessage({    channel: channel,    thread_ts: thread_ts,    text: result,    unfurl_links:false,    blocks:[{type:'section',        text:{type:'mrkdwn',          text: result,},},],});updateStatus('');}
```

With the event handlers in place, let's now implement the AI logic.


## [Implementing AI Logic](#implementing-ai-logic)


The core of our application is the `generateResponse` function in `lib/generate-response.ts`, which processes messages and generates responses using the AI SDK.

Here's how to implement it:

lib/generate-response.ts

```
import{ openai }from'@ai-sdk/openai';import{CoreMessage, generateText }from'ai';exportconstgenerateResponse=async(  messages:CoreMessage[],  updateStatus?:(status:string)=>void,)=>{const{ text }=awaitgenerateText({    model:openai('gpt-4o-mini'),    system:`You are a Slack bot assistant. Keep your responses concise and to the point.    - Do not tag users.    - Current date is: ${newDate().toISOString().split('T')[0]}`,    messages,});// Convert markdown to Slack mrkdwn formatreturn text.replace(/\[(.*?)\]\((.*?)\)/g,'<$2|$1>').replace(/\*\*/g,'*');};
```

This basic implementation:

1.  Uses the AI SDK's `generateText` function to call OpenAI's `gpt-4o` model
2.  Provides a system prompt to guide the model's behavior
3.  Formats the response for Slack's markdown format


## [Enhancing with Tools](#enhancing-with-tools)


The real power of the AI SDK comes from tools that enable your bot to perform actions. Let's add two useful tools:

lib/generate-response.ts

```
import{ openai }from'@ai-sdk/openai';import{CoreMessage, generateText, tool }from'ai';import{ z }from'zod';import{ exa }from'./utils';exportconstgenerateResponse=async(  messages:CoreMessage[],  updateStatus?:(status:string)=>void,)=>{const{ text }=awaitgenerateText({    model:openai('gpt-4o'),    system:`You are a Slack bot assistant. Keep your responses concise and to the point.    - Do not tag users.    - Current date is: ${newDate().toISOString().split('T')[0]}    - Always include sources in your final response if you use web search.`,    messages,    maxSteps:10,    tools:{      getWeather:tool({        description:'Get the current weather at a location',        parameters: z.object({          latitude: z.number(),          longitude: z.number(),          city: z.string(),}),execute:async({ latitude, longitude, city })=>{          updateStatus?.(`is getting weather for ${city}...`);const response =awaitfetch(`https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,weathercode,relativehumidity_2m&timezone=auto`,);const weatherData =await response.json();return{            temperature: weatherData.current.temperature_2m,            weatherCode: weatherData.current.weathercode,            humidity: weatherData.current.relativehumidity_2m,            city,};},}),      searchWeb:tool({        description:'Use this to search the web for information',        parameters: z.object({          query: z.string(),          specificDomain: z.string().nullable().describe('a domain to search if the user specifies e.g. bbc.com. Should be only the domain name without the protocol',),}),execute:async({ query, specificDomain })=>{          updateStatus?.(`is searching the web for ${query}...`);const{ results }=await exa.searchAndContents(query,{            livecrawl:'always',            numResults:3,            includeDomains: specificDomain ?[specificDomain]:undefined,});return{            results: results.map(result =>({              title: result.title,              url: result.url,              snippet: result.text.slice(0,1000),})),};},}),},});// Convert markdown to Slack mrkdwn formatreturn text.replace(/\[(.*?)\]\((.*?)\)/g,'<$2|$1>').replace(/\*\*/g,'*');};
```

In this updated implementation:

1.  You added two tools:

    -   `getWeather`: Fetches weather data for a specified location
    -   `searchWeb`: Searches the web for information using the Exa API
2.  You set `maxSteps: 10` to enable multi-step conversations. This will automatically send any tool results back to the LLM to trigger additional tool calls or responses as the LLM deems necessary. This turns your LLM call from a one-off operation into a multi-step agentic flow.



## [How It Works](#how-it-works)


When a user interacts with your bot:

1.  The Slack event is received and processed by your API endpoint
2.  The user's message and the thread history is passed to the `generateResponse` function
3.  The AI SDK processes the message and may invoke tools as needed
4.  The response is formatted for Slack and sent back to the user

The tools are automatically invoked based on the user's intent. For example, if a user asks "What's the weather in London?", the AI will:

1.  Recognize this as a weather query
2.  Call the `getWeather` tool with London's coordinates (inferred by the LLM)
3.  Process the weather data
4.  Generate a final response, answering the user's question


## [Deploying the App](#deploying-the-app)


1.  Install the Vercel CLI

pnpm install -g vercel

2.  Deploy the app

vercel deploy

3.  Copy the deployment URL and update the Slack app's Event Subscriptions to point to your Vercel URL
4.  Go to your project's deployment settings (Your project -> Settings -> Environment Variables) and add your environment variables

```
SLACK_BOT_TOKEN=your_slack_bot_tokenSLACK_SIGNING_SECRET=your_slack_signing_secretOPENAI_API_KEY=your_openai_api_keyEXA_API_KEY=your_exa_api_key
```

Make sure to redeploy your app after updating environment variables.

5.  Head back to the [https://api.slack.com/](https://api.slack.com/) and navigate to the "Event Subscriptions" page. Enable events and add your deployment URL.

```
https://your-vercel-url.vercel.app/api/events
```

6.  On the Events Subscription page, subscribe to the following events.
    -   `app_mention`
    -   `assistant_thread_started`
    -   `message:im`

Finally, head to Slack and test the app by sending a message to the bot.


## [Next Steps](#next-steps)


You've built a Slack chatbot powered by the AI SDK! Here are some ways you could extend it:

1.  Add memory for specific users to give the LLM context of previous interactions
2.  Implement more tools like database queries or knowledge base searches
3.  Add support for rich message formatting with blocks
4.  Add analytics to track usage patterns

In a production environment, it is recommended to implement a robust queueing system to ensure messages are properly handled.
```

### 145. `docs/guides/sonnet-3-7.md`

```markdown
# Get started with Claude 3.7 Sonnet


---
url: https://ai-sdk.dev/docs/guides/sonnet-3-7
description: Get started with Claude 3.7 Sonnet using the AI SDK.
---


# [Get started with Claude 3.7 Sonnet](#get-started-with-claude-37-sonnet)


With the [release of Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet), there has never been a better time to start building AI applications, particularly those that require complex reasoning capabilities.

The [AI SDK](/) is a powerful TypeScript toolkit for building AI applications with large language models (LLMs) like Claude 3.7 Sonnet alongside popular frameworks like React, Next.js, Vue, Svelte, Node.js, and more.


## [Claude 3.7 Sonnet](#claude-37-sonnet)


Claude 3.7 Sonnet is Anthropic's most intelligent model to date and the first Claude model to offer extended thinking—the ability to solve complex problems with careful, step-by-step reasoning. With Claude 3.7 Sonnet, you can balance speed and quality by choosing between standard thinking for near-instant responses or extended thinking or advanced reasoning. Claude 3.7 Sonnet is state-of-the-art for coding, and delivers advancements in computer use, agentic capabilities, complex reasoning, and content generation. With frontier performance and more control over speed, Claude 3.7 Sonnet is a great choice for powering AI agents, especially customer-facing agents, and complex AI workflows.


## [Getting Started with the AI SDK](#getting-started-with-the-ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more. Integrating LLMs into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK abstracts away the differences between model providers, eliminates boilerplate code for building chatbots, and allows you to go beyond text output to generate rich, interactive components.

At the center of the AI SDK is [AI SDK Core](/docs/ai-sdk-core/overview), which provides a unified API to call any LLM. The code snippet below is all you need to call Claude 3.7 Sonnet with the AI SDK:

```
import{ anthropic }from'@ai-sdk/anthropic';import{ generateText }from'ai';const{ text, reasoning, reasoningDetails }=awaitgenerateText({  model:anthropic('claude-3-7-sonnet-20250219'),  prompt:'How many people will live in the world in 2040?',});console.log(text);// text response
```

The unified interface also means that you can easily switch between providers by changing just two lines of code. For example, to use Claude 3.7 Sonnet via Amazon Bedrock:

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ generateText }from'ai';const{ reasoning, text }=awaitgenerateText({  model:bedrock('anthropic.claude-3-7-sonnet-20250219-v1:0'),  prompt:'How many people will live in the world in 2040?',});
```


### [Reasoning Ability](#reasoning-ability)


Claude 3.7 Sonnet introduces a new extended thinking—the ability to solve complex problems with careful, step-by-step reasoning. You can enable it using the `thinking` provider option and specifying a thinking budget in tokens:

```
import{ anthropic,AnthropicProviderOptions}from'@ai-sdk/anthropic';import{ generateText }from'ai';const{ text, reasoning, reasoningDetails }=awaitgenerateText({  model:anthropic('claude-3-7-sonnet-20250219'),  prompt:'How many people will live in the world in 2040?',  providerOptions:{    anthropic:{      thinking:{type:'enabled', budgetTokens:12000},} satisfies AnthropicProviderOptions,},});console.log(reasoning);// reasoning textconsole.log(reasoningDetails);// reasoning details including redacted reasoningconsole.log(text);// text response
```


### [Building Interactive Interfaces](#building-interactive-interfaces)


AI SDK Core can be paired with [AI SDK UI](/docs/ai-sdk-ui/overview), another powerful component of the AI SDK, to streamline the process of building chat, completion, and assistant interfaces with popular frameworks like Next.js, Nuxt, SvelteKit, and SolidStart.

AI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently.

With four main hooks — [`useChat`](/docs/reference/ai-sdk-ui/use-chat), [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), [`useObject`](/docs/reference/ai-sdk-ui/use-object), and [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.

Let's explore building a chatbot with [Next.js](https://nextjs.org), the AI SDK, and Claude 3.7 Sonnet:

In a new Next.js application, first install the AI SDK and the Anthropic provider:

npm install ai @ai-sdk/anthropic

Then, create a route handler for the chat endpoint:

app/api/chat/route.ts

```
import{ anthropic,AnthropicProviderOptions}from'@ai-sdk/anthropic';import{ streamText }from'ai';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:anthropic('claude-3-7-sonnet-20250219'),    messages,    providerOptions:{      anthropic:{        thinking:{type:'enabled', budgetTokens:12000},} satisfies AnthropicProviderOptions,},});return result.toDataStreamResponse({    sendReasoning:true,});}
```

You can forward the model's reasoning tokens to the client with `sendReasoning: true` in the `toDataStreamResponse` method.

Finally, update the root page (`app/page.tsx`) to use the `useChat` hook:

app/page.tsx

```
'use client';import{ useChat }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ messages, input, handleInputChange, handleSubmit, error }=useChat();return(<>{messages.map(message=>(<divkey={message.id}>{message.role ==='user'?'User: ':'AI: '}{message.parts.map((part, index)=>{// text parts:if(part.type==='text'){return<divkey={index}>{part.text}</div>;}// reasoning parts:if(part.type==='reasoning'){return(<prekey={index}>{part.details.map(detail=>                    detail.type==='text'? detail.text :'<redacted>',)}</pre>);}})}</div>))}<formonSubmit={handleSubmit}><inputname="prompt"value={input}onChange={handleInputChange}/><buttontype="submit">Submit</button></form></>);}
```

You can access the model's reasoning tokens with the `reasoning` part on the message `parts`.

The useChat hook on your root page (`app/page.tsx`) will make a request to your AI provider endpoint (`app/api/chat/route.ts`) whenever the user submits a message. The messages are then displayed in the chat UI.


## [Get Started](#get-started)


Ready to dive in? Here's how you can begin:

1.  Explore the documentation at [ai-sdk.dev/docs](/docs) to understand the capabilities of the AI SDK.
2.  Check out practical examples at [ai-sdk.dev/examples](/examples) to see the SDK in action.
3.  Dive deeper with advanced guides on topics like Retrieval-Augmented Generation (RAG) at [ai-sdk.dev/docs/guides](/docs/guides).
4.  Use ready-to-deploy AI templates at [vercel.com/templates?type=ai](https://vercel.com/templates?type=ai).

Claude 3.7 Sonnet opens new opportunities for reasoning-intensive AI applications. Start building today and leverage the power of advanced reasoning in your AI projects.
```

### 146. `docs/guides.md`

```markdown
# Guides


---
url: https://ai-sdk.dev/docs/guides
description: Learn how to build AI applications with the AI SDK
---


# [Guides](#guides)


These use-case specific guides are intended to help you build real applications with the AI SDK.

[

RAG Chatbot

Learn how to build a retrieval-augmented generation chatbot with the AI SDK.

](/docs/guides/rag-chatbot)[

Multimodal Chatbot

Learn how to build a multimodal chatbot with the AI SDK.

](/docs/guides/multi-modal-chatbot)[

Get started with Llama 3.1

Get started with Llama 3.1 using the AI SDK.

](/docs/guides/llama-3_1)[

Get started with OpenAI o1

Get started with OpenAI o1 using the AI SDK.

](/docs/guides/o1)
```

### 147. `docs/introduction.md`

```markdown
# AI SDK


---
url: https://ai-sdk.dev/docs/introduction
description: The AI SDK is the TypeScript toolkit for building AI applications and agents with React, Next.js, Vue, Svelte, Node.js, and more.
---


# [AI SDK](#ai-sdk)


The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications and agents with React, Next.js, Vue, Svelte, Node.js, and more.


## [Why use the AI SDK?](#why-use-the-ai-sdk)


Integrating large language models (LLMs) into applications is complicated and heavily dependent on the specific model provider you use.

The AI SDK standardizes integrating artificial intelligence (AI) models across [supported providers](/docs/foundations/providers-and-models). This enables developers to focus on building great AI applications, not waste time on technical details.

For example, here’s how you can generate text with various models using the AI SDK:

xAI

OpenAI

Anthropic

Google

Custom

import { generateText } from "ai"

import { xai } from "@ai-sdk/xai"

const { text } = await generateText({

model: xai("grok-3-beta"),

prompt: "What is love?"

})

Love is a universal emotion that is characterized by feelings of affection, attachment, and warmth towards someone or something. It is a complex and multifaceted experience that can take many different forms, including romantic love, familial love, platonic love, and self-love.

The AI SDK has two main libraries:

-   **[AI SDK Core](/docs/ai-sdk-core):** A unified API for generating text, structured objects, tool calls, and building agents with LLMs.
-   **[AI SDK UI](/docs/ai-sdk-ui):** A set of framework-agnostic hooks for quickly building chat and generative user interface.


## [Model Providers](#model-providers)


The AI SDK supports [multiple model providers](/providers).

[

xAI Grok

Image InputImage GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/xai)[

OpenAI

Image InputImage GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/openai)[

Azure

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/azure)[

Anthropic

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/anthropic)[

Amazon Bedrock

Image InputImage GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/amazon-bedrock)[

Groq

Object GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/groq)[

Fal AI

Image Generation

](/providers/ai-sdk-providers/fal)[

DeepInfra

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/deepinfra)[

Google Generative AI

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/google-generative-ai)[

Google Vertex AI

Image InputImage GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/google-vertex)[

Mistral

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/mistral)[

Together.ai

Object GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/togetherai)[

Cohere

Tool UsageTool Streaming

](/providers/ai-sdk-providers/cohere)[

Fireworks

Image GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/fireworks)[

DeepSeek

Object GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/deepseek)[

Cerebras

Object GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/cerebras)[

Perplexity

](/providers/ai-sdk-providers/perplexity)[

Luma AI

Image Generation

](/providers/ai-sdk-providers/luma)


## [Templates](#templates)


We've built some [templates](https://vercel.com/templates?type=ai) that include AI SDK integrations for different use cases, providers, and frameworks. You can use these templates to get started with your AI-powered application.


### [Starter Kits](#starter-kits)


[

Chatbot Starter Template

Uses the AI SDK and Next.js. Features persistence, multi-modal chat, and more.

](https://vercel.com/templates/next.js/nextjs-ai-chatbot)[

Internal Knowledge Base (RAG)

Uses AI SDK Language Model Middleware for RAG and enforcing guardrails.

](https://vercel.com/templates/next.js/ai-sdk-internal-knowledge-base)[

Multi-Modal Chat

Uses Next.js and AI SDK useChat hook for multi-modal message chat interface.

](https://vercel.com/templates/next.js/multi-modal-chatbot)[

Semantic Image Search

An AI semantic image search app template built with Next.js, AI SDK, and Postgres.

](https://vercel.com/templates/next.js/semantic-image-search)[

Natural Language PostgreSQL

Query PostgreSQL using natural language with AI SDK and GPT-4o.

](https://vercel.com/templates/next.js/natural-language-postgres)


### [Feature Exploration](#feature-exploration)


[

Feature Flags Example

AI SDK with Next.js, Feature Flags, and Edge Config for dynamic model switching.

](https://vercel.com/templates/next.js/ai-sdk-feature-flags-edge-config)[

Chatbot with Telemetry

AI SDK chatbot with OpenTelemetry support.

](https://vercel.com/templates/next.js/ai-chatbot-telemetry)[

Structured Object Streaming

Uses AI SDK useObject hook to stream structured object generation.

](https://vercel.com/templates/next.js/use-object)[

Multi-Step Tools

Uses AI SDK streamText function to handle multiple tool steps automatically.

](https://vercel.com/templates/next.js/ai-sdk-roundtrips)


### [Frameworks](#frameworks)


[

Next.js OpenAI Starter

Uses OpenAI GPT-4, AI SDK, and Next.js.

](https://github.com/vercel/ai/tree/main/examples/next-openai)[

Nuxt OpenAI Starter

Uses OpenAI GPT-4, AI SDK, and Nuxt.js.

](https://github.com/vercel/ai/tree/main/examples/nuxt-openai)[

SvelteKit OpenAI Starter

Uses OpenAI GPT-4, AI SDK, and SvelteKit.

](https://github.com/vercel/ai/tree/main/examples/sveltekit-openai)[

Solid OpenAI Starter

Uses OpenAI GPT-4, AI SDK, and Solid.

](https://github.com/vercel/ai/tree/main/examples/solidstart-openai)


### [Generative UI](#generative-ui)


[

Gemini Chatbot

Uses Google Gemini, AI SDK, and Next.js.

](https://vercel.com/templates/next.js/gemini-ai-chatbot)[

Generative UI with RSC (experimental)

Uses Next.js, AI SDK, and streamUI to create generative UIs with React Server Components.

](https://vercel.com/templates/next.js/rsc-genui)


### [Security](#security)


[

Bot Protection

Uses Kasada, OpenAI GPT-4, AI SDK, and Next.js.

](https://vercel.com/templates/next.js/advanced-ai-bot-protection)[

Rate Limiting

Uses Vercel KV, OpenAI GPT-4, AI SDK, and Next.js.

](https://github.com/vercel/ai/tree/main/examples/next-openai-upstash-rate-limits)


## [Join our Community](#join-our-community)


If you have questions about anything related to the AI SDK, you're always welcome to ask our community on [GitHub Discussions](https://github.com/vercel/ai/discussions).


## [`llms.txt` (for Cursor, Windsurf, Copilot, Claude etc.)](#llmstxt-for-cursor-windsurf-copilot-claude-etc)


You can access the entire AI SDK documentation in Markdown format at [ai-sdk.dev/llms.txt](/llms.txt). This can be used to ask any LLM (assuming it has a big enough context window) questions about the AI SDK based on the most up-to-date documentation.


### [Example Usage](#example-usage)


For instance, to prompt an LLM with questions about the AI SDK:

1.  Copy the documentation contents from [ai-sdk.dev/llms.txt](/llms.txt)
2.  Use the following prompt format:

```
Documentation:{paste documentation here}---Based on the above documentation, answer the following:{your question}
```
```

### 148. `docs/migration-guides/migration-guide-3-1.md`

```markdown
# Migrate AI SDK 3.0 to 3.1


---
url: https://ai-sdk.dev/docs/migration-guides/migration-guide-3-1
description: Learn how to upgrade AI SDK 3.0 to 3.1.
---


# [Migrate AI SDK 3.0 to 3.1](#migrate-ai-sdk-30-to-31)


Check out the [AI SDK 3.1 release blog post](https://vercel.com/blog/vercel-ai-sdk-3-1-modelfusion-joins-the-team) for more information about the release.

This guide will help you:

-   Upgrade to AI SDK 3.1
-   Migrate from Legacy Providers to AI SDK Core
-   Migrate from [`render`](/docs/reference/ai-sdk-rsc/render) to [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui)

Upgrading to AI SDK 3.1 does not require using the newly released AI SDK Core API or [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function.


## [Upgrading](#upgrading)



### [AI SDK](#ai-sdk)


To update to AI SDK version 3.1, run the following command using your preferred package manager:

pnpm add ai@3.1


## [Next Steps](#next-steps)


The release of AI SDK 3.1 introduces several new features that improve the way you build AI applications with the SDK:

-   AI SDK Core, a brand new unified API for interacting with large language models (LLMs).
-   [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui), a new abstraction, built upon AI SDK Core functions that simplifies building streaming UIs.


## [Migrating from Legacy Providers to AI SDK Core](#migrating-from-legacy-providers-to-ai-sdk-core)


Prior to AI SDK Core, you had to use a model provider's SDK to query their models.

In the following Route Handler, you use the OpenAI SDK to query their model. You then pipe that response into the [`OpenAIStream`](/docs/reference/stream-helpers/openai-stream) function which returns a [`ReadableStream`](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream) that you can pass to the client using a new [`StreamingTextResponse`](/docs/reference/stream-helpers/streaming-text-response).

```
importOpenAIfrom'openai';import{OpenAIStream,StreamingTextResponse}from'ai';const openai =newOpenAI({  apiKey: process.env.OPENAI_API_KEY!,});exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const response =await openai.chat.completions.create({    model:'gpt-4-turbo',    stream:true,    messages,});const stream =OpenAIStream(response);returnnewStreamingTextResponse(stream);}
```

With AI SDK Core you have a unified API for any provider that implements the [AI SDK Language Model Specification](/providers/community-providers/custom-providers).

Let’s take a look at the example above, but refactored to utilize the AI SDK Core API alongside the AI SDK OpenAI provider. In this example, you import the LLM function you want to use from the `ai` package, import the OpenAI provider from `@ai-sdk/openai`, and then you call the model and return the response using the `toDataStreamResponse()` helper function.

```
import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =awaitstreamText({    model:openai('gpt-4-turbo'),    messages,});return result.toDataStreamResponse();}
```


## [Migrating from `render` to `streamUI`](#migrating-from-render-to-streamui)


The AI SDK RSC API was launched as part of version 3.0. This API introduced the [`render`](/docs/reference/ai-sdk-rsc/render) function, a helper function to create streamable UIs with OpenAI models. With the new AI SDK Core API, it became possible to make streamable UIs possible with any compatible provider.

The following example Server Action uses the `render` function using the model provider directly from OpenAI. You first create an OpenAI provider instance with the OpenAI SDK. Then, you pass it to the provider key of the render function alongside a tool that returns a React Server Component, defined in the `render` key of the tool.

```
import{ render }from'ai/rsc';importOpenAIfrom'openai';import{ z }from'zod';import{Spinner,Weather}from'@/components';import{ getWeather }from'@/utils';const openai =newOpenAI();asyncfunctionsubmitMessage(userInput ='What is the weather in SF?'){'use server';returnrender({    provider: openai,    model:'gpt-4-turbo',    messages:[{ role:'system', content:'You are a helpful assistant'},{ role:'user', content: userInput },],text:({ content })=><p>{content}</p>,    tools:{      get_city_weather:{        description:'Get the current weather for a city',        parameters: z.object({            city: z.string().describe('the city'),}).required(),render:asyncfunction*({ city }){yield<Spinner/>;const weather =awaitgetWeather(city);return<Weatherinfo={weather}/>;},},},});}
```

With the new [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function, you can now use any compatible AI SDK provider. In this example, you import the AI SDK OpenAI provider. Then, you pass it to the [`model`](/docs/reference/ai-sdk-rsc/stream-ui#model) key of the new [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function. Finally, you declare a tool and return a React Server Component, defined in the [`generate`](/docs/reference/ai-sdk-rsc/stream-ui#tools-generate) key of the tool.

```
import{ streamUI }from'ai/rsc';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';import{Spinner,Weather}from'@/components';import{ getWeather }from'@/utils';asyncfunctionsubmitMessage(userInput ='What is the weather in SF?'){'use server';const result =awaitstreamUI({    model:openai('gpt-4-turbo'),    system:'You are a helpful assistant',    messages:[{ role:'user', content: userInput }],text:({ content })=><p>{content}</p>,    tools:{      get_city_weather:{        description:'Get the current weather for a city',        parameters: z.object({            city: z.string().describe('Name of the city'),}).required(),generate:asyncfunction*({ city }){yield<Spinner/>;const weather =awaitgetWeather(city);return<Weatherinfo={weather}/>;},},},});return result.value;}
```
```

### 149. `docs/migration-guides/migration-guide-3-2.md`

```markdown
# Migrate AI SDK 3.1 to 3.2


---
url: https://ai-sdk.dev/docs/migration-guides/migration-guide-3-2
description: Learn how to upgrade AI SDK 3.1 to 3.2.
---


# [Migrate AI SDK 3.1 to 3.2](#migrate-ai-sdk-31-to-32)


Check out the [AI SDK 3.2 release blog post](https://vercel.com/blog/introducing-vercel-ai-sdk-3-2) for more information about the release.

This guide will help you upgrade to AI SDK 3.2:

-   Experimental `StreamingReactResponse` functionality has been removed
-   Several features have been deprecated
-   UI framework integrations have moved to their own Node modules


## [Upgrading](#upgrading)



### [AI SDK](#ai-sdk)


To update to AI SDK version 3.2, run the following command using your preferred package manager:

pnpm add ai@latest


## [Removed Functionality](#removed-functionality)


The experimental `StreamingReactResponse` has been removed. You can use [AI SDK RSC](/docs/ai-sdk-rsc/overview) to build streaming UIs.


## [Deprecated Functionality](#deprecated-functionality)


The `nanoid` export has been deprecated. Please use [`generateId`](/docs/reference/ai-sdk-core/generate-id) instead.


## [UI Package Separation](#ui-package-separation)


AI SDK UI supports several frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), [Vue.js](https://vuejs.org/), and [SolidJS](https://www.solidjs.com/).

The integrations (other than React and RSC) have moved to separate Node modules. You need to update the import and require statements as follows:

-   Change `ai/svelte` to `@ai-sdk/svelte`
-   Change `ai/vue` to `@ai-sdk/vue`
-   Change `ai/solid` to `@ai-sdk/solid`

The old exports are still available but will be removed in a future release.
```

### 150. `docs/migration-guides/migration-guide-3-3.md`

```markdown
# Migrate AI SDK 3.2 to 3.3


---
url: https://ai-sdk.dev/docs/migration-guides/migration-guide-3-3
description: Learn how to upgrade AI SDK 3.2 to 3.3.
---


# [Migrate AI SDK 3.2 to 3.3](#migrate-ai-sdk-32-to-33)


Check out the [AI SDK 3.3 release blog post](https://vercel.com/blog/vercel-ai-sdk-3-3) for more information about the release.

No breaking changes in this release.

The following changelog encompasses all changes made in the 3.2.x series, introducing significant improvements and new features across the AI SDK and its associated libraries:


## [New Features](#new-features)



### [Open Telemetry Support](#open-telemetry-support)


-   Added experimental [OpenTelemetry support](/docs/ai-sdk-core/telemetry#telemetry) for all [AI SDK Core functions](/docs/ai-sdk-core/overview#ai-sdk-core-functions), enabling better observability and tracing capabilities.


### [AI SDK UI Improvements](#ai-sdk-ui-improvements)


-   Introduced the experimental **`useObject`** hook (for React) that can be used in conjunction with **`streamObject`** on the backend to enable seamless streaming of structured data.
-   Enhanced **`useChat`** with experimental support for attachments and streaming tool calls, providing more versatile chat functionalities.
-   Patched **`useChat`** to prevent empty submissions, improving the quality of user interactions by ensuring that only intended inputs are processed.
-   Fix **`useChat`**'s **`reload`** function, now correctly sending data, body, and headers.
-   Implemented **`setThreadId`** helper for **`useAssistant`**, simplifying thread management.
-   Documented the stream data protocol for **`useChat`** and **`useCompletion`**, allowing developers to use these functions with any backend. The stream data protocol also enables the use of custom frontends with **`streamText`**.
-   Added support for custom fetch functions and request body customization, offering greater control over API interactions.
-   Added **`onFinish`** to **`useChat`** hook for access to token usage and finish reason.


### [Core Enhancements](#core-enhancements)


-   Implemented support for sending custom request headers, enabling more tailored API requests.
-   Added raw JSON schema support alongside existing Zod support, providing more options for schema and data validation.
-   Introduced usage information for **`embed`** and **`embedMany`** functions, offering insights into token usage.
-   Added support for additional settings including **`stopSequences`** and **`topK`**, allowing for finer control over text generation.
-   Provided access to information for all steps on **`generateText`**, providing access to intermediate tool calls and results.


### [New Providers](#new-providers)


-   [AWS Bedrock provider](/providers/ai-sdk-providers/amazon-bedrock).
-   [Chrome AI provider](/providers/community-providers/chrome-ai) (community-maintained).


### [Provider Improvements](#provider-improvements)


-   Enhanced existing providers including Anthropic, Google, Azure, and OpenAI with various improvements and bug fixes.
-   Upgraded the LangChain adapter with StreamEvent v2 support and introduced the **`toDataStreamResponse`** function, enabling conversion of LangChain output streams to data stream responses.
-   Added legacy function calling support to the OpenAI provider.
-   Updated Mistral AI provider with fixes and improvements for tool calling support.


### [UI Framework Support Expansion](#ui-framework-support-expansion)


-   SolidJS: Updated **`useChat`** and **`useCompletion`** to achieve feature parity with React implementations.
-   Vue.js: Introduced **`useAssistant`** hook.
-   Vue.js / Nuxt: [Updated examples](https://github.com/vercel/ai/tree/main/examples/nuxt-openai) to showcase latest features and best practices.
-   Svelte: Added tool calling support to **`useChat`.**


## [Fixes and Improvements](#fixes-and-improvements)


-   Resolved various issues across different components of the SDK, including race conditions, error handling, and state management.
```

### 151. `docs/migration-guides/migration-guide-3-4.md`

```markdown
# Migrate AI SDK 3.3 to 3.4


---
url: https://ai-sdk.dev/docs/migration-guides/migration-guide-3-4
description: Learn how to upgrade AI SDK 3.3 to 3.4.
---


# [Migrate AI SDK 3.3 to 3.4](#migrate-ai-sdk-33-to-34)


Check out the [AI SDK 3.4 release blog post](https://vercel.com/blog/ai-sdk-3-4) for more information about the release.

No breaking changes in this release.
```

### 152. `docs/migration-guides/migration-guide-4-0.md`

```markdown
# Migrate AI SDK 3.4 to 4.0


---
url: https://ai-sdk.dev/docs/migration-guides/migration-guide-4-0
description: Learn how to upgrade AI SDK 3.4 to 4.0.
---


# [Migrate AI SDK 3.4 to 4.0](#migrate-ai-sdk-34-to-40)


Check out the [AI SDK 4.0 release blog post](https://vercel.com/blog/ai-sdk-4-0) for more information about the release.


## [Recommended Migration Process](#recommended-migration-process)


1.  Backup your project. If you use a versioning control system, make sure all previous versions are committed.
2.  [Migrate to AI SDK 3.4](/docs/troubleshooting/migration-guide/migration-guide-3-4).
3.  Upgrade to AI SDK 4.0.
4.  Automatically migrate your code using [codemods](#codemods).

    > If you don't want to use codemods, we recommend resolving all deprecation warnings before upgrading to AI SDK 4.0.

5.  Follow the breaking changes guide below.
6.  Verify your project is working as expected.
7.  Commit your changes.


## [AI SDK 4.0 package versions](#ai-sdk-40-package-versions)


You need to update the following packages to the following versions in your `package.json` file(s):

-   `ai` package: `4.0.*`
-   `ai-sdk@provider-utils` package: `2.0.*`
-   `ai-sdk/*` packages: `1.0.*` (other `@ai-sdk` packages)


## [Codemods](#codemods)


The AI SDK provides Codemod transformations to help upgrade your codebase when a feature is deprecated, removed, or otherwise changed.

Codemods are transformations that run on your codebase programmatically. They allow you to easily apply many changes without having to manually go through every file.

Codemods are intended as a tool to help you with the upgrade process. They may not cover all of the changes you need to make. You may need to make additional changes manually.

You can run all codemods provided as part of the 4.0 upgrade process by running the following command from the root of your project:

```
npx @ai-sdk/codemod upgrade
```

Individual codemods can be run by specifying the name of the codemod:

```
npx @ai-sdk/codemod <codemod-name> <path>
```

See also the [table of codemods](#codemod-table). In addition, the latest set of codemods can be found in the [`@ai-sdk/codemod`](https://github.com/vercel/ai/tree/main/packages/codemod/src/codemods) repository.


## [Provider Changes](#provider-changes)



### [Removed `baseUrl` option](#removed-baseurl-option)


The `baseUrl` option has been removed from all providers. Please use the `baseURL` option instead.

AI SDK 3.4

```
const perplexity =createOpenAI({// ...  baseUrl:'https://api.perplexity.ai/',});
```

AI SDK 4.0

```
const perplexity =createOpenAI({// ...  baseURL:'https://api.perplexity.ai/',});
```


### [Anthropic Provider](#anthropic-provider)



#### [Removed `Anthropic` facade](#removed-anthropic-facade)


The `Anthropic` facade has been removed from the Anthropic provider. Please use the `anthropic` object or the `createAnthropic` function instead.

AI SDK 3.4

```
const anthropic =newAnthropic({// ...});
```

AI SDK 4.0

```
const anthropic =createAnthropic({// ...});
```


#### [Removed `topK` setting](#removed-topk-setting)


There is no codemod available for this change. Please review and update your code manually.

The model specific `topK` setting has been removed from the Anthropic provider. You can use the standard `topK` setting instead.

AI SDK 3.4

```
const result =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-latest',{    topK:0.5,}),});
```

AI SDK 4.0

```
const result =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-latest'),  topK:0.5,});
```


### [Google Generative AI Provider](#google-generative-ai-provider)



#### [Removed `Google` facade](#removed-google-facade)


The `Google` facade has been removed from the Google Generative AI provider. Please use the `google` object or the `createGoogleGenerativeAI` function instead.

AI SDK 3.4

```
const google =newGoogle({// ...});
```

AI SDK 4.0

```
const google =createGoogleGenerativeAI({// ...});
```


#### [Removed `topK` setting](#removed-topk-setting-1)


There is no codemod available for this change. Please review and update your code manually.

The model-specific `topK` setting has been removed from the Google Generative AI provider. You can use the standard `topK` setting instead.

AI SDK 3.4

```
const result =awaitgenerateText({  model:google('gemini-1.5-flash',{    topK:0.5,}),});
```

AI SDK 4.0

```
const result =awaitgenerateText({  model:google('gemini-1.5-flash'),  topK:0.5,});
```


### [Google Vertex Provider](#google-vertex-provider)



#### [Removed `topK` setting](#removed-topk-setting-2)


There is no codemod available for this change. Please review and update your code manually.

The model-specific `topK` setting has been removed from the Google Vertex provider. You can use the standard `topK` setting instead.

AI SDK 3.4

```
const result =awaitgenerateText({  model:vertex('gemini-1.5-flash',{    topK:0.5,}),});
```

AI SDK 4.0

```
const result =awaitgenerateText({  model:vertex('gemini-1.5-flash'),  topK:0.5,});
```


### [Mistral Provider](#mistral-provider)



#### [Removed `Mistral` facade](#removed-mistral-facade)


The `Mistral` facade has been removed from the Mistral provider. Please use the `mistral` object or the `createMistral` function instead.

AI SDK 3.4

```
const mistral =newMistral({// ...});
```

AI SDK 4.0

```
const mistral =createMistral({// ...});
```


### [OpenAI Provider](#openai-provider)



#### [Removed `OpenAI` facade](#removed-openai-facade)


The `OpenAI` facade has been removed from the OpenAI provider. Please use the `openai` object or the `createOpenAI` function instead.

AI SDK 3.4

```
const openai =newOpenAI({// ...});
```

AI SDK 4.0

```
const openai =createOpenAI({// ...});
```


### [LangChain Adapter](#langchain-adapter)



#### [Removed `toAIStream`](#removed-toaistream)


The `toAIStream` function has been removed from the LangChain adapter. Please use the `toDataStream` function instead.

AI SDK 3.4

```
LangChainAdapter.toAIStream(stream);
```

AI SDK 4.0

```
LangChainAdapter.toDataStream(stream);
```


## [AI SDK Core Changes](#ai-sdk-core-changes)



### [`streamText` returns immediately](#streamtext-returns-immediately)


Instead of returning a Promise, the `streamText` function now returns immediately. It is not necessary to await the result of `streamText`.

AI SDK 3.4

```
const result =awaitstreamText({// ...});
```

AI SDK 4.0

```
const result =streamText({// ...});
```


### [`streamObject` returns immediately](#streamobject-returns-immediately)


Instead of returning a Promise, the `streamObject` function now returns immediately. It is not necessary to await the result of `streamObject`.

AI SDK 3.4

```
const result =awaitstreamObject({// ...});
```

AI SDK 4.0

```
const result =streamObject({// ...});
```


### [Remove roundtrips](#remove-roundtrips)


The `maxToolRoundtrips` and `maxAutomaticRoundtrips` options have been removed from the `generateText` and `streamText` functions. Please use the `maxSteps` option instead.

The `roundtrips` property has been removed from the `GenerateTextResult` type. Please use the `steps` property instead.

AI SDK 3.4

```
const{ text, roundtrips }=awaitgenerateText({  maxToolRoundtrips:1,// or maxAutomaticRoundtrips// ...});
```

AI SDK 4.0

```
const{ text, steps }=awaitgenerateText({  maxSteps:2,// ...});
```


### [Removed `nanoid` export](#removed-nanoid-export)


The `nanoid` export has been removed. Please use [`generateId`](/docs/reference/ai-sdk-core/generate-id) instead.

AI SDK 3.4

```
import{ nanoid }from'ai';
```

AI SDK 4.0

```
import{ generateId }from'ai';
```


### [Increased default size of generated IDs](#increased-default-size-of-generated-ids)


There is no codemod available for this change. Please review and update your code manually.

The [`generateId`](/docs/reference/ai-sdk-core/generate-id) function now generates 16-character IDs. The previous default was 7 characters.

This might e.g. require updating your database schema if you limit the length of IDs.

AI SDK 4.0

```
import{ generateId }from'ai';const id =generateId();// now 16 characters
```


### [Removed `ExperimentalMessage` types](#removed-experimentalmessage-types)


The following types have been removed:

-   `ExperimentalMessage` (use `CoreMessage` instead)
-   `ExperimentalUserMessage` (use `CoreUserMessage` instead)
-   `ExperimentalAssistantMessage` (use `CoreAssistantMessage` instead)
-   `ExperimentalToolMessage` (use `CoreToolMessage` instead)

AI SDK 3.4

```
import{ExperimentalMessage,ExperimentalUserMessage,ExperimentalAssistantMessage,ExperimentalToolMessage,}from'ai';
```

AI SDK 4.0

```
import{CoreMessage,CoreUserMessage,CoreAssistantMessage,CoreToolMessage,}from'ai';
```


### [Removed `ExperimentalTool` type](#removed-experimentaltool-type)


The `ExperimentalTool` type has been removed. Please use the `CoreTool` type instead.

AI SDK 3.4

```
import{ExperimentalTool}from'ai';
```

AI SDK 4.0

```
import{CoreTool}from'ai';
```


### [Removed experimental AI function exports](#removed-experimental-ai-function-exports)


The following exports have been removed:

-   `experimental_generateText` (use `generateText` instead)
-   `experimental_streamText` (use `streamText` instead)
-   `experimental_generateObject` (use `generateObject` instead)
-   `experimental_streamObject` (use `streamObject` instead)

AI SDK 3.4

```
import{  experimental_generateText,  experimental_streamText,  experimental_generateObject,  experimental_streamObject,}from'ai';
```

AI SDK 4.0

```
import{ generateText, streamText, generateObject, streamObject }from'ai';
```


### [Removed AI-stream related methods from `streamText`](#removed-ai-stream-related-methods-from-streamtext)


The following methods have been removed from the `streamText` result:

-   `toAIStream`
-   `pipeAIStreamToResponse`
-   `toAIStreamResponse`

Use the `toDataStream`, `pipeDataStreamToResponse`, and `toDataStreamResponse` functions instead.

AI SDK 3.4

```
const result =awaitstreamText({// ...});result.toAIStream();result.pipeAIStreamToResponse(response);result.toAIStreamResponse();
```

AI SDK 4.0

```
const result =streamText({// ...});result.toDataStream();result.pipeDataStreamToResponse(response);result.toDataStreamResponse();
```


### [Renamed "formatStreamPart" to "formatDataStreamPart"](#renamed-formatstreampart-to-formatdatastreampart)


The `formatStreamPart` function has been renamed to `formatDataStreamPart`.

AI SDK 3.4

```
formatStreamPart('text','Hello, world!');
```

AI SDK 4.0

```
formatDataStreamPart('text','Hello, world!');
```


### [Renamed "parseStreamPart" to "parseDataStreamPart"](#renamed-parsestreampart-to-parsedatastreampart)


The `parseStreamPart` function has been renamed to `parseDataStreamPart`.

AI SDK 3.4

```
const part =parseStreamPart(line);
```

AI SDK 4.0

```
const part =parseDataStreamPart(line);
```


### [Renamed `TokenUsage`, `CompletionTokenUsage` and `EmbeddingTokenUsage` types](#renamed-tokenusage-completiontokenusage-and-embeddingtokenusage-types)


The `TokenUsage`, `CompletionTokenUsage` and `EmbeddingTokenUsage` types have been renamed to `LanguageModelUsage` (for the first two) and `EmbeddingModelUsage` (for the last).

AI SDK 3.4

```
import{TokenUsage,CompletionTokenUsage,EmbeddingTokenUsage}from'ai';
```

AI SDK 4.0

```
import{LanguageModelUsage,EmbeddingModelUsage}from'ai';
```


### [Removed deprecated telemetry data](#removed-deprecated-telemetry-data)


There is no codemod available for this change. Please review and update your code manually.

The following telemetry data values have been removed:

-   `ai.finishReason` (now in `ai.response.finishReason`)
-   `ai.result.object` (now in `ai.response.object`)
-   `ai.result.text` (now in `ai.response.text`)
-   `ai.result.toolCalls` (now in `ai.response.toolCalls`)
-   `ai.stream.msToFirstChunk` (now in `ai.response.msToFirstChunk`)

This change will apply to observability providers and any scripts or automation that you use for processing telemetry data.


### [Provider Registry](#provider-registry)



#### [Removed experimental\_Provider, experimental\_ProviderRegistry, and experimental\_ModelRegistry](#removed-experimental_provider-experimental_providerregistry-and-experimental_modelregistry)


The `experimental_Provider` interface, `experimental_ProviderRegistry` interface, and `experimental_ModelRegistry` interface have been removed. Please use the `Provider` interface instead.

AI SDK 3.4

```
import{ experimental_Provider, experimental_ProviderRegistry }from'ai';
```

AI SDK 4.0

```
import{Provider}from'ai';
```

The model registry is not available any more. Please [register providers](/docs/reference/ai-sdk-core/provider-registry#setup) instead.


#### [Removed `experimental_​createModelRegistry` function](#removed-experimental_createmodelregistry-function)


The `experimental_createModelRegistry` function has been removed. Please use the `experimental_createProviderRegistry` function instead.

AI SDK 3.4

```
import{ experimental_createModelRegistry }from'ai';
```

AI SDK 4.0

```
import{ experimental_createProviderRegistry }from'ai';
```

The model registry is not available any more. Please [register providers](/docs/reference/ai-sdk-core/provider-registry#setup) instead.


### [Removed `rawResponse` from results](#removed-rawresponse-from-results)


There is no codemod available for this change. Please review and update your code manually.

The `rawResponse` property has been removed from the `generateText`, `streamText`, `generateObject`, and `streamObject` results. You can use the `response` property instead.

AI SDK 3.4

```
const{ text, rawResponse }=awaitgenerateText({// ...});
```

AI SDK 4.0

```
const{ text, response }=awaitgenerateText({// ...});
```


### [Removed `init` option from `pipeDataStreamToResponse` and `toDataStreamResponse`](#removed-init-option-from-pipedatastreamtoresponse-and-todatastreamresponse)


There is no codemod available for this change. Please review and update your code manually.

The `init` option has been removed from the `pipeDataStreamToResponse` and `toDataStreamResponse` functions. You can set the values from `init` directly into the `options` object.

AI SDK 3.4

```
const result =awaitstreamText({// ...});result.toDataStreamResponse(response,{  init:{    headers:{'X-Custom-Header':'value',},},// ...});
```

AI SDK 4.0

```
const result =streamText({// ...});result.toDataStreamResponse(response,{  headers:{'X-Custom-Header':'value',},// ...});
```


### [Removed `responseMessages` from `generateText` and `streamText`](#removed-responsemessages-from-generatetext-and-streamtext)


There is no codemod available for this change. Please review and update your code manually.

The `responseMessages` property has been removed from the `generateText` and `streamText` results. This includes the `onFinish` callback. Please use the `response.messages` property instead.

AI SDK 3.4

```
const{ text, responseMessages }=awaitgenerateText({// ...});
```

AI SDK 4.0

```
const{ text, response }=awaitgenerateText({// ...});const responseMessages = response.messages;
```


### [Removed `experimental_​continuationSteps` option](#removed-experimental_continuationsteps-option)


The `experimental_continuationSteps` option has been removed from the `generateText` function. Please use the `experimental_continueSteps` option instead.

AI SDK 3.4

```
const result =awaitgenerateText({  experimental_continuationSteps:true,// ...});
```

AI SDK 4.0

```
const result =awaitgenerateText({  experimental_continueSteps:true,// ...});
```


### [Removed `LanguageModelResponseMetadataWithHeaders` type](#removed-languagemodelresponsemetadatawithheaders-type)


The `LanguageModelResponseMetadataWithHeaders` type has been removed. Please use the `LanguageModelResponseMetadata` type instead.

AI SDK 3.4

```
import{LanguageModelResponseMetadataWithHeaders}from'ai';
```

AI SDK 4.0

```
import{LanguageModelResponseMetadata}from'ai';
```


#### [Changed `streamText` warnings result to Promise](#changed-streamtext-warnings-result-to-promise)


There is no codemod available for this change. Please review and update your code manually.

The `warnings` property of the `StreamTextResult` type is now a Promise.

AI SDK 3.4

```
const result =awaitstreamText({// ...});const warnings = result.warnings;
```

AI SDK 4.0

```
const result =streamText({// ...});const warnings =await result.warnings;
```


#### [Changed `streamObject` warnings result to Promise](#changed-streamobject-warnings-result-to-promise)


There is no codemod available for this change. Please review and update your code manually.

The `warnings` property of the `StreamObjectResult` type is now a Promise.

AI SDK 3.4

```
const result =awaitstreamObject({// ...});const warnings = result.warnings;
```

AI SDK 4.0

```
const result =streamObject({// ...});const warnings =await result.warnings;
```


#### [Renamed `simulateReadableStream` `values` to `chunks`](#renamed-simulatereadablestream-values-to-chunks)


There is no codemod available for this change. Please review and update your code manually.

The `simulateReadableStream` function from `ai/test` has been renamed to `chunks`.

AI SDK 3.4

```
import{ simulateReadableStream }from'ai/test';const stream =simulateReadableStream({  values:[1,2,3],  chunkDelayInMs:100,});
```

AI SDK 4.0

```
import{ simulateReadableStream }from'ai/test';const stream =simulateReadableStream({  chunks:[1,2,3],  chunkDelayInMs:100,});
```


## [AI SDK RSC Changes](#ai-sdk-rsc-changes)


There are no codemods available for the changes in this section. Please review and update your code manually.


### [Removed `render` function](#removed-render-function)


The AI SDK RSC 3.0 `render` function has been removed. Please use the `streamUI` function instead or [switch to AI SDK UI](/docs/ai-sdk-rsc/migrating-to-ui).

AI SDK 3.0

```
import{ render }from'ai/rsc';
```

AI SDK 4.0

```
import{ streamUI }from'ai/rsc';
```


## [AI SDK UI Changes](#ai-sdk-ui-changes)



### [Removed Svelte, Vue, and SolidJS exports](#removed-svelte-vue-and-solidjs-exports)


This codemod only operates on `.ts` and `.tsx` files. If you have code in files with other suffixes, please review and update your code manually.

The `ai` package no longer exports Svelte, Vue, and SolidJS UI integrations. You need to install the `@ai-sdk/svelte`, `@ai-sdk/vue`, and `@ai-sdk/solid` packages directly.

AI SDK 3.4

```
import{ useChat }from'ai/svelte';
```

AI SDK 4.0

```
import{ useChat }from'@ai-sdk/svelte';
```


### [Removed `experimental_StreamData`](#removed-experimental_streamdata)


The `experimental_StreamData` export has been removed. Please use the `StreamData` export instead.

AI SDK 3.4

```
import{ experimental_StreamData }from'ai';
```

AI SDK 4.0

```
import{StreamData}from'ai';
```


### [`useChat` hook](#usechat-hook)


There are no codemods available for the changes in this section. Please review and update your code manually.


#### [Removed `streamMode` setting](#removed-streammode-setting)


The `streamMode` options has been removed from the `useChat` hook. Please use the `streamProtocol` parameter instead.

AI SDK 3.4

```
const{ messages }=useChat({  streamMode:'text',// ...});
```

AI SDK 4.0

```
const{ messages }=useChat({  streamProtocol:'text',// ...});
```


#### [Replaced roundtrip setting with `maxSteps`](#replaced-roundtrip-setting-with-maxsteps)


The following options have been removed from the `useChat` hook:

-   `experimental_maxAutomaticRoundtrips`
-   `maxAutomaticRoundtrips`
-   `maxToolRoundtrips`

Please use the [`maxSteps`](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls) option instead. The value of `maxSteps` is equal to roundtrips + 1.

AI SDK 3.4

```
const{ messages }=useChat({  experimental_maxAutomaticRoundtrips:2,// or maxAutomaticRoundtrips// or maxToolRoundtrips// ...});
```

AI SDK 4.0

```
const{ messages }=useChat({  maxSteps:3,// 2 roundtrips + 1// ...});
```


#### [Removed `options` setting](#removed-options-setting)


The `options` parameter in the `useChat` hook has been removed. Please use the `headers` and `body` parameters instead.

AI SDK 3.4

```
const{ messages }=useChat({  options:{    headers:{'X-Custom-Header':'value',},},// ...});
```

AI SDK 4.0

```
const{ messages }=useChat({  headers:{'X-Custom-Header':'value',},// ...});
```


#### [Removed `experimental_addToolResult` method](#removed-experimental_addtoolresult-method)


The `experimental_addToolResult` method has been removed from the `useChat` hook. Please use the `addToolResult` method instead.

AI SDK 3.4

```
const{ messages, experimental_addToolResult }=useChat({// ...});
```

AI SDK 4.0

```
const{ messages, addToolResult }=useChat({// ...});
```


#### [Changed default value of `keepLastMessageOnError` to true and deprecated the option](#changed-default-value-of-keeplastmessageonerror-to-true-and-deprecated-the-option)


The `keepLastMessageOnError` option has been changed to default to `true`. The option will be removed in the next major release.

AI SDK 3.4

```
const{ messages }=useChat({  keepLastMessageOnError:true,// ...});
```

AI SDK 4.0

```
const{ messages }=useChat({// ...});
```


### [`useCompletion` hook](#usecompletion-hook)


There are no codemods available for the changes in this section. Please review and update your code manually.


#### [Removed `streamMode` setting](#removed-streammode-setting-1)


The `streamMode` options has been removed from the `useCompletion` hook. Please use the `streamProtocol` parameter instead.

AI SDK 3.4

```
const{ text }=useCompletion({  streamMode:'text',// ...});
```

AI SDK 4.0

```
const{ text }=useCompletion({  streamProtocol:'text',// ...});
```


### [`useAssistant` hook](#useassistant-hook)



#### [Removed `experimental_useAssistant` export](#removed-experimental_useassistant-export)


The `experimental_useAssistant` export has been removed from the `useAssistant` hook. Please use the `useAssistant` hook directly instead.

AI SDK 3.4

```
import{ experimental_useAssistant }from'@ai-sdk/react';
```

AI SDK 4.0

```
import{ useAssistant }from'@ai-sdk/react';
```


#### [Removed `threadId` and `messageId` from `AssistantResponse`](#removed-threadid-and-messageid-from-assistantresponse)


There is no codemod available for this change. Please review and update your code manually.

The `threadId` and `messageId` parameters have been removed from the `AssistantResponse` function. Please use the `threadId` and `messageId` variables from the outer scope instead.

AI SDK 3.4

```
returnAssistantResponse({ threadId: myThreadId, messageId: myMessageId },async({ forwardStream, sendDataMessage, threadId, messageId })=>{// use threadId and messageId here},);
```

AI SDK 4.0

```
returnAssistantResponse({ threadId: myThreadId, messageId: myMessageId },async({ forwardStream, sendDataMessage })=>{// use myThreadId and myMessageId here},);
```


#### [Removed `experimental_​AssistantResponse` export](#removed-experimental_assistantresponse-export)


There is no codemod available for this change. Please review and update your code manually.

The `experimental_AssistantResponse` export has been removed. Please use the `AssistantResponse` function directly instead.

AI SDK 3.4

```
import{ experimental_AssistantResponse }from'ai';
```

AI SDK 4.0

```
import{AssistantResponse}from'ai';
```


### [`experimental_useObject` hook](#experimental_useobject-hook)


There are no codemods available for the changes in this section. Please review and update your code manually.

The `setInput` helper has been removed from the `experimental_useObject` hook. Please use the `submit` helper instead.

AI SDK 3.4

```
const{ object, setInput }=useObject({// ...});
```

AI SDK 4.0

```
const{ object, submit }=useObject({// ...});
```


## [AI SDK Errors](#ai-sdk-errors)



### [Removed `isXXXError` static methods](#removed-isxxxerror-static-methods)


The `isXXXError` static methods have been removed from AI SDK errors. Please use the `isInstance` method of the corresponding error class instead.

AI SDK 3.4

```
import{APICallError}from'ai';APICallError.isAPICallError(error);
```

AI SDK 4.0

```
import{APICallError}from'ai';APICallError.isInstance(error);
```


### [Removed `toJSON` method](#removed-tojson-method)


There is no codemod available for this change. Please review and update your code manually.

The `toJSON` method has been removed from AI SDK errors.


## [AI SDK 2.x Legacy Changes](#ai-sdk-2x-legacy-changes)


There are no codemods available for the changes in this section. Please review and update your code manually.


### [Removed 2.x legacy providers](#removed-2x-legacy-providers)


Legacy providers from AI SDK 2.x have been removed. Please use the new [AI SDK provider architecture](/docs/foundations/providers-and-models) instead.


#### [Removed 2.x legacy function and tool calling](#removed-2x-legacy-function-and-tool-calling)


The legacy `function_call` and `tools` options have been removed from `useChat` and `Message`. The `name` property from the `Message` type has been removed. Please use the [AI SDK Core tool calling](/docs/ai-sdk-core/tools-and-tool-calling) instead.


### [Removed 2.x prompt helpers](#removed-2x-prompt-helpers)


Prompt helpers for constructing message prompts are no longer needed with the AI SDK provider architecture and have been removed.


### [Removed 2.x `AIStream`](#removed-2x-aistream)


The `AIStream` function and related exports have been removed. Please use the [`streamText`](/docs/reference/ai-sdk-core/stream-text) function and its `toDataStream()` method instead.


### [Removed 2.x `StreamingTextResponse`](#removed-2x-streamingtextresponse)


The `StreamingTextResponse` function has been removed. Please use the [`streamText`](/docs/reference/ai-sdk-core/stream-text) function and its `toDataStreamResponse()` method instead.


### [Removed 2.x `streamToResponse`](#removed-2x-streamtoresponse)


The `streamToResponse` function has been removed. Please use the [`streamText`](/docs/reference/ai-sdk-core/stream-text) function and its `pipeDataStreamToResponse()` method instead.


### [Removed 2.x RSC `Tokens` streaming](#removed-2x-rsc-tokens-streaming)


The legacy `Tokens` RSC streaming from 2.x has been removed. `Tokens` were implemented prior to AI SDK RSC and are no longer needed.


## [Codemod Table](#codemod-table)


The following table lists codemod availability for the AI SDK 4.0 upgrade process. Note the codemod `upgrade` command will run all of them for you. This list is provided to give visibility into which migrations have some automation. It can also be helpful to find the codemod names if you'd like to run a subset of codemods. For more, see the [Codemods](#codemods) section.

Change

Codemod

**Provider Changes**

Removed baseUrl option

`replace-baseurl`

**Anthropic Provider**

Removed Anthropic facade

`remove-anthropic-facade`

Removed topK setting

*N/A*

**Google Generative AI Provider**

Removed Google facade

`remove-google-facade`

Removed topK setting

*N/A*

**Google Vertex Provider**

Removed topK setting

*N/A*

**Mistral Provider**

Removed Mistral facade

`remove-mistral-facade`

**OpenAI Provider**

Removed OpenAI facade

`remove-openai-facade`

**LangChain Adapter**

Removed toAIStream

`replace-langchain-toaistream`

**AI SDK Core Changes**

streamText returns immediately

`remove-await-streamtext`

streamObject returns immediately

`remove-await-streamobject`

Remove roundtrips

`replace-roundtrips-with-maxsteps`

Removed nanoid export

`replace-nanoid`

Increased default size of generated IDs

*N/A*

Removed ExperimentalMessage types

`remove-experimental-message-types`

Removed ExperimentalTool type

`remove-experimental-tool`

Removed experimental AI function exports

`remove-experimental-ai-fn-exports`

Removed AI-stream related methods from streamText

`remove-ai-stream-methods-from-stream-text-result`

Renamed "formatStreamPart" to "formatDataStreamPart"

`rename-format-stream-part`

Renamed "parseStreamPart" to "parseDataStreamPart"

`rename-parse-stream-part`

Renamed TokenUsage, CompletionTokenUsage and EmbeddingTokenUsage types

`replace-token-usage-types`

Removed deprecated telemetry data

*N/A*

**Provider Registry**

→ Removed experimental\_Provider, experimental\_ProviderRegistry, and experimental\_ModelRegistry

`remove-deprecated-provider-registry-exports`

→ Removed experimental\_createModelRegistry function

*N/A*

Removed rawResponse from results

*N/A*

Removed init option from pipeDataStreamToResponse and toDataStreamResponse

*N/A*

Removed responseMessages from generateText and streamText

*N/A*

Removed experimental\_continuationSteps option

`replace-continuation-steps`

Removed LanguageModelResponseMetadataWithHeaders type

`remove-metadata-with-headers`

Changed streamText warnings result to Promise

*N/A*

Changed streamObject warnings result to Promise

*N/A*

Renamed simulateReadableStream values to chunks

*N/A*

**AI SDK RSC Changes**

Removed render function

*N/A*

**AI SDK UI Changes**

Removed Svelte, Vue, and SolidJS exports

`rewrite-framework-imports`

Removed experimental\_StreamData

`remove-experimental-streamdata`

**useChat hook**

Removed streamMode setting

*N/A*

Replaced roundtrip setting with maxSteps

`replace-roundtrips-with-maxsteps`

Removed options setting

*N/A*

Removed experimental\_addToolResult method

*N/A*

Changed default value of keepLastMessageOnError to true and deprecated the option

*N/A*

**useCompletion hook**

Removed streamMode setting

*N/A*

**useAssistant hook**

Removed experimental\_useAssistant export

`remove-experimental-useassistant`

Removed threadId and messageId from AssistantResponse

*N/A*

Removed experimental\_AssistantResponse export

*N/A*

**experimental\_useObject hook**

Removed setInput helper

*N/A*

**AI SDK Errors**

Removed isXXXError static methods

`remove-isxxxerror`

Removed toJSON method

*N/A*

**AI SDK 2.x Legacy Changes**

Removed 2.x legacy providers

*N/A*

Removed 2.x legacy function and tool calling

*N/A*

Removed 2.x prompt helpers

*N/A*

Removed 2.x AIStream

*N/A*

Removed 2.x StreamingTextResponse

*N/A*

Removed 2.x streamToResponse

*N/A*

Removed 2.x RSC Tokens streaming

*N/A*
```

### 153. `docs/migration-guides/migration-guide-4-1.md`

```markdown
# Migrate AI SDK 4.0 to 4.1


---
url: https://ai-sdk.dev/docs/migration-guides/migration-guide-4-1
description: Learn how to upgrade AI SDK 4.0 to 4.1.
---


# [Migrate AI SDK 4.0 to 4.1](#migrate-ai-sdk-40-to-41)


Check out the [AI SDK 4.1 release blog post](https://vercel.com/blog/ai-sdk-4-1) for more information about the release.

No breaking changes in this release.
```

### 154. `docs/migration-guides/migration-guide-4-2.md`

```markdown
# Migrate AI SDK 4.1 to 4.2


---
url: https://ai-sdk.dev/docs/migration-guides/migration-guide-4-2
description: Learn how to upgrade AI SDK 4.1 to 4.2.
---


# [Migrate AI SDK 4.1 to 4.2](#migrate-ai-sdk-41-to-42)


Check out the [AI SDK 4.2 release blog post](https://vercel.com/blog/ai-sdk-4-2) for more information about the release.

This guide will help you upgrade to AI SDK 4.2:


## [Stable APIs](#stable-apis)


The following APIs have been moved to stable and no longer have the `experimental_` prefix:

-   `customProvider`
-   `providerOptions` (renamed from `providerMetadata` for provider-specific inputs)
-   `providerMetadata` (for provider-specific outputs)
-   `toolCallStreaming` option for `streamText`


## [Dependency Versions](#dependency-versions)


AI SDK requires a non-optional `zod` dependency with version `^3.23.8`.


## [UI Message Parts](#ui-message-parts)


In AI SDK 4.2, we've redesigned how `useChat` handles model outputs with message parts and multiple steps. This is a significant improvement that simplifies rendering complex, multi-modal AI responses in your UI.


### [What's Changed](#whats-changed)


Assistant messages with tool calling now get combined into a single message with multiple parts, rather than creating separate messages for each step. This change addresses two key developments in AI applications:

1.  **Diverse Output Types**: Models now generate more than just text; they produce reasoning steps, sources, and tool calls.
2.  **Interleaved Outputs**: In multi-step agent use-cases, these different output types are frequently interleaved.


### [Benefits of the New Approach](#benefits-of-the-new-approach)


Previously, `useChat` stored different output types separately, which made it challenging to maintain the correct sequence in your UI when these elements were interleaved in a response, and led to multiple consecutive assistant messages when there were tool calls. For example:

```
message.content="Final answer: 42";message.reasoning="First I'll calculate X, then Y...";message.toolInvocations=[{toolName:"calculator", args:{...}}];
```

This structure was limiting. The new message parts approach replaces separate properties with an ordered array that preserves the exact sequence:

```
message.parts=[{ type:"text", text:"Final answer: 42"},{ type:"reasoning", reasoning:"First I'll calculate X, then Y..."},{ type:"tool-invocation", toolInvocation:{ toolName:"calculator", args:{...}}},];
```


### [Migration](#migration)


Existing applications using the previous message format will need to update their UI components to handle the new `parts` array. The fields from the previous format are still available for backward compatibility, but we recommend migrating to the new format for better support of multi-modal and multi-step interactions.

You can use the `useChat` hook with the new message parts as follows:

```
functionChat(){const{ messages }=useChat();return(<div>{messages.map(message=>        message.parts.map((part, i)=>{switch(part.type){case'text':return<p key={i}>{part.text}</p>;case'source':return<p key={i}>{part.source.url}</p>;case'reasoning':return<div key={i}>{part.reasoning}</div>;case'tool-invocation':return<div key={i}>{part.toolInvocation.toolName}</div>;case'file':return(<img                  key={i}                  src={`data:${part.mimeType};base64,${part.data}`}/>);}}),)}</div>);}
```
```

### 155. `docs/migration-guides/versioning.md`

```markdown
# Versioning


---
url: https://ai-sdk.dev/docs/migration-guides/versioning
description: Understand how the AI SDK approaches versioning.
---


# [Versioning](#versioning)


Each version number follows the format: `MAJOR.MINOR.PATCH`

-   **Major**: Breaking API updates that require code changes.
-   **Minor**: Blog post that aggregates new features and improvements into a public release that highlights benefits.
-   **Patch**: New features and bug fixes.


## [API Stability](#api-stability)


We communicate the stability of our APIs as follows:


### [Stable APIs](#stable-apis)


All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases.


### [Experimental APIs](#experimental-apis)


APIs prefixed with `experimental_` or `Experimental_` (e.g. `experimental_generateImage()`) are in development and can change in any releases. To use experimental APIs safely:

1.  Test them first in development, not production
2.  Review release notes before upgrading
3.  Prepare for potential code updates

If you use experimental APIs, make sure to pin your AI SDK version number exactly (avoid using ^ or ~ version ranges) to prevent unexpected breaking changes.


### [Deprecated APIs](#deprecated-apis)


APIs marked as `deprecated` will be removed in future major releases. You can wait until the major release to update your code. To handle deprecations:

1.  Switch to the recommended alternative API
2.  Follow the migration guide (released alongside major releases)

For major releases, we provide automated codemods where possible to help migrate your code to the new version.
```

### 156. `docs/migration-guides.md`

```markdown
# Migration Guides


---
url: https://ai-sdk.dev/docs/migration-guides
description: Learn how to upgrade between Vercel AI versions.
---


# [Migration Guides](#migration-guides)


-   [Migrate AI SDK 4.1 to 4.2](/docs/migration-guides/migration-guide-4-2)
-   [Migrate AI SDK 4.0 to 4.1](/docs/migration-guides/migration-guide-4-1)
-   [Migrate AI SDK 3.4 to 4.0](/docs/migration-guides/migration-guide-4-0)
-   [Migrate AI SDK 3.3 to 3.4](/docs/migration-guides/migration-guide-3-4)
-   [Migrate AI SDK 3.2 to 3.3](/docs/migration-guides/migration-guide-3-3)
-   [Migrate AI SDK 3.1 to 3.2](/docs/migration-guides/migration-guide-3-2)
-   [Migrate AI SDK 3.0 to 3.1](/docs/migration-guides/migration-guide-3-1)


## [Versioning](#versioning)


-   [Versioning](/docs/migration-guides/versioning)
```

### 157. `docs/reference/ai-sdk-core/core-message.md`

```markdown
# CoreMessage


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/core-message
description: Message types for AI SDK Core (API Reference)
---


# [`CoreMessage`](#coremessage)


`CoreMessage` represents the fundamental message structure used with AI SDK Core functions. It encompasses various message types that can be used in the `messages` field of any AI SDK Core functions.

You can access the Zod schema for `CoreMessage` with the `coreMessageSchema` export.


## [`CoreMessage` Types](#coremessage-types)



### [`CoreSystemMessage`](#coresystemmessage)


A system message that can contain system information.

```
typeCoreSystemMessage={  role:'system';  content:string;};
```

You can access the Zod schema for `CoreSystemMessage` with the `coreSystemMessageSchema` export.

Using the "system" property instead of a system message is recommended to enhance resilience against prompt injection attacks.


### [`CoreUserMessage`](#coreusermessage)


A user message that can contain text or a combination of text, images, and files.

```
typeCoreUserMessage={  role:'user';  content:UserContent;};typeUserContent=string|Array<TextPart|ImagePart|FilePart>;
```

You can access the Zod schema for `CoreUserMessage` with the `coreUserMessageSchema` export.


### [`CoreAssistantMessage`](#coreassistantmessage)


An assistant message that can contain text, tool calls, or a combination of both.

```
typeCoreAssistantMessage={  role:'assistant';  content:AssistantContent;};typeAssistantContent=string|Array<TextPart|ToolCallPart>;
```

You can access the Zod schema for `CoreAssistantMessage` with the `coreAssistantMessageSchema` export.


### [`CoreToolMessage`](#coretoolmessage)


A tool message that contains the result of one or more tool calls.

```
typeCoreToolMessage={  role:'tool';  content:ToolContent;};typeToolContent=Array<ToolResultPart>;
```

You can access the Zod schema for `CoreToolMessage` with the `coreToolMessageSchema` export.


## [`CoreMessage` Parts](#coremessage-parts)



### [`TextPart`](#textpart)


Represents a text content part of a prompt. It contains a string of text.

```
exportinterfaceTextPart{type:'text';/**   * The text content.   */  text:string;}
```


### [`ImagePart`](#imagepart)


Represents an image part in a user message.

```
exportinterfaceImagePart{type:'image';/**   * Image data. Can either be:   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer   * - URL: a URL that points to the image   */  image:DataContent|URL;/**   * Optional mime type of the image.   * We recommend leaving this out as it will be detected automatically.   */  mimeType?:string;}
```


### [`FilePart`](#filepart)


Represents an file part in a user message.

```
exportinterfaceFilePart{type:'file';/**   * File data. Can either be:   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer   * - URL: a URL that points to the file   */  data:DataContent|URL;/**   * Mime type of the file.   */  mimeType:string;}
```


### [`ToolCallPart`](#toolcallpart)


Represents a tool call content part of a prompt, typically generated by the AI model.

```
exportinterfaceToolCallPart{type:'tool-call';/**   * ID of the tool call. This ID is used to match the tool call with the tool result.   */  toolCallId:string;/**   * Name of the tool that is being called.   */  toolName:string;/**   * Arguments of the tool call. This is a JSON-serializable object that matches the tool's input schema.   */  args:unknown;}
```


### [`ToolResultPart`](#toolresultpart)


Represents the result of a tool call in a tool message.

```
exportinterfaceToolResultPart{type:'tool-result';/**   * ID of the tool call that this result is associated with.   */  toolCallId:string;/**   * Name of the tool that generated this result.   */  toolName:string;/**   * Result of the tool call. This is a JSON-serializable object.   */  result:unknown;/**   * Multi-part content of the tool result. Only for tools that support multipart results.   */  experimental_content?:ToolResultContent;/**   * Optional flag if the result is an error or an error message.   */  isError?:boolean;}
```


### [`ToolResultContent`](#toolresultcontent)


```
exporttypeToolResultContent=Array<|{type:'text';      text:string;}|{type:'image';      data:string;// base64 encoded png image, e.g. screenshot      mimeType?:string;// e.g. 'image/png';}>;
```
```

### 158. `docs/reference/ai-sdk-core/cosine-similarity.md`

```markdown
# cosineSimilarity()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/cosine-similarity
description: Calculate the cosine similarity between two vectors (API Reference)
---


# [`cosineSimilarity()`](#cosinesimilarity)


When you want to compare the similarity of embeddings, standard vector similarity metrics like cosine similarity are often used.

`cosineSimilarity` calculates the cosine similarity between two vectors. A high value (close to 1) indicates that the vectors are very similar, while a low value (close to -1) indicates that they are different.

```
import{ openai }from'@ai-sdk/openai';import{ cosineSimilarity, embedMany }from'ai';const{ embeddings }=awaitembedMany({  model: openai.embedding('text-embedding-3-small'),  values:['sunny day at the beach','rainy afternoon in the city'],});console.log(`cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,);
```


## [Import](#import)


import { cosineSimilarity } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### vector1:


number\[\]

The first vector to compare


### vector2:


number\[\]

The second vector to compare


### options?:


Object

Optional configuration.

Object


### throwErrorForEmptyVectors?:


boolean

Set throwErrorForEmptyVectors to true to throw an error when vectors are empty (default: false)


### [Returns](#returns)


A number between -1 and 1 representing the cosine similarity between the two vectors.
```

### 159. `docs/reference/ai-sdk-core/create-id-generator.md`

```markdown
# createIdGenerator()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/create-id-generator
description: Create a customizable unique identifier generator (API Reference)
---


# [`createIdGenerator()`](#createidgenerator)


Creates a customizable ID generator function. You can configure the alphabet, prefix, separator, and default size of the generated IDs.

```
import{ createIdGenerator }from'ai';const generateCustomId =createIdGenerator({  prefix:'user',  separator:'_',});const id =generateCustomId();// Example: "user_1a2b3c4d5e6f7g8h"
```


## [Import](#import)


import { createIdGenerator } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### options:


object

Optional configuration object with the following properties:


### options.alphabet:


string

The characters to use for generating the random part of the ID. Defaults to alphanumeric characters (0-9, A-Z, a-z).


### options.prefix:


string

A string to prepend to all generated IDs. Defaults to none.


### options.separator:


string

The character(s) to use between the prefix and the random part. Defaults to "-".


### options.size:


number

The default length of the random part of the ID. Defaults to 16.


### [Returns](#returns)


Returns a function that generates IDs based on the configured options.


### [Notes](#notes)


-   The generator uses non-secure random generation and should not be used for security-critical purposes.
-   The separator character must not be part of the alphabet to ensure reliable prefix checking.


## [Example](#example)


```
// Create a custom ID generator for user IDsconst generateUserId =createIdGenerator({  prefix:'user',  separator:'_',  size:8,});// Generate IDsconst id1 =generateUserId();// e.g., "user_1a2b3c4d"
```


## [See also](#see-also)


-   [`generateId()`](/docs/reference/ai-sdk-core/generate-id)
```

### 160. `docs/reference/ai-sdk-core/create-mcp-client.md`

```markdown
# experimental_createMCPClient()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/create-mcp-client
description: Create a client for connecting to MCP servers
---


# [`experimental_createMCPClient()`](#experimental_createmcpclient)


Creates a lightweight Model Context Protocol (MCP) client that connects to an MCP server. The client's primary purpose is tool conversion between MCP tools and AI SDK tools.

It currently does not support accepting notifications from an MCP server, and custom configuration of the client.

This feature is experimental and may change or be removed in the future.


## [Import](#import)


import { experimental\_createMCPClient } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### config:


MCPClientConfig

Configuration for the MCP client.

MCPClientConfig


### transport:


TransportConfig = MCPTransport | McpSSEServerConfig

Configuration for the message transport layer.

MCPTransport


### start:


() => Promise<void>

A method that starts the transport


### send:


(message: JSONRPCMessage) => Promise<void>

A method that sends a message through the transport


### close:


() => Promise<void>

A method that closes the transport


### onclose:


() => void

A method that is called when the transport is closed


### onerror:


(error: Error) => void

A method that is called when the transport encounters an error


### onmessage:


(message: JSONRPCMessage) => void

A method that is called when the transport receives a message

McpSSEServerConfig


### type:


'sse'

Use Server-Sent Events for communication


### url:


string

URL of the MCP server


### headers?:


Record<string, string>

Additional HTTP headers to be sent with requests.


### name?:


string

Client name. Defaults to "ai-sdk-mcp-client"


### onUncaughtError?:


(error: unknown) => void

Handler for uncaught errors


### [Returns](#returns)


Returns a Promise that resolves to an `MCPClient` with the following methods:


### tools:


async (options?: { schemas?: TOOL\_SCHEMAS }) => Promise<McpToolSet<TOOL\_SCHEMAS>>

Gets the tools available from the MCP server.

options


### schemas?:


TOOL\_SCHEMAS

Schema definitions for compile-time type checking. When not provided, schemas are inferred from the server.


### close:


async () => void

Closes the connection to the MCP server and cleans up resources.


## [Example](#example)


```
import{ experimental_createMCPClient, generateText }from'ai';import{ openai }from'@ai-sdk/openai';try{const client =awaitexperimental_createMCPClient({    transport:{type:'stdio',      command:'node server.js',},});const tools =await client.tools();const response =awaitgenerateText({    model:openai('gpt-4o-mini'),    tools,    messages:[{ role:'user', content:'Query the data'}],});console.log(response);}finally{await client.close();}
```


## [Error Handling](#error-handling)


The client throws `MCPClientError` for:

-   Client initialization failures
-   Protocol version mismatches
-   Missing server capabilities
-   Connection failures

For tool execution, errors are propagated as `CallToolError` errors.

For unknown errors, the client exposes an `onUncaughtError` callback that can be used to manually log or handle errors that are not covered by known error types.
```

### 161. `docs/reference/ai-sdk-core/custom-provider.md`

```markdown
# customProvider()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/custom-provider
description: Custom provider that uses models from a different provider (API Reference)
---


# [`customProvider()`](#customprovider)


With a custom provider, you can map ids to any model. This allows you to set up custom model configurations, alias names, and more. The custom provider also supports a fallback provider, which is useful for wrapping existing providers and adding additional functionality.


### [Example: custom model settings](#example-custom-model-settings)


You can create a custom provider using `customProvider`.

```
import{ openai }from'@ai-sdk/openai';import{ customProvider }from'ai';// custom provider with different model settings:exportconst myOpenAI =customProvider({  languageModels:{// replacement model with custom settings:'gpt-4':openai('gpt-4',{ structuredOutputs:true}),// alias model with custom settings:'gpt-4o-structured':openai('gpt-4o',{ structuredOutputs:true}),},  fallbackProvider: openai,});
```


## [Import](#import)


import {  customProvider } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### languageModels?:


Record<string, LanguageModel>

A record of language models, where keys are model IDs and values are LanguageModel instances.


### textEmbeddingModels?:


Record<string, EmbeddingModelV1<string>>

A record of text embedding models, where keys are model IDs and values are EmbeddingModel<string> instances.


### imageModels?:


Record<string, ImageModelV1>

A record of image models, where keys are model IDs and values are ImageModelV1 instances.


### fallbackProvider?:


Provider

An optional fallback provider to use when a requested model is not found in the custom provider.


### [Returns](#returns)


The `customProvider` function returns a `Provider` instance. It has the following methods:


### languageModel:


(id: string) => LanguageModel

A function that returns a language model by its id (format: providerId:modelId)


### textEmbeddingModel:


(id: string) => EmbeddingModel<string>

A function that returns a text embedding model by its id (format: providerId:modelId)


### imageModel:


(id: string) => ImageModel

A function that returns an image model by its id (format: providerId:modelId)
```

### 162. `docs/reference/ai-sdk-core/default-settings-middleware.md`

```markdown
# defaultSettingsMiddleware()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/default-settings-middleware
description: Middleware that applies default settings for language models
---


# [`defaultSettingsMiddleware()`](#defaultsettingsmiddleware)


`defaultSettingsMiddleware` is a middleware function that applies default settings to language model calls. This is useful when you want to establish consistent default parameters across multiple model invocations.

```
import{ defaultSettingsMiddleware }from'ai';const middleware =defaultSettingsMiddleware({  settings:{    temperature:0.7,    maxTokens:1000,// other settings...},});
```


## [Import](#import)


import { defaultSettingsMiddleware } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)


The middleware accepts a configuration object with the following properties:

-   `settings`: An object containing default parameter values to apply to language model calls. These can include any valid `LanguageModelV1CallOptions` properties and optional provider metadata.


### [Returns](#returns)


Returns a middleware object that:

-   Merges the default settings with the parameters provided in each model call
-   Ensures that explicitly provided parameters take precedence over defaults
-   Merges provider metadata objects


### [Usage Example](#usage-example)


```
import{ streamText }from'ai';import{ wrapLanguageModel }from'ai';import{ defaultSettingsMiddleware }from'ai';import{ openai }from'ai';// Create a model with default settingsconst modelWithDefaults =wrapLanguageModel({  model: openai.ChatTextGenerator({ model:'gpt-4'}),  middleware:defaultSettingsMiddleware({    settings:{      temperature:0.5,      maxTokens:800,      providerMetadata:{        openai:{          tags:['production'],},},},}),});// Use the model - default settings will be appliedconst result =awaitstreamText({  model: modelWithDefaults,  prompt:'Your prompt here',// These parameters will override the defaults  temperature:0.8,});
```


## [How It Works](#how-it-works)


The middleware:

1.  Takes a set of default settings as configuration
2.  Merges these defaults with the parameters provided in each model call
3.  Ensures that explicitly provided parameters take precedence over defaults
4.  Merges provider metadata objects from both sources
```

### 163. `docs/reference/ai-sdk-core/embed-many.md`

```markdown
# embedMany()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/embed-many
description: API Reference for embedMany.
---


# [`embedMany()`](#embedmany)


Embed several values using an embedding model. The type of the value is defined by the embedding model.

`embedMany` automatically splits large requests into smaller chunks if the model has a limit on how many embeddings can be generated in a single call.

```
import{ openai }from'@ai-sdk/openai';import{ embedMany }from'ai';const{ embeddings }=awaitembedMany({  model: openai.embedding('text-embedding-3-small'),  values:['sunny day at the beach','rainy afternoon in the city','snowy night in the mountains',],});
```


## [Import](#import)


import { embedMany } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


EmbeddingModel

The embedding model to use. Example: openai.embedding('text-embedding-3-small')


### values:


Array<VALUE>

The values to embed. The type depends on the model.


### maxRetries?:


number

Maximum number of retries. Set to 0 to disable retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal that can be used to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.


### experimental\_telemetry?:


TelemetrySettings

Telemetry configuration. Experimental feature.

TelemetrySettings


### isEnabled?:


boolean

Enable or disable telemetry. Disabled by default while experimental.


### recordInputs?:


boolean

Enable or disable input recording. Enabled by default.


### recordOutputs?:


boolean

Enable or disable output recording. Enabled by default.


### functionId?:


string

Identifier for this function. Used to group telemetry data by function.


### metadata?:


Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>

Additional information to include in the telemetry data.


### [Returns](#returns)



### values:


Array<VALUE>

The values that were embedded.


### embeddings:


number\[\]\[\]

The embeddings. They are in the same order as the values.


### usage:


EmbeddingTokenUsage

The token usage for generating the embeddings.

EmbeddingTokenUsage


### tokens:


number

The total number of input tokens.
```

### 164. `docs/reference/ai-sdk-core/embed.md`

```markdown
# embed()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/embed
description: API Reference for embed.
---


# [`embed()`](#embed)


Generate an embedding for a single value using an embedding model.

This is ideal for use cases where you need to embed a single value to e.g. retrieve similar items or to use the embedding in a downstream task.

```
import{ openai }from'@ai-sdk/openai';import{ embed }from'ai';const{ embedding }=awaitembed({  model: openai.embedding('text-embedding-3-small'),  value:'sunny day at the beach',});
```


## [Import](#import)


import { embed } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


EmbeddingModel

The embedding model to use. Example: openai.embedding('text-embedding-3-small')


### value:


VALUE

The value to embed. The type depends on the model.


### maxRetries?:


number

Maximum number of retries. Set to 0 to disable retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal that can be used to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.


### experimental\_telemetry?:


TelemetrySettings

Telemetry configuration. Experimental feature.

TelemetrySettings


### isEnabled?:


boolean

Enable or disable telemetry. Disabled by default while experimental.


### recordInputs?:


boolean

Enable or disable input recording. Enabled by default.


### recordOutputs?:


boolean

Enable or disable output recording. Enabled by default.


### functionId?:


string

Identifier for this function. Used to group telemetry data by function.


### metadata?:


Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>

Additional information to include in the telemetry data.


### [Returns](#returns)



### value:


VALUE

The value that was embedded.


### embedding:


number\[\]

The embedding of the value.


### usage:


EmbeddingTokenUsage

The token usage for generating the embeddings.

EmbeddingTokenUsage


### tokens:


number

The total number of input tokens.


### rawResponse?:


RawResponse

Optional raw response data.

RawResponse


### headers?:


Record<string, string>

Response headers.
```

### 165. `docs/reference/ai-sdk-core/extract-reasoning-middleware.md`

```markdown
# extractReasoningMiddleware()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/extract-reasoning-middleware
description: Middleware that extracts XML-tagged reasoning sections from generated text
---


# [`extractReasoningMiddleware()`](#extractreasoningmiddleware)


`extractReasoningMiddleware` is a middleware function that extracts XML-tagged reasoning sections from generated text and exposes them separately from the main text content. This is particularly useful when you want to separate an AI model's reasoning process from its final output.

```
import{ extractReasoningMiddleware }from'ai';const middleware =extractReasoningMiddleware({  tagName:'reasoning',  separator:'\n',});
```


## [Import](#import)


import { extractReasoningMiddleware } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### tagName:


string

The name of the XML tag to extract reasoning from (without angle brackets)


### separator?:


string

The separator to use between reasoning and text sections. Defaults to "\\n"


### startWithReasoning?:


boolean

Starts with reasoning tokens. Set to true when the response always starts with reasoning and the initial tag is omitted. Defaults to false.


### [Returns](#returns)


Returns a middleware object that:

-   Processes both streaming and non-streaming responses
-   Extracts content between specified XML tags as reasoning
-   Removes the XML tags and reasoning from the main text
-   Adds a `reasoning` property to the result containing the extracted content
-   Maintains proper separation between text sections using the specified separator


### [Type Parameters](#type-parameters)


The middleware works with the `LanguageModelV1StreamPart` type for streaming responses.
```

### 166. `docs/reference/ai-sdk-core/generate-id.md`

```markdown
# generateId()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-id
description: Generate a unique identifier (API Reference)
---


# [`generateId()`](#generateid)


Generates a unique identifier. You can optionally provide the length of the ID.

This is the same id generator used by the AI SDK.

```
import{ generateId }from'ai';const id =generateId();
```


## [Import](#import)


import { generateId } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### size:


number

The length of the generated ID. It defaults to 16. This parameter is deprecated and will be removed in the next major version.


### [Returns](#returns)


A string representing the generated ID.


## [See also](#see-also)


-   [`createIdGenerator()`](/docs/reference/ai-sdk-core/create-id-generator)
```

### 167. `docs/reference/ai-sdk-core/generate-image.md`

```markdown
# generateImage()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-image
description: API Reference for generateImage.
---


# [`generateImage()`](#generateimage)


`generateImage` is an experimental feature.

Generates images based on a given prompt using an image model.

It is ideal for use cases where you need to generate images programmatically, such as creating visual content or generating images for data augmentation.

```
import{ experimental_generateImage as generateImage }from'ai';const{ images }=awaitgenerateImage({  model: openai.image('dall-e-3'),  prompt:'A futuristic cityscape at sunset',  n:3,  size:'1024x1024',});console.log(images);
```


## [Import](#import)


import { experimental\_generateImage as generateImage } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


ImageModelV1

The image model to use.


### prompt:


string

The input prompt to generate the image from.


### n?:


number

Number of images to generate.


### size?:


string

Size of the images to generate. Format: \`{width}x{height}\`.


### aspectRatio?:


string

Aspect ratio of the images to generate. Format: \`{width}:{height}\`.


### seed?:


number

Seed for the image generation.


### providerOptions?:


Record<string, Record<string, JSONValue>>

Additional provider-specific options.


### maxRetries?:


number

Maximum number of retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers for the request.


### [Returns](#returns)



### image:


GeneratedFile

The first image that was generated.

GeneratedFile


### base64:


string

Image as a base64 encoded string.


### uint8Array:


Uint8Array

Image as a Uint8Array.


### mimeType:


string

MIME type of the image.


### images:


Array<GeneratedFile>

All images that were generated.

GeneratedFile


### base64:


string

Image as a base64 encoded string.


### uint8Array:


Uint8Array

Image as a Uint8Array.


### mimeType:


string

MIME type of the image.


### warnings:


ImageGenerationWarning\[\]

Warnings from the model provider (e.g. unsupported settings).


### responses:


Array<ImageModelResponseMetadata>

Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.

ImageModelResponseMetadata


### timestamp:


Date

Timestamp for the start of the generated response.


### modelId:


string

The ID of the response model that was used to generate the response.


### headers?:


Record<string, string>

Response headers.
```

### 168. `docs/reference/ai-sdk-core/generate-object.md`

```markdown
# generateObject()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-object
description: API Reference for generateObject.
---


# [`generateObject()`](#generateobject)


Generates a typed, structured object for a given prompt and schema using a language model.

It can be used to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.


#### [Example: generate an object using a schema](#example-generate-an-object-using-a-schema)


```
import{ openai }from'@ai-sdk/openai';import{ generateObject }from'ai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model:openai('gpt-4-turbo'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.string()),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});console.log(JSON.stringify(object,null,2));
```


#### [Example: generate an array using a schema](#example-generate-an-array-using-a-schema)


For arrays, you specify the schema of the array items.

```
import{ openai }from'@ai-sdk/openai';import{ generateObject }from'ai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model:openai('gpt-4-turbo'),  output:'array',  schema: z.object({    name: z.string(),class: z.string().describe('Character class, e.g. warrior, mage, or thief.'),    description: z.string(),}),  prompt:'Generate 3 hero descriptions for a fantasy role playing game.',});
```


#### [Example: generate an enum](#example-generate-an-enum)


When you want to generate a specific enum value, you can set the output strategy to `enum` and provide the list of possible values in the `enum` parameter.

```
import{ generateObject }from'ai';const{ object }=awaitgenerateObject({  model: yourModel,  output:'enum',enum:['action','comedy','drama','horror','sci-fi'],  prompt:'Classify the genre of this movie plot: '+'"A group of astronauts travel through a wormhole in search of a '+'new habitable planet for humanity."',});
```


#### [Example: generate JSON without a schema](#example-generate-json-without-a-schema)


```
import{ openai }from'@ai-sdk/openai';import{ generateObject }from'ai';const{ object }=awaitgenerateObject({  model:openai('gpt-4-turbo'),  output:'no-schema',  prompt:'Generate a lasagna recipe.',});
```

To see `generateObject` in action, check out the [additional examples](#more-examples).


## [Import](#import)


import { generateObject } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


LanguageModel

The language model to use. Example: openai('gpt-4-turbo')


### output:


'object' | 'array' | 'enum' | 'no-schema' | undefined

The type of output to generate. Defaults to 'object'.


### mode:


'auto' | 'json' | 'tool'

The mode to use for object generation. Not every model supports all modes. Defaults to 'auto' for 'object' output and to 'json' for 'no-schema' output. Must be 'json' for 'no-schema' output.


### schema:


Zod Schema | JSON Schema

The schema that describes the shape of the object to generate. It is sent to the model to generate the object and used to validate the output. You can either pass in a Zod schema or a JSON schema (using the \`jsonSchema\` function). In 'array' mode, the schema is used to describe an array element. Not available with 'no-schema' or 'enum' output.


### schemaName:


string | undefined

Optional name of the output that should be generated. Used by some providers for additional LLM guidance, e.g. via tool or schema name. Not available with 'no-schema' or 'enum' output.


### schemaDescription:


string | undefined

Optional description of the output that should be generated. Used by some providers for additional LLM guidance, e.g. via tool or schema name. Not available with 'no-schema' or 'enum' output.


### enum:


string\[\]

List of possible values to generate. Only available with 'enum' output.


### system:


string

The system prompt to use that specifies the behavior of the model.


### prompt:


string

The input prompt to generate the text from.


### messages:


Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>

A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.

CoreSystemMessage


### role:


'system'

The role for the system message.


### content:


string

The content of the message.

CoreUserMessage


### role:


'user'

The role for the user message.


### content:


string | Array<TextPart | ImagePart | FilePart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ImagePart


### type:


'image'

The type of the message part.


### image:


string | Uint8Array | Buffer | ArrayBuffer | URL

The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType?:


string

The mime type of the image. Optional.

FilePart


### type:


'file'

The type of the message part.


### data:


string | Uint8Array | Buffer | ArrayBuffer | URL

The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType:


string

The mime type of the file.

CoreAssistantMessage


### role:


'assistant'

The role for the assistant message.


### content:


string | Array<TextPart | ReasoningPart | RedactedReasoningPart | ToolCallPart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ReasoningPart


### type:


'reasoning'


### text:


string

The reasoning text.


### signature?:


string

The signature for the reasoning.

RedactedReasoningPart


### type:


'redacted-reasoning'


### data:


string

The redacted data.

ToolCallPart


### type:


'tool-call'

The type of the message part.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on schema

Parameters generated by the model to be used by the tool.

CoreToolMessage


### role:


'tool'

The role for the assistant message.


### content:


Array<ToolResultPart>

The content of the message.

ToolResultPart


### type:


'tool-result'

The type of the message part.


### toolCallId:


string

The id of the tool call the result corresponds to.


### toolName:


string

The name of the tool the result corresponds to.


### result:


unknown

The result returned by the tool after execution.


### isError?:


boolean

Whether the result is an error or an error message.


### maxTokens?:


number

Maximum number of tokens to generate.


### temperature?:


number

Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topP?:


number

Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topK?:


number

Only sample from the top K options for each subsequent token. Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.


### presencePenalty?:


number

Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.


### frequencyPenalty?:


number

Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.


### seed?:


number

The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.


### maxRetries?:


number

Maximum number of retries. Set to 0 to disable retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal that can be used to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.


### experimental\_repairText?:


(options: RepairTextOptions) => Promise<string>

A function that attempts to repair the raw output of the model to enable JSON parsing. Should return the repaired text or null if the text cannot be repaired.

RepairTextOptions


### text:


string

The text that was generated by the model.


### error:


JSONParseError | TypeValidationError

The error that occurred while parsing the text.


### experimental\_telemetry?:


TelemetrySettings

Telemetry configuration. Experimental feature.

TelemetrySettings


### isEnabled?:


boolean

Enable or disable telemetry. Disabled by default while experimental.


### recordInputs?:


boolean

Enable or disable input recording. Enabled by default.


### recordOutputs?:


boolean

Enable or disable output recording. Enabled by default.


### functionId?:


string

Identifier for this function. Used to group telemetry data by function.


### metadata?:


Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>

Additional information to include in the telemetry data.


### providerOptions?:


Record<string,Record<string,JSONValue>> | undefined

Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### [Returns](#returns)



### object:


based on the schema

The generated object, validated by the schema (if it supports validation).


### finishReason:


'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'

The reason the model finished generating the text.


### usage:


CompletionTokenUsage

The token usage of the generated text.

CompletionTokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### request?:


RequestMetadata

Request metadata.

RequestMetadata


### body:


string

Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).


### response?:


ResponseMetadata

Response metadata.

ResponseMetadata


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### modelId:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### body?:


unknown

Optional response body.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### providerMetadata:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### toJsonResponse:


(init?: ResponseInit) => Response

Converts the object to a JSON response. The response will have a status code of 200 and a content type of \`application/json; charset=utf-8\`.


## [More Examples](#more-examples)


[

Learn to generate structured data using a language model in Next.js

](/examples/next-app/basics/generating-object)[

Learn to generate structured data using a language model in Node.js

](/examples/node/generating-structured-data/generate-object)
```

### 169. `docs/reference/ai-sdk-core/generate-speech.md`

```markdown
# generateSpeech()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-speech
description: API Reference for generateSpeech.
---


# [`generateSpeech()`](#generatespeech)


`generateSpeech` is an experimental feature.

Generates speech audio from text.

```
import{ experimental_generateSpeech as generateSpeech }from'ai';import{ openai }from'@ai-sdk/openai';import{ readFile }from'fs/promises';const{ audio }=awaitgenerateSpeech({  model: openai.speech('tts-1'),  text:'Hello from the AI SDK!',});console.log(audio);
```


## [Import](#import)


import { experimental\_generateSpeech as generateSpeech } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


SpeechModelV1

The speech model to use.


### text:


string

The text to generate the speech from.


### voice?:


string

The voice to use for the speech.


### outputFormat?:


string

The output format to use for the speech e.g. "mp3", "wav", etc.


### instructions?:


string

Instructions for the speech generation.


### speed?:


number

The speed of the speech generation.


### providerOptions?:


Record<string, Record<string, JSONValue>>

Additional provider-specific options.


### maxRetries?:


number

Maximum number of retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers for the request.


### [Returns](#returns)



### audio:


GeneratedAudioFile

The generated audio.

GeneratedAudioFile


### base64:


string

Audio as a base64 encoded string.


### uint8Array:


Uint8Array

Audio as a Uint8Array.


### mimeType:


string

MIME type of the audio (e.g. "audio/mpeg").


### format:


string

Format of the audio (e.g. "mp3").


### warnings:


SpeechWarning\[\]

Warnings from the model provider (e.g. unsupported settings).


### responses:


Array<SpeechModelResponseMetadata>

Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.

SpeechModelResponseMetadata


### timestamp:


Date

Timestamp for the start of the generated response.


### modelId:


string

The ID of the response model that was used to generate the response.


### headers?:


Record<string, string>

Response headers.
```

### 170. `docs/reference/ai-sdk-core/generate-text.md`

```markdown
# generateText()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text
description: API Reference for generateText.
---


# [`generateText()`](#generatetext)


Generates text and calls tools for a given prompt using a language model.

It is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Invent a new holiday and describe its traditions.',});console.log(text);
```

To see `generateText` in action, check out [these examples](#examples).


## [Import](#import)


import { generateText } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


LanguageModel

The language model to use. Example: openai('gpt-4o')


### system:


string

The system prompt to use that specifies the behavior of the model.


### prompt:


string

The input prompt to generate the text from.


### messages:


Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>

A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.

CoreSystemMessage


### role:


'system'

The role for the system message.


### content:


string

The content of the message.

CoreUserMessage


### role:


'user'

The role for the user message.


### content:


string | Array<TextPart | ImagePart | FilePart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ImagePart


### type:


'image'

The type of the message part.


### image:


string | Uint8Array | Buffer | ArrayBuffer | URL

The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType?:


string

The mime type of the image. Optional.

FilePart


### type:


'file'

The type of the message part.


### data:


string | Uint8Array | Buffer | ArrayBuffer | URL

The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType:


string

The mime type of the file.

CoreAssistantMessage


### role:


'assistant'

The role for the assistant message.


### content:


string | Array<TextPart | ReasoningPart | RedactedReasoningPart | ToolCallPart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ReasoningPart


### type:


'reasoning'


### text:


string

The reasoning text.


### signature?:


string

The signature for the reasoning.

RedactedReasoningPart


### type:


'redacted-reasoning'


### data:


string

The redacted data.

ToolCallPart


### type:


'tool-call'

The type of the message part.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on zod schema

Parameters generated by the model to be used by the tool.

CoreToolMessage


### role:


'tool'

The role for the assistant message.


### content:


Array<ToolResultPart>

The content of the message.

ToolResultPart


### type:


'tool-result'

The type of the message part.


### toolCallId:


string

The id of the tool call the result corresponds to.


### toolName:


string

The name of the tool the result corresponds to.


### result:


unknown

The result returned by the tool after execution.


### isError?:


boolean

Whether the result is an error or an error message.


### tools:


ToolSet

Tools that are accessible to and can be called by the model. The model needs to support calling tools.

Tool


### description?:


string

Information about the purpose of the tool including details on how and when it can be used by the model.


### parameters:


Zod Schema | JSON Schema

The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the \`jsonSchema\` function).


### execute?:


async (parameters: T, options: ToolExecutionOptions) => RESULT

An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.

ToolExecutionOptions


### toolCallId:


string

The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.


### messages:


CoreMessage\[\]

Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.


### abortSignal:


AbortSignal

An optional abort signal that indicates that the overall operation should be aborted.


### toolChoice?:


"auto" | "none" | "required" | { "type": "tool", "toolName": string }

The tool choice setting. It specifies how tools are selected for execution. The default is "auto". "none" disables tool execution. "required" requires tools to be executed. { "type": "tool", "toolName": string } specifies a specific tool to execute.


### maxTokens?:


number

Maximum number of tokens to generate.


### temperature?:


number

Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topP?:


number

Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topK?:


number

Only sample from the top K options for each subsequent token. Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.


### presencePenalty?:


number

Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.


### frequencyPenalty?:


number

Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.


### stopSequences?:


string\[\]

Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.


### seed?:


number

The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.


### maxRetries?:


number

Maximum number of retries. Set to 0 to disable retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal that can be used to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.


### maxSteps?:


number

Maximum number of sequential LLM calls (steps), e.g. when you use tool calls. A maximum number is required to prevent infinite loops in the case of misconfigured tools. By default, it is set to 1.


### experimental\_generateMessageId?:


() => string

Function used to generate a unique ID for each message. This is an experimental feature.


### experimental\_continueSteps?:


boolean

Enable or disable continue steps. Disabled by default.


### experimental\_telemetry?:


TelemetrySettings

Telemetry configuration. Experimental feature.

TelemetrySettings


### isEnabled?:


boolean

Enable or disable telemetry. Disabled by default while experimental.


### recordInputs?:


boolean

Enable or disable input recording. Enabled by default.


### recordOutputs?:


boolean

Enable or disable output recording. Enabled by default.


### functionId?:


string

Identifier for this function. Used to group telemetry data by function.


### metadata?:


Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>

Additional information to include in the telemetry data.


### providerOptions?:


Record<string,Record<string,JSONValue>> | undefined

Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### experimental\_activeTools?:


Array<TOOLNAME> | undefined

The tools that are currently active. All tools are active by default.


### experimental\_repairToolCall?:


(options: ToolCallRepairOptions) => Promise<LanguageModelV1FunctionToolCall | null>

A function that attempts to repair a tool call that failed to parse. Return either a repaired tool call or null if the tool call cannot be repaired.

ToolCallRepairOptions


### system:


string | undefined

The system prompt.


### messages:


CoreMessage\[\]

The messages in the current generation step.


### toolCall:


LanguageModelV1FunctionToolCall

The tool call that failed to parse.


### tools:


TOOLS

The tools that are available.


### parameterSchema:


(options: { toolName: string }) => JSONSchema7

A function that returns the JSON Schema for a tool.


### error:


NoSuchToolError | InvalidToolArgumentsError

The error that occurred while parsing the tool call.


### experimental\_output?:


Output

Experimental setting for generating structured outputs.

Output


### Output.text():


Output

Forward text output.


### Output.object():


Output

Generate a JSON object of type OBJECT.

Options


### schema:


Schema<OBJECT>

The schema of the JSON object to generate.


### onStepFinish?:


(result: OnStepFinishResult) => Promise<void> | void

Callback that is called when a step is finished.

OnStepFinishResult


### stepType:


"initial" | "continue" | "tool-result"

The type of step. The first step is always an "initial" step, and subsequent steps are either "continue" steps or "tool-result" steps.


### finishReason:


"stop" | "length" | "content-filter" | "tool-calls" | "error" | "other" | "unknown"

The reason the model finished generating the text for the step.


### usage:


TokenUsage

The token usage of the step.

TokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### text:


string

The full text that has been generated.


### toolCalls:


ToolCall\[\]

The tool calls that have been executed.


### toolResults:


ToolResult\[\]

The tool results that have been generated.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### response?:


Response

Response metadata.

Response


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### body?:


unknown

Optional response body.


### isContinued:


boolean

True when there will be a continuation step with a continuation text.


### providerMetadata?:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### [Returns](#returns)



### text:


string

The generated text by the model.


### reasoning:


string | undefined

The reasoning text of the model (only available for some models).


### reasoningDetails:


Array<ReasoningDetail>

The reasoning details of the model (only available for some models).

ReasoningDetail


### type:


'text'

The type of the reasoning detail.


### text:


string

The text content (only for type "text").


### signature?:


string

Optional signature (only for type "text").

ReasoningDetail


### type:


'redacted'

The type of the reasoning detail.


### data:


string

The redacted data content (only for type "redacted").


### sources:


Array<Source>

Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.

Source


### sourceType:


'url'

A URL source. This is return by web search RAG models.


### id:


string

The ID of the source.


### url:


string

The URL of the source.


### title?:


string

The title of the source.


### providerMetadata?:


LanguageModelV1ProviderMetadata

Additional provider metadata for the source.


### files:


Array<GeneratedFile>

Files that were generated in the final step.

GeneratedFile


### base64:


string

File as a base64 encoded string.


### uint8Array:


Uint8Array

File as a Uint8Array.


### mimeType:


string

MIME type of the file.


### toolCalls:


array

A list of tool calls made by the model.


### toolResults:


array

A list of tool results returned as responses to earlier tool calls.


### finishReason:


'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'

The reason the model finished generating the text.


### usage:


CompletionTokenUsage

The token usage of the generated text.

CompletionTokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### request?:


RequestMetadata

Request metadata.

RequestMetadata


### body:


string

Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).


### response?:


ResponseMetadata

Response metadata.

ResponseMetadata


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### body?:


unknown

Optional response body.


### messages:


Array<ResponseMessage>

The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls. When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### providerMetadata:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### experimental\_output?:


Output

Experimental setting for generating structured outputs.


### steps:


Array<StepResult>

Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.

StepResult


### stepType:


"initial" | "continue" | "tool-result"

The type of step. The first step is always an "initial" step, and subsequent steps are either "continue" steps or "tool-result" steps.


### text:


string

The generated text by the model.


### reasoning:


string | undefined

The reasoning text of the model (only available for some models).


### reasoningDetails:


Array<ReasoningDetail>

The reasoning details of the model (only available for some models).

ReasoningDetail


### type:


'text'

The type of the reasoning detail.


### text:


string

The text content (only for type "text").


### signature?:


string

Optional signature (only for type "text").

ReasoningDetail


### type:


'redacted'

The type of the reasoning detail.


### data:


string

The redacted data content (only for type "redacted").


### sources:


Array<Source>

Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.

Source


### sourceType:


'url'

A URL source. This is return by web search RAG models.


### id:


string

The ID of the source.


### url:


string

The URL of the source.


### title?:


string

The title of the source.


### providerMetadata?:


LanguageModelV1ProviderMetadata

Additional provider metadata for the source.


### files:


Array<GeneratedFile>

Files that were generated in this step.

GeneratedFile


### base64:


string

File as a base64 encoded string.


### uint8Array:


Uint8Array

File as a Uint8Array.


### mimeType:


string

MIME type of the file.


### toolCalls:


array

A list of tool calls made by the model.


### toolResults:


array

A list of tool results returned as responses to earlier tool calls.


### finishReason:


'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'

The reason the model finished generating the text.


### usage:


CompletionTokenUsage

The token usage of the generated text.

CompletionTokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### request?:


RequestMetadata

Request metadata.

RequestMetadata


### body:


string

Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).


### response?:


ResponseMetadata

Response metadata.

ResponseMetadata


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### body?:


unknown

Optional response body.


### messages:


Array<ResponseMessage>

The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls. When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### isContinued:


boolean

True when there will be a continuation step with a continuation text.


### providerMetadata?:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


## [Examples](#examples)


[

Learn to generate text using a language model in Next.js

](/examples/next-app/basics/generating-text)[

Learn to generate a chat completion using a language model in Next.js

](/examples/next-app/basics/generating-text)[

Learn to call tools using a language model in Next.js

](/examples/next-app/tools/call-tool)[

Learn to render a React component as a tool call using a language model in Next.js

](/examples/next-app/tools/render-interface-during-tool-call)[

Learn to generate text using a language model in Node.js

](/examples/node/generating-text/generate-text)[

Learn to generate chat completions using a language model in Node.js

](/examples/node/generating-text/generate-text-with-chat-prompt)
```

### 171. `docs/reference/ai-sdk-core/json-schema.md`

```markdown
# jsonSchema()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/json-schema
description: Helper function for creating JSON schemas
---


# [`jsonSchema()`](#jsonschema)


`jsonSchema` is a helper function that creates a JSON schema object that is compatible with the AI SDK. It takes the JSON schema and an optional validation function as inputs, and can be typed.

You can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).

`jsonSchema` is an alternative to using Zod schemas that provides you with flexibility in dynamic situations (e.g. when using OpenAPI definitions) or for using other validation libraries.

```
import{ jsonSchema }from'ai';const mySchema =jsonSchema<{  recipe:{    name:string;    ingredients:{ name:string; amount:string}[];    steps:string[];};}>({type:'object',  properties:{    recipe:{type:'object',      properties:{        name:{type:'string'},        ingredients:{type:'array',          items:{type:'object',            properties:{              name:{type:'string'},              amount:{type:'string'},},            required:['name','amount'],},},        steps:{type:'array',          items:{type:'string'},},},      required:['name','ingredients','steps'],},},  required:['recipe'],});
```


## [Import](#import)


import { jsonSchema } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### schema:


JSONSchema7

The JSON schema definition.


### options:


SchemaOptions

Additional options for the JSON schema.

SchemaOptions


### validate?:


(value: unknown) => { success: true; value: OBJECT } | { success: false; error: Error };

A function that validates the value against the JSON schema. If the value is valid, the function should return an object with a \`success\` property set to \`true\` and a \`value\` property set to the validated value. If the value is invalid, the function should return an object with a \`success\` property set to \`false\` and an \`error\` property set to the error.


### [Returns](#returns)


A JSON schema object that is compatible with the AI SDK.
```

### 172. `docs/reference/ai-sdk-core/language-model-v1-middleware.md`

```markdown
# LanguageModelV1Middleware


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/language-model-v1-middleware
description: Middleware for enhancing language model behavior (API Reference)
---


# [`LanguageModelV1Middleware`](#languagemodelv1middleware)


Language model middleware is an experimental feature.

Language model middleware provides a way to enhance the behavior of language models by intercepting and modifying the calls to the language model. It can be used to add features like guardrails, RAG, caching, and logging in a language model agnostic way.

See [Language Model Middleware](/docs/ai-sdk-core/middleware) for more information.


## [Import](#import)


import { LanguageModelV1Middleware } from "ai"


## [API Signature](#api-signature)



### transformParams:


({ type: "generate" | "stream", params: LanguageModelV1CallOptions }) => Promise<LanguageModelV1CallOptions>

Transforms the parameters before they are passed to the language model.


### wrapGenerate:


({ doGenerate: DoGenerateFunction, params: LanguageModelV1CallOptions, model: LanguageModelV1 }) => Promise<DoGenerateResult>

Wraps the generate operation of the language model.


### wrapStream:


({ doStream: DoStreamFunction, params: LanguageModelV1CallOptions, model: LanguageModelV1 }) => Promise<DoStreamResult>

Wraps the stream operation of the language model.
```

### 173. `docs/reference/ai-sdk-core/mcp-stdio-transport.md`

```markdown
# Experimental_StdioMCPTransport


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/mcp-stdio-transport
description: Create a transport for Model Context Protocol (MCP) clients to communicate with MCP servers using standard input and output streams
---


# [`Experimental_StdioMCPTransport`](#experimental_stdiomcptransport)


Creates a transport for Model Context Protocol (MCP) clients to communicate with MCP servers using standard input and output streams. This transport is only supported in Node.js environments.

This feature is experimental and may change or be removed in the future.


## [Import](#import)


import { Experimental\_StdioMCPTransport } from "ai/mcp-stdio"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### config:


StdioConfig

Configuration for the MCP client.

StdioConfig


### command:


string

The command to run the MCP server.


### args?:


string\[\]

The arguments to pass to the MCP server.


### env?:


Record<string, string>

The environment variables to set for the MCP server.


### stderr?:


IOType | Stream | number

The stream to write the MCP server's stderr to.


### cwd?:


string

The current working directory for the MCP server.
```

### 174. `docs/reference/ai-sdk-core/provider-registry.md`

```markdown
# createProviderRegistry()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/provider-registry
description: Registry for managing multiple providers and models (API Reference)
---


# [`createProviderRegistry()`](#createproviderregistry)


When you work with multiple providers and models, it is often desirable to manage them in a central place and access the models through simple string ids.

`createProviderRegistry` lets you create a registry with multiple providers that you can access by their ids in the format `providerId:modelId`.


### [Setup](#setup)


You can create a registry with multiple providers and models using `createProviderRegistry`.

```
import{ anthropic }from'@ai-sdk/anthropic';import{ createOpenAI }from'@ai-sdk/openai';import{ createProviderRegistry }from'ai';exportconst registry =createProviderRegistry({// register provider with prefix and default setup:  anthropic,// register provider with prefix and custom setup:  openai:createOpenAI({    apiKey: process.env.OPENAI_API_KEY,}),});
```


### [Custom Separator](#custom-separator)


By default, the registry uses `:` as the separator between provider and model IDs. You can customize this separator by passing a `separator` option:

```
const registry =createProviderRegistry({    anthropic,    openai,},{ separator:' > '},);// Now you can use the custom separatorconst model = registry.languageModel('anthropic > claude-3-opus-20240229');
```


### [Language models](#language-models)


You can access language models by using the `languageModel` method on the registry. The provider id will become the prefix of the model id: `providerId:modelId`.

```
import{ generateText }from'ai';import{ registry }from'./registry';const{ text }=awaitgenerateText({  model: registry.languageModel('openai:gpt-4-turbo'),  prompt:'Invent a new holiday and describe its traditions.',});
```


### [Text embedding models](#text-embedding-models)


You can access text embedding models by using the `textEmbeddingModel` method on the registry. The provider id will become the prefix of the model id: `providerId:modelId`.

```
import{ embed }from'ai';import{ registry }from'./registry';const{ embedding }=awaitembed({  model: registry.textEmbeddingModel('openai:text-embedding-3-small'),  value:'sunny day at the beach',});
```


### [Image models](#image-models)


You can access image models by using the `imageModel` method on the registry. The provider id will become the prefix of the model id: `providerId:modelId`.

```
import{ generateImage }from'ai';import{ registry }from'./registry';const{ image }=awaitgenerateImage({  model: registry.imageModel('openai:dall-e-3'),  prompt:'A beautiful sunset over a calm ocean',});
```


## [Import](#import)


import { createProviderRegistry } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### providers:


Record<string, Provider>

The unique identifier for the provider. It should be unique within the registry.

Provider


### languageModel:


(id: string) => LanguageModel

A function that returns a language model by its id.


### textEmbeddingModel:


(id: string) => EmbeddingModel<string>

A function that returns a text embedding model by its id.


### imageModel:


(id: string) => ImageModel

A function that returns an image model by its id.


### options:


object

Optional configuration for the registry.

Options


### separator:


string

Custom separator between provider and model IDs. Defaults to ":".


### [Returns](#returns)


The `createProviderRegistry` function returns a `Provider` instance. It has the following methods:


### languageModel:


(id: string) => LanguageModel

A function that returns a language model by its id (format: providerId:modelId)


### textEmbeddingModel:


(id: string) => EmbeddingModel<string>

A function that returns a text embedding model by its id (format: providerId:modelId)


### imageModel:


(id: string) => ImageModel

A function that returns an image model by its id (format: providerId:modelId)
```

### 175. `docs/reference/ai-sdk-core/simulate-readable-stream.md`

```markdown
# simulateReadableStream()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-readable-stream
description: Create a ReadableStream that emits values with configurable delays
---


# [`simulateReadableStream()`](#simulatereadablestream)


`simulateReadableStream` is a utility function that creates a ReadableStream which emits provided values sequentially with configurable delays. This is particularly useful for testing streaming functionality or simulating time-delayed data streams.

```
import{ simulateReadableStream }from'ai';const stream =simulateReadableStream({  chunks:['Hello',' ','World'],  initialDelayInMs:100,  chunkDelayInMs:50,});
```


## [Import](#import)


import { simulateReadableStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### chunks:


T\[\]

Array of values to be emitted by the stream


### initialDelayInMs?:


number | null

Initial delay in milliseconds before emitting the first value. Defaults to 0. Set to null to skip the initial delay entirely.


### chunkDelayInMs?:


number | null

Delay in milliseconds between emitting each value. Defaults to 0. Set to null to skip delays between chunks.


### [Returns](#returns)


Returns a `ReadableStream<T>` that:

-   Emits each value from the provided `chunks` array sequentially
-   Waits for `initialDelayInMs` before emitting the first value (if not `null`)
-   Waits for `chunkDelayInMs` between emitting subsequent values (if not `null`)
-   Closes automatically after all chunks have been emitted


### [Type Parameters](#type-parameters)


-   `T`: The type of values contained in the chunks array and emitted by the stream


## [Examples](#examples)



### [Basic Usage](#basic-usage)


```
const stream =simulateReadableStream({  chunks:['Hello',' ','World'],});
```


### [With Delays](#with-delays)


```
const stream =simulateReadableStream({  chunks:['Hello',' ','World'],  initialDelayInMs:1000,// Wait 1 second before first chunk  chunkDelayInMs:500,// Wait 0.5 seconds between chunks});
```


### [Without Delays](#without-delays)


```
const stream =simulateReadableStream({  chunks:['Hello',' ','World'],  initialDelayInMs:null,// No initial delay  chunkDelayInMs:null,// No delay between chunks});
```
```

### 176. `docs/reference/ai-sdk-core/simulate-streaming-middleware.md`

```markdown
# simulateStreamingMiddleware()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-streaming-middleware
description: Middleware that simulates streaming for non-streaming language models
---


# [`simulateStreamingMiddleware()`](#simulatestreamingmiddleware)


`simulateStreamingMiddleware` is a middleware function that simulates streaming behavior with responses from non-streaming language models. This is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.

```
import{ simulateStreamingMiddleware }from'ai';const middleware =simulateStreamingMiddleware();
```


## [Import](#import)


import { simulateStreamingMiddleware } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)


This middleware doesn't accept any parameters.


### [Returns](#returns)


Returns a middleware object that:

-   Takes a complete response from a language model
-   Converts it into a simulated stream of chunks
-   Properly handles various response components including:
    -   Text content
    -   Reasoning (as string or array of objects)
    -   Tool calls
    -   Metadata and usage information
    -   Warnings


### [Usage Example](#usage-example)


```
import{ streamText }from'ai';import{ wrapLanguageModel }from'ai';import{ simulateStreamingMiddleware }from'ai';// Example with a non-streaming modelconst result =streamText({  model:wrapLanguageModel({    model: nonStreamingModel,    middleware:simulateStreamingMiddleware(),}),  prompt:'Your prompt here',});// Now you can use the streaming interfaceforawait(const chunk of result.fullStream){// Process streaming chunks}
```


## [How It Works](#how-it-works)


The middleware:

1.  Awaits the complete response from the language model
2.  Creates a `ReadableStream` that emits chunks in the correct sequence
3.  Simulates streaming by breaking down the response into appropriate chunk types
4.  Preserves all metadata, reasoning, tool calls, and other response properties
```

### 177. `docs/reference/ai-sdk-core/smooth-stream.md`

```markdown
# smoothStream()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/smooth-stream
description: Stream transformer for smoothing text output
---


# [`smoothStream()`](#smoothstream)


`smoothStream` is a utility function that creates a TransformStream for the `streamText` `transform` option to smooth out text streaming by buffering and releasing complete words with configurable delays. This creates a more natural reading experience when streaming text responses.

```
import{ smoothStream, streamText }from'ai';const result =streamText({  model,  prompt,  experimental_transform:smoothStream({    delayInMs:20,// optional: defaults to 10ms    chunking:'line',// optional: defaults to 'word'}),});
```


## [Import](#import)


import { smoothStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### delayInMs?:


number | null

The delay in milliseconds between outputting each chunk. Defaults to 10ms. Set to \`null\` to disable delays.


### chunking?:


"word" | "line" | RegExp | (buffer: string) => string | undefined | null

Controls how the text is chunked for streaming. Use "word" to stream word by word (default), "line" to stream line by line, or provide a custom callback or RegExp pattern for custom chunking.


#### [Word chunking caveats with non-latin languages](#word-chunking-caveats-with-non-latin-languages)


The word based chunking **does not work well** with the following languages that do not delimit words with spaces:

For these languages we recommend using a custom regex, like the following:

-   Chinese - `/[\u4E00-\u9FFF]|\S+\s+/`
-   Japanese - `/[\u3040-\u309F\u30A0-\u30FF]|\S+\s+/`

For these languages you could pass your own language aware chunking function:

-   Vietnamese
-   Thai
-   Javanese (Aksara Jawa)


#### [Regex based chunking](#regex-based-chunking)


To use regex based chunking, pass a `RegExp` to the `chunking` option.

```
// To split on underscores:smoothStream({  chunking:/_+/,});// Also can do it like this, same behaviorsmoothStream({  chunking:/[^_]*_/,});
```


#### [Custom callback chunking](#custom-callback-chunking)


To use a custom callback for chunking, pass a function to the `chunking` option.

```
smoothStream({chunking: text =>{const findString ='some string';const index = text.indexOf(findString);if(index ===-1){returnnull;}return text.slice(0, index)+ findString;},});
```


### [Returns](#returns)


Returns a `TransformStream` that:

-   Buffers incoming text chunks
-   Releases text when the chunking pattern is encountered
-   Adds configurable delays between chunks for smooth output
-   Passes through non-text chunks (like step-finish events) immediately
```

### 178. `docs/reference/ai-sdk-core/stream-object.md`

```markdown
# streamObject()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-object
description: API Reference for streamObject
---


# [`streamObject()`](#streamobject)


Streams a typed, structured object for a given prompt and schema using a language model.

It can be used to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.


#### [Example: stream an object using a schema](#example-stream-an-object-using-a-schema)


```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ z }from'zod';const{ partialObjectStream }=streamObject({  model:openai('gpt-4-turbo'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(z.string()),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});forawait(const partialObject of partialObjectStream){console.clear();console.log(partialObject);}
```


#### [Example: stream an array using a schema](#example-stream-an-array-using-a-schema)


For arrays, you specify the schema of the array items. You can use `elementStream` to get the stream of complete array elements.

```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';import{ z }from'zod';const{ elementStream }=streamObject({  model:openai('gpt-4-turbo'),  output:'array',  schema: z.object({    name: z.string(),class: z.string().describe('Character class, e.g. warrior, mage, or thief.'),    description: z.string(),}),  prompt:'Generate 3 hero descriptions for a fantasy role playing game.',});forawait(const hero of elementStream){console.log(hero);}
```


#### [Example: generate JSON without a schema](#example-generate-json-without-a-schema)


```
import{ openai }from'@ai-sdk/openai';import{ streamObject }from'ai';const{ partialObjectStream }=streamObject({  model:openai('gpt-4-turbo'),  output:'no-schema',  prompt:'Generate a lasagna recipe.',});forawait(const partialObject of partialObjectStream){console.clear();console.log(partialObject);}
```

To see `streamObject` in action, check out the [additional examples](#more-examples).


## [Import](#import)


import { streamObject } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


LanguageModel

The language model to use. Example: openai('gpt-4-turbo')


### output:


'object' | 'array' | 'no-schema' | undefined

The type of output to generate. Defaults to 'object'.


### mode:


'auto' | 'json' | 'tool'

The mode to use for object generation. Not every model supports all modes. Defaults to 'auto' for 'object' output and to 'json' for 'no-schema' output. Must be 'json' for 'no-schema' output.


### schema:


Zod Schema | JSON Schema

The schema that describes the shape of the object to generate. It is sent to the model to generate the object and used to validate the output. You can either pass in a Zod schema or a JSON schema (using the \`jsonSchema\` function). In 'array' mode, the schema is used to describe an array element. Not available with 'no-schema' output.


### schemaName:


string | undefined

Optional name of the output that should be generated. Used by some providers for additional LLM guidance, e.g. via tool or schema name. Not available with 'no-schema' output.


### schemaDescription:


string | undefined

Optional description of the output that should be generated. Used by some providers for additional LLM guidance, e.g. via tool or schema name. Not available with 'no-schema' output.


### system:


string

The system prompt to use that specifies the behavior of the model.


### prompt:


string

The input prompt to generate the text from.


### messages:


Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>

A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.

CoreSystemMessage


### role:


'system'

The role for the system message.


### content:


string

The content of the message.

CoreUserMessage


### role:


'user'

The role for the user message.


### content:


string | Array<TextPart | ImagePart | FilePart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ImagePart


### type:


'image'

The type of the message part.


### image:


string | Uint8Array | Buffer | ArrayBuffer | URL

The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType?:


string

The mime type of the image. Optional.

FilePart


### type:


'file'

The type of the message part.


### data:


string | Uint8Array | Buffer | ArrayBuffer | URL

The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType:


string

The mime type of the file.

CoreAssistantMessage


### role:


'assistant'

The role for the assistant message.


### content:


string | Array<TextPart | ReasoningPart | RedactedReasoningPart | ToolCallPart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ReasoningPart


### type:


'reasoning'


### text:


string

The reasoning text.


### signature?:


string

The signature for the reasoning.

RedactedReasoningPart


### type:


'redacted-reasoning'


### data:


string

The redacted data.

ToolCallPart


### type:


'tool-call'

The type of the message part.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on schema

Parameters generated by the model to be used by the tool.

CoreToolMessage


### role:


'tool'

The role for the assistant message.


### content:


Array<ToolResultPart>

The content of the message.

ToolResultPart


### type:


'tool-result'

The type of the message part.


### toolCallId:


string

The id of the tool call the result corresponds to.


### toolName:


string

The name of the tool the result corresponds to.


### result:


unknown

The result returned by the tool after execution.


### isError?:


boolean

Whether the result is an error or an error message.


### maxTokens?:


number

Maximum number of tokens to generate.


### temperature?:


number

Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topP?:


number

Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topK?:


number

Only sample from the top K options for each subsequent token. Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.


### presencePenalty?:


number

Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.


### frequencyPenalty?:


number

Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.


### seed?:


number

The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.


### maxRetries?:


number

Maximum number of retries. Set to 0 to disable retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal that can be used to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.


### experimental\_telemetry?:


TelemetrySettings

Telemetry configuration. Experimental feature.

TelemetrySettings


### isEnabled?:


boolean

Enable or disable telemetry. Disabled by default while experimental.


### recordInputs?:


boolean

Enable or disable input recording. Enabled by default.


### recordOutputs?:


boolean

Enable or disable output recording. Enabled by default.


### functionId?:


string

Identifier for this function. Used to group telemetry data by function.


### metadata?:


Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>

Additional information to include in the telemetry data.


### providerOptions?:


Record<string,Record<string,JSONValue>> | undefined

Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### onError?:


(event: OnErrorResult) => Promise<void> |void

Callback that is called when an error occurs during streaming. You can use it to log errors.

OnErrorResult


### error:


unknown

The error that occurred.


### onFinish?:


(result: OnFinishResult) => void

Callback that is called when the LLM response has finished.

OnFinishResult


### usage:


CompletionTokenUsage

The token usage of the generated text.

CompletionTokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### providerMetadata:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### object:


T | undefined

The generated object (typed according to the schema). Can be undefined if the final object does not match the schema.


### error:


unknown | undefined

Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### response?:


Response

Response metadata.

Response


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### [Returns](#returns)



### usage:


Promise<CompletionTokenUsage>

The token usage of the generated text. Resolved when the response is finished.

CompletionTokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### providerMetadata:


Promise<Record<string,Record<string,JSONValue>> | undefined>

Optional metadata from the provider. Resolved whe the response is finished. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### object:


Promise<T>

The generated object (typed according to the schema). Resolved when the response is finished.


### partialObjectStream:


AsyncIterableStream<DeepPartial<T>>

Stream of partial objects. It gets more complete as the stream progresses. Note that the partial object is not validated. If you want to be certain that the actual content matches your schema, you need to implement your own validation for partial results.


### elementStream:


AsyncIterableStream<ELEMENT>

Stream of array elements. Only available in "array" mode.


### textStream:


AsyncIterableStream<string>

Text stream of the JSON representation of the generated object. It contains text chunks. When the stream is finished, the object is valid JSON that can be parsed.


### fullStream:


AsyncIterableStream<ObjectStreamPart<T>>

Stream of different types of events, including partial objects, errors, and finish events. Only errors that stop the stream, such as network errors, are thrown.

ObjectPart


### type:


'object'


### object:


DeepPartial<T>

The partial object that was generated.

TextDeltaPart


### type:


'text-delta'


### textDelta:


string

The text delta for the underlying raw JSON text.

ErrorPart


### type:


'error'


### error:


unknown

The error that occurred.

FinishPart


### type:


'finish'


### finishReason:


FinishReason


### logprobs?:


Logprobs


### usage:


Usage

Token usage.


### response?:


Response

Response metadata.

Response


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### request?:


Promise<RequestMetadata>

Request metadata.

RequestMetadata


### body:


string

Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).


### response?:


Promise<ResponseMetadata>

Response metadata. Resolved when the response is finished.

ResponseMetadata


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### pipeTextStreamToResponse:


(response: ServerResponse, init?: ResponseInit => void

Writes text delta output to a Node.js response-like object. It sets a \`Content-Type\` header to \`text/plain; charset=utf-8\` and writes each text delta as a separate chunk.

ResponseInit


### status?:


number

The response status code.


### statusText?:


string

The response status text.


### headers?:


Record<string, string>

The response headers.


### toTextStreamResponse:


(init?: ResponseInit) => Response

Creates a simple text stream response. Each text delta is encoded as UTF-8 and sent as a separate chunk. Non-text-delta events are ignored.

ResponseInit


### status?:


number

The response status code.


### statusText?:


string

The response status text.


### headers?:


Record<string, string>

The response headers.


## [More Examples](#more-examples)


[

Streaming Object Generation with RSC

](/examples/next-app/basics/streaming-object-generation)[

Streaming Object Generation with useObject

](/examples/next-pages/basics/streaming-object-generation)[

Streaming Partial Objects

](/examples/node/streaming-structured-data/stream-object)[

Recording Token Usage

](/examples/node/streaming-structured-data/token-usage)[

Recording Final Object

](/examples/node/streaming-structured-data/object)
```

### 179. `docs/reference/ai-sdk-core/stream-text.md`

```markdown
# streamText()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text
description: API Reference for streamText.
---


# [`streamText()`](#streamtext)


Streams text generations from a language model.

You can use the streamText function for interactive use cases such as chat bots and other real-time applications. You can also generate UI components with tools.

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';const{ textStream }=streamText({  model:openai('gpt-4o'),  prompt:'Invent a new holiday and describe its traditions.',});forawait(const textPart of textStream){  process.stdout.write(textPart);}
```

To see `streamText` in action, check out [these examples](#examples).


## [Import](#import)


import { streamText } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


LanguageModel

The language model to use. Example: openai('gpt-4-turbo')


### system:


string

The system prompt to use that specifies the behavior of the model.


### prompt:


string

The input prompt to generate the text from.


### messages:


Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>

A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.

CoreSystemMessage


### role:


'system'

The role for the system message.


### content:


string

The content of the message.

CoreUserMessage


### role:


'user'

The role for the user message.


### content:


string | Array<TextPart | ImagePart | FilePart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ImagePart


### type:


'image'

The type of the message part.


### image:


string | Uint8Array | Buffer | ArrayBuffer | URL

The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType?:


string

The mime type of the image. Optional.

FilePart


### type:


'file'

The type of the message part.


### data:


string | Uint8Array | Buffer | ArrayBuffer | URL

The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType:


string

The mime type of the file.

CoreAssistantMessage


### role:


'assistant'

The role for the assistant message.


### content:


string | Array<TextPart | ReasoningPart | RedactedReasoningPart | ToolCallPart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ReasoningPart


### type:


'reasoning'


### text:


string

The reasoning text.


### signature?:


string

The signature for the reasoning.

RedactedReasoningPart


### type:


'redacted-reasoning'


### data:


string

The redacted data.

ToolCallPart


### type:


'tool-call'

The type of the message part.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on zod schema

Parameters generated by the model to be used by the tool.

CoreToolMessage


### role:


'tool'

The role for the assistant message.


### content:


Array<ToolResultPart>

The content of the message.

ToolResultPart


### type:


'tool-result'

The type of the message part.


### toolCallId:


string

The id of the tool call the result corresponds to.


### toolName:


string

The name of the tool the result corresponds to.


### result:


unknown

The result returned by the tool after execution.


### isError?:


boolean

Whether the result is an error or an error message.


### tools:


ToolSet

Tools that are accessible to and can be called by the model. The model needs to support calling tools.

Tool


### description?:


string

Information about the purpose of the tool including details on how and when it can be used by the model.


### parameters:


Zod Schema | JSON Schema

The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the \`jsonSchema\` function).


### execute?:


async (parameters: T, options: ToolExecutionOptions) => RESULT

An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.

ToolExecutionOptions


### toolCallId:


string

The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.


### messages:


CoreMessage\[\]

Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.


### abortSignal:


AbortSignal

An optional abort signal that indicates that the overall operation should be aborted.


### toolChoice?:


"auto" | "none" | "required" | { "type": "tool", "toolName": string }

The tool choice setting. It specifies how tools are selected for execution. The default is "auto". "none" disables tool execution. "required" requires tools to be executed. { "type": "tool", "toolName": string } specifies a specific tool to execute.


### maxTokens?:


number

Maximum number of tokens to generate.


### temperature?:


number

Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topP?:


number

Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topK?:


number

Only sample from the top K options for each subsequent token. Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.


### presencePenalty?:


number

Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.


### frequencyPenalty?:


number

Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.


### stopSequences?:


string\[\]

Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.


### seed?:


number

The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.


### maxRetries?:


number

Maximum number of retries. Set to 0 to disable retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal that can be used to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.


### maxSteps?:


number

Maximum number of sequential LLM calls (steps), e.g. when you use tool calls. A maximum number is required to prevent infinite loops in the case of misconfigured tools. By default, it is set to 1.


### experimental\_generateMessageId?:


() => string

Function used to generate a unique ID for each message. This is an experimental feature.


### experimental\_continueSteps?:


boolean

Enable or disable continue steps. Disabled by default.


### experimental\_telemetry?:


TelemetrySettings

Telemetry configuration. Experimental feature.

TelemetrySettings


### isEnabled?:


boolean

Enable or disable telemetry. Disabled by default while experimental.


### recordInputs?:


boolean

Enable or disable input recording. Enabled by default.


### recordOutputs?:


boolean

Enable or disable output recording. Enabled by default.


### functionId?:


string

Identifier for this function. Used to group telemetry data by function.


### metadata?:


Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>

Additional information to include in the telemetry data.


### toolCallStreaming?:


boolean

Enable streaming of tool call deltas as they are generated. Disabled by default.


### experimental\_transform?:


StreamTextTransform | Array<StreamTextTransform>

Optional stream transformations. They are applied in the order they are provided. The stream transformations must maintain the stream structure for streamText to work correctly.

StreamTextTransform


### transform:


(options: TransformOptions) => TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>

A transformation that is applied to the stream.

TransformOptions


### stopStream:


() => void

A function that stops the stream.


### tools:


TOOLS

The tools that are available.


### providerOptions?:


Record<string,Record<string,JSONValue>> | undefined

Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### experimental\_activeTools?:


Array<TOOLNAME> | undefined

The tools that are currently active. All tools are active by default.


### experimental\_repairToolCall?:


(options: ToolCallRepairOptions) => Promise<LanguageModelV1FunctionToolCall | null>

A function that attempts to repair a tool call that failed to parse. Return either a repaired tool call or null if the tool call cannot be repaired.

ToolCallRepairOptions


### system:


string | undefined

The system prompt.


### messages:


CoreMessage\[\]

The messages in the current generation step.


### toolCall:


LanguageModelV1FunctionToolCall

The tool call that failed to parse.


### tools:


TOOLS

The tools that are available.


### parameterSchema:


(options: { toolName: string }) => JSONSchema7

A function that returns the JSON Schema for a tool.


### error:


NoSuchToolError | InvalidToolArgumentsError

The error that occurred while parsing the tool call.


### onChunk?:


(event: OnChunkResult) => Promise<void> |void

Callback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.

OnChunkResult


### chunk:


TextStreamPart

The chunk of the stream.

TextStreamPart


### type:


'text-delta'

The type to identify the object as text delta.


### textDelta:


string

The text delta.

TextStreamPart


### type:


'reasoning'

The type to identify the object as reasoning.


### textDelta:


string

The reasoning text delta.

TextStreamPart


### type:


'source'

The type to identify the object as source.


### source:


Source

The source.

TextStreamPart


### type:


'tool-call'

The type to identify the object as tool call.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on zod schema

Parameters generated by the model to be used by the tool.

TextStreamPart


### type:


'tool-call-streaming-start'

Indicates the start of a tool call streaming. Only available when streaming tool calls.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.

TextStreamPart


### type:


'tool-call-delta'

The type to identify the object as tool call delta. Only available when streaming tool calls.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### argsTextDelta:


string

The text delta of the tool call arguments.

TextStreamPart


### type:


'tool-result'

The type to identify the object as tool result.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on zod schema

Parameters generated by the model to be used by the tool.


### result:


any

The result returned by the tool after execution has completed.


### onError?:


(event: OnErrorResult) => Promise<void> |void

Callback that is called when an error occurs during streaming. You can use it to log errors.

OnErrorResult


### error:


unknown

The error that occurred.


### experimental\_output?:


Output

Experimental setting for generating structured outputs.

Output


### Output.text():


Output

Forward text output.


### Output.object():


Output

Generate a JSON object of type OBJECT.

Options


### schema:


Schema<OBJECT>

The schema of the JSON object to generate.


### onStepFinish?:


(result: onStepFinishResult) => Promise<void> | void

Callback that is called when a step is finished.

onStepFinishResult


### stepType:


"initial" | "continue" | "tool-result"

The type of step. The first step is always an "initial" step, and subsequent steps are either "continue" steps or "tool-result" steps.


### finishReason:


"stop" | "length" | "content-filter" | "tool-calls" | "error" | "other" | "unknown"

The reason the model finished generating the text for the step.


### usage:


TokenUsage

The token usage of the step.

TokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### text:


string

The full text that has been generated.


### reasoning:


string | undefined

The reasoning text of the model (only available for some models).


### sources:


Array<Source>

Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.

Source


### sourceType:


'url'

A URL source. This is return by web search RAG models.


### id:


string

The ID of the source.


### url:


string

The URL of the source.


### title?:


string

The title of the source.


### providerMetadata?:


LanguageModelV1ProviderMetadata

Additional provider metadata for the source.


### files:


Array<GeneratedFile>

All files that were generated in this step.

GeneratedFile


### base64:


string

File as a base64 encoded string.


### uint8Array:


Uint8Array

File as a Uint8Array.


### mimeType:


string

MIME type of the file.


### toolCalls:


ToolCall\[\]

The tool calls that have been executed.


### toolResults:


ToolResult\[\]

The tool results that have been generated.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### response?:


Response

Response metadata.

Response


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### isContinued:


boolean

True when there will be a continuation step with a continuation text.


### providerMetadata?:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### onFinish?:


(result: OnFinishResult) => Promise<void> | void

Callback that is called when the LLM response and all request tool executions (for tools that have an \`execute\` function) are finished.

OnFinishResult


### finishReason:


"stop" | "length" | "content-filter" | "tool-calls" | "error" | "other" | "unknown"

The reason the model finished generating the text.


### usage:


TokenUsage

The token usage of the generated text.

TokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### providerMetadata:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### text:


string

The full text that has been generated.


### reasoning:


string | undefined

The reasoning text of the model (only available for some models).


### reasoningDetails:


Array<ReasoningDetail>

The reasoning details of the model (only available for some models).

ReasoningDetail


### type:


'text'

The type of the reasoning detail.


### text:


string

The text content (only for type "text").


### signature?:


string

Optional signature (only for type "text").

ReasoningDetail


### type:


'redacted'

The type of the reasoning detail.


### data:


string

The redacted data content (only for type "redacted").


### sources:


Array<Source>

Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.

Source


### sourceType:


'url'

A URL source. This is return by web search RAG models.


### id:


string

The ID of the source.


### url:


string

The URL of the source.


### title?:


string

The title of the source.


### providerMetadata?:


LanguageModelV1ProviderMetadata

Additional provider metadata for the source.


### files:


Array<GeneratedFile>

Files that were generated in the final step.

GeneratedFile


### base64:


string

File as a base64 encoded string.


### uint8Array:


Uint8Array

File as a Uint8Array.


### mimeType:


string

MIME type of the file.


### toolCalls:


ToolCall\[\]

The tool calls that have been executed.


### toolResults:


ToolResult\[\]

The tool results that have been generated.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### response?:


Response

Response metadata.

Response


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### messages:


Array<ResponseMessage>

The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls. When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.


### steps:


Array<StepResult>

Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.


### [Returns](#returns)



### finishReason:


Promise<'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'>

The reason why the generation finished. Resolved when the response is finished.


### usage:


Promise<CompletionTokenUsage>

The token usage of the generated text. Resolved when the response is finished.

CompletionTokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### providerMetadata:


Promise<Record<string,Record<string,JSONValue>> | undefined>

Optional metadata from the provider. Resolved whe the response is finished. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### text:


Promise<string>

The full text that has been generated. Resolved when the response is finished.


### reasoning:


Promise<string | undefined>

The reasoning text of the model (only available for some models). Resolved when the response is finished.


### reasoningDetails:


Promise<Array<ReasoningDetail>>

The reasoning details of the model (only available for some models). Resolved when the response is finished.

ReasoningDetail


### type:


'text'

The type of the reasoning detail.


### text:


string

The text content (only for type "text").


### signature?:


string

Optional signature (only for type "text").

ReasoningDetail


### type:


'redacted'

The type of the reasoning detail.


### data:


string

The redacted data content (only for type "redacted").


### sources:


Promise<Array<Source>>

Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps. Resolved when the response is finished.

Source


### sourceType:


'url'

A URL source. This is return by web search RAG models.


### id:


string

The ID of the source.


### url:


string

The URL of the source.


### title?:


string

The title of the source.


### providerMetadata?:


LanguageModelV1ProviderMetadata

Additional provider metadata for the source.


### files:


Promise<Array<GeneratedFile>>

Files that were generated in the final step. Resolved when the response is finished.

GeneratedFile


### base64:


string

File as a base64 encoded string.


### uint8Array:


Uint8Array

File as a Uint8Array.


### mimeType:


string

MIME type of the file.


### toolCalls:


Promise<ToolCall\[\]>

The tool calls that have been executed. Resolved when the response is finished.


### toolResults:


Promise<ToolResult\[\]>

The tool results that have been generated. Resolved when the all tool executions are finished.


### request?:


Promise<RequestMetadata>

Request metadata.

RequestMetadata


### body:


string

Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).


### response?:


Promise<ResponseMetadata>

Response metadata. Resolved when the response is finished.

ResponseMetadata


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### messages:


Array<ResponseMessage>

The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls. When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### steps:


Promise<Array<StepResult>>

Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.

StepResult


### stepType:


"initial" | "continue" | "tool-result"

The type of step. The first step is always an "initial" step, and subsequent steps are either "continue" steps or "tool-result" steps.


### text:


string

The generated text by the model.


### reasoning:


string | undefined

The reasoning text of the model (only available for some models).


### sources:


Array<Source>

Sources that have been used as input.

Source


### sourceType:


'url'

A URL source. This is return by web search RAG models.


### id:


string

The ID of the source.


### url:


string

The URL of the source.


### title?:


string

The title of the source.


### providerMetadata?:


LanguageModelV1ProviderMetadata

Additional provider metadata for the source.


### files:


Array<GeneratedFile>

Files that were generated in this step.

GeneratedFile


### base64:


string

File as a base64 encoded string.


### uint8Array:


Uint8Array

File as a Uint8Array.


### mimeType:


string

MIME type of the file.


### toolCalls:


array

A list of tool calls made by the model.


### toolResults:


array

A list of tool results returned as responses to earlier tool calls.


### finishReason:


'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'

The reason the model finished generating the text.


### usage:


CompletionTokenUsage

The token usage of the generated text.

CompletionTokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### request?:


RequestMetadata

Request metadata.

RequestMetadata


### body:


string

Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).


### response?:


ResponseMetadata

Response metadata.

ResponseMetadata


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers?:


Record<string, string>

Optional response headers.


### messages:


Array<ResponseMessage>

The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls. When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### isContinued:


boolean

True when there will be a continuation step with a continuation text.


### providerMetadata?:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### textStream:


AsyncIterable<string> & ReadableStream<string>

A text stream that returns only the generated text deltas. You can use it as either an AsyncIterable or a ReadableStream. Errors are suppressed to prevent server crashes. Use the onError callback to log errors.


### fullStream:


AsyncIterable<TextStreamPart> & ReadableStream<TextStreamPart>

A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. Only errors that stop the stream, such as network errors, are thrown.

TextStreamPart


### type:


'text-delta'

The type to identify the object as text delta.


### textDelta:


string

The text delta.

TextStreamPart


### type:


'reasoning'

The type to identify the object as reasoning.


### textDelta:


string

The reasoning text delta.

TextStreamPart


### type:


'reasoning-signature'

The signature for the previous reasoning chunks. Send by some providers.


### signature:


string

The signature.

TextStreamPart


### type:


'redacted-reasoning'

The type to identify the object as redacted reasoning.


### data:


string

The redacted data.

TextStreamPart


### type:


'source'

The type to identify the object as source.


### source:


Source

The source.

TextStreamPart


### type:


'file'

The type to identify the object as file.


### base64:


string

File as a base64 encoded string.


### uint8Array:


Uint8Array

File as a Uint8Array.


### mimeType:


string

MIME type of the file.

TextStreamPart


### type:


'tool-call'

The type to identify the object as tool call.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on zod schema

Parameters generated by the model to be used by the tool.

TextStreamPart


### type:


'tool-call-streaming-start'

Indicates the start of a tool call streaming. Only available when streaming tool calls.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.

TextStreamPart


### type:


'tool-call-delta'

The type to identify the object as tool call delta. Only available when streaming tool calls.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### argsTextDelta:


string

The text delta of the tool call arguments.

TextStreamPart


### type:


'tool-result'

The type to identify the object as tool result.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on zod schema

Parameters generated by the model to be used by the tool.


### result:


any

The result returned by the tool after execution has completed.

TextStreamPart


### type:


'step-start'

Indicates the start of a new step in the stream.


### messageId:


string

The ID of the assistant message that started the step.


### request:


RequestMetadata

Information about the request that was sent to the language model provider.

RequestMetadata


### body:


string

Raw request HTTP body that was sent to the provider API as a string.


### warnings:


Warning\[\]

Warnings from the model provider (e.g. unsupported settings).

TextStreamPart


### type:


'step-finish'

Indicates the end of the current step in the stream.


### messageId:


string

The ID of the assistant message that ended the step.


### logprobs?:


LogProbs

Optional log probabilities for tokens returned by some providers.


### request:


RequestMetadata

Information about the request that was sent to the language model provider.

RequestMetadata


### body:


string

Raw request HTTP body that was sent to the provider API as a string.


### warnings?:


Warning\[\]

Warnings from the model provider (e.g. unsupported settings).


### response:


ResponseMetadata

Response metadata from the language model provider.

ResponseMetadata


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers:


Record<string, string>

The response headers.


### usage:


TokenUsage

The token usage of the generated text.

TokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### finishReason:


'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'

The reason the model finished generating the text.


### providerMetadata?:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### isContinued:


boolean

True when there will be a continuation step with a continuation text.

TextStreamPart


### type:


'finish'

The type to identify the object as finish.


### finishReason:


'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'

The reason the model finished generating the text.


### usage:


TokenUsage

The token usage of the generated text.

TokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### providerMetadata?:


Record<string,Record<string,JSONValue>> | undefined

Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### logprobs?:


LogProbs

Optional log probabilities for tokens returned by some providers.


### response:


ResponseMetadata

Response metadata from the language model provider.

ResponseMetadata


### id:


string

The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.


### model:


string

The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.


### timestamp:


Date

The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.


### headers:


Record<string, string>

The response headers.

TextStreamPart


### type:


'error'

The type to identify the object as error.


### error:


unknown

Describes the error that may have occurred during execution.


### experimental\_partialOutputStream:


AsyncIterableStream<PARTIAL\_OUTPUT>

A stream of partial outputs. It uses the \`experimental\_output\` specification.


### consumeStream:


(options?: ConsumeStreamOptions) => Promise<void>

Consumes the stream without processing the parts. This is useful to force the stream to finish. If an error occurs, it is passed to the optional \`onError\` callback.

ConsumeStreamOptions


### onError?:


(error: unknown) => void

The error callback.


### pipeDataStreamToResponse:


(response: ServerResponse, options: PipeDataStreamToResponseOptions) => void

Writes stream data output to a Node.js response-like object. It sets a \`Content-Type\` header to \`text/plain; charset=utf-8\` and writes each stream data part as a separate chunk.

PipeDataStreamToResponseOptions


### status?:


number

The response status code.


### statusText?:


string

The response status text.


### headers?:


HeadersInit

The response headers.


### data?:


StreamData

The stream data object.


### getErrorMessage?:


(error: unknown) => string

A function to get the error message from the error object. By default, all errors are masked as "" for safety reasons.


### sendUsage?:


boolean

Whether to send the usage information in the stream. Defaults to true.


### sendReasoning?:


boolean

Whether to send the reasoning information in the stream. Defaults to false.


### sendSources?:


boolean

Whether to send the sources information in the stream. Defaults to false.


### experimental\_sendFinish?:


boolean

Send the finish event to the client. Set to false if you are using additional streamText calls that send additional data. Default to true.


### experimental\_sendStart?:


boolean

Send the message start event to the client. Set to false if you are using additional streamText calls and the message start event has already been sent. Default to true.


### pipeTextStreamToResponse:


(response: ServerResponse, init?: ResponseInit => void

Writes text delta output to a Node.js response-like object. It sets a \`Content-Type\` header to \`text/plain; charset=utf-8\` and writes each text delta as a separate chunk.

ResponseInit


### status?:


number

The response status code.


### statusText?:


string

The response status text.


### headers?:


Record<string, string>

The response headers.


### toDataStream:


(options?: ToDataStreamOptions) => Response

Converts the result to a data stream.

ToDataStreamOptions


### data?:


StreamData

The stream data object.


### getErrorMessage?:


(error: unknown) => string

A function to get the error message from the error object. By default, all errors are masked as "" for safety reasons.


### sendUsage?:


boolean

Whether to send the usage information in the stream. Defaults to true.


### sendReasoning?:


boolean

Whether to send the reasoning information in the stream. Defaults to false.


### sendSources?:


boolean

Whether to send the sources information in the stream. Defaults to false.


### experimental\_sendFinish?:


boolean

Send the finish event to the client. Set to false if you are using additional streamText calls that send additional data. Default to true.


### experimental\_sendStart?:


boolean

Send the message start event to the client. Set to false if you are using additional streamText calls and the message start event has already been sent. Default to true.


### toDataStreamResponse:


(options?: ToDataStreamResponseOptions) => Response

Converts the result to a streamed response object with a stream data part stream. It can be used with the \`useChat\` and \`useCompletion\` hooks.

ToDataStreamResponseOptions


### status?:


number

The response status code.


### statusText?:


string

The response status text.


### headers?:


Record<string, string>

The response headers.


### data?:


StreamData

The stream data object.


### getErrorMessage?:


(error: unknown) => string

A function to get the error message from the error object. By default, all errors are masked as "" for safety reasons.


### sendUsage?:


boolean

Whether to send the usage information in the stream. Defaults to true.


### sendReasoning?:


boolean

Whether to send the reasoning information in the stream. Defaults to false.


### sendSources?:


boolean

Whether to send the sources information in the stream. Defaults to false.


### experimental\_sendFinish?:


boolean

Send the finish event to the client. Set to false if you are using additional streamText calls that send additional data. Default to true.


### experimental\_sendStart?:


boolean

Send the message start event to the client. Set to false if you are using additional streamText calls and the message start event has already been sent. Default to true.


### toTextStreamResponse:


(init?: ResponseInit) => Response

Creates a simple text stream response. Each text delta is encoded as UTF-8 and sent as a separate chunk. Non-text-delta events are ignored.

ResponseInit


### status?:


number

The response status code.


### statusText?:


string

The response status text.


### headers?:


Record<string, string>

The response headers.


### mergeIntoDataStream:


(dataStream: DataStreamWriter, options?: DataStreamOptions) => void

Merges the result as a data stream into another data stream.

DataStreamOptions


### sendUsage?:


boolean

Whether to send the usage information to the client. Defaults to true.


### sendReasoning?:


boolean

Whether to send the reasoning information in the stream. Defaults to false.


### sendSources?:


boolean

Whether to send the sources information in the stream. Defaults to false.


### experimental\_sendFinish?:


boolean

Send the finish event to the client. Set to false if you are using additional streamText calls that send additional data. Default to true.


### experimental\_sendStart?:


boolean

Send the message start event to the client. Set to false if you are using additional streamText calls and the message start event has already been sent. Default to true.


## [Examples](#examples)


[

Learn to stream text generated by a language model in Next.js

](/examples/next-app/basics/streaming-text-generation)[

Learn to stream chat completions generated by a language model in Next.js

](/examples/next-app/chat/stream-chat-completion)[

Learn to stream text generated by a language model in Node.js

](/examples/node/generating-text/stream-text)[

Learn to stream chat completions generated by a language model in Node.js

](/examples/node/generating-text/stream-text-with-chat-prompt)
```

### 180. `docs/reference/ai-sdk-core/tool.md`

```markdown
# tool()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/tool
description: Helper function for tool type inference
---


# [`tool()`](#tool)


Tool is a helper function that infers the tool parameters for its `execute` method.

It does not have any runtime behavior, but it helps TypeScript infer the types of the parameters for the `execute` method.

Without this helper function, TypeScript is unable to connect the `parameters` property to the `execute` method, and the argument types of `execute` cannot be inferred.

```
import{ tool }from'ai';import{ z }from'zod';exportconst weatherTool =tool({  description:'Get the weather in a location',  parameters: z.object({location: z.string().describe('The location to get the weather for'),}),// location below is inferred to be a string:execute:async({location})=>({location,    temperature:72+Math.floor(Math.random()*21)-10,}),});
```


## [Import](#import)


import { tool } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### tool:


Tool

The tool definition.

Tool


### description?:


string

Information about the purpose of the tool including details on how and when it can be used by the model.


### parameters:


Zod Schema | JSON Schema

The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the \`jsonSchema\` function).


### execute?:


async (parameters: T, options: ToolExecutionOptions) => RESULT

An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.

ToolExecutionOptions


### toolCallId:


string

The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.


### messages:


CoreMessage\[\]

Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.


### abortSignal:


AbortSignal

An optional abort signal that indicates that the overall operation should be aborted.


### experimental\_toToolResultContent?:


(result: RESULT) => TextToolResultContent | ImageToolResultContent

An optional function that converts the result of the tool call to a content object that can be used in LLM messages.

TextToolResultContent


### type:


'text'

The type of the tool result content.


### text:


string

The content of the message.

ImageToolResultContent


### type:


'image'

The type of the tool result content.


### data:


string

The base64 encoded png image.


### mimeType?:


string

The mime type of the image.


### [Returns](#returns)


The tool that was passed in.
```

### 181. `docs/reference/ai-sdk-core/transcribe.md`

```markdown
# transcribe()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/transcribe
description: API Reference for transcribe.
---


# [`transcribe()`](#transcribe)


`transcribe` is an experimental feature.

Generates a transcript from an audio file.

```
import{ experimental_transcribe as transcribe }from'ai';import{ openai }from'@ai-sdk/openai';import{ readFile }from'fs/promises';const{ transcript }=awaittranscribe({  model: openai.transcription('whisper-1'),  audio:awaitreadFile('audio.mp3'),});console.log(transcript);
```


## [Import](#import)


import { experimental\_transcribe as transcribe } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


TranscriptionModelV1

The transcription model to use.


### audio:


DataContent (string | Uint8Array | ArrayBuffer | Buffer) | URL

The audio file to generate the transcript from.


### providerOptions?:


Record<string, Record<string, JSONValue>>

Additional provider-specific options.


### maxRetries?:


number

Maximum number of retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers for the request.


### [Returns](#returns)



### text:


string

The complete transcribed text from the audio input.


### segments:


Array<{ text: string; startSecond: number; endSecond: number }>

An array of transcript segments, each containing a portion of the transcribed text along with its start and end times in seconds.


### language:


string | undefined

The language of the transcript in ISO-639-1 format e.g. "en" for English.


### durationInSeconds:


number | undefined

The duration of the transcript in seconds.


### warnings:


TranscriptionWarning\[\]

Warnings from the model provider (e.g. unsupported settings).


### responses:


Array<TranscriptionModelResponseMetadata>

Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.

TranscriptionModelResponseMetadata


### timestamp:


Date

Timestamp for the start of the generated response.


### modelId:


string

The ID of the response model that was used to generate the response.


### headers?:


Record<string, string>

Response headers.
```

### 182. `docs/reference/ai-sdk-core/valibot-schema.md`

```markdown
# valibotSchema()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/valibot-schema
description: Helper function for creating Valibot schemas
---


# [`valibotSchema()`](#valibotschema)


`valibotSchema` is currently experimental.

`valibotSchema` is a helper function that converts a Valibot schema into a JSON schema object that is compatible with the AI SDK. It takes a Valibot schema as input, and returns a typed schema.

You can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).


## [Example](#example)


```
import{ valibotSchema }from'@ai-sdk/valibot';import{ object,string, array }from'valibot';const recipeSchema =valibotSchema(object({    name:string(),    ingredients:array(object({        name:string(),        amount:string(),}),),    steps:array(string()),}),);
```


## [Import](#import)


import { valibotSchema } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### valibotSchema:


GenericSchema<unknown, T>

The Valibot schema definition.


### [Returns](#returns)


A Schema object that is compatible with the AI SDK, containing both the JSON schema representation and validation functionality.
```

### 183. `docs/reference/ai-sdk-core/wrap-language-model.md`

```markdown
# wrapLanguageModel()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/wrap-language-model
description: Function for wrapping a language model with middleware (API Reference)
---


# [`wrapLanguageModel()`](#wraplanguagemodel)


The `wrapLanguageModel` function provides a way to enhance the behavior of language models by wrapping them with middleware. See [Language Model Middleware](/docs/ai-sdk-core/middleware) for more information on middleware.

```
import{ wrapLanguageModel }from'ai';const wrappedLanguageModel =wrapLanguageModel({  model: yourModel,  middleware: yourLanguageModelMiddleware,});
```


## [Import](#import)


import { wrapLanguageModel } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


LanguageModelV1

The original LanguageModelV1 instance to be wrapped.


### middleware:


LanguageModelV1Middleware | LanguageModelV1Middleware\[\]

The middleware to be applied to the language model. When multiple middlewares are provided, the first middleware will transform the input first, and the last middleware will be wrapped directly around the model.


### modelId:


string

Optional custom model ID to override the original model's ID.


### providerId:


string

Optional custom provider ID to override the original model's provider.


### [Returns](#returns)


A new `LanguageModelV1` instance with middleware applied.
```

### 184. `docs/reference/ai-sdk-core/zod-schema.md`

```markdown
# zodSchema()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core/zod-schema
description: Helper function for creating Zod schemas
---


# [`zodSchema()`](#zodschema)


`zodSchema` is a helper function that converts a Zod schema into a JSON schema object that is compatible with the AI SDK. It takes a Zod schema and optional configuration as inputs, and returns a typed schema.

You can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).

You can also pass Zod objects directly to the AI SDK functions. Internally, the AI SDK will convert the Zod schema to a JSON schema using `zodSchema()`. However, if you want to specify options such as `useReferences`, you can pass the `zodSchema()` helper function instead.


## [Example with recursive schemas](#example-with-recursive-schemas)


```
import{ zodSchema }from'ai';import{ z }from'zod';// Define a base category schemaconst baseCategorySchema = z.object({  name: z.string(),});// Define the recursive Category typetypeCategory= z.infer<typeof baseCategorySchema>&{  subcategories:Category[];};// Create the recursive schema using z.lazyconst categorySchema: z.ZodType<Category>= baseCategorySchema.extend({  subcategories: z.lazy(()=> categorySchema.array()),});// Create the final schema with useReferences enabled for recursive supportconst mySchema =zodSchema(  z.object({    category: categorySchema,}),{ useReferences:true},);
```


## [Import](#import)


import { zodSchema } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### zodSchema:


z.Schema

The Zod schema definition.


### options:


object

Additional options for the schema conversion.

object


### useReferences?:


boolean

Enables support for references in the schema. This is required for recursive schemas, e.g. with \`z.lazy\`. However, not all language models and providers support such references. Defaults to \`false\`.


### [Returns](#returns)


A Schema object that is compatible with the AI SDK, containing both the JSON schema representation and validation functionality.
```

### 185. `docs/reference/ai-sdk-core.md`

```markdown
# AI SDK Core


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-core
description: Reference documentation for the AI SDK Core
---


# [AI SDK Core](#ai-sdk-core)


[AI SDK Core](/docs/ai-sdk-core) is a set of functions that allow you to interact with language models and other AI models. These functions are designed to be easy-to-use and flexible, allowing you to generate text, structured data, and embeddings from language models and other AI models.

AI SDK Core contains the following main functions:

[

generateText()

Generate text and call tools from a language model.

](/docs/reference/ai-sdk-core/generate-text)[

streamText()

Stream text and call tools from a language model.

](/docs/reference/ai-sdk-core/stream-text)[

generateObject()

Generate structured data from a language model.

](/docs/reference/ai-sdk-core/generate-object)[

streamObject()

Stream structured data from a language model.

](/docs/reference/ai-sdk-core/stream-object)[

embed()

Generate an embedding for a single value using an embedding model.

](/docs/reference/ai-sdk-core/embed)[

embedMany()

Generate embeddings for several values using an embedding model (batch embedding).

](/docs/reference/ai-sdk-core/embed-many)[

generateImage()

Generate images based on a given prompt using an image model.

](/docs/reference/ai-sdk-core/generate-image)[

transcribe()

Generate a transcript from an audio file.

](/docs/reference/ai-sdk-core/transcribe)[

generateSpeech()

Generate speech audio from text.

](/docs/reference/ai-sdk-core/generate-speech)

It also contains the following helper functions:

[

tool()

Type inference helper function for tools.

](/docs/reference/ai-sdk-core/tool)[

experimental\_createMCPClient()

Creates a client for connecting to MCP servers.

](/docs/reference/ai-sdk-core/create-mcp-client)[

jsonSchema()

Creates AI SDK compatible JSON schema objects.

](/docs/reference/ai-sdk-core/json-schema)[

zodSchema()

Creates AI SDK compatible Zod schema objects.

](/docs/reference/ai-sdk-core/zod-schema)[

createProviderRegistry()

Creates a registry for using models from multiple providers.

](/docs/reference/ai-sdk-core/provider-registry)[

cosineSimilarity()

Calculates the cosine similarity between two vectors, e.g. embeddings.

](/docs/reference/ai-sdk-core/cosine-similarity)[

simulateReadableStream()

Creates a ReadableStream that emits values with configurable delays.

](/docs/reference/ai-sdk-core/simulate-readable-stream)[

wrapLanguageModel()

Wraps a language model with middleware.

](/docs/reference/ai-sdk-core/wrap-language-model)[

extractReasoningMiddleware()

Extracts reasoning from the generated text and exposes it as a \`reasoning\` property on the result.

](/docs/reference/ai-sdk-core/extract-reasoning-middleware)[

simulateStreamingMiddleware()

Simulates streaming behavior with responses from non-streaming language models.

](/docs/reference/ai-sdk-core/simulate-streaming-middleware)[

defaultSettingsMiddleware()

Applies default settings to a language model.

](/docs/reference/ai-sdk-core/default-settings-middleware)[

smoothStream()

Smooths text streaming output.

](/docs/reference/ai-sdk-core/smooth-stream)[

generateId()

Helper function for generating unique IDs

](/docs/reference/ai-sdk-core/generate-id)[

createIdGenerator()

Creates an ID generator

](/docs/reference/ai-sdk-core/create-id-generator)
```

### 186. `docs/reference/ai-sdk-errors/ai-api-call-error.md`

```markdown
# AI_APICallError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-api-call-error
description: Learn how to fix AI_APICallError
---


# [AI\_APICallError](#ai_apicallerror)


This error occurs when an API call fails.


## [Properties](#properties)


-   `url`: The URL of the API request that failed
-   `requestBodyValues`: The request body values sent to the API
-   `statusCode`: The HTTP status code returned by the API
-   `responseHeaders`: The response headers returned by the API
-   `responseBody`: The response body returned by the API
-   `isRetryable`: Whether the request can be retried based on the status code
-   `data`: Any additional data associated with the error


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_APICallError` using:

```
import{APICallError}from'ai';if(APICallError.isInstance(error)){// Handle the error}
```
```

### 187. `docs/reference/ai-sdk-errors/ai-download-error.md`

```markdown
# AI_DownloadError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-download-error
description: Learn how to fix AI_DownloadError
---


# [AI\_DownloadError](#ai_downloaderror)


This error occurs when a download fails.


## [Properties](#properties)


-   `url`: The URL that failed to download
-   `statusCode`: The HTTP status code returned by the server
-   `statusText`: The HTTP status text returned by the server
-   `message`: The error message containing details about the download failure


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_DownloadError` using:

```
import{DownloadError}from'ai';if(DownloadError.isInstance(error)){// Handle the error}
```
```

### 188. `docs/reference/ai-sdk-errors/ai-empty-response-body-error.md`

```markdown
# AI_EmptyResponseBodyError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-empty-response-body-error
description: Learn how to fix AI_EmptyResponseBodyError
---


# [AI\_EmptyResponseBodyError](#ai_emptyresponsebodyerror)


This error occurs when the server returns an empty response body.


## [Properties](#properties)


-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_EmptyResponseBodyError` using:

```
import{EmptyResponseBodyError}from'ai';if(EmptyResponseBodyError.isInstance(error)){// Handle the error}
```
```

### 189. `docs/reference/ai-sdk-errors/ai-invalid-argument-error.md`

```markdown
# AI_InvalidArgumentError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-argument-error
description: Learn how to fix AI_InvalidArgumentError
---


# [AI\_InvalidArgumentError](#ai_invalidargumenterror)


This error occurs when an invalid argument was provided.


## [Properties](#properties)


-   `parameter`: The name of the parameter that is invalid
-   `value`: The invalid value
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_InvalidArgumentError` using:

```
import{InvalidArgumentError}from'ai';if(InvalidArgumentError.isInstance(error)){// Handle the error}
```
```

### 190. `docs/reference/ai-sdk-errors/ai-invalid-data-content-error.md`

```markdown
# AI_InvalidDataContentError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-data-content-error
description: How to fix AI_InvalidDataContentError
---


# [AI\_InvalidDataContentError](#ai_invaliddatacontenterror)


This error occurs when the data content provided in a multi-modal message part is invalid. Check out the [prompt examples for multi-modal messages](/docs/foundations/prompts#message-prompts) .


## [Properties](#properties)


-   `content`: The invalid content value
-   `message`: The error message describing the expected and received content types


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_InvalidDataContentError` using:

```
import{InvalidDataContentError}from'ai';if(InvalidDataContentError.isInstance(error)){// Handle the error}
```
```

### 191. `docs/reference/ai-sdk-errors/ai-invalid-data-content.md`

```markdown
# AI_InvalidDataContent


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-data-content
description: Learn how to fix AI_InvalidDataContent
---


# [AI\_InvalidDataContent](#ai_invaliddatacontent)


This error occurs when invalid data content is provided.


## [Properties](#properties)


-   `content`: The invalid content value
-   `message`: The error message
-   `cause`: The cause of the error


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_InvalidDataContent` using:

```
import{InvalidDataContent}from'ai';if(InvalidDataContent.isInstance(error)){// Handle the error}
```
```

### 192. `docs/reference/ai-sdk-errors/ai-invalid-message-role-error.md`

```markdown
# AI_InvalidMessageRoleError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-message-role-error
description: Learn how to fix AI_InvalidMessageRoleError
---


# [AI\_InvalidMessageRoleError](#ai_invalidmessageroleerror)


This error occurs when an invalid message role is provided.


## [Properties](#properties)


-   `role`: The invalid role value
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_InvalidMessageRoleError` using:

```
import{InvalidMessageRoleError}from'ai';if(InvalidMessageRoleError.isInstance(error)){// Handle the error}
```
```

### 193. `docs/reference/ai-sdk-errors/ai-invalid-prompt-error.md`

```markdown
# AI_InvalidPromptError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-prompt-error
description: Learn how to fix AI_InvalidPromptError
---


# [AI\_InvalidPromptError](#ai_invalidprompterror)


This error occurs when the prompt provided is invalid.


## [Properties](#properties)


-   `prompt`: The invalid prompt value
-   `message`: The error message
-   `cause`: The cause of the error


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_InvalidPromptError` using:

```
import{InvalidPromptError}from'ai';if(InvalidPromptError.isInstance(error)){// Handle the error}
```
```

### 194. `docs/reference/ai-sdk-errors/ai-invalid-response-data-error.md`

```markdown
# AI_InvalidResponseDataError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-response-data-error
description: Learn how to fix AI_InvalidResponseDataError
---


# [AI\_InvalidResponseDataError](#ai_invalidresponsedataerror)


This error occurs when the server returns a response with invalid data content.


## [Properties](#properties)


-   `data`: The invalid response data value
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_InvalidResponseDataError` using:

```
import{InvalidResponseDataError}from'ai';if(InvalidResponseDataError.isInstance(error)){// Handle the error}
```
```

### 195. `docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error.md`

```markdown
# AI_InvalidToolArgumentsError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error
description: Learn how to fix AI_InvalidToolArgumentsError
---


# [AI\_InvalidToolArgumentsError](#ai_invalidtoolargumentserror)


This error occurs when invalid tool argument was provided.


## [Properties](#properties)


-   `toolName`: The name of the tool with invalid arguments
-   `toolArgs`: The invalid tool arguments
-   `message`: The error message
-   `cause`: The cause of the error


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_InvalidToolArgumentsError` using:

```
import{InvalidToolArgumentsError}from'ai';if(InvalidToolArgumentsError.isInstance(error)){// Handle the error}
```
```

### 196. `docs/reference/ai-sdk-errors/ai-json-parse-error.md`

```markdown
# AI_JSONParseError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-json-parse-error
description: Learn how to fix AI_JSONParseError
---


# [AI\_JSONParseError](#ai_jsonparseerror)


This error occurs when JSON fails to parse.


## [Properties](#properties)


-   `text`: The text value that could not be parsed
-   `message`: The error message including parse error details


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_JSONParseError` using:

```
import{JSONParseError}from'ai';if(JSONParseError.isInstance(error)){// Handle the error}
```
```

### 197. `docs/reference/ai-sdk-errors/ai-load-api-key-error.md`

```markdown
# AI_LoadAPIKeyError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-load-api-key-error
description: Learn how to fix AI_LoadAPIKeyError
---


# [AI\_LoadAPIKeyError](#ai_loadapikeyerror)


This error occurs when API key is not loaded successfully.


## [Properties](#properties)


-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_LoadAPIKeyError` using:

```
import{LoadAPIKeyError}from'ai';if(LoadAPIKeyError.isInstance(error)){// Handle the error}
```
```

### 198. `docs/reference/ai-sdk-errors/ai-load-setting-error.md`

```markdown
# AI_LoadSettingError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-load-setting-error
description: Learn how to fix AI_LoadSettingError
---


# [AI\_LoadSettingError](#ai_loadsettingerror)


This error occurs when a setting is not loaded successfully.


## [Properties](#properties)


-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_LoadSettingError` using:

```
import{LoadSettingError}from'ai';if(LoadSettingError.isInstance(error)){// Handle the error}
```
```

### 199. `docs/reference/ai-sdk-errors/ai-message-conversion-error.md`

```markdown
# AI_MessageConversionError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-message-conversion-error
description: Learn how to fix AI_MessageConversionError
---


# [AI\_MessageConversionError](#ai_messageconversionerror)


This error occurs when message conversion fails.


## [Properties](#properties)


-   `originalMessage`: The original message that failed conversion
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_MessageConversionError` using:

```
import{MessageConversionError}from'ai';if(MessageConversionError.isInstance(error)){// Handle the error}
```
```

### 200. `docs/reference/ai-sdk-errors/ai-no-audio-generated-error.md`

```markdown
# AI_NoAudioGeneratedError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-audio-generated-error
description: Learn how to fix AI_NoAudioGeneratedError
---


# [AI\_NoAudioGeneratedError](#ai_noaudiogeneratederror)


This error occurs when no audio could be generated from the input.


## [Properties](#properties)


-   `responses`: Array of responses
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoAudioGeneratedError` using:

```
import{NoAudioGeneratedError}from'ai';if(NoAudioGeneratedError.isInstance(error)){// Handle the error}
```
```

### 201. `docs/reference/ai-sdk-errors/ai-no-content-generated-error.md`

```markdown
# AI_NoContentGeneratedError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-content-generated-error
description: Learn how to fix AI_NoContentGeneratedError
---


# [AI\_NoContentGeneratedError](#ai_nocontentgeneratederror)


This error occurs when the AI provider fails to generate content.


## [Properties](#properties)


-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoContentGeneratedError` using:

```
import{NoContentGeneratedError}from'ai';if(NoContentGeneratedError.isInstance(error)){// Handle the error}
```
```

### 202. `docs/reference/ai-sdk-errors/ai-no-image-generated-error.md`

```markdown
# AI_NoImageGeneratedError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-image-generated-error
description: Learn how to fix AI_NoImageGeneratedError
---


# [AI\_NoImageGeneratedError](#ai_noimagegeneratederror)


This error occurs when the AI provider fails to generate an image. It can arise due to the following reasons:

-   The model failed to generate a response.
-   The model generated an invalid response.


## [Properties](#properties)


-   `message`: The error message.
-   `responses`: Metadata about the image model responses, including timestamp, model, and headers.
-   `cause`: The cause of the error. You can use this for more detailed error handling.


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoImageGeneratedError` using:

```
import{ generateImage,NoImageGeneratedError}from'ai';try{awaitgenerateImage({ model, prompt });}catch(error){if(NoImageGeneratedError.isInstance(error)){console.log('NoImageGeneratedError');console.log('Cause:', error.cause);console.log('Responses:', error.responses);}}
```
```

### 203. `docs/reference/ai-sdk-errors/ai-no-object-generated-error.md`

```markdown
# AI_NoObjectGeneratedError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-object-generated-error
description: Learn how to fix AI_NoObjectGeneratedError
---


# [AI\_NoObjectGeneratedError](#ai_noobjectgeneratederror)


This error occurs when the AI provider fails to generate a parsable object that conforms to the schema. It can arise due to the following reasons:

-   The model failed to generate a response.
-   The model generated a response that could not be parsed.
-   The model generated a response that could not be validated against the schema.


## [Properties](#properties)


-   `message`: The error message.
-   `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.
-   `response`: Metadata about the language model response, including response id, timestamp, and model.
-   `usage`: Request token usage.
-   `finishReason`: Request finish reason. For example 'length' if model generated maximum number of tokens, this could result in a JSON parsing error.
-   `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoObjectGeneratedError` using:

```
import{ generateObject,NoObjectGeneratedError}from'ai';try{awaitgenerateObject({ model, schema, prompt });}catch(error){if(NoObjectGeneratedError.isInstance(error)){console.log('NoObjectGeneratedError');console.log('Cause:', error.cause);console.log('Text:', error.text);console.log('Response:', error.response);console.log('Usage:', error.usage);console.log('Finish Reason:', error.finishReason);}}
```
```

### 204. `docs/reference/ai-sdk-errors/ai-no-output-specified-error.md`

```markdown
# AI_NoOutputSpecifiedError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-output-specified-error
description: Learn how to fix AI_NoOutputSpecifiedError
---


# [AI\_NoOutputSpecifiedError](#ai_nooutputspecifiederror)


This error occurs when no output format was specified for the AI response, and output-related methods are called.


## [Properties](#properties)


-   `message`: The error message (defaults to 'No output specified.')


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoOutputSpecifiedError` using:

```
import{NoOutputSpecifiedError}from'ai';if(NoOutputSpecifiedError.isInstance(error)){// Handle the error}
```
```

### 205. `docs/reference/ai-sdk-errors/ai-no-such-model-error.md`

```markdown
# AI_NoSuchModelError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-model-error
description: Learn how to fix AI_NoSuchModelError
---


# [AI\_NoSuchModelError](#ai_nosuchmodelerror)


This error occurs when a model ID is not found.


## [Properties](#properties)


-   `modelId`: The ID of the model that was not found
-   `modelType`: The type of model
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoSuchModelError` using:

```
import{NoSuchModelError}from'ai';if(NoSuchModelError.isInstance(error)){// Handle the error}
```
```

### 206. `docs/reference/ai-sdk-errors/ai-no-such-provider-error.md`

```markdown
# AI_NoSuchProviderError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-provider-error
description: Learn how to fix AI_NoSuchProviderError
---


# [AI\_NoSuchProviderError](#ai_nosuchprovidererror)


This error occurs when a provider ID is not found.


## [Properties](#properties)


-   `providerId`: The ID of the provider that was not found
-   `availableProviders`: Array of available provider IDs
-   `modelId`: The ID of the model
-   `modelType`: The type of model
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoSuchProviderError` using:

```
import{NoSuchProviderError}from'ai';if(NoSuchProviderError.isInstance(error)){// Handle the error}
```
```

### 207. `docs/reference/ai-sdk-errors/ai-no-such-tool-error.md`

```markdown
# AI_NoSuchToolError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-tool-error
description: Learn how to fix AI_NoSuchToolError
---


# [AI\_NoSuchToolError](#ai_nosuchtoolerror)


This error occurs when a model tries to call an unavailable tool.


## [Properties](#properties)


-   `toolName`: The name of the tool that was not found
-   `availableTools`: Array of available tool names
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoSuchToolError` using:

```
import{NoSuchToolError}from'ai';if(NoSuchToolError.isInstance(error)){// Handle the error}
```
```

### 208. `docs/reference/ai-sdk-errors/ai-no-transcript-generated-error.md`

```markdown
# AI_NoTranscriptGeneratedError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error
description: Learn how to fix AI_NoTranscriptGeneratedError
---


# [AI\_NoTranscriptGeneratedError](#ai_notranscriptgeneratederror)


This error occurs when no transcript could be generated from the input.


## [Properties](#properties)


-   `responses`: Array of responses
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_NoTranscriptGeneratedError` using:

```
import{NoTranscriptGeneratedError}from'ai';if(NoTranscriptGeneratedError.isInstance(error)){// Handle the error}
```
```

### 209. `docs/reference/ai-sdk-errors/ai-retry-error.md`

```markdown
# AI_RetryError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-retry-error
description: Learn how to fix AI_RetryError
---


# [AI\_RetryError](#ai_retryerror)


This error occurs when a retry operation fails.


## [Properties](#properties)


-   `reason`: The reason for the retry failure
-   `lastError`: The most recent error that occurred during retries
-   `errors`: Array of all errors that occurred during retry attempts
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_RetryError` using:

```
import{RetryError}from'ai';if(RetryError.isInstance(error)){// Handle the error}
```
```

### 210. `docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error.md`

```markdown
# AI_TooManyEmbeddingValuesForCallError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error
description: Learn how to fix AI_TooManyEmbeddingValuesForCallError
---


# [AI\_TooManyEmbeddingValuesForCallError](#ai_toomanyembeddingvaluesforcallerror)


This error occurs when too many values are provided in a single embedding call.


## [Properties](#properties)


-   `provider`: The AI provider name
-   `modelId`: The ID of the embedding model
-   `maxEmbeddingsPerCall`: The maximum number of embeddings allowed per call
-   `values`: The array of values that was provided


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_TooManyEmbeddingValuesForCallError` using:

```
import{TooManyEmbeddingValuesForCallError}from'ai';if(TooManyEmbeddingValuesForCallError.isInstance(error)){// Handle the error}
```
```

### 211. `docs/reference/ai-sdk-errors/ai-tool-call-repair-error.md`

```markdown
# ToolCallRepairError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-call-repair-error
description: Learn how to fix AI SDK ToolCallRepairError
---


# [ToolCallRepairError](#toolcallrepairerror)


This error occurs when there is a failure while attempting to repair an invalid tool call. This typically happens when the AI attempts to fix either a `NoSuchToolError` or `InvalidToolArgumentsError`.


## [Properties](#properties)


-   `originalError`: The original error that triggered the repair attempt (either `NoSuchToolError` or `InvalidToolArgumentsError`)
-   `message`: The error message
-   `cause`: The underlying error that caused the repair to fail


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `ToolCallRepairError` using:

```
import{ToolCallRepairError}from'ai';if(ToolCallRepairError.isInstance(error)){// Handle the error}
```
```

### 212. `docs/reference/ai-sdk-errors/ai-tool-execution-error.md`

```markdown
# AI_ToolExecutionError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-execution-error
description: Learn how to fix AI_ToolExecutionError
---


# [AI\_ToolExecutionError](#ai_toolexecutionerror)


This error occurs when there is a failure during the execution of a tool.


## [Properties](#properties)


-   `toolName`: The name of the tool that failed
-   `toolArgs`: The arguments passed to the tool
-   `toolCallId`: The ID of the tool call that failed
-   `message`: The error message
-   `cause`: The underlying error that caused the tool execution to fail


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_ToolExecutionError` using:

```
import{ToolExecutionError}from'ai';if(ToolExecutionError.isInstance(error)){// Handle the error}
```
```

### 213. `docs/reference/ai-sdk-errors/ai-type-validation-error.md`

```markdown
# AI_TypeValidationError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-type-validation-error
description: Learn how to fix AI_TypeValidationError
---


# [AI\_TypeValidationError](#ai_typevalidationerror)


This error occurs when type validation fails.


## [Properties](#properties)


-   `value`: The value that failed validation
-   `message`: The error message including validation details


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_TypeValidationError` using:

```
import{TypeValidationError}from'ai';if(TypeValidationError.isInstance(error)){// Handle the error}
```
```

### 214. `docs/reference/ai-sdk-errors/ai-unsupported-functionality-error.md`

```markdown
# AI_UnsupportedFunctionalityError


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error
description: Learn how to fix AI_UnsupportedFunctionalityError
---


# [AI\_UnsupportedFunctionalityError](#ai_unsupportedfunctionalityerror)


This error occurs when functionality is not unsupported.


## [Properties](#properties)


-   `functionality`: The name of the unsupported functionality
-   `message`: The error message


## [Checking for this Error](#checking-for-this-error)


You can check if an error is an instance of `AI_UnsupportedFunctionalityError` using:

```
import{UnsupportedFunctionalityError}from'ai';if(UnsupportedFunctionalityError.isInstance(error)){// Handle the error}
```
```

### 215. `docs/reference/ai-sdk-errors.md`

```markdown
# AI SDK Errors


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-errors
description: Troubleshooting information for common AI SDK errors.
---


# [AI SDK Errors](#ai-sdk-errors)


-   [AI\_APICallError](/docs/reference/ai-sdk-errors/ai-api-call-error)
-   [AI\_DownloadError](/docs/reference/ai-sdk-errors/ai-download-error)
-   [AI\_EmptyResponseBodyError](/docs/reference/ai-sdk-errors/ai-empty-response-body-error)
-   [AI\_InvalidArgumentError](/docs/reference/ai-sdk-errors/ai-invalid-argument-error)
-   [AI\_InvalidDataContent](/docs/reference/ai-sdk-errors/ai-invalid-data-content)
-   [AI\_InvalidDataContentError](/docs/reference/ai-sdk-errors/ai-invalid-data-content-error)
-   [AI\_InvalidMessageRoleError](/docs/reference/ai-sdk-errors/ai-invalid-message-role-error)
-   [AI\_InvalidPromptError](/docs/reference/ai-sdk-errors/ai-invalid-prompt-error)
-   [AI\_InvalidResponseDataError](/docs/reference/ai-sdk-errors/ai-invalid-response-data-error)
-   [AI\_InvalidToolArgumentsError](/docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error)
-   [AI\_JSONParseError](/docs/reference/ai-sdk-errors/ai-json-parse-error)
-   [AI\_LoadAPIKeyError](/docs/reference/ai-sdk-errors/ai-load-api-key-error)
-   [AI\_LoadSettingError](/docs/reference/ai-sdk-errors/ai-load-setting-error)
-   [AI\_MessageConversionError](/docs/reference/ai-sdk-errors/ai-message-conversion-error)
-   [AI\_NoContentGeneratedError](/docs/reference/ai-sdk-errors/ai-no-content-generated-error)
-   [AI\_NoImageGeneratedError](/docs/reference/ai-sdk-errors/ai-no-image-generated-error)
-   [AI\_NoObjectGeneratedError](/docs/reference/ai-sdk-errors/ai-no-object-generated-error)
-   [AI\_NoOutputSpecifiedError](/docs/reference/ai-sdk-errors/ai-no-output-specified-error)
-   [AI\_NoSuchModelError](/docs/reference/ai-sdk-errors/ai-no-such-model-error)
-   [AI\_NoSuchProviderError](/docs/reference/ai-sdk-errors/ai-no-such-provider-error)
-   [AI\_NoSuchToolError](/docs/reference/ai-sdk-errors/ai-no-such-tool-error)
-   [AI\_RetryError](/docs/reference/ai-sdk-errors/ai-retry-error)
-   [AI\_ToolCallRepairError](/docs/reference/ai-sdk-errors/ai-tool-call-repair-error)
-   [AI\_ToolExecutionError](/docs/reference/ai-sdk-errors/ai-tool-execution-error)
-   [AI\_TooManyEmbeddingValuesForCallError](/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error)
-   [AI\_TypeValidationError](/docs/reference/ai-sdk-errors/ai-type-validation-error)
-   [AI\_UnsupportedFunctionalityError](/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error)
```

### 216. `docs/reference/ai-sdk-rsc/create-ai.md`

```markdown
# createAI


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-ai
description: Reference for the createAI function from the AI SDK RSC
---


# [`createAI`](#createai)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

Creates a client-server context provider that can be used to wrap parts of your application tree to easily manage both UI and AI states of your application.


## [Import](#import)


import { createAI } from "ai/rsc"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### actions:


Record<string, Action>

Server side actions that can be called from the client.


### initialAIState:


any

Initial AI state to be used in the client.


### initialUIState:


any

Initial UI state to be used in the client.


### onGetUIState:


() => UIState

is called during SSR to compare and update UI state.


### onSetAIState:


(Event) => void

is triggered whenever an update() or done() is called by the mutable AI state in your action, so you can safely store your AI state in the database.

Event


### state:


AIState

The resulting AI state after the update.


### done:


boolean

Whether the AI state updates have been finalized or not.


### [Returns](#returns)


It returns an `<AI/>` context provider.


## [Examples](#examples)


[

Learn to manage AI and UI states in Next.js

](/examples/next-app/state-management/ai-ui-states)[

Learn to persist and restore states UI/AI states in Next.js

](/examples/next-app/state-management/save-and-restore-states)
```

### 217. `docs/reference/ai-sdk-rsc/create-streamable-ui.md`

```markdown
# createStreamableUI


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-ui
description: Reference for the createStreamableUI function from the AI SDK RSC
---


# [`createStreamableUI`](#createstreamableui)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

Create a stream that sends UI from the server to the client. On the client side, it can be rendered as a normal React node.


## [Import](#import)


import { createStreamableUI } from "ai/rsc"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### initialValue?:


ReactNode

The initial value of the streamable UI.


### [Returns](#returns)



### value:


ReactNode

The value of the streamable UI. This can be returned from a Server Action and received by the client.


### [Methods](#methods)



### update:


(ReactNode) => void

Updates the current UI node. It takes a new UI node and replaces the old one.


### append:


(ReactNode) => void

Appends a new UI node to the end of the old one. Once appended a new UI node, the previous UI node cannot be updated anymore.


### done:


(ReactNode | null) => void

Marks the UI node as finalized and closes the stream. Once called, the UI node cannot be updated or appended anymore. This method is always required to be called, otherwise the response will be stuck in a loading state.


### error:


(Error) => void

Signals that there is an error in the UI stream. It will be thrown on the client side and caught by the nearest error boundary component.


## [Examples](#examples)


[

Render a React component during a tool call

](/examples/next-app/tools/render-interface-during-tool-call)
```

### 218. `docs/reference/ai-sdk-rsc/create-streamable-value.md`

```markdown
# createStreamableValue


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-value
description: Reference for the createStreamableValue function from the AI SDK RSC
---


# [`createStreamableValue`](#createstreamablevalue)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

Create a stream that sends values from the server to the client. The value can be any serializable data.


## [Import](#import)


import { createStreamableValue } from "ai/rsc"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### value:


any

Any data that RSC supports. Example, JSON.


### [Returns](#returns)



### value:


streamable

This creates a special value that can be returned from Actions to the client. It holds the data inside and can be updated via the update method.
```

### 219. `docs/reference/ai-sdk-rsc/get-ai-state.md`

```markdown
# getAIState


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/get-ai-state
description: Reference for the getAIState function from the AI SDK RSC
---


# [`getAIState`](#getaistate)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

Get the current AI state.


## [Import](#import)


import { getAIState } from "ai/rsc"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### key?:


string

Returns the value of the specified key in the AI state, if it's an object.


### [Returns](#returns)


The AI state.


## [Examples](#examples)


[

Learn to render a React component during a tool call made by a language model in Next.js

](/examples/next-app/tools/render-interface-during-tool-call)
```

### 220. `docs/reference/ai-sdk-rsc/get-mutable-ai-state.md`

```markdown
# getMutableAIState


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/get-mutable-ai-state
description: Reference for the getMutableAIState function from the AI SDK RSC
---


# [`getMutableAIState`](#getmutableaistate)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

Get a mutable copy of the AI state. You can use this to update the state in the server.


## [Import](#import)


import { getMutableAIState } from "ai/rsc"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### key?:


string

Returns the value of the specified key in the AI state, if it's an object.


### [Returns](#returns)


The mutable AI state.


### [Methods](#methods)



### update:


(newState: any) => void

Updates the AI state with the new state.


### done:


(newState: any) => void

Updates the AI state with the new state, marks it as finalized and closes the stream.


## [Examples](#examples)


[

Learn to persist and restore states AI and UI states in Next.js

](/examples/next-app/state-management/save-and-restore-states)
```

### 221. `docs/reference/ai-sdk-rsc/read-streamable-value.md`

```markdown
# readStreamableValue


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/read-streamable-value
description: Reference for the readStreamableValue function from the AI SDK RSC
---


# [`readStreamableValue`](#readstreamablevalue)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

It is a function that helps you read the streamable value from the client that was originally created using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) on the server.


## [Import](#import)


import { readStreamableValue } from "ai/rsc"


## [Example](#example)


app/actions.ts

```
asyncfunctiongenerate(){'use server';const streamable =createStreamableValue();  streamable.update(1);  streamable.update(2);  streamable.done(3);return streamable.value;}
```

app/page.tsx

```
import{ readStreamableValue }from'ai/rsc';exportdefaultfunctionPage(){const[generation, setGeneration]=useState('');return(<div><buttononClick={async()=>{const stream =awaitgenerate();forawait(const delta ofreadStreamableValue(stream)){setGeneration(generation=> generation + delta);}}}>Generate</button></div>);}
```


## [API Signature](#api-signature)



### [Parameters](#parameters)



### stream:


StreamableValue

The streamable value to read from.


### [Returns](#returns)


It returns an async iterator that contains the values emitted by the streamable value.
```

### 222. `docs/reference/ai-sdk-rsc/render.md`

```markdown
# render (Removed)


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/render
description: Reference for the render function from the AI SDK RSC
---


# [`render` (Removed)](#render-removed)


"render" has been removed in AI SDK 4.0.

AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

A helper function to create a streamable UI from LLM providers. This function is similar to AI SDK Core APIs and supports the same model interfaces.

> **Note**: `render` has been deprecated in favor of [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui). During migration, please ensure that the `messages` parameter follows the updated [specification](/docs/reference/ai-sdk-rsc/stream-ui#messages).


## [Import](#import)


import { render } from "ai/rsc"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### model:


string

Model identifier, must be OpenAI SDK compatible.


### provider:


provider client

Currently the only provider available is OpenAI. This needs to match the model name.


### initial?:


ReactNode

The initial UI to render.


### messages:


Array<SystemMessage | UserMessage | AssistantMessage | ToolMessage>

A list of messages that represent a conversation.

SystemMessage


### role:


'system'

The role for the system message.


### content:


string

The content of the message.

UserMessage


### role:


'user'

The role for the user message.


### content:


string

The content of the message.

AssistantMessage


### role:


'assistant'

The role for the assistant message.


### content:


string

The content of the message.


### tool\_calls:


ToolCall\[\]

A list of tool calls made by the model.

ToolCall


### id:


string

The id of the tool call.


### type:


'function'

The type of the tool call.


### function:


Function

The function to call.

Function


### name:


string

The name of the function.


### arguments:


string

The arguments of the function.

ToolMessage


### role:


'tool'

The role for the tool message.


### content:


string

The content of the message.


### toolCallId:


string

The id of the tool call.


### functions?:


ToolSet

Tools that are accessible to and can be called by the model.

Tool


### description?:


string

Information about the purpose of the tool including details on how and when it can be used by the model.


### parameters:


zod schema

The typed schema that describes the parameters of the tool that can also be used to validation and error handling.


### render?:


async (parameters) => any

An async function that is called with the arguments from the tool call and produces a result.


### tools?:


ToolSet

Tools that are accessible to and can be called by the model.

Tool


### description?:


string

Information about the purpose of the tool including details on how and when it can be used by the model.


### parameters:


zod schema

The typed schema that describes the parameters of the tool that can also be used to validation and error handling.


### render?:


async (parameters) => any

An async function that is called with the arguments from the tool call and produces a result.


### text?:


(Text) => ReactNode

Callback to handle the generated tokens from the model.

Text


### content:


string

The full content of the completion.


### delta:


string

The delta.


### done:


boolean

Is it done?


### temperature?:


number

The temperature to use for the model.


### [Returns](#returns)


It can return any valid ReactNode.
```

### 223. `docs/reference/ai-sdk-rsc/stream-ui.md`

```markdown
# streamUI


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/stream-ui
description: Reference for the streamUI function from the AI SDK RSC
---


# [`streamUI`](#streamui)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

A helper function to create a streamable UI from LLM providers. This function is similar to AI SDK Core APIs and supports the same model interfaces.

To see `streamUI` in action, check out [these examples](#examples).


## [Import](#import)


import { streamUI } from "ai/rsc"


## [Parameters](#parameters)



### model:


LanguageModel

The language model to use. Example: openai("gpt-4-turbo")


### initial?:


ReactNode

The initial UI to render.


### system:


string

The system prompt to use that specifies the behavior of the model.


### prompt:


string

The input prompt to generate the text from.


### messages:


Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>

A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.

CoreSystemMessage


### role:


'system'

The role for the system message.


### content:


string

The content of the message.

CoreUserMessage


### role:


'user'

The role for the user message.


### content:


string | Array<TextPart | ImagePart | FilePart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ImagePart


### type:


'image'

The type of the message part.


### image:


string | Uint8Array | Buffer | ArrayBuffer | URL

The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType?:


string

The mime type of the image. Optional.

FilePart


### type:


'file'

The type of the message part.


### data:


string | Uint8Array | Buffer | ArrayBuffer | URL

The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.


### mimeType:


string

The mime type of the file.

CoreAssistantMessage


### role:


'assistant'

The role for the assistant message.


### content:


string | Array<TextPart | ToolCallPart>

The content of the message.

TextPart


### type:


'text'

The type of the message part.


### text:


string

The text content of the message part.

ToolCallPart


### type:


'tool-call'

The type of the message part.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on zod schema

Parameters generated by the model to be used by the tool.

CoreToolMessage


### role:


'tool'

The role for the assistant message.


### content:


Array<ToolResultPart>

The content of the message.

ToolResultPart


### type:


'tool-result'

The type of the message part.


### toolCallId:


string

The id of the tool call the result corresponds to.


### toolName:


string

The name of the tool the result corresponds to.


### result:


unknown

The result returned by the tool after execution.


### isError?:


boolean

Whether the result is an error or an error message.


### maxTokens?:


number

Maximum number of tokens to generate.


### temperature?:


number

Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topP?:


number

Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either \`temperature\` or \`topP\`, but not both.


### topK?:


number

Only sample from the top K options for each subsequent token. Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.


### presencePenalty?:


number

Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.


### frequencyPenalty?:


number

Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.


### stopSequences?:


string\[\]

Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.


### seed?:


number

The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.


### maxRetries?:


number

Maximum number of retries. Set to 0 to disable retries. Default: 2.


### abortSignal?:


AbortSignal

An optional abort signal that can be used to cancel the call.


### headers?:


Record<string, string>

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.


### tools:


ToolSet

Tools that are accessible to and can be called by the model.

Tool


### description?:


string

Information about the purpose of the tool including details on how and when it can be used by the model.


### parameters:


zod schema

The typed schema that describes the parameters of the tool that can also be used to validation and error handling.


### generate?:


(async (parameters) => ReactNode) | AsyncGenerator<ReactNode, ReactNode, void>

A function or a generator function that is called with the arguments from the tool call and yields React nodes as the UI.


### toolChoice?:


"auto" | "none" | "required" | { "type": "tool", "toolName": string }

The tool choice setting. It specifies how tools are selected for execution. The default is "auto". "none" disables tool execution. "required" requires tools to be executed. { "type": "tool", "toolName": string } specifies a specific tool to execute.


### text?:


(Text) => ReactNode

Callback to handle the generated tokens from the model.

Text


### content:


string

The full content of the completion.


### delta:


string

The delta.


### done:


boolean

Is it done?


### providerOptions?:


Record<string,Record<string,JSONValue>> | undefined

Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.


### onFinish?:


(result: OnFinishResult) => void

Callback that is called when the LLM response and all request tool executions (for tools that have a \`generate\` function) are finished.

OnFinishResult


### usage:


TokenUsage

The token usage of the generated text.

TokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### value:


ReactNode

The final ui node that was generated.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### rawResponse:


RawResponse

Optional raw response data.

RawResponse


### headers?:


Record<string, string>

Response headers.


## [Returns](#returns)



### value:


ReactNode

The user interface based on the stream output.


### rawResponse?:


RawResponse

Optional raw response data.

RawResponse


### headers?:


Record<string, string>

Response headers.


### warnings:


Warning\[\] | undefined

Warnings from the model provider (e.g. unsupported settings).


### stream:


AsyncIterable<StreamPart> & ReadableStream<StreamPart>

A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. When an error occurs, the stream will throw the error.

StreamPart


### type:


'text-delta'

The type to identify the object as text delta.


### textDelta:


string

The text delta.

StreamPart


### type:


'tool-call'

The type to identify the object as tool call.


### toolCallId:


string

The id of the tool call.


### toolName:


string

The name of the tool, which typically would be the name of the function.


### args:


object based on zod schema

Parameters generated by the model to be used by the tool.

StreamPart


### type:


'error'

The type to identify the object as error.


### error:


Error

Describes the error that may have occurred during execution.

StreamPart


### type:


'finish'

The type to identify the object as finish.


### finishReason:


'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'

The reason the model finished generating the text.


### usage:


TokenUsage

The token usage of the generated text.

TokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


## [Examples](#examples)


[

Learn to render a React component as a function call using a language model in Next.js

](/examples/next-app/state-management/ai-ui-states)[

Learn to persist and restore states UI/AI states in Next.js

](/examples/next-app/state-management/save-and-restore-states)[

Learn to route React components using a language model in Next.js

](/examples/next-app/interface/route-components)[

Learn to stream component updates to the client in Next.js

](/examples/next-app/interface/stream-component-updates)
```

### 224. `docs/reference/ai-sdk-rsc/use-actions.md`

```markdown
# useActions


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-actions
description: Reference for the useActions function from the AI SDK RSC
---


# [`useActions`](#useactions)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

It is a hook to help you access your Server Actions from the client. This is particularly useful for building interfaces that require user interactions with the server.

It is required to access these server actions via this hook because they are patched when passed through the context. Accessing them directly may result in a [Cannot find Client Component error](/docs/troubleshooting/common-issues/server-actions-in-client-components).


## [Import](#import)


import { useActions } from "ai/rsc"


## [API Signature](#api-signature)



### [Returns](#returns)


`Record<string, Action>`, a dictionary of server actions.


## [Examples](#examples)


[

Learn to manage AI and UI states in Next.js

](/examples/next-app/state-management/ai-ui-states)[

Learn to route React components using a language model in Next.js

](/examples/next-app/interface/route-components)
```

### 225. `docs/reference/ai-sdk-rsc/use-ai-state.md`

```markdown
# useAIState


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-ai-state
description: Reference for the useAIState function from the AI SDK RSC
---


# [`useAIState`](#useaistate)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

It is a hook that enables you to read and update the AI state. The AI state is shared globally between all `useAIState` hooks under the same `<AI/>` provider.

The AI state is intended to contain context and information shared with the AI model, such as system messages, function responses, and other relevant data.


## [Import](#import)


import { useAIState } from "ai/rsc"


## [API Signature](#api-signature)



### [Returns](#returns)


A single element array where the first element is the current AI state.
```

### 226. `docs/reference/ai-sdk-rsc/use-streamable-value.md`

```markdown
# useStreamableValue


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-streamable-value
description: Reference for the useStreamableValue function from the AI SDK RSC
---


# [`useStreamableValue`](#usestreamablevalue)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

It is a React hook that takes a streamable value created using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) and returns the current value, error, and pending state.


## [Import](#import)


import { useStreamableValue } from "ai/rsc"


## [Example](#example)


This is useful for consuming streamable values received from a component's props.

```
functionMyComponent({ streamableValue }){const[data, error, pending]=useStreamableValue(streamableValue);if(pending)return<div>Loading...</div>;if(error)return<div>Error:{error.message}</div>;return<div>Data:{data}</div>;}
```


## [API Signature](#api-signature)



### [Parameters](#parameters)


It accepts a streamable value created using `createStreamableValue`.


### [Returns](#returns)


It is an array, where the first element contains the data, the second element contains an error if it is thrown anytime during the stream, and the third is a boolean indicating if the value is pending.
```

### 227. `docs/reference/ai-sdk-rsc/use-ui-state.md`

```markdown
# useUIState


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-ui-state
description: Reference for the useUIState function from the AI SDK RSC
---


# [`useUIState`](#useuistate)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

It is a hook that enables you to read and update the UI State. The state is client-side and can contain functions, React nodes, and other data. UIState is the visual representation of the AI state.


## [Import](#import)


import { useUIState } from "ai/rsc"


## [API Signature](#api-signature)



### [Returns](#returns)


Similar to useState, it is an array, where the first element is the current UI state and the second element is the function that updates the state.


## [Examples](#examples)


[

Learn to manage AI and UI states in Next.js

](/examples/next-app/state-management/ai-ui-states)
```

### 228. `docs/reference/ai-sdk-rsc.md`

```markdown
# AI SDK RSC


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-rsc
description: Reference documentation for the AI SDK UI
---


# [AI SDK RSC](#ai-sdk-rsc)


AI SDK RSC is currently experimental. We recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).

[

streamUI

Use a helper function that streams React Server Components on tool execution.

](/docs/reference/ai-sdk-rsc/stream-ui)[

createAI

Create a context provider that wraps your application and shares state between the client and language model on the server.

](/docs/reference/ai-sdk-rsc/create-ai)[

createStreamableUI

Create a streamable UI component that can be rendered on the server and streamed to the client.

](/docs/reference/ai-sdk-rsc/create-streamable-ui)[

createStreamableValue

Create a streamable value that can be rendered on the server and streamed to the client.

](/docs/reference/ai-sdk-rsc/create-streamable-value)[

getAIState

Read the AI state on the server.

](/docs/reference/ai-sdk-rsc/get-ai-state)[

getMutableAIState

Read and update the AI state on the server.

](/docs/reference/ai-sdk-rsc/get-mutable-ai-state)[

useAIState

Get the AI state on the client from the context provider.

](/docs/reference/ai-sdk-rsc/use-ai-state)[

useUIState

Get the UI state on the client from the context provider.

](/docs/reference/ai-sdk-rsc/use-ui-state)[

useActions

Call server actions from the client.

](/docs/reference/ai-sdk-rsc/use-actions)
```

### 229. `docs/reference/ai-sdk-ui/append-client-message.md`

```markdown
# appendClientMessage()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/append-client-message
description: Appends or updates a client Message to an existing array of UI messages for useChat (API Reference)
---


# [`appendClientMessage()`](#appendclientmessage)


Appends a client Message object to an existing array of UI messages. If the last message in the array has the same ID as the new message, it will replace the existing message instead of appending. This is useful for maintaining a unified message history in a client-side chat application, especially when updating existing messages.


## [Import](#import)


import { appendClientMessage } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### messages:


Message\[\]

An existing array of UI messages for useChat (usually from state).


### message:


Message

The new client message to be appended or used to replace an existing message with the same ID.


### [Returns](#returns)



### Message\[\]:


Array

A new array of UI messages with either the appended message or the updated message replacing the previous one with the same ID.
```

### 230. `docs/reference/ai-sdk-ui/append-response-messages.md`

```markdown
# appendResponseMessages()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/append-response-messages
description: Appends ResponseMessage[] from an AI response to an existing array of UI messages, generating timestamps and reusing IDs for useChat (API Reference)
---


# [`appendResponseMessages()`](#appendresponsemessages)


Appends an array of ResponseMessage objects (from the AI response) to an existing array of UI messages. It reuses the existing IDs from the response messages, generates new timestamps, and merges tool-call results with the previous assistant message (if any). This is useful for maintaining a unified message history when working with AI responses in a client-side chat application.


## [Import](#import)


import { appendResponseMessages } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### messages:


Message\[\]

An existing array of UI messages for useChat (usually from state).


### responseMessages:


ResponseMessage\[\]

The new array of AI messages returned from the AI service to be appended. For example, "assistant" messages get added as new items, while tool-call results (role: "tool") are merged with the previous assistant message.


### [Returns](#returns)


An updated array of Message objects.


### Message\[\]:


Array

A new array of UI messages with the appended AI response messages (and updated tool-call results for the preceding assistant message).
```

### 231. `docs/reference/ai-sdk-ui/assistant-response.md`

```markdown
# AssistantResponse


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/assistant-response
description: API reference for the AssistantResponse streaming helper.
---


# [`AssistantResponse`](#assistantresponse)


The AssistantResponse class is designed to facilitate streaming assistant responses to the [`useAssistant`](/docs/reference/ai-sdk-ui/use-assistant) hook. It receives an assistant thread and a current message, and can send messages and data messages to the client.


## [Import](#import)


import { AssistantResponse } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### settings:


Settings

You can pass the id of thread and the latest message which helps establish the context for the response.

Settings


### threadId:


string

The thread ID that the response is associated with.


### messageId:


string

The id of the latest message the response is associated with.


### process:


AssistantResponseCallback

A callback in which you can run the assistant on threads, and send messages and data messages to the client.

AssistantResponseCallback


### forwardStream:


(stream: AssistantStream) => Run | undefined

Forwards the assistant response stream to the client. Returns the Run object after it completes, or when it requires an action.


### sendDataMessage:


(message: DataMessage) => void

Send a data message to the client. You can use this to provide information for rendering custom UIs while the assistant is processing the thread.
```

### 232. `docs/reference/ai-sdk-ui/convert-to-core-messages.md`

```markdown
# convertToCoreMessages()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/convert-to-core-messages
description: Convert useChat messages to CoreMessages for AI core functions (API Reference)
---


# [`convertToCoreMessages()`](#converttocoremessages)


The `convertToCoreMessages` function is no longer required. The AI SDK now automatically converts the incoming messages to the `CoreMessage` format.

The `convertToCoreMessages` function is used to transform an array of UI messages from the `useChat` hook into an array of `CoreMessage` objects. These `CoreMessage` objects are compatible with AI core functions like `streamText`.

app/api/chat/route.ts

```
import{ openai }from'@ai-sdk/openai';import{ convertToCoreMessages, streamText }from'ai';exportasyncfunctionPOST(req:Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages:convertToCoreMessages(messages),});return result.toDataStreamResponse();}
```


## [Import](#import)


import { convertToCoreMessages } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### messages:


Message\[\]

An array of UI messages from the useChat hook to be converted


### options:


{ tools?: ToolSet }

Optional configuration object. Provide tools to enable multi-modal tool responses.


### [Returns](#returns)


An array of [`CoreMessage`](/docs/reference/ai-sdk-core/core-message) objects.


### CoreMessage\[\]:


Array

An array of CoreMessage objects


## [Multi-modal Tool Responses](#multi-modal-tool-responses)


The `convertToCoreMessages` function supports tools that can return multi-modal content. This is useful when tools need to return non-text content like images.

```
import{ tool }from'ai';import{ z }from'zod';const screenshotTool =tool({  parameters: z.object({}),execute:async()=>'imgbase64',experimental_toToolResultContent: result =>[{type:'image', data: result }],});const result =streamText({  model:openai('gpt-4'),  messages:convertToCoreMessages(messages,{    tools:{      screenshot: screenshotTool,},}),});
```

Tools can implement the optional `experimental_toToolResultContent` method to transform their results into multi-modal content. The content is an array of content parts, where each part has a `type` (e.g., 'text', 'image') and corresponding data.
```

### 233. `docs/reference/ai-sdk-ui/create-data-stream-response.md`

```markdown
# createDataStreamResponse


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/create-data-stream-response
description: Learn to use createDataStreamResponse helper function to create a Response object with streaming data.
---


# [`createDataStreamResponse`](#createdatastreamresponse)


The `createDataStreamResponse` function creates a Response object that streams data to the client (see [Streaming Data](/docs/ai-sdk-ui/streaming-data)).


## [Import](#import)


import { createDataStreamResponse } from "ai"


## [Example](#example)


```
const response =createDataStreamResponse({  status:200,  statusText:'OK',  headers:{'Custom-Header':'value',},asyncexecute(dataStream){// Write data    dataStream.writeData({ value:'Hello'});// Write annotation    dataStream.writeMessageAnnotation({type:'status', value:'processing'});// Merge another streamconst otherStream =getAnotherStream();    dataStream.merge(otherStream);},onError:error=>`Custom error: ${error.message}`,});
```


## [API Signature](#api-signature)



### [Parameters](#parameters)



### status:


number

The status code for the response.


### statusText:


string

The status text for the response.


### headers:


Headers | Record<string, string>

Additional headers for the response.


### execute:


(dataStream: DataStreamWriter) => Promise<void> | void

A function that receives a DataStreamWriter instance and can use it to write data to the stream.

DataStreamWriter


### write:


(data: DataStreamString) => void

Appends a data part to the stream.


### writeData:


(value: JSONValue) => void

Appends a data part to the stream.


### writeMessageAnnotation:


(value: JSONValue) => void

Appends a message annotation to the stream.


### writeSource:


(source: Source) => void

Appends a source part to the stream.


### merge:


(stream: ReadableStream<DataStreamString>) => void

Merges the contents of another stream to this stream.


### onError:


((error: unknown) => string) | undefined

Error handler that is used by the data stream writer. This is intended for forwarding when merging streams to prevent duplicated error masking.


### onError:


(error: unknown) => string

A function that handles errors and returns an error message string. By default, it returns "An error occurred."


### [Returns](#returns)


`Response`

A Response object that streams formatted data stream parts with the specified status, headers, and content.
```

### 234. `docs/reference/ai-sdk-ui/create-data-stream.md`

```markdown
# createDataStream


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/create-data-stream
description: Learn to use createDataStream helper function to stream additional data in your application.
---


# [`createDataStream`](#createdatastream)


The `createDataStream` function allows you to stream additional data to the client (see [Streaming Data](/docs/ai-sdk-ui/streaming-data)).


## [Import](#import)


import { createDataStream } from "ai"


## [Example](#example)


```
const stream =createDataStream({asyncexecute(dataStream){// Write data    dataStream.writeData({ value:'Hello'});// Write annotation    dataStream.writeMessageAnnotation({type:'status', value:'processing'});// Merge another streamconst otherStream =getAnotherStream();    dataStream.merge(otherStream);},onError:error=>`Custom error: ${error.message}`,});
```


## [API Signature](#api-signature)



### [Parameters](#parameters)



### execute:


(dataStream: DataStreamWriter) => Promise<void> | void

A function that receives a DataStreamWriter instance and can use it to write data to the stream.

DataStreamWriter


### write:


(data: DataStreamString) => void

Appends a data part to the stream.


### writeData:


(value: JSONValue) => void

Appends a data part to the stream.


### writeMessageAnnotation:


(value: JSONValue) => void

Appends a message annotation to the stream.


### writeSource:


(source: Source) => void

Appends a source part to the stream.


### merge:


(stream: ReadableStream<DataStreamString>) => void

Merges the contents of another stream to this stream.


### onError:


((error: unknown) => string) | undefined

Error handler that is used by the data stream writer. This is intended for forwarding when merging streams to prevent duplicated error masking.


### onError:


(error: unknown) => string

A function that handles errors and returns an error message string. By default, it returns "An error occurred."


### [Returns](#returns)


`ReadableStream<DataStreamString>`

A readable stream that emits formatted data stream parts.
```

### 235. `docs/reference/ai-sdk-ui/pipe-data-stream-to-response.md`

```markdown
# pipeDataStreamToResponse


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/pipe-data-stream-to-response
description: Learn to use pipeDataStreamToResponse helper function to pipe streaming data to a ServerResponse object.
---


# [`pipeDataStreamToResponse`](#pipedatastreamtoresponse)


The `pipeDataStreamToResponse` function pipes streaming data to a Node.js ServerResponse object (see [Streaming Data](/docs/ai-sdk-ui/streaming-data)).


## [Import](#import)


import { pipeDataStreamToResponse } from "ai"


## [Example](#example)


```
pipeDataStreamToResponse(serverResponse,{  status:200,  statusText:'OK',  headers:{'Custom-Header':'value',},asyncexecute(dataStream){// Write data    dataStream.writeData({ value:'Hello'});// Write annotation    dataStream.writeMessageAnnotation({type:'status', value:'processing'});// Merge another streamconst otherStream =getAnotherStream();    dataStream.merge(otherStream);},onError:error=>`Custom error: ${error.message}`,});
```


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


ServerResponse

The Node.js ServerResponse object to pipe the data to.


### status:


number

The status code for the response.


### statusText:


string

The status text for the response.


### headers:


Headers | Record<string, string>

Additional headers for the response.


### execute:


(dataStream: DataStreamWriter) => Promise<void> | void

A function that receives a DataStreamWriter instance and can use it to write data to the stream.

DataStreamWriter


### writeData:


(value: JSONValue) => void

Appends a data part to the stream.


### writeMessageAnnotation:


(value: JSONValue) => void

Appends a message annotation to the stream.


### merge:


(stream: ReadableStream<DataStreamString>) => void

Merges the contents of another stream to this stream.


### onError:


((error: unknown) => string) | undefined

Error handler that is used by the data stream writer. This is intended for forwarding when merging streams to prevent duplicated error masking.


### onError:


(error: unknown) => string

A function that handles errors and returns an error message string. By default, it returns "An error occurred."
```

### 236. `docs/reference/ai-sdk-ui/stream-data.md`

```markdown
# StreamData


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/stream-data
description: Learn to use streamData helper function in your application.
---


# [`StreamData`](#streamdata)


The `StreamData` class is deprecated and will be removed in a future version of AI SDK. Please use `createDataStream`, `createDataStreamResponse`, and `pipeDataStreamToResponse` instead.

The `StreamData` class allows you to stream additional data to the client (see [Streaming Data](/docs/ai-sdk-ui/streaming-data)).


## [Import](#import)



### [React](#react)


import { StreamData } from "ai"


## [API Signature](#api-signature)



### [Constructor](#constructor)


```
const data =newStreamData();
```


### [Methods](#methods)



#### [`append`](#append)


Appends a value to the stream data.

```
data.append(value:JSONValue)
```


#### [`appendMessageAnnotation`](#appendmessageannotation)


Appends a message annotation to the stream data.

```
data.appendMessageAnnotation(annotation:JSONValue)
```


#### [`close`](#close)


Closes the stream data.

```
data.close();
```
```

### 237. `docs/reference/ai-sdk-ui/use-assistant.md`

```markdown
# useAssistant()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-assistant
description: API reference for the useAssistant hook.
---


# [`useAssistant()`](#useassistant)


Allows you to handle the client state when interacting with an OpenAI compatible assistant API. This hook is useful when you want to integrate assistant capabilities into your application, with the UI updated automatically as the assistant is streaming its execution.

This works in conjunction with [`AssistantResponse`](./assistant-response) in the backend.


## [Import](#import)


React

Svelte

import { useAssistant } from '@ai-sdk/react'


## [API Signature](#api-signature)



### [Parameters](#parameters)



### api:


string

The API endpoint that accepts a threadId and message object and returns an AssistantResponse stream. It can be a relative path (starting with \`/\`) or an absolute URL.


### threadId?:


string | undefined

Represents the ID of an existing thread. If not provided, a new thread will be created.


### credentials?:


'omit' | 'same-origin' | 'include' = 'same-origin'

Sets the mode of credentials to be used on the request.


### headers?:


Record<string, string> | Headers

Headers to be passed to the API endpoint.


### body?:


any

Additional body to be passed to the API endpoint.


### onError?:


(error: Error) => void

Callback that will be called when the assistant encounters an error


### fetch?:


FetchFunction

Optional. A custom fetch function to be used for the API call. Defaults to the global fetch function.


### [Returns](#returns)



### messages:


Message\[\]

The current array of chat messages.


### setMessages:


React.Dispatch<React.SetStateAction<Message>>

Function to update the \`messages\` array.


### threadId:


string | undefined

The current thread ID.


### setThreadId:


(threadId: string | undefined) => void

Set the current thread ID. Specifying a thread ID will switch to that thread, if it exists. If set to 'undefined', a new thread will be created. For both cases, \`threadId\` will be updated with the new value and \`messages\` will be cleared.


### input:


string

The current value of the input field.


### setInput:


React.Dispatch<React.SetStateAction<string>>

Function to update the \`input\` value.


### handleInputChange:


(event: any) => void

Handler for the \`onChange\` event of the input field to control the input's value.


### submitMessage:


(event?: { preventDefault?: () => void }) => void

Form submission handler that automatically resets the input field and appends a user message.


### status:


'awaiting\_message' | 'in\_progress'

The current status of the assistant. This can be used to show a loading indicator.


### append:


(message: Message | CreateMessage, chatRequestOptions: { options: { headers, body } }) => Promise<string | undefined>

Function to append a user message to the current thread. This triggers the API call to fetch the assistant's response.


### stop:


() => void

Function to abort the current request from streaming the assistant response. Note that the run will still be in progress.


### error:


undefined | Error

The error thrown during the assistant message processing, if any.
```

### 238. `docs/reference/ai-sdk-ui/use-chat.md`

```markdown
# useChat()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat
description: API reference for the useChat hook.
---


# [`useChat()`](#usechat)


Allows you to easily create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.


## [Import](#import)


React

Svelte

Vue

Solid

import { useChat } from '@ai-sdk/react'


## [API Signature](#api-signature)



### [Parameters](#parameters)



### api?:


string = '/api/chat'

The API endpoint that is called to generate chat responses. It can be a relative path (starting with \`/\`) or an absolute URL.


### id?:


string

An unique identifier for the chat. If not provided, a random one will be generated. When provided, the \`useChat\` hook with the same \`id\` will have shared states across components. This is useful when you have multiple components showing the same chat stream.


### initialInput?:


string = ''

An optional string for the initial prompt input.


### initialMessages?:


Messages\[\] = \[\]

An optional array of initial chat messages


### onToolCall?:


({toolCall: ToolCall}) => void | unknown| Promise<unknown>

Optional callback function that is invoked when a tool call is received. Intended for automatic client-side tool execution. You can optionally return a result for the tool call, either synchronously or asynchronously.


### onResponse?:


(response: Response) => void

An optional callback that will be called with the response from the API endpoint. Useful for throwing customized errors or logging


### onFinish?:


(message: Message, options: OnFinishOptions) => void

An optional callback function that is called when the completion stream ends.

OnFinishOptions


### usage:


CompletionTokenUsage

The token usage for the completion.

CompletionTokenUsage


### promptTokens:


number

The total number of tokens in the prompt.


### completionTokens:


number

The total number of tokens in the completion.


### totalTokens:


number

The total number of tokens generated.


### finishReason:


'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other' | 'unknown'

The reason why the generation ended.


### onError?:


(error: Error) => void

A callback that will be called when the chat stream encounters an error. Optional.


### generateId?:


() => string

A custom id generator for message ids and the chat id. Optional.


### headers?:


Record<string, string> | Headers

Additional headers to be passed to the API endpoint. Optional.


### body?:


any

Additional body object to be passed to the API endpoint. Optional.


### credentials?:


'omit' | 'same-origin' | 'include'

An optional literal that sets the mode of credentials to be used on the request. Defaults to same-origin.


### sendExtraMessageFields?:


boolean

An optional boolean that determines whether to send extra fields you've added to \`messages\`. Defaults to \`false\` and only the \`content\` and \`role\` fields will be sent to the API endpoint. If set to \`true\`, the \`name\`, \`data\`, and \`annotations\` fields will also be sent.


### maxSteps?:


number

Maximum number of backend calls to generate a response. A maximum number is required to prevent infinite loops in the case of misconfigured tools. By default, it is set to 1.


### streamProtocol?:


'text' | 'data'

An optional literal that sets the type of stream to be used. Defaults to \`data\`. If set to \`text\`, the stream will be treated as a text stream.


### fetch?:


FetchFunction

Optional. A custom fetch function to be used for the API call. Defaults to the global fetch function.


### experimental\_prepareRequestBody?:


(options: { messages: UIMessage\[\]; requestData?: JSONValue; requestBody?: object, id: string }) => unknown

Experimental (React, Solid & Vue only). When a function is provided, it will be used to prepare the request body for the chat API. This can be useful for customizing the request body based on the messages and data in the chat.


### experimental\_throttle?:


number

React only. Custom throttle wait time in milliseconds for the message and data updates. When specified, updates will be throttled using this interval. Defaults to undefined (no throttling).


### [Returns](#returns)



### messages:


UIMessage\[\]

The current array of chat messages.

UIMessage


### id:


string

The unique identifier of the message.


### role:


'system' | 'user' | 'assistant' | 'data'

The role of the message.


### createdAt?:


Date

The creation date of the message.


### content:


string

The content of the message.


### annotations?:


Array<JSONValue>

Additional annotations sent along with the message.


### parts:


Array<TextUIPart | ReasoningUIPart | ToolInvocationUIPart | SourceUIPart | StepStartUIPart>

An array of message parts that are associated with the message.

TextUIPart


### type:


"text"


### text:


string

The text content of the part.

ReasoningUIPart


### type:


"reasoning"


### reasoning:


string

The reasoning content of the part.

ToolInvocationUIPart


### type:


"tool-invocation"


### toolInvocation:


ToolInvocation

ToolInvocation


### state:


'partial-call'

The state of the tool call when it was partially created.


### toolCallId:


string

ID of the tool call. This ID is used to match the tool call with the tool result.


### toolName:


string

Name of the tool that is being called.


### args:


any

Partial arguments of the tool call. This is a JSON-serializable object.

ToolInvocation


### state:


'call'

The state of the tool call when it was fully created.


### toolCallId:


string

ID of the tool call. This ID is used to match the tool call with the tool result.


### toolName:


string

Name of the tool that is being called.


### args:


any

Arguments of the tool call. This is a JSON-serializable object that matches the tools input schema.

ToolInvocation


### state:


'result'

The state of the tool call when the result is available.


### toolCallId:


string

ID of the tool call. This ID is used to match the tool call with the tool result.


### toolName:


string

Name of the tool that is being called.


### args:


any

Arguments of the tool call. This is a JSON-serializable object that matches the tools input schema.


### result:


any

The result of the tool call.

SourceUIPart


### type:


"source"


### source:


Source

Source


### sourceType:


'url'

The type of the source.


### id:


string

ID of the source.


### url:


string

URL of the source.


### title?:


string

The title of the source.

StepStartUIPart


### type:


"step-start"


### experimental\_attachments?:


Array<Attachment>

Additional attachments sent along with the message.

Attachment


### name?:


string

The name of the attachment, usually the file name.


### contentType?:


string

A string indicating the media type of the file.


### url:


string

The URL of the attachment. It can either be a URL to a hosted file or a Data URL.


### error:


Error | undefined

An error object returned by SWR, if any.


### append:


(message: Message | CreateMessage, options?: ChatRequestOptions) => Promise<string | undefined>

Function to append a message to the chat, triggering an API call for the AI response. It returns a promise that resolves to full response message content when the API call is successfully finished, or throws an error when the API call fails.

ChatRequestOptions


### headers:


Record<string, string> | Headers

Additional headers that should be to be passed to the API endpoint.


### body:


object

Additional body JSON properties that should be sent to the API endpoint.


### data:


JSONValue

Additional data to be sent to the API endpoint.


### experimental\_attachments?:


FileList | Array<Attachment>

An array of attachments to be sent to the API endpoint.

FileList

A list of files that have been selected by the user using an <input type='file'> element. It's also used for a list of files dropped into web content when using the drag and drop API.

Attachment


### name?:


string

The name of the attachment, usually the file name.


### contentType?:


string

A string indicating the media type of the file.


### url:


string

The URL of the attachment. It can either be a URL to a hosted file or a Data URL.


### reload:


(options?: ChatRequestOptions) => Promise<string | undefined>

Function to reload the last AI chat response for the given chat history. If the last message isn't from the assistant, it will request the API to generate a new response.

ChatRequestOptions


### headers:


Record<string, string> | Headers

Additional headers that should be to be passed to the API endpoint.


### body:


object

Additional body JSON properties that should be sent to the API endpoint.


### data:


JSONValue

Additional data to be sent to the API endpoint.


### stop:


() => void

Function to abort the current API request.


### experimental\_resume:


() => void

Function to resume an ongoing chat generation stream.


### setMessages:


(messages: Message\[\] | ((messages: Message\[\]) => Message\[\]) => void

Function to update the \`messages\` state locally without triggering an API call.


### input:


string

The current value of the input field.


### setInput:


React.Dispatch<React.SetStateAction<string>>

Function to update the \`input\` value.


### handleInputChange:


(event: any) => void

Handler for the \`onChange\` event of the input field to control the input's value.


### handleSubmit:


(event?: { preventDefault?: () => void }, options?: ChatRequestOptions) => void

Form submission handler that automatically resets the input field and appends a user message. You can use the \`options\` parameter to send additional data, headers and more to the server.

ChatRequestOptions


### headers:


Record<string, string> | Headers

Additional headers that should be to be passed to the API endpoint.


### body:


object

Additional body JSON properties that should be sent to the API endpoint.


### data:


JSONValue

Additional data to be sent to the API endpoint.


### allowEmptySubmit?:


boolean

A boolean that determines whether to allow submitting an empty input that triggers a generation. Defaults to \`false\`.


### experimental\_attachments?:


FileList | Array<Attachment>

An array of attachments to be sent to the API endpoint.

FileList

A list of files that have been selected by the user using an <input type='file'> element. It's also used for a list of files dropped into web content when using the drag and drop API.

Attachment


### name?:


string

The name of the attachment, usually the file name.


### contentType?:


string

A string indicating the media type of the file.


### url:


string

The URL of the attachment. It can either be a URL to a hosted file or a Data URL.


### status:


"submitted" | "streaming" | "ready" | "error"

Status of the chat request: submitted (message sent to API), streaming (receiving response chunks), ready (response complete), or error (request failed).


### id:


string

The unique identifier of the chat.


### data:


JSONValue\[\]

Data returned from StreamData.


### setData:


(data: JSONValue\[\] | undefined | ((data: JSONValue\[\] | undefined) => JSONValue\[\] | undefined)) => void

Function to update the \`data\` state which contains data from StreamData.


### addToolResult:


({toolCallId: string; result: any;}) => void

Function to add a tool result to the chat. This will update the chat messages with the tool result and call the API route if all tool results for the last message are available.


## [Learn more](#learn-more)


-   [Chatbot](/docs/ai-sdk-ui/chatbot)
-   [Chatbot with Tools](/docs/ai-sdk-ui/chatbot-with-tool-calling)
```

### 239. `docs/reference/ai-sdk-ui/use-completion.md`

```markdown
# useCompletion()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-completion
description: API reference for the useCompletion hook.
---


# [`useCompletion()`](#usecompletion)


Allows you to create text completion based capabilities for your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.


## [Import](#import)


React

Svelte

Vue

Solid

import { useCompletion } from '@ai-sdk/react'


## [API Signature](#api-signature)



### [Parameters](#parameters)



### api:


string = '/api/completion'

The API endpoint that is called to generate text. It can be a relative path (starting with \`/\`) or an absolute URL.


### id:


string

An unique identifier for the completion. If not provided, a random one will be generated. When provided, the \`useCompletion\` hook with the same \`id\` will have shared states across components. This is useful when you have multiple components showing the same chat stream


### initialInput:


string

An optional string for the initial prompt input.


### initialCompletion:


string

An optional string for the initial completion result.


### onResponse:


(response: Response) => void

An optional callback function that is called with the response from the API endpoint. Useful for throwing customized errors or logging.


### onFinish:


(prompt: string, completion: string) => void

An optional callback function that is called when the completion stream ends.


### onError:


(error: Error) => void

An optional callback that will be called when the chat stream encounters an error.


### headers:


Record<string, string> | Headers

An optional object of headers to be passed to the API endpoint.


### body:


any

An optional, additional body object to be passed to the API endpoint.


### credentials:


'omit' | 'same-origin' | 'include'

An optional literal that sets the mode of credentials to be used on the request. Defaults to same-origin.


### sendExtraMessageFields:


boolean

An optional boolean that determines whether to send extra fields you've added to \`messages\`. Defaults to \`false\` and only the \`content\` and \`role\` fields will be sent to the API endpoint.


### streamProtocol?:


'text' | 'data'

An optional literal that sets the type of stream to be used. Defaults to \`data\`. If set to \`text\`, the stream will be treated as a text stream.


### fetch?:


FetchFunction

Optional. A custom fetch function to be used for the API call. Defaults to the global fetch function.


### experimental\_throttle?:


number

React only. Custom throttle wait time in milliseconds for the completion and data updates. When specified, throttles how often the UI updates during streaming. Default is undefined, which disables throttling.


### [Returns](#returns)



### completion:


string

The current text completion.


### complete:


(prompt: string, options: { headers, body }) => void

Function to execute text completion based on the provided prompt.


### error:


undefined | Error

The error thrown during the completion process, if any.


### setCompletion:


(completion: string) => void

Function to update the \`completion\` state.


### stop:


() => void

Function to abort the current API request.


### input:


string

The current value of the input field.


### setInput:


React.Dispatch<React.SetStateAction<string>>

The current value of the input field.


### handleInputChange:


(event: any) => void

Handler for the \`onChange\` event of the input field to control the input's value.


### handleSubmit:


(event?: { preventDefault?: () => void }) => void

Form submission handler that automatically resets the input field and appends a user message.


### isLoading:


boolean

Boolean flag indicating whether a fetch operation is currently in progress.
```

### 240. `docs/reference/ai-sdk-ui/use-object.md`

```markdown
# experimental_useObject()


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object
description: API reference for the useObject hook.
---


# [`experimental_useObject()`](#experimental_useobject)


`useObject` is an experimental feature and only available in React and SolidJS.

Allows you to consume text streams that represent a JSON object and parse them into a complete object based on a schema. You can use it together with [`streamObject`](/docs/reference/ai-sdk-core/stream-object) in the backend.

```
'use client';import{ experimental_useObject as useObject }from'@ai-sdk/react';exportdefaultfunctionPage(){const{ object, submit }=useObject({    api:'/api/use-object',    schema: z.object({ content: z.string()}),});return(<div><buttononClick={()=>submit('example input')}>Generate</button>{object?.content &&<p>{object.content}</p>}</div>);}
```


## [Import](#import)


import { experimental\_useObject as useObject } from '@ai-sdk/react'


## [API Signature](#api-signature)



### [Parameters](#parameters)



### api:


string

The API endpoint that is called to generate objects. It should stream JSON that matches the schema as chunked text. It can be a relative path (starting with \`/\`) or an absolute URL.


### schema:


Zod Schema | JSON Schema

A schema that defines the shape of the complete object. You can either pass in a Zod schema or a JSON schema (using the \`jsonSchema\` function).


### id?:


string

A unique identifier. If not provided, a random one will be generated. When provided, the \`useObject\` hook with the same \`id\` will have shared states across components.


### initialValue?:


DeepPartial<RESULT> | undefined

An value for the initial object. Optional.


### fetch?:


FetchFunction

A custom fetch function to be used for the API call. Defaults to the global fetch function. Optional.


### headers?:


Record<string, string> | Headers

A headers object to be passed to the API endpoint. Optional.


### credentials?:


RequestCredentials

The credentials mode to be used for the fetch request. Possible values are: "omit", "same-origin", "include". Optional.


### onError?:


(error: Error) => void

Callback function to be called when an error is encountered. Optional.


### onFinish?:


(result: OnFinishResult) => void

Called when the streaming response has finished.

OnFinishResult


### object:


T | undefined

The generated object (typed according to the schema). Can be undefined if the final object does not match the schema.


### error:


unknown | undefined

Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.


### [Returns](#returns)



### submit:


(input: INPUT) => void

Calls the API with the provided input as JSON body.


### object:


DeepPartial<RESULT> | undefined

The current value for the generated object. Updated as the API streams JSON chunks.


### error:


Error | unknown

The error object if the API call fails.


### isLoading:


boolean

Boolean flag indicating whether a request is currently in progress.


### stop:


() => void

Function to abort the current API request.


## [Examples](#examples)


[

Streaming Object Generation with useObject

](/examples/next-pages/basics/streaming-object-generation)
```

### 241. `docs/reference/ai-sdk-ui.md`

```markdown
# AI SDK UI


---
url: https://ai-sdk.dev/docs/reference/ai-sdk-ui
description: Reference documentation for the AI SDK UI
---


# [AI SDK UI](#ai-sdk-ui)


[AI SDK UI](/docs/ai-sdk-ui) is designed to help you build interactive chat, completion, and assistant applications with ease. It is framework-agnostic toolkit, streamlining the integration of advanced AI functionalities into your applications.

AI SDK UI contains the following hooks:

[

useChat

Use a hook to interact with language models in a chat interface.

](/docs/reference/ai-sdk-ui/use-chat)[

useCompletion

Use a hook to interact with language models in a completion interface.

](/docs/reference/ai-sdk-ui/use-completion)[

useObject

Use a hook for consuming a streamed JSON objects.

](/docs/reference/ai-sdk-ui/use-object)[

useAssistant

Use a hook to interact with OpenAI assistants.

](/docs/reference/ai-sdk-ui/use-assistant)[

convertToCoreMessages

Convert useChat messages to CoreMessages for AI core functions.

](/docs/reference/ai-sdk-ui/convert-to-core-messages)[

appendResponseMessages

Append CoreMessage\[\] from an AI response to an existing array of UI messages.

](/docs/reference/ai-sdk-ui/append-response-messages)[

appendClientMessage

Append a client message to an existing array of UI messages.

](/docs/reference/ai-sdk-ui/append-client-message)[

createDataStream

Create a data stream to stream additional data to the client.

](/docs/reference/ai-sdk-ui/create-data-stream)[

createDataStreamResponse

Create a response object to stream additional data to the client.

](/docs/reference/ai-sdk-ui/create-data-stream-response)[

pipeDataStreamToResponse

Pipe a data stream to a Node.js ServerResponse object.

](/docs/reference/ai-sdk-ui/pipe-data-stream-to-response)[

streamData

Stream additional data to the client along with generations.

](/docs/reference/ai-sdk-ui/stream-data)

It also contains the following helper functions:

[

AssistantResponse

Streaming helper for assistant responses.

](/docs/reference/ai-sdk-ui/assistant-response)


## [UI Framework Support](#ui-framework-support)


AI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), [Vue.js](https://vuejs.org/), and [SolidJS](https://www.solidjs.com/) (deprecated). Here is a comparison of the supported functions across these frameworks:

Function

React

Svelte

Vue.js

SolidJS (deprecated)

[useChat](/docs/reference/ai-sdk-ui/use-chat)

Chat

[useCompletion](/docs/reference/ai-sdk-ui/use-completion)

Completion

[useObject](/docs/reference/ai-sdk-ui/use-object)

StructuredObject

[useAssistant](/docs/reference/ai-sdk-ui/use-assistant)

[Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are welcome to implement missing features for non-React frameworks.
```

### 242. `docs/reference/stream-helpers/ai-stream.md`

```markdown
# AIStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/ai-stream
description: Learn to use AIStream helper function in your application.
---


# [`AIStream`](#aistream)


AIStream has been removed in AI SDK 4.0. Use `streamText.toDataStreamResponse()` instead.

Creates a readable stream for AI responses. This is based on the responses returned by fetch and serves as the basis for the OpenAIStream and AnthropicStream. It allows you to handle AI response streams in a controlled and customized manner that will work with useChat and useCompletion.

AIStream will throw an error if response doesn't have a 2xx status code. This is to ensure that the stream is only created for successful responses.


## [Import](#import)



### [React](#react)


import { AIStream } from "ai"


## [API Signature](#api-signature)



### response:


Response

This is the response object returned by fetch. It's used as the source of the readable stream.


### customParser:


(AIStreamParser) => void

This is a function that is used to parse the events in the stream. It should return a function that receives a stringified chunk from the LLM and extracts the message content. The function is expected to return nothing (void) or a string.

AIStreamParser

(data: string) => string | void


### callbacks:


AIStreamCallbacksAndOptions

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.
```

### 243. `docs/reference/stream-helpers/anthropic-stream.md`

```markdown
# AnthropicStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/anthropic-stream
description: Learn to use AnthropicStream helper function in your application.
---


# [`AnthropicStream`](#anthropicstream)


AnthropicStream has been removed in AI SDK 4.0.

AnthropicStream is part of the legacy Anthropic integration. It is not compatible with the AI SDK 3.1 functions. It is recommended to use the [AI SDK Anthropic Provider](/providers/ai-sdk-providers/anthropic) instead.

It is a utility function that transforms the output from Anthropic's SDK into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Anthropic's response data structure.


## [Import](#import)



### [React](#react)


import { AnthropicStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


Response

The response object returned by a call made by the Provider SDK.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 244. `docs/reference/stream-helpers/aws-bedrock-anthropic-stream.md`

```markdown
# AWSBedrockAnthropicStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-anthropic-stream
description: Learn to use AWSBedrockAnthropicStream helper function in your application.
---


# [`AWSBedrockAnthropicStream`](#awsbedrockanthropicstream)


AWSBedrockAnthropicStream has been removed in AI SDK 4.0.

AWSBedrockAnthropicStream is part of the legacy AWS Bedrock integration. It is not compatible with the AI SDK 3.1 functions.

The AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock's response.


## [Import](#import)



### [React](#react)


import { AWSBedrockAnthropicStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


AWSBedrockResponse

The response object returned from AWS Bedrock.

AWSBedrockResponse


### body?:


AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>

An optional async iterable of objects containing optional binary data chunks.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 245. `docs/reference/stream-helpers/aws-bedrock-cohere-stream.md`

```markdown
# AWSBedrockCohereStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-cohere-stream
description: Learn to use AWSBedrockCohereStream helper function in your application.
---


# [`AWSBedrockCohereStream`](#awsbedrockcoherestream)


AWSBedrockCohereStream has been removed in AI SDK 4.0.

AWSBedrockCohereStream is part of the legacy AWS Bedrock integration. It is not compatible with the AI SDK 3.1 functions.


## [Import](#import)


The AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handles parsing Bedrock's response.


### [React](#react)


import { AWSBedrockCohereStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


AWSBedrockResponse

The response object returned from AWS Bedrock.

AWSBedrockResponse


### body?:


AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>

An optional async iterable of objects containing optional binary data chunks.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 246. `docs/reference/stream-helpers/aws-bedrock-llama-2-stream.md`

```markdown
# AWSBedrockLlama2Stream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-llama-2-stream
description: Learn to use AWSBedrockLlama2Stream helper function in your application.
---


# [`AWSBedrockLlama2Stream`](#awsbedrockllama2stream)


AWSBedrockLlama2Stream has been removed in AI SDK 4.0.

AWSBedrockLlama2Stream is part of the legacy AWS Bedrock integration. It is not compatible with the AI SDK 3.1 functions.

The AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock's response.


## [Import](#import)



### [React](#react)


import { AWSBedrockLlama2Stream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


AWSBedrockResponse

The response object returned from AWS Bedrock.

AWSBedrockResponse


### body?:


AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>

An optional async iterable of objects containing optional binary data chunks.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 247. `docs/reference/stream-helpers/aws-bedrock-messages-stream.md`

```markdown
# AWSBedrockAnthropicMessagesStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-messages-stream
description: Learn to use AWSBedrockAnthropicMessagesStream helper function in your application.
---


# [`AWSBedrockAnthropicMessagesStream`](#awsbedrockanthropicmessagesstream)


AWSBedrockAnthropicMessagesStream has been removed in AI SDK 4.0.

AWSBedrockAnthropicMessagesStream is part of the legacy AWS Bedrock integration. It is not compatible with the AI SDK 3.1 functions.

The AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock's response.


## [Import](#import)



### [React](#react)


import { AWSBedrockAnthropicMessagesStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


AWSBedrockResponse

The response object returned from AWS Bedrock.

AWSBedrockResponse


### body?:


AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>

An optional async iterable of objects containing optional binary data chunks.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 248. `docs/reference/stream-helpers/aws-bedrock-stream.md`

```markdown
# AWSBedrockStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-stream
description: Learn to use AWSBedrockStream helper function in your application.
---


# [`AWSBedrockStream`](#awsbedrockstream)


AWSBedrockStream has been removed in AI SDK 4.0.

AWSBedrockStream is part of the legacy AWS Bedrock integration. It is not compatible with the AI SDK 3.1 functions.

The AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock's response.


## [Import](#import)



### [React](#react)


import { AWSBedrockStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


AWSBedrockResponse

The response object returned from AWS Bedrock.

AWSBedrockResponse


### body?:


AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>

An optional async iterable of objects containing optional binary data chunks.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 249. `docs/reference/stream-helpers/cohere-stream.md`

```markdown
# CohereStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/cohere-stream
description: Learn to use CohereStream helper function in your application.
---


# [`CohereStream`](#coherestream)


CohereStream has been removed in AI SDK 4.0.

CohereStream is part of the legacy Cohere integration. It is not compatible with the AI SDK 3.1 functions.

The CohereStream function is a utility that transforms the output from Cohere's API into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Cohere's response data structure. This works with the official Cohere API, and it's supported in both Node.js, the Edge Runtime, and browser environments.


## [Import](#import)



### [React](#react)


import { CohereStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


Response

The response object returned by a call made by the Provider SDK.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 250. `docs/reference/stream-helpers/google-generative-ai-stream.md`

```markdown
# GoogleGenerativeAIStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/google-generative-ai-stream
description: Learn to use GoogleGenerativeAIStream helper function in your application.
---


# [`GoogleGenerativeAIStream`](#googlegenerativeaistream)


GoogleGenerativeAIStream has been removed in AI SDK 4.0.

GoogleGenerativeAIStream is part of the legacy Google Generative AI integration. It is not compatible with the AI SDK 3.1 functions. It is recommended to use the [AI SDK Google Generative AI Provider](/providers/ai-sdk-providers/google-generative-ai) instead.

The GoogleGenerativeAIStream function is a utility that transforms the output from Google's Generative AI SDK into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Google's response data structure. This works with the official Generative AI SDK, and it's supported in both Node.js, Edge Runtime, and browser environments.


## [Import](#import)



### [React](#react)


import { GoogleGenerativeAIStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


{ stream: AsyncIterable<GenerateContentResponse> }

The response object returned by the Google Generative AI API.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 251. `docs/reference/stream-helpers/hugging-face-stream.md`

```markdown
# HuggingFaceStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/hugging-face-stream
description: Learn to use HuggingFaceStream helper function in your application.
---


# [`HuggingFaceStream`](#huggingfacestream)


HuggingFaceStream has been removed in AI SDK 4.0.

HuggingFaceStream is part of the legacy Hugging Face integration. It is not compatible with the AI SDK 3.1 functions.

Converts the output from language models hosted on Hugging Face into a ReadableStream.

While HuggingFaceStream is compatible with most Hugging Face language models, the rapidly evolving landscape of models may result in certain new or niche models not being supported. If you encounter a model that isn't supported, we encourage you to open an issue.

To ensure that AI responses are comprised purely of text without any delimiters that could pose issues when rendering in chat or completion modes, we standardize and remove special end-of-response tokens. If your use case requires a different handling of responses, you can fork and modify this stream to meet your specific needs.

Currently, `</s>` and `<|endoftext|>` are recognized as end-of-stream tokens.


## [Import](#import)



### [React](#react)


import { HuggingFaceStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### iter:


AsyncGenerator<any>

This parameter should be the generator function returned by the hf.textGenerationStream method in the Hugging Face Inference SDK.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 252. `docs/reference/stream-helpers/inkeep-stream.md`

```markdown
# InkeepStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/inkeep-stream
description: Learn to use InkeepStream helper function in your application.
---


# [`InkeepStream`](#inkeepstream)


InkeepStream has been removed in AI SDK 4.0.

InkeepStream is part of the legacy Inkeep integration. It is not compatible with the AI SDK 3.1 functions.

The InkeepStream function is a utility that transforms the output from [Inkeep](https://inkeep.com)'s API into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Inkeep's response data structure.

This works with the official Inkeep API, and it's supported in both Node.js, the Edge Runtime, and browser environments.


## [Import](#import)



### [React](#react)


import { InkeepStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


Response

The response object returned by a call made by the Provider SDK.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 253. `docs/reference/stream-helpers/langchain-adapter.md`

```markdown
# LangChainAdapter


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/langchain-adapter
description: API Reference for LangChainAdapter.
---


# [`LangChainAdapter`](#langchainadapter)


The `LangChainAdapter` module provides helper functions to transform LangChain output streams into data streams and data stream responses. See the [LangChain Adapter documentation](/providers/adapters/langchain) for more information.

It supports:

-   LangChain StringOutputParser streams
-   LangChain AIMessageChunk streams
-   LangChain StreamEvents v2 streams


## [Import](#import)


import { LangChainAdapter } from "ai"


## [API Signature](#api-signature)



### [Methods](#methods)



### toDataStream:


(stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, AIStreamCallbacksAndOptions) => AIStream

Converts LangChain output streams to data stream.


### toDataStreamResponse:


(stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, options?: {init?: ResponseInit, data?: StreamData, callbacks?: AIStreamCallbacksAndOptions}) => Response

Converts LangChain output streams to data stream response.


### mergeIntoDataStream:


(stream: ReadableStream<LangChainStreamEvent> | ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, options: { dataStream: DataStreamWriter; callbacks?: StreamCallbacks }) => void

Merges LangChain output streams into an existing data stream.


## [Examples](#examples)



### [Convert LangChain Expression Language Stream](#convert-langchain-expression-language-stream)


app/api/completion/route.ts

```
import{ChatOpenAI}from'@langchain/openai';import{LangChainAdapter}from'ai';exportasyncfunctionPOST(req: Request){const{ prompt }=await req.json();const model =newChatOpenAI({    model:'gpt-3.5-turbo-0125',    temperature:0,});const stream =await model.stream(prompt);returnLangChainAdapter.toDataStreamResponse(stream);}
```


### [Convert StringOutputParser Stream](#convert-stringoutputparser-stream)


app/api/completion/route.ts

```
import{ChatOpenAI}from'@langchain/openai';import{LangChainAdapter}from'ai';import{StringOutputParser}from'@langchain/core/output_parsers';exportasyncfunctionPOST(req: Request){const{ prompt }=await req.json();const model =newChatOpenAI({    model:'gpt-3.5-turbo-0125',    temperature:0,});const parser =newStringOutputParser();const stream =await model.pipe(parser).stream(prompt);returnLangChainAdapter.toDataStreamResponse(stream);}
```
```

### 254. `docs/reference/stream-helpers/langchain-stream.md`

```markdown
# LangChainStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/langchain-stream
description: API Reference for LangChainStream.
---


# [`LangChainStream`](#langchainstream)


LangChainStream has been removed in AI SDK 4.0.

LangChainStream is part of the legacy LangChain integration. It is recommended to use the [LangChain Adapter](/providers/adapters/langchain) instead.

Helps with the integration of LangChain. It is compatible with useChat and useCompletion.


## [Import](#import)


import { LangChainStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)



### stream:


ReadableStream

This is the readable stream that can be piped into another stream. This stream contains the results of the LangChain process.


### handlers:


LangChainCallbacks

This object contains handlers that can be used to handle certain callbacks provided by LangChain.
```

### 255. `docs/reference/stream-helpers/llamaindex-adapter.md`

```markdown
# LlamaIndexAdapter


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/llamaindex-adapter
description: API Reference for LlamaIndexAdapter.
---


# [`LlamaIndexAdapter`](#llamaindexadapter)


The `LlamaIndexAdapter` module provides helper functions to transform LlamaIndex output streams into data streams and data stream responses. See the [LlamaIndex Adapter documentation](/providers/adapters/llamaindex) for more information.

It supports:

-   LlamaIndex ChatEngine streams
-   LlamaIndex QueryEngine streams


## [Import](#import)


import { LlamaIndexAdapter } from "ai"


## [API Signature](#api-signature)



### [Methods](#methods)



### toDataStream:


(stream: AsyncIterable<EngineResponse>, AIStreamCallbacksAndOptions) => AIStream

Converts LlamaIndex output streams to data stream.


### toDataStreamResponse:


(stream: AsyncIterable<EngineResponse>, options?: {init?: ResponseInit, data?: StreamData, callbacks?: AIStreamCallbacksAndOptions}) => Response

Converts LlamaIndex output streams to data stream response.


### mergeIntoDataStream:


(stream: AsyncIterable<EngineResponse>, options: { dataStream: DataStreamWriter; callbacks?: StreamCallbacks }) => void

Merges LlamaIndex output streams into an existing data stream.


## [Examples](#examples)



### [Convert LlamaIndex ChatEngine Stream](#convert-llamaindex-chatengine-stream)


app/api/completion/route.ts

```
import{OpenAI,SimpleChatEngine}from'llamaindex';import{LlamaIndexAdapter}from'ai';exportasyncfunctionPOST(req: Request){const{ prompt }=await req.json();const llm =newOpenAI({ model:'gpt-4o'});const chatEngine =newSimpleChatEngine({ llm });const stream =await chatEngine.chat({    message: prompt,    stream:true,});returnLlamaIndexAdapter.toDataStreamResponse(stream);}
```
```

### 256. `docs/reference/stream-helpers/mistral-stream.md`

```markdown
# MistralStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/mistral-stream
description: Learn to use MistralStream helper function in your application.
---


# [`MistralStream`](#mistralstream)


MistralStream has been removed in AI SDK 4.0.

MistralStream is part of the legacy Mistral integration. It is not compatible with the AI SDK 3.1 functions. It is recommended to use the [AI SDK Mistral Provider](/providers/ai-sdk-providers/mistral) instead.

Transforms the output from Mistral's language models into a ReadableStream.

This works with the official Mistral API, and it's supported in both Node.js, the Edge Runtime, and browser environments.


## [Import](#import)



### [React](#react)


import { MistralStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


Response

The response object returned by a call made by the Provider SDK.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### [Returns](#returns)


A `ReadableStream`.
```

### 257. `docs/reference/stream-helpers/openai-stream.md`

```markdown
# OpenAIStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/openai-stream
description: Learn to use OpenAIStream helper function in your application.
---


# [`OpenAIStream`](#openaistream)


OpenAIStream has been removed in AI SDK 4.0

OpenAIStream is part of the legacy OpenAI integration. It is not compatible with the AI SDK 3.1 functions. It is recommended to use the [AI SDK OpenAI Provider](/providers/ai-sdk-providers/openai) instead.

Transforms the response from OpenAI's language models into a ReadableStream.

Note: Prior to v4, the official OpenAI API SDK does not support the Edge Runtime and only works in serverless environments. The openai-edge package is based on fetch instead of axios (and thus works in the Edge Runtime) so we recommend using openai v4+ or openai-edge.


## [Import](#import)



### [React](#react)


import { OpenAIStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### response:


Response

The response object returned by a call made by the Provider SDK.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.
```

### 258. `docs/reference/stream-helpers/replicate-stream.md`

```markdown
# ReplicateStream


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/replicate-stream
description: Learn to use ReplicateStream helper function in your application.
---


# [`ReplicateStream`](#replicatestream)


ReplicateStream has been removed in AI SDK 4.0.

ReplicateStream is part of the legacy Replicate integration. It is not compatible with the AI SDK 3.1 functions.

The ReplicateStream function is a utility that handles extracting the stream from the output of [Replicate](https://replicate.com)'s API. It expects a Prediction object as returned by the [Replicate JavaScript SDK](https://github.com/replicate/replicate-javascript), and returns a ReadableStream. Unlike other wrappers, ReplicateStream returns a Promise because it makes a fetch call to the [Replicate streaming API](https://github.com/replicate/replicate-javascript#streaming) under the hood.


## [Import](#import)



### [React](#react)


import { ReplicateStream } from "ai"


## [API Signature](#api-signature)



### [Parameters](#parameters)



### pre:


Prediction

Object returned by the Replicate JavaScript SDK.


### callbacks?:


AIStreamCallbacksAndOptions

An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.

AIStreamCallbacksAndOptions


### onStart:


() => Promise<void>

An optional function that is called at the start of the stream processing.


### onCompletion:


(completion: string) => Promise<void>

An optional function that is called for every completion. It's passed the completion as a string.


### onFinal:


(completion: string) => Promise<void>

An optional function that is called once when the stream is closed with the final completion message.


### onToken:


(token: string) => Promise<void>

An optional function that is called for each token in the stream. It's passed the token as a string.


### options:


{ headers?: Record<string, string> }

An optional parameter for passing additional headers.


### [Returns](#returns)


A `ReadableStream` wrapped in a promise.
```

### 259. `docs/reference/stream-helpers/stream-to-response.md`

```markdown
# streamToResponse


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/stream-to-response
description: Learn to use streamToResponse helper function in your application.
---


# [`streamToResponse`](#streamtoresponse)


`streamToResponse` has been removed in AI SDK 4.0. Use `pipeDataStreamToResponse` from [streamText](/docs/reference/ai-sdk-core/stream-text) instead.

`streamToResponse` pipes a data stream to a Node.js `ServerResponse` object and sets the status code and headers.

This is useful to create data stream responses in environments that use `ServerResponse` objects, such as Node.js HTTP servers.

The status code and headers can be configured using the `options` parameter. By default, the status code is set to 200 and the Content-Type header is set to `text/plain; charset=utf-8`.


## [Import](#import)


import { streamToResponse } from "ai"


## [Example](#example)


You can e.g. use `streamToResponse` to pipe a data stream to a Node.js HTTP server response:

```
import{ openai }from'@ai-sdk/openai';import{StreamData, streamText, streamToResponse }from'ai';import{ createServer }from'http';createServer(async(req, res)=>{const result =streamText({    model:openai('gpt-4-turbo'),    prompt:'What is the weather in San Francisco?',});// use stream dataconst data =newStreamData();  data.append('initialized call');streamToResponse(    result.toAIStream({onFinal(){        data.append('call completed');        data.close();},}),    res,{},    data,);}).listen(8080);
```


## [API Signature](#api-signature)



### [Parameters](#parameters)



### stream:


ReadableStream

The Web Stream to pipe to the response. It can be the return value of OpenAIStream, HuggingFaceStream, AnthropicStream, or an AIStream instance.


### response:


ServerResponse

The Node.js ServerResponse object to pipe the stream to. This is usually the second argument of a Node.js HTTP request handler.


### options:


Options

Configure the response

Options


### status:


number

The status code to set on the response. Defaults to \`200\`.


### headers:


Record<string, string>

Additional headers to set on the response. Defaults to \`{ 'Content-Type': 'text/plain; charset=utf-8' }\`.


### data:


StreamData

StreamData object for forwarding additional data to the client.
```

### 260. `docs/reference/stream-helpers/streaming-text-response.md`

```markdown
# StreamingTextResponse


---
url: https://ai-sdk.dev/docs/reference/stream-helpers/streaming-text-response
description: Learn to use StreamingTextResponse helper function in your application.
---


# [`StreamingTextResponse`](#streamingtextresponse)


`StreamingTextResponse` has been removed in AI SDK 4.0. Use [`streamText.toDataStreamResponse()`](/docs/reference/ai-sdk-core/stream-text) instead.

It is a utility class that simplifies the process of returning a ReadableStream of text in HTTP responses. It is a lightweight wrapper around the native Response class, automatically setting the status code to 200 and the Content-Type header to 'text/plain; charset=utf-8'.


## [Import](#import)


import { StreamingTextResponse } from "ai"


## [API Signature](#api-signature)



## [Parameters](#parameters)



### stream:


ReadableStream

The stream of content which represents the HTTP response.


### init?:


ResponseInit

It can be used to customize the properties of the HTTP response. It is an object that corresponds to the ResponseInit object used in the Response constructor.

ResponseInit


### status?:


number

The status code for the response. StreamingTextResponse will overwrite this value with 200.


### statusText?:


string

The status message associated with the status code.


### headers?:


HeadersInit

Any headers you want to add to your response. StreamingTextResponse will add 'Content-Type': 'text/plain; charset=utf-8' to these headers.


### data?:


StreamData

StreamData object that you are using to generate additional data for the response.


### [Returns](#returns)


An instance of Response with the provided ReadableStream as the body, the status set to 200, and the Content-Type header set to 'text/plain; charset=utf-8'. Additional headers and properties can be added using the init parameter
```

### 261. `docs/reference/stream-helpers.md`

```markdown
# Reference: Stream Helpers


---
url: https://ai-sdk.dev/docs/reference/stream-helpers
description: Learn to use help functions that help stream generations from different providers.
---

[

AIStream

Create a readable stream for AI responses.

](/docs/reference/stream-helpers/ai-stream)[

StreamingTextResponse

Create a streaming response for text generations.

](/docs/reference/stream-helpers/streaming-text-response)[

streamtoResponse

Pipe a ReadableStream to a Node.js ServerResponse object.

](/docs/reference/stream-helpers/stream-to-response)[

OpenAIStream

Transforms the response from OpenAI's language models into a readable stream.

](/docs/reference/stream-helpers/openai-stream)[

AnthropicStream

Transforms the response from Anthropic's language models into a readable stream.

](/docs/reference/stream-helpers/anthropic-stream)[

AWSBedrockStream

Transforms the response from AWS Bedrock's language models into a readable stream.

](/docs/reference/stream-helpers/aws-bedrock-stream)[

AWSBedrockMessagesStream

Transforms the response from AWS Bedrock Message's language models into a readable stream.

](/docs/reference/stream-helpers/aws-bedrock-messages-stream)[

AWSBedrockCohereStream

Transforms the response from AWS Bedrock Cohere's language models into a readable stream.

](/docs/reference/stream-helpers/aws-bedrock-cohere-stream)[

AWSBedrockLlama-2Stream

Transforms the response from AWS Bedrock Llama-2's language models into a readable stream.

](/docs/reference/stream-helpers/aws-bedrock-llama-2-stream)[

CohereStream

Transforms the response from Cohere's language models into a readable stream.

](/docs/reference/stream-helpers/cohere-stream)[

GoogleGenerativeAIStream

Transforms the response from Google's language models into a readable stream.

](/docs/reference/stream-helpers/google-generative-ai-stream)[

HuggingFaceStream

Transforms the response from Hugging Face's language models into a readable stream.

](/docs/reference/stream-helpers/hugging-face-stream)[

LangChainStream

Transforms the response from LangChain's language models into a readable stream.

](/docs/reference/stream-helpers/langchain-stream)[

LangChainAdapter

Transforms the response from LangChain's stream into data streams.

](/docs/reference/stream-helpers/langchain-adapter)[

LlamaIndexAdapter

Transforms the response from LlamaIndex's streams into data streams.

](/docs/reference/stream-helpers/llamaindex-adapter)[

MistralStream

Transforms the response from Mistral's language models into a readable stream.

](/docs/reference/stream-helpers/mistral-stream)[

ReplicateStream

Transforms the response from Replicate's language models into a readable stream.

](/docs/reference/stream-helpers/replicate-stream)[

InkeepsStream

Transforms the response from Inkeeps's language models into a readable stream.

](/docs/reference/stream-helpers/inkeep-stream)
```

### 262. `docs/reference.md`

```markdown
# API Reference


---
url: https://ai-sdk.dev/docs/reference
description: Reference documentation for the AI SDK
---


# [API Reference](#api-reference)


[

AI SDK Core

Switch between model providers without changing your code.

](/docs/reference/ai-sdk-core)[

AI SDK RSC

Use React Server Components to stream user interfaces to the client.

](/docs/reference/ai-sdk-rsc)[

AI SDK UI

Use hooks to integrate user interfaces that interact with language models.

](/docs/reference/ai-sdk-ui)[

Stream Helpers

Use special functions that help stream model generations from various providers.

](/docs/reference/stream-helpers)
```

### 263. `docs/troubleshooting/azure-stream-slow.md`

```markdown
# Azure OpenAI Slow To Stream


---
url: https://ai-sdk.dev/docs/troubleshooting/azure-stream-slow
description: Learn to troubleshoot Azure OpenAI slow to stream issues.
---


# [Azure OpenAI Slow To Stream](#azure-openai-slow-to-stream)



## [Issue](#issue)


When using OpenAI hosted on Azure, streaming is slow and in big chunks.


## [Cause](#cause)


This is a Microsoft Azure issue. Some users have reported the following solutions:

-   **Update Content Filtering Settings**: Inside [Azure AI Studio](https://ai.azure.com/), within "Shared resources" > "Content filters", create a new content filter and set the "Streaming mode (Preview)" under "Output filter" from "Default" to "Asynchronous Filter".


## [Solution](#solution)


You can use the [`smoothStream` transformation](/docs/ai-sdk-core/generating-text#smoothing-streams) to stream each word individually.

```
import{ smoothStream, streamText }from'ai';const result =streamText({  model,  prompt,  experimental_transform:smoothStream(),});
```
```

### 264. `docs/troubleshooting/client-side-function-calls-not-invoked.md`

```markdown
# Client-Side Function Calls Not Invoked


---
url: https://ai-sdk.dev/docs/troubleshooting/client-side-function-calls-not-invoked
description: Troubleshooting client-side function calls not being invoked.
---


# [Client-Side Function Calls Not Invoked](#client-side-function-calls-not-invoked)



## [Issue](#issue)


I upgraded the AI SDK to v3.0.20 or newer. I am using [`OpenAIStream`](/docs/reference/stream-helpers/openai-stream). Client-side function calls are no longer invoked.


## [Solution](#solution)


You will need to add a stub for `experimental_onFunctionCall` to [`OpenAIStream`](/docs/reference/stream-helpers/openai-stream) to enable the correct forwarding of the function calls to the client.

```
const stream =OpenAIStream(response,{asyncexperimental_onFunctionCall(){return;},});
```
```

### 265. `docs/troubleshooting/client-stream-error.md`

```markdown
# "Only plain objects can be passed from client components" Server Action Error


---
url: https://ai-sdk.dev/docs/troubleshooting/client-stream-error
description: Troubleshooting errors related to using AI SDK Core functions with Server Actions.
---


# ["Only plain objects can be passed from client components" Server Action Error](#only-plain-objects-can-be-passed-from-client-components-server-action-error)



## [Issue](#issue)


I am using [`streamText`](/docs/reference/ai-sdk-core/stream-text) or [`streamObject`](/docs/reference/ai-sdk-core/stream-object) with Server Actions, and I am getting a `"only plain objects and a few built ins can be passed from client components"` error.


## [Background](#background)


This error occurs when you're trying to return a non-serializable object from a Server Action to a Client Component. The streamText function likely returns an object with methods or complex structures that can't be directly serialized and passed to the client.


## [Solution](#solution)


To fix this issue, you need to ensure that you're only returning serializable data from your Server Action. Here's how you can modify your approach:

1.  Instead of returning the entire result object from streamText, extract only the necessary serializable data.
2.  Use the [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) function to create a streamable value that can be safely passed to the client.

Here's an example that demonstrates how to implement this solution: [Streaming Text Generation](/examples/next-app/basics/streaming-text-generation).

This approach ensures that only serializable data (the text) is passed to the client, avoiding the "only plain objects" error.
```

### 266. `docs/troubleshooting/jest-cannot-find-module-ai-rsc.md`

```markdown
# Jest: cannot find module 'ai/rsc'


---
url: https://ai-sdk.dev/docs/troubleshooting/jest-cannot-find-module-ai-rsc
description: Troubleshooting AI SDK errors related to the Jest: cannot find module 'ai/rsc' error
---


# [Jest: cannot find module 'ai/rsc'](#jest-cannot-find-module-airsc)



## [Issue](#issue)


I am using AI SDK RSC and am writing tests for my RSC components with Jest.

I am getting the following error: `Cannot find module 'ai/rsc'`.


## [Solution](#solution)


Configure the module resolution via `jest config update` in `moduleNameMapper`:

jest.config.js

```
"moduleNameMapper":{"^ai/rsc$":"<rootDir>/node_modules/ai/rsc/dist"}
```
```

### 267. `docs/troubleshooting/model-is-not-assignable-to-type.md`

```markdown
# Model is not assignable to type "LanguageModelV1"


---
url: https://ai-sdk.dev/docs/troubleshooting/model-is-not-assignable-to-type
description: Troubleshooting errors related to incompatible models.
---


# [Model is not assignable to type "LanguageModelV1"](#model-is-not-assignable-to-type-languagemodelv1)



## [Issue](#issue)


I have updated the AI SDK and now I get the following error: `Type 'SomeModel' is not assignable to type 'LanguageModelV1'.`

Similar errors can occur with `EmbeddingModelV1` as well.


## [Background](#background)


Sometimes new features are being added to the model specification. This can cause incompatibilities with older provider versions.


## [Solution](#solution)


Update your provider packages and the AI SDK to the latest version.
```

### 268. `docs/troubleshooting/nan-token-counts-openai-streaming.md`

```markdown
# NaN token counts when using streamText with OpenAI models


---
url: https://ai-sdk.dev/docs/troubleshooting/nan-token-counts-openai-streaming
description: Troubleshooting errors related to NaN token counts in OpenAI streaming.
---


# [`NaN` token counts when using `streamText` with OpenAI models](#nan-token-counts-when-using-streamtext-with-openai-models)



## [Issue](#issue)


I am using `streamText` with the [OpenAI provider for the AI SDK](/providers/ai-sdk-providers/openai) and OpenAI models. I use [`createOpenAI`](/providers/ai-sdk-providers/openai#provider-instance) to create the provider instance. When I try to get the token counts, I get `NaN` values.


## [Background](#background)


OpenAI introduced `streamOptions` parameters to enable token counts in the stream. However, this was a breaking change for OpenAI-compatible providers, and we therefore made it opt-in.


## [Solution](#solution)


When you use [`createOpenAI`](/providers/ai-sdk-providers/openai#provider-instance), you can enable a `strict` compatibility model:

```
import{ createOpenAI }from'@ai-sdk/openai';const openai =createOpenAI({  compatibility:'strict',});
```

This will enable the token counts in the stream. When you use the default `openai` provider instance, the setting is enabled by default.
```

### 269. `docs/troubleshooting/react-maximum-update-depth-exceeded.md`

```markdown
# React error "Maximum update depth exceeded"


---
url: https://ai-sdk.dev/docs/troubleshooting/react-maximum-update-depth-exceeded
description: Troubleshooting errors related to the "Maximum update depth exceeded" error.
---


# [React error "Maximum update depth exceeded"](#react-error-maximum-update-depth-exceeded)



## [Issue](#issue)


I am using the AI SDK in a React project with the `useChat` or `useCompletion` hooks and I get the following error when AI responses stream in: `Maximum update depth exceeded`.


## [Background](#background)


By default, the UI is re-rendered on every chunk that arrives. This can overload the rendering, especially on slower devices or when complex components need updating (e.g. Markdown). Throttling can mitigate this.


## [Solution](#solution)


Use the `experimental_throttle` option to throttle the UI updates:


### [`useChat`](#usechat)


page.tsx

```
const{ messages,...}=useChat({// Throttle the messages and data updates to 50ms:  experimental_throttle:50})
```


### [`useCompletion`](#usecompletion)


page.tsx

```
const{ completion,...}=useCompletion({// Throttle the completion and data updates to 50ms:  experimental_throttle:50})
```
```

### 270. `docs/troubleshooting/server-actions-in-client-components.md`

```markdown
# Server Actions in Client Components


---
url: https://ai-sdk.dev/docs/troubleshooting/server-actions-in-client-components
description: Troubleshooting errors related to server actions in client components.
---


# [Server Actions in Client Components](#server-actions-in-client-components)


You may use Server Actions in client components, but sometimes you may encounter the following issues.


## [Issue](#issue)


It is not allowed to define inline `"use server"` annotated Server Actions in Client Components.


## [Solution](#solution)


To use Server Actions in a Client Component, you can either:

-   Export them from a separate file with `"use server"` at the top.
-   Pass them down through props from a Server Component.
-   Implement a combination of [`createAI`](/docs/reference/ai-sdk-rsc/create-ai) and [`useActions`](/docs/reference/ai-sdk-rsc/use-actions) hooks to access them.

Learn more about [Server Actions and Mutations](https://nextjs.org/docs/app/api-reference/functions/server-actions#with-client-components).

```
'use server';import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctiongetAnswer(question:string){'use server';const{ text }=awaitgenerateText({    model: openai.chat('gpt-3.5-turbo'),    prompt: question,});return{ answer: text };}
```
```

### 271. `docs/troubleshooting/strange-stream-output.md`

```markdown
# useChat/useCompletion stream output contains 0:... instead of text


---
url: https://ai-sdk.dev/docs/troubleshooting/strange-stream-output
description: How to fix strange stream output in the UI
---


# [useChat/useCompletion stream output contains 0:... instead of text](#usechatusecompletion-stream-output-contains-0-instead-of-text)



## [Issue](#issue)


I am using custom client code to process a server response that is sent using [`StreamingTextResponse`](/docs/reference/stream-helpers/streaming-text-response). I am using version `3.0.20` or newer of the AI SDK. When I send a query, the UI streams text such as `0: "Je"`, `0: " suis"`, `0: "des"...` instead of the text that I’m looking for.


## [Background](#background)


The AI SDK has switched to the stream data protocol in version `3.0.20`. It sends different stream parts to support data, tool calls, etc. What you see is the raw stream data protocol response.


## [Solution](#solution)


You have several options:

1.  Use the AI Core [`streamText`](/docs/reference/ai-sdk-core/stream-text) function to send a raw text stream:

    ```
    exportasyncfunctionPOST(req: Request){const{ prompt }=await req.json();const result =streamText({    model: openai.completion('gpt-3.5-turbo-instruct'),    maxTokens:2000,    prompt,});return result.toTextStreamResponse();}
    ```

2.  Pin the AI SDK version to `3.0.19` . This will keep the raw text stream.
```

### 272. `docs/troubleshooting/stream-text-not-working.md`

```markdown
# streamText is not working


---
url: https://ai-sdk.dev/docs/troubleshooting/stream-text-not-working
description: Troubleshooting errors related to the streamText function not working.
---


# [`streamText` is not working](#streamtext-is-not-working)



## [Issue](#issue)


I am using [`streamText`](/docs/reference/ai-sdk-core/stream-text) function, and it does not work. It does not throw any errors and the stream is only containing error parts.


## [Background](#background)


`streamText` immediately starts streaming to enable sending data without waiting for the model. Errors become part of the stream and are not thrown to prevent e.g. servers from crashing.


## [Solution](#solution)


To log errors, you can provide an `onError` callback that is triggered when an error occurs.

```
import{ streamText }from'ai';const result =streamText({  model: yourModel,  prompt:'Invent a new holiday and describe its traditions.',onError({ error }){console.error(error);// your error logging logic here},});
```
```

### 273. `docs/troubleshooting/streamable-ui-errors.md`

```markdown
# Streamable UI Component Error


---
url: https://ai-sdk.dev/docs/troubleshooting/streamable-ui-errors
description: Troubleshooting errors related to streamable UI.
---


# [Streamable UI Component Error](#streamable-ui-component-error)



## [Issue](#issue)


-   Variable Not Found
-   Cannot find `div`
-   `Component` refers to a value, but is being used as a type


## [Solution](#solution)


If you encounter these errors when working with streamable UIs within server actions, it is likely because the file ends in `.ts` instead of `.tsx`.
```

### 274. `docs/troubleshooting/streaming-not-working-when-deployed.md`

```markdown
# Streaming Not Working When Deployed


---
url: https://ai-sdk.dev/docs/troubleshooting/streaming-not-working-when-deployed
description: Troubleshooting streaming issues in deployed apps.
---


# [Streaming Not Working When Deployed](#streaming-not-working-when-deployed)



## [Issue](#issue)


Streaming with the AI SDK works in my local development environment. However, when deploying, streaming does not work in the deployed app. Instead of streaming, only the full response is returned after a while.


## [Cause](#cause)


The causes of this issue are varied and depend on the deployment environment.


## [Solution](#solution)


You can try the following:

-   add `'Transfer-Encoding': 'chunked'` and/or `Connection: 'keep-alive'` headers

    ```
    return result.toDataStreamResponse({  headers:{'Transfer-Encoding':'chunked',Connection:'keep-alive',},});
    ```
```

### 275. `docs/troubleshooting/streaming-not-working-when-proxied.md`

```markdown
# Streaming Not Working When Proxied


---
url: https://ai-sdk.dev/docs/troubleshooting/streaming-not-working-when-proxied
description: Troubleshooting streaming issues in proxied apps.
---


# [Streaming Not Working When Proxied](#streaming-not-working-when-proxied)



## [Issue](#issue)


Streaming with the AI SDK doesn't work in local development environment, or deployed in some proxy environments. Instead of streaming, only the full response is returned after a while.


## [Cause](#cause)


The causes of this issue are caused by the proxy middleware.

If the middleware is configured to compress the response, it will cause the streaming to fail.


## [Solution](#solution)


You can try the following, the solution only affects the streaming API:

-   add `'Content-Encoding': 'none'` headers

    ```
    return result.toDataStreamResponse({  headers:{'Content-Encoding':'none',},});
    ```
```

### 276. `docs/troubleshooting/timeout-on-vercel.md`

```markdown
# Getting Timeouts When Deploying on Vercel


---
url: https://ai-sdk.dev/docs/troubleshooting/timeout-on-vercel
description: Learn how to fix timeouts and cut off responses when deploying to Vercel.
---


# [Getting Timeouts When Deploying on Vercel](#getting-timeouts-when-deploying-on-vercel)



## [Issue](#issue)


Streaming with the AI SDK works in my local development environment. However, when I'm deploying to Vercel, longer responses get chopped off in the UI and I'm seeing timeouts in the Vercel logs or I'm seeing the error: `Uncaught (in promise) Error: Connection closed`.


## [Solution](#solution)


If you are using Next.js with the App Router, you can add the following to your route file or the page you are calling your Server Action from:

```
exportconst maxDuration =30;
```

This increases the maximum duration of the function to 30 seconds.

For other frameworks such as Svelte, you can set timeouts in your `vercel.json` file:

```
{"functions":{"api/chat/route.ts":{"maxDuration":30}}}
```


## [Learn more](#learn-more)


-   [Configuring Maximum Duration for Vercel Functions](https://vercel.com/docs/functions/configuring-functions/duration)
-   [Maximum Duration Limits](https://vercel.com/docs/functions/runtimes#max-duration)
```

### 277. `docs/troubleshooting/tool-invocation-missing-result.md`

```markdown
# Tool Invocation Missing Result Error


---
url: https://ai-sdk.dev/docs/troubleshooting/tool-invocation-missing-result
description: How to fix the "ToolInvocation must have a result" error when using tools without execute functions
---


# [Tool Invocation Missing Result Error](#tool-invocation-missing-result-error)



## [Issue](#issue)


When using `generateText()` or `streamText()`, you may encounter the error "ToolInvocation must have a result" when a tool without an `execute` function is called.


## [Cause](#cause)


The error occurs when you define a tool without an `execute` function and don't provide the result through other means (like `useChat`'s `onToolCall` or `addToolResult` functions).

Each time a tool is invoked, the model expects to receive a result before continuing the conversation. Without a result, the model cannot determine if the tool call succeeded or failed and the conversation state becomes invalid.


## [Solution](#solution)


You have two options for handling tool results:

1.  Server-side execution using tools with an `execute` function:

```
const tools ={  weather:tool({    description:'Get the weather in a location',    parameters: z.object({location: z.string().describe('The city and state, e.g. "San Francisco, CA"'),}),execute:async({ location })=>{// Fetch and return weather datareturn{ temperature:72, conditions:'sunny',location};},}),};
```

2.  Client-side execution with `useChat` (omitting the `execute` function), but you must provide results in one of these ways:

```
const{ messages }=useChat({// Option 1: Handle using onToolCallonToolCall:async({ toolCall })=>{if(toolCall.toolName ==='getLocation'){const result =awaitgetLocationData();return result;// This becomes the tool result}},});}
```

```
// Option 2: Use addToolResult (e.g. with interactive UI elements)const{ messages, addToolResult }=useChat();// Inside your JSX, when rendering tool calls:<buttononClick={()=>addToolResult({      toolCallId,// must provide tool call ID      result:{/* your tool result */},})}>Confirm</button>;
```

Whether handling tools on the server or client, each tool call must have a corresponding result before the conversation can continue.
```

### 278. `docs/troubleshooting/typescript-cannot-find-namespace-jsx.md`

```markdown
# TypeScript error "Cannot find namespace 'JSX'"


---
url: https://ai-sdk.dev/docs/troubleshooting/typescript-cannot-find-namespace-jsx
description: Troubleshooting errors related to TypeScript and JSX.
---


# [TypeScript error "Cannot find namespace 'JSX'"](#typescript-error-cannot-find-namespace-jsx)



## [Issue](#issue)


I am using the AI SDK in a project without React, e.g. an Hono server, and I get the following error: `error TS2503: Cannot find namespace 'JSX'.`


## [Background](#background)


The AI SDK has a dependency on `@types/react` which defines the `JSX` namespace. It will be removed in the next major version of the AI SDK.


## [Solution](#solution)


You can install the `@types/react` package as a dependency to fix the error.

```
npminstall @types/react
```
```

### 279. `docs/troubleshooting/unclosed-streams.md`

```markdown
# Unclosed Streams


---
url: https://ai-sdk.dev/docs/troubleshooting/unclosed-streams
description: Troubleshooting errors related to unclosed streams.
---


# [Unclosed Streams](#unclosed-streams)


Sometimes streams are not closed properly, which can lead to unexpected behavior. The following are some common issues that can occur when streams are not closed properly.


## [Issue](#issue)


The streamable UI has been slow to update.


## [Solution](#solution)


This happens when you create a streamable UI using [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) and fail to close the stream. In order to fix this, you must ensure you close the stream by calling the [`.done()`](/docs/reference/ai-sdk-rsc/create-streamable-ui#done) method. This will ensure the stream is closed.

```
import{ createStreamableUI }from'ai/rsc';constsubmitMessage=async()=>{'use server';const stream =createStreamableUI('1');  stream.update('2');  stream.append('3');  stream.done('4');// [!code ++]return stream.value;};
```
```

### 280. `docs/troubleshooting/use-chat-an-error-occurred.md`

```markdown
# useChat "An error occurred"


---
url: https://ai-sdk.dev/docs/troubleshooting/use-chat-an-error-occurred
description: Troubleshooting errors related to the "An error occurred" error in useChat.
---


# [`useChat` "An error occurred"](#usechat-an-error-occurred)



## [Issue](#issue)


I am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and I get the error "An error occurred".


## [Background](#background)


Error messages from `streamText` are masked by default when using `toDataStreamResponse` for security reasons (secure-by-default). This prevents leaking sensitive information to the client.


## [Solution](#solution)


To forward error details to the client or to log errors, use the `getErrorMessage` function when calling `toDataStreamResponse`.

```
exportfunctionerrorHandler(error: unknown){if(error ==null){return'unknown error';}if(typeof error ==='string'){return error;}if(error instanceofError){return error.message;}returnJSON.stringify(error);}
```

```
const result =streamText({// ...});return result.toDataStreamResponse({  getErrorMessage: errorHandler,});
```

In case you are using `createDataStreamResponse`, you can use the `onError` function when calling `toDataStreamResponse`:

```
const response =createDataStreamResponse({// ...asyncexecute(dataStream){// ...},  onError: errorHandler,});
```
```

### 281. `docs/troubleshooting/use-chat-failed-to-parse-stream.md`

```markdown
# useChat "Failed to Parse Stream String" Error


---
url: https://ai-sdk.dev/docs/troubleshooting/use-chat-failed-to-parse-stream
description: Troubleshooting errors related to the Use Chat Failed to Parse Stream error.
---


# [`useChat` "Failed to Parse Stream String" Error](#usechat-failed-to-parse-stream-string-error)



## [Issue](#issue)


I am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat) or [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and I am getting a `"Failed to parse stream string. Invalid code"` error. I am using version `3.0.20` or newer of the AI SDK.


## [Background](#background)


The AI SDK has switched to the stream data protocol in version `3.0.20`. [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) expect stream parts that support data, tool calls, etc. What you see is a failure to parse the stream. This can be caused by using an older version of the AI SDK in the backend, by providing a text stream using a custom provider, or by using a raw LangChain stream result.


## [Solution](#solution)


You can switch [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) to raw text stream processing with the [`streamProtocol`](/docs/reference/ai-sdk-ui/use-completion#stream-protocol) parameter. Set it to `text` as follows:

```
const{ messages, append }=useChat({ streamProtocol:'text'});
```
```

### 282. `docs/troubleshooting/use-chat-tools-no-response.md`

```markdown
# useChat No Response with maxSteps


---
url: https://ai-sdk.dev/docs/troubleshooting/use-chat-tools-no-response
description: Troubleshooting errors related to the Use Chat Failed to Parse Stream error.
---


# [`useChat` No Response with maxSteps](#usechat-no-response-with-maxsteps)



## [Issue](#issue)


I am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat) with [`maxSteps`](/docs/reference/ai-sdk-ui/use-chat#max-steps). When I log the incoming messages on the server, I can see the tool call and the tool result, but the model does not respond with anything.


## [Background](#background)


The `useChat` hook uses a message structure (`Message`) that pre-dates the AI SDK Core message structure (`CoreMessage`).


## [Solution](#solution)


This solution is outdated. The AI SDK now automatically converts the incoming messages to the `CoreMessage` format.

To resolve this issue, convert the incoming messages to the `CoreMessage` format using the [`convertToCoreMessages`](/docs/reference/ai-sdk-ui/convert-to-core-messages) function.

```
import{ openai }from'@ai-sdk/openai';import{ convertToCoreMessages, streamText }from'ai';exportasyncfunctionPOST(req: Request){const{ messages }=await req.json();const result =streamText({    model:openai('gpt-4o'),    messages:convertToCoreMessages(messages),});return result.toDataStreamResponse();}
```
```

### 283. `docs/troubleshooting.md`

```markdown
# Troubleshooting


---
url: https://ai-sdk.dev/docs/troubleshooting
description: Troubleshooting information for common issues encountered with the AI SDK.
---


# [Troubleshooting](#troubleshooting)


This section is designed to help you quickly identify and resolve common issues encountered with the AI SDK, ensuring a smoother and more efficient development experience.

Report Issues

Found a bug? We'd love to hear about it in our GitHub issues.

[

Open GitHub Issue

](https://github.com/vercel/ai/issues/new?assignees=&labels=&projects=&template=1.bug_report.yml)

Feature Requests

Want to suggest a new feature? Share it with us and the community.

[

Request Feature

](https://github.com/vercel/ai/issues/new?assignees=&labels=&projects=&template=2.feature_request.yml)

Ask the Community

Join our GitHub discussions to browse for help and best practices.

[

Ask a question

](https://github.com/vercel/ai/discussions)

Migration Guides

Check out our migration guides to help you upgrade to the latest version.

[

Migration Guides

](/docs/migration-guides)
```

### 284. `getting-started.md`

```markdown
# Getting Started with the AI SDK


---
url: https://ai-sdk.dev/getting-started
description: The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more.
---

[

AI SDK

](/)

Announcing AI SDK 5 Alpha!

[Learn more](https://ai-sdk.dev/docs/announcing-ai-sdk-5-alpha)

Menu

[AI SDK by Vercel](/docs/introduction)

[AI SDK 5 Alpha](/docs/announcing-ai-sdk-5-alpha)

[Foundations](/docs/foundations)

[Overview](/docs/foundations/overview)

[Providers and Models](/docs/foundations/providers-and-models)

[Prompts](/docs/foundations/prompts)

[Tools](/docs/foundations/tools)

[Streaming](/docs/foundations/streaming)

[Agents](/docs/foundations/agents)

[Getting Started](/docs/getting-started)

[Navigating the Library](/docs/getting-started/navigating-the-library)

[Next.js App Router](/docs/getting-started/nextjs-app-router)

[Next.js Pages Router](/docs/getting-started/nextjs-pages-router)

[Svelte](/docs/getting-started/svelte)

[Vue.js (Nuxt)](/docs/getting-started/nuxt)

[Node.js](/docs/getting-started/nodejs)

[Expo](/docs/getting-started/expo)

[Guides](/docs/guides)

[RAG Chatbot](/docs/guides/rag-chatbot)

[Multi-Modal Chatbot](/docs/guides/multi-modal-chatbot)

[Slackbot Guide](/docs/guides/slackbot)

[Natural Language Postgres](/docs/guides/natural-language-postgres)

[Get started with Computer Use](/docs/guides/computer-use)

[Get started with Claude 4](/docs/guides/claude-4)

[OpenAI Responses API](/docs/guides/openai-responses)

[Get started with Claude 3.7 Sonnet](/docs/guides/sonnet-3-7)

[Get started with Llama 3.1](/docs/guides/llama-3_1)

[Get started with OpenAI GPT-4.5](/docs/guides/gpt-4-5)

[Get started with OpenAI o1](/docs/guides/o1)

[Get started with OpenAI o3-mini](/docs/guides/o3)

[Get started with DeepSeek R1](/docs/guides/r1)

[AI SDK Core](/docs/ai-sdk-core)

[Overview](/docs/ai-sdk-core/overview)

[Generating Text](/docs/ai-sdk-core/generating-text)

[Generating Structured Data](/docs/ai-sdk-core/generating-structured-data)

[Tool Calling](/docs/ai-sdk-core/tools-and-tool-calling)

[Prompt Engineering](/docs/ai-sdk-core/prompt-engineering)

[Settings](/docs/ai-sdk-core/settings)

[Embeddings](/docs/ai-sdk-core/embeddings)

[Image Generation](/docs/ai-sdk-core/image-generation)

[Transcription](/docs/ai-sdk-core/transcription)

[Speech](/docs/ai-sdk-core/speech)

[Language Model Middleware](/docs/ai-sdk-core/middleware)

[Provider & Model Management](/docs/ai-sdk-core/provider-management)

[Error Handling](/docs/ai-sdk-core/error-handling)

[Testing](/docs/ai-sdk-core/testing)

[Telemetry](/docs/ai-sdk-core/telemetry)

[AI SDK UI](/docs/ai-sdk-ui)

[Overview](/docs/ai-sdk-ui/overview)

[Chatbot](/docs/ai-sdk-ui/chatbot)

[Chatbot Message Persistence](/docs/ai-sdk-ui/chatbot-message-persistence)

[Chatbot Tool Usage](/docs/ai-sdk-ui/chatbot-tool-usage)

[Generative User Interfaces](/docs/ai-sdk-ui/generative-user-interfaces)

[Completion](/docs/ai-sdk-ui/completion)

[Object Generation](/docs/ai-sdk-ui/object-generation)

[OpenAI Assistants](/docs/ai-sdk-ui/openai-assistants)

[Streaming Custom Data](/docs/ai-sdk-ui/streaming-data)

[Error Handling](/docs/ai-sdk-ui/error-handling)

[Smooth streaming japanese text](/docs/ai-sdk-ui/smooth-stream-japanese)

[Smooth streaming chinese text](/docs/ai-sdk-ui/smooth-stream-chinese)

[Stream Protocols](/docs/ai-sdk-ui/stream-protocol)

[AI SDK RSC](/docs/ai-sdk-rsc)

[Advanced](/docs/advanced)

[Reference](/docs/reference)

[AI SDK Core](/docs/reference/ai-sdk-core)

[AI SDK UI](/docs/reference/ai-sdk-ui)

[AI SDK RSC](/docs/reference/ai-sdk-rsc)

[Stream Helpers](/docs/reference/stream-helpers)

[AI SDK Errors](/docs/reference/ai-sdk-errors)

[Migration Guides](/docs/migration-guides)

[Troubleshooting](/docs/troubleshooting)


# Getting Started with the AI SDK


Get started by installing the AI SDK:

npm i ai


## Where to next?


[

I'm new to AI

New to AI? Start here to learn core concepts before diving into the AI SDK.

](/docs/foundations/overview)[

Try the AI SDK

Discover what the AI SDK can do with quick start guides for your preferred framework.

](/docs/getting-started)[

Show Me Examples

Find code examples for common AI tasks to jumpstart your project.

](/examples)[

I Need Help

Stuck on a problem? Find solutions to common issues and get support here.

](/docs/troubleshooting)

Elevate your AI applications with Vercel.

Trusted by OpenAI, Replicate, Suno, Pinecone, and more.

Vercel provides tools and infrastructure to deploy AI apps and features at scale.

[Talk to an expert](https://vercel.com/contact/sales?utm_source=ai_sdk&utm_medium=web&utm_campaign=contact_sales_cta&utm_content=talk_to_an_expert_sdk_docs)
```

### 285. `index.md`

```markdown
# AI SDK


---
url: https://ai-sdk.dev/
description: The AI Toolkit for TypeScript, from the creators of Next.js.
---

[

AI SDK

](/)

Announcing AI SDK 5 Alpha!

[Learn more](https://ai-sdk.dev/docs/announcing-ai-sdk-5-alpha)

The AI Toolkit for TypeScript

From the creators of Next.js, the AI SDK is a free open-source library that gives you the tools you need to build AI-powered products.

[Get Started](/getting-started)

npm i ai

[Visit Playground](/playground)

npm i ai

[Get Started](/getting-started)[Visit Playground](/playground)


### Trusted by builders at


[

OpenAI

Claude

Hugging Face

The next big thing

Unified Provider API

Switch between AI providers by changing a single line of code.

](/docs/foundations/providers-and-models)[

Make a music player

Blowin’ in the Wind

Bob Dylan

Generative UI

Create dynamic, AI-powered user interfaces that amaze your users.

](/docs/ai-sdk-ui/chatbot-with-tool-calling)[

Framework-agnostic

Build with React, Next, Vue, Nuxt, SvelteKit, and more.

](/docs/getting-started)[

Streaming AI Responses

Don't let your users wait for AI responses. Send them instantly.

](/docs/advanced/why-streaming)

What builders say about the AI SDK

[

Sully

@SullyOmarr

the @aisdk is probably the best way to build an ai app right now

you can go from idea -> working ai app in 15 mins.

its made working with llms 10x more enjoyable ( and we ship faster)

](https://x.com/SullyOmarr/status/1885049394342310335)[

Max Baines

@maxbaines

Hands down the @aisdk is by far the best SDK I have worked with, thats pretty much all things since jQuery.

](https://x.com/maxbaines/status/1875207718618935302)[

Micky

@rasmickyy

vercel ai sdk is just sooo good it hurts man...

can literally build ai features within any of my apps in mins

](https://x.com/rasmickyy/status/1855398758554439824)[

morgan

@morganlinton

The AI SDK is 🔥🔥🔥

](https://x.com/morganlinton/status/1884257150761066894)[

Tom Watkins

@TomWatkins1994

the AI SDK is S tier software

](https://x.com/TomWatkins1994/status/1829200829150339230)

[

EGOIST

@localhost\_5173

Vercel AI SDK is so good, there's no reason to directly use npm/openai or npm/claude anymore

](https://x.com/localhost_5173/status/1794004340375802108)[

sunil "yeah no" pai

@threepointone

vercel ai sdk is very good actually

](https://x.com/threepointone/status/1818719331276276189)

[

Matt Pocock

@mattpocockuk

Vercel's AI SDK is one of the first tools I reach for when I'm building an AI-powered feature in TypeScript.

](https://x.com/mattpocockuk/status/1874822204824731717)[

Kyle Mistele 🏴‍☠️

@0xblacklight

Vercel's @aisdk is insanely good. Docs are fantastic. Great abstractions where you want them, doesn't force unnecessary ones, and lets you get under the hood where appropriate. Solves the hard stuff (stream parsing, tool streaming, multi-turn tool execution, error handling and healing/recovery) without forcing you into dumb patterns

It just works, it's fantastic software and delightful to use. The team ships insanely fast, and has turned PRs from me around in like 2 days, and frequently ships requested features in < 1w

](https://x.com/0xblacklight/status/1866886257055342787)[

Ryan Carson

@ryancarson

I love @vercel and their @aisdk - so freaking easy to deploy.

](https://x.com/ryancarson/status/1877170074538180811)[

Olivier

@StonkyOli

Big fan of the AI SDK

It has blown away my expectations since i started using it, way better than raw dogging oai

](https://x.com/StonkyOli/status/1858922730181324938)

[

Pontus Abrahamsson

@pontusab

With the ai sdk available i'm always thinking: "How can I make this process as automatic as possible for the user?" because the barrier to implementing it is just a matter of minutes.

](https://x.com/pontusab/status/1824398511099510791)[

ben

@benhylak

@aisdk has made it possible to just call "generateObject" across any model provider, and it returns a properly typed json object.

it's pure magic.

](https://x.com/benhylak/status/1866931335291629995)

[

EGOIST

@localhost\_5173

Vercel AI SDK is so good, there's no reason to directly use npm/openai or npm/claude anymore

](https://x.com/localhost_5173/status/1794004340375802108)[

sunil "yeah no" pai

@threepointone

vercel ai sdk is very good actually

](https://x.com/threepointone/status/1818719331276276189)

[

Pontus Abrahamsson

@pontusab

With the ai sdk available i'm always thinking: "How can I make this process as automatic as possible for the user?" because the barrier to implementing it is just a matter of minutes.

](https://x.com/pontusab/status/1824398511099510791)[

ben

@benhylak

@aisdk has made it possible to just call "generateObject" across any model provider, and it returns a properly typed json object.

it's pure magic.

](https://x.com/benhylak/status/1866931335291629995)[

sami

@svmisvhn

huge shoutout to @vercel for the ai sdk, one API for all LLMs is 🤯

](https://x.com/svmisvhn/status/1808950039676342339)

FAQs


### Is the AI SDK free to use?


Yes, the AI SDK is free and open source.


### How do I get started?


Visit our [getting started page](/getting-started) to learn how to install the AI SDK with your preferred framework.


### How can I contribute to the project?


We welcome contributions from the community! You can contribute by submitting bug reports, feature requests, or pull requests on our [GitHub repository](https://github.com/vercel/ai).
```

### 286. `playground/anthropic_claude-3.5-haiku.md`

```markdown
# Claude 3.5 Haiku by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-3.5-haiku
description: Test and compare Claude 3.5 Haiku by Anthropic
---
```

### 287. `playground/anthropic_claude-3.7-sonnet-reasoning.md`

```markdown
# Claude 3.7 Sonnet Reasoning by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-3.7-sonnet-reasoning
description: Test and compare Claude 3.7 Sonnet Reasoning by Anthropic
---
```

### 288. `playground/anthropic_claude-3.7-sonnet.md`

```markdown
# Claude 3.7 Sonnet by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-3.7-sonnet
description: Test and compare Claude 3.7 Sonnet by Anthropic
---
```

### 289. `playground/anthropic_claude-4-opus-20250514.md`

```markdown
# Claude 4 Opus by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-4-opus-20250514
description: Test and compare Claude 4 Opus by Anthropic
---
```

### 290. `playground/anthropic_claude-4-sonnet-20250514.md`

```markdown
# Claude 4 Sonnet by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-4-sonnet-20250514
description: Test and compare Claude 4 Sonnet by Anthropic
---
```

### 291. `playground/anthropic_claude-v2.md`

```markdown
# Claude 2 by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-v2
description: Test and compare Claude 2 by Anthropic
---
```

### 292. `playground/anthropic_claude-v3-haiku.md`

```markdown
# Claude 3 Haiku by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-v3-haiku
description: Test and compare Claude 3 Haiku by Anthropic
---
```

### 293. `playground/anthropic_claude-v3-opus.md`

```markdown
# Claude 3 Opus by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-v3-opus
description: Test and compare Claude 3 Opus by Anthropic
---
```

### 294. `playground/anthropic_claude-v3-sonnet.md`

```markdown
# Claude 3 Sonnet by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-v3-sonnet
description: Test and compare Claude 3 Sonnet by Anthropic
---
```

### 295. `playground/anthropic_claude-v3.5-sonnet.md`

```markdown
# Claude 3.5 Sonnet by Anthropic on the AI Playground


---
url: https://ai-sdk.dev/playground/anthropic:claude-v3.5-sonnet
description: Test and compare Claude 3.5 Sonnet by Anthropic
---
```

### 296. `playground/bedrock_amazon.nova-lite-v1_0.md`

```markdown
# Nova Lite by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:amazon.nova-lite-v1:0
description: Test and compare Nova Lite by Amazon
---
```

### 297. `playground/bedrock_amazon.nova-micro-v1_0.md`

```markdown
# Nova Micro by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:amazon.nova-micro-v1:0
description: Test and compare Nova Micro by Amazon
---
```

### 298. `playground/bedrock_amazon.nova-pro-v1_0.md`

```markdown
# Nova Pro by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:amazon.nova-pro-v1:0
description: Test and compare Nova Pro by Amazon
---
```

### 299. `playground/bedrock_claude-3-5-haiku-20241022.md`

```markdown
# Claude 3.5 Haiku (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:claude-3-5-haiku-20241022
description: Test and compare Claude 3.5 Haiku (Bedrock) by Amazon
---
```

### 300. `playground/bedrock_claude-3-5-sonnet-20240620-v1.md`

```markdown
# Claude 3.5 Sonnet (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:claude-3-5-sonnet-20240620-v1
description: Test and compare Claude 3.5 Sonnet (Bedrock) by Amazon
---
```

### 301. `playground/bedrock_claude-3-5-sonnet-20241022-v2.md`

```markdown
# Claude 3.5 Sonnet v2 (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:claude-3-5-sonnet-20241022-v2
description: Test and compare Claude 3.5 Sonnet v2 (Bedrock) by Amazon
---
```

### 302. `playground/bedrock_claude-3-7-sonnet-20250219.md`

```markdown
# Claude 3.7 Sonnet (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:claude-3-7-sonnet-20250219
description: Test and compare Claude 3.7 Sonnet (Bedrock) by Amazon
---
```

### 303. `playground/bedrock_claude-3-haiku-20240307-v1.md`

```markdown
# Claude 3 Haiku (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:claude-3-haiku-20240307-v1
description: Test and compare Claude 3 Haiku (Bedrock) by Amazon
---
```

### 304. `playground/bedrock_claude-4-opus-20250514-v1.md`

```markdown
# Claude 4 Opus (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:claude-4-opus-20250514-v1
description: Test and compare Claude 4 Opus (Bedrock) by Amazon
---
```

### 305. `playground/bedrock_claude-4-sonnet-20250514-v1.md`

```markdown
# Claude 4 Sonnet (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:claude-4-sonnet-20250514-v1
description: Test and compare Claude 4 Sonnet (Bedrock) by Amazon
---
```

### 306. `playground/bedrock_deepseek.r1-v1.md`

```markdown
# DeepSeek-R1 (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:deepseek.r1-v1
description: Test and compare DeepSeek-R1 (Bedrock) by Amazon
---
```

### 307. `playground/bedrock_meta.llama3-1-70b-instruct-v1.md`

```markdown
# Llama 3.1 70B Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama3-1-70b-instruct-v1
description: Test and compare Llama 3.1 70B Instruct (Bedrock) by Amazon
---
```

### 308. `playground/bedrock_meta.llama3-1-8b-instruct-v1.md`

```markdown
# Llama 3.1 8B Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama3-1-8b-instruct-v1
description: Test and compare Llama 3.1 8B Instruct (Bedrock) by Amazon
---
```

### 309. `playground/bedrock_meta.llama3-2-11b-instruct-v1.md`

```markdown
# Llama 3.2 11B Vision Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama3-2-11b-instruct-v1
description: Test and compare Llama 3.2 11B Vision Instruct (Bedrock) by Amazon
---
```

### 310. `playground/bedrock_meta.llama3-2-1b-instruct-v1.md`

```markdown
# Llama 3.2 1B Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama3-2-1b-instruct-v1
description: Test and compare Llama 3.2 1B Instruct (Bedrock) by Amazon
---
```

### 311. `playground/bedrock_meta.llama3-2-3b-instruct-v1.md`

```markdown
# Llama 3.2 3B Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama3-2-3b-instruct-v1
description: Test and compare Llama 3.2 3B Instruct (Bedrock) by Amazon
---
```

### 312. `playground/bedrock_meta.llama3-2-90b-instruct-v1.md`

```markdown
# Llama 3.2 90B Vision Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama3-2-90b-instruct-v1
description: Test and compare Llama 3.2 90B Vision Instruct (Bedrock) by Amazon
---
```

### 313. `playground/bedrock_meta.llama3-3-70b-instruct-v1.md`

```markdown
# Llama 3.3 70B Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama3-3-70b-instruct-v1
description: Test and compare Llama 3.3 70B Instruct (Bedrock) by Amazon
---
```

### 314. `playground/bedrock_meta.llama4-maverick-17b-instruct-v1.md`

```markdown
# Llama 4 Maverick 17B Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama4-maverick-17b-instruct-v1
description: Test and compare Llama 4 Maverick 17B Instruct (Bedrock) by Amazon
---
```

### 315. `playground/bedrock_meta.llama4-scout-17b-instruct-v1.md`

```markdown
# Llama 4 Scout 17B Instruct (Bedrock) by Amazon on the AI Playground


---
url: https://ai-sdk.dev/playground/bedrock:meta.llama4-scout-17b-instruct-v1
description: Test and compare Llama 4 Scout 17B Instruct (Bedrock) by Amazon
---
```

### 316. `playground/cerebras_llama-3.3-70b.md`

```markdown
# Llama 3.3 70B by Cerebras on the AI Playground


---
url: https://ai-sdk.dev/playground/cerebras:llama-3.3-70b
description: Test and compare Llama 3.3 70B by Cerebras
---
```

### 317. `playground/cerebras_llama-4-scout-17b-16e-instruct.md`

```markdown
# Llama 4 Scout by Cerebras on the AI Playground


---
url: https://ai-sdk.dev/playground/cerebras:llama-4-scout-17b-16e-instruct
description: Test and compare Llama 4 Scout by Cerebras
---
```

### 318. `playground/cerebras_llama3.1-8b.md`

```markdown
# Llama 3.1 8B by Cerebras on the AI Playground


---
url: https://ai-sdk.dev/playground/cerebras:llama3.1-8b
description: Test and compare Llama 3.1 8B by Cerebras
---
```

### 319. `playground/cerebras_qwen-3-32b.md`

```markdown
# Qwen 3.32B by Cerebras on the AI Playground


---
url: https://ai-sdk.dev/playground/cerebras:qwen-3-32b
description: Test and compare Qwen 3.32B by Cerebras
---
```

### 320. `playground/cohere_command-a.md`

```markdown
# Command A by Cohere on the AI Playground


---
url: https://ai-sdk.dev/playground/cohere:command-a
description: Test and compare Command A by Cohere
---
```

### 321. `playground/cohere_command-light-nightly.md`

```markdown
# Command Light Nightly by Cohere on the AI Playground


---
url: https://ai-sdk.dev/playground/cohere:command-light-nightly
description: Test and compare Command Light Nightly by Cohere
---
```

### 322. `playground/cohere_command-nightly.md`

```markdown
# Command Nightly by Cohere on the AI Playground


---
url: https://ai-sdk.dev/playground/cohere:command-nightly
description: Test and compare Command Nightly by Cohere
---
```

### 323. `playground/cohere_command-r-plus.md`

```markdown
# Command R+ by Cohere on the AI Playground


---
url: https://ai-sdk.dev/playground/cohere:command-r-plus
description: Test and compare Command R+ by Cohere
---
```

### 324. `playground/cohere_command-r.md`

```markdown
# Command R by Cohere on the AI Playground


---
url: https://ai-sdk.dev/playground/cohere:command-r
description: Test and compare Command R by Cohere
---
```

### 325. `playground/deepinfra_llama-4-maverick-17b-128e-instruct-fp8.md`

```markdown
# Llama 4 Maverick 17B 128E Instruct FP8 by DeepInfra on the AI Playground


---
url: https://ai-sdk.dev/playground/deepinfra:llama-4-maverick-17b-128e-instruct-fp8
description: Test and compare Llama 4 Maverick 17B 128E Instruct FP8 by DeepInfra
---
```

### 326. `playground/deepinfra_llama-4-scout-17b-16e-instruct.md`

```markdown
# Llama 4 Scout 17B 16E Instruct by DeepInfra on the AI Playground


---
url: https://ai-sdk.dev/playground/deepinfra:llama-4-scout-17b-16e-instruct
description: Test and compare Llama 4 Scout 17B 16E Instruct by DeepInfra
---
```

### 327. `playground/deepinfra_qwen3-14b.md`

```markdown
# Qwen3-14B by DeepInfra on the AI Playground


---
url: https://ai-sdk.dev/playground/deepinfra:qwen3-14b
description: Test and compare Qwen3-14B by DeepInfra
---
```

### 328. `playground/deepinfra_qwen3-235b-a22b.md`

```markdown
# Qwen3-235B-A22B by DeepInfra on the AI Playground


---
url: https://ai-sdk.dev/playground/deepinfra:qwen3-235b-a22b
description: Test and compare Qwen3-235B-A22B by DeepInfra
---
```

### 329. `playground/deepinfra_qwen3-30b-a3b.md`

```markdown
# Qwen3-30B-A3B by DeepInfra on the AI Playground


---
url: https://ai-sdk.dev/playground/deepinfra:qwen3-30b-a3b
description: Test and compare Qwen3-30B-A3B by DeepInfra
---
```

### 330. `playground/deepinfra_qwen3-32b.md`

```markdown
# Qwen3-32B by DeepInfra on the AI Playground


---
url: https://ai-sdk.dev/playground/deepinfra:qwen3-32b
description: Test and compare Qwen3-32B by DeepInfra
---
```

### 331. `playground/deepseek_chat.md`

```markdown
# DeepSeek-V3 by DeepSeek on the AI Playground


---
url: https://ai-sdk.dev/playground/deepseek:chat
description: Test and compare DeepSeek-V3 by DeepSeek
---
```

### 332. `playground/deepseek_deepseek-r1-0528.md`

```markdown
# DeepSeek R1 0528 by DeepSeek on the AI Playground


---
url: https://ai-sdk.dev/playground/deepseek:deepseek-r1-0528
description: Test and compare DeepSeek R1 0528 by DeepSeek
---
```

### 333. `playground/deepseek_deepseek-r1.md`

```markdown
# DeepSeek R1 by DeepSeek on the AI Playground


---
url: https://ai-sdk.dev/playground/deepseek:deepseek-r1
description: Test and compare DeepSeek R1 by DeepSeek
---
```

### 334. `playground/fireworks_deepseek-r1.md`

```markdown
# DeepSeek R1 by Fireworks on the AI Playground


---
url: https://ai-sdk.dev/playground/fireworks:deepseek-r1
description: Test and compare DeepSeek R1 by Fireworks
---
```

### 335. `playground/fireworks_deepseek-v3.md`

```markdown
# DeepSeek-V3 by Fireworks on the AI Playground


---
url: https://ai-sdk.dev/playground/fireworks:deepseek-v3
description: Test and compare DeepSeek-V3 by Fireworks
---
```

### 336. `playground/fireworks_firefunction-v1.md`

```markdown
# FireFunction V1 by Fireworks on the AI Playground


---
url: https://ai-sdk.dev/playground/fireworks:firefunction-v1
description: Test and compare FireFunction V1 by Fireworks
---
```

### 337. `playground/fireworks_mixtral-8x22b-instruct.md`

```markdown
# Mixtral MoE 8x22B Instruct by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/fireworks:mixtral-8x22b-instruct
description: Test and compare Mixtral MoE 8x22B Instruct by Mistral
---
```

### 338. `playground/fireworks_mixtral-8x7b-instruct.md`

```markdown
# Mixtral MoE 8x7B Instruct by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/fireworks:mixtral-8x7b-instruct
description: Test and compare Mixtral MoE 8x7B Instruct by Mistral
---
```

### 339. `playground/fireworks_qwen3-235b-a22b.md`

```markdown
# Qwen3-235B-A22B by Fireworks on the AI Playground


---
url: https://ai-sdk.dev/playground/fireworks:qwen3-235b-a22b
description: Test and compare Qwen3-235B-A22B by Fireworks
---
```

### 340. `playground/fireworks_qwq-32b.md`

```markdown
# QwQ-32B by Fireworks on the AI Playground


---
url: https://ai-sdk.dev/playground/fireworks:qwq-32b
description: Test and compare QwQ-32B by Fireworks
---
```

### 341. `playground/google_gemini-1.5-flash-002.md`

```markdown
# Gemini 1.5 Flash 002 by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-1.5-flash-002
description: Test and compare Gemini 1.5 Flash 002 by Google
---
```

### 342. `playground/google_gemini-1.5-flash-8b.md`

```markdown
# Gemini 1.5 Flash 8b by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-1.5-flash-8b
description: Test and compare Gemini 1.5 Flash 8b by Google
---
```

### 343. `playground/google_gemini-1.5-flash.md`

```markdown
# Gemini 1.5 Flash 001 by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-1.5-flash
description: Test and compare Gemini 1.5 Flash 001 by Google
---
```

### 344. `playground/google_gemini-1.5-pro-002.md`

```markdown
# Gemini 1.5 Pro 002 by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-1.5-pro-002
description: Test and compare Gemini 1.5 Pro 002 by Google
---
```

### 345. `playground/google_gemini-1.5-pro.md`

```markdown
# Gemini 1.5 Pro 001 by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-1.5-pro
description: Test and compare Gemini 1.5 Pro 001 by Google
---
```

### 346. `playground/google_gemini-2.0-flash-001.md`

```markdown
# Gemini 2.0 Flash by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-2.0-flash-001
description: Test and compare Gemini 2.0 Flash by Google
---
```

### 347. `playground/google_gemini-2.0-flash-lite-preview-02-05.md`

```markdown
# Gemini 2.0 Flash Lite Preview by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-2.0-flash-lite-preview-02-05
description: Test and compare Gemini 2.0 Flash Lite Preview by Google
---
```

### 348. `playground/google_gemini-2.5-flash-preview-04-17.md`

```markdown
# Gemini 2.5 Flash Preview by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-2.5-flash-preview-04-17
description: Test and compare Gemini 2.5 Flash Preview by Google
---
```

### 349. `playground/google_gemini-2.5-pro-preview-03-25.md`

```markdown
# Gemini 2.5 Pro Preview by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemini-2.5-pro-preview-03-25
description: Test and compare Gemini 2.5 Pro Preview by Google
---
```

### 350. `playground/google_gemma-3-27b-it.md`

```markdown
# Gemma 3 27B by Google on the AI Playground


---
url: https://ai-sdk.dev/playground/google:gemma-3-27b-it
description: Test and compare Gemma 3 27B by Google
---
```

### 351. `playground/groq_gemma2-9b-it.md`

```markdown
# Gemma 2 9B IT by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:gemma2-9b-it
description: Test and compare Gemma 2 9B IT by Groq
---
```

### 352. `playground/groq_llama-3-70b-instruct.md`

```markdown
# Llama 3 70B Instruct by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-3-70b-instruct
description: Test and compare Llama 3 70B Instruct by Groq
---
```

### 353. `playground/groq_llama-3-8b-instruct.md`

```markdown
# Llama 3 8B Instruct by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-3-8b-instruct
description: Test and compare Llama 3 8B Instruct by Groq
---
```

### 354. `playground/groq_llama-3.1-8b.md`

```markdown
# Llama 3.1 8B Instant by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-3.1-8b
description: Test and compare Llama 3.1 8B Instant by Groq
---
```

### 355. `playground/groq_llama-3.2-11b-vision-preview.md`

```markdown
# Llama 3.2 11B by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-3.2-11b-vision-preview
description: Test and compare Llama 3.2 11B by Groq
---
```

### 356. `playground/groq_llama-3.2-1b.md`

```markdown
# Llama 3.2 1B by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-3.2-1b
description: Test and compare Llama 3.2 1B by Groq
---
```

### 357. `playground/groq_llama-3.2-3b.md`

```markdown
# Llama 3.2 3B by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-3.2-3b
description: Test and compare Llama 3.2 3B by Groq
---
```

### 358. `playground/groq_llama-3.2-90b-vision-preview.md`

```markdown
# Llama 3.2 90B by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-3.2-90b-vision-preview
description: Test and compare Llama 3.2 90B by Groq
---
```

### 359. `playground/groq_llama-3.3-70b-versatile.md`

```markdown
# Llama 3.3 70B Versatile by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-3.3-70b-versatile
description: Test and compare Llama 3.3 70B Versatile by Groq
---
```

### 360. `playground/groq_llama-4-scout-17b-16e-instruct.md`

```markdown
# Llama 4 Scout 17B 16E Instruct by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:llama-4-scout-17b-16e-instruct
description: Test and compare Llama 4 Scout 17B 16E Instruct by Groq
---
```

### 361. `playground/groq_mistral-saba-24b.md`

```markdown
# Mistral Saba 24B by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:mistral-saba-24b
description: Test and compare Mistral Saba 24B by Groq
---
```

### 362. `playground/groq_qwen-qwq-32b.md`

```markdown
# QWQ-32B by Groq on the AI Playground


---
url: https://ai-sdk.dev/playground/groq:qwen-qwq-32b
description: Test and compare QWQ-32B by Groq
---
```

### 363. `playground/inception_mercury-coder-small.md`

```markdown
# Mercury Coder Small Beta by Inception on the AI Playground


---
url: https://ai-sdk.dev/playground/inception:mercury-coder-small
description: Test and compare Mercury Coder Small Beta by Inception
---
```

### 364. `playground/mistral_codestral-2501.md`

```markdown
# Mistral Codestral 25.01 by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/mistral:codestral-2501
description: Test and compare Mistral Codestral 25.01 by Mistral
---
```

### 365. `playground/mistral_ministral-3b-latest.md`

```markdown
# Ministral 3B by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/mistral:ministral-3b-latest
description: Test and compare Ministral 3B by Mistral
---
```

### 366. `playground/mistral_ministral-8b-latest.md`

```markdown
# Ministral 8B by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/mistral:ministral-8b-latest
description: Test and compare Ministral 8B by Mistral
---
```

### 367. `playground/mistral_mistral-large.md`

```markdown
# Mistral Large by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/mistral:mistral-large
description: Test and compare Mistral Large by Mistral
---
```

### 368. `playground/mistral_mistral-small-2503.md`

```markdown
# Mistral Small 2503 by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/mistral:mistral-small-2503
description: Test and compare Mistral Small 2503 by Mistral
---
```

### 369. `playground/mistral_mistral-small.md`

```markdown
# Mistral Small by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/mistral:mistral-small
description: Test and compare Mistral Small by Mistral
---
```

### 370. `playground/mistral_pixtral-12b-2409.md`

```markdown
# Pixtral 12B 2409 by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/mistral:pixtral-12b-2409
description: Test and compare Pixtral 12B 2409 by Mistral
---
```

### 371. `playground/mistral_pixtral-large-latest.md`

```markdown
# Pixtral Large by Mistral on the AI Playground


---
url: https://ai-sdk.dev/playground/mistral:pixtral-large-latest
description: Test and compare Pixtral Large by Mistral
---
```

### 372. `playground/openai_gpt-3.5-turbo-instruct.md`

```markdown
# GPT-3.5 Turbo Instruct by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-3.5-turbo-instruct
description: Test and compare GPT-3.5 Turbo Instruct by OpenAI
---
```

### 373. `playground/openai_gpt-3.5-turbo.md`

```markdown
# GPT-3.5 Turbo by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-3.5-turbo
description: Test and compare GPT-3.5 Turbo by OpenAI
---
```

### 374. `playground/openai_gpt-4-turbo.md`

```markdown
# GPT-4 Turbo by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-4-turbo
description: Test and compare GPT-4 Turbo by OpenAI
---
```

### 375. `playground/openai_gpt-4.1-mini.md`

```markdown
# GPT-4.1 mini by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-4.1-mini
description: Test and compare GPT-4.1 mini by OpenAI
---
```

### 376. `playground/openai_gpt-4.1-nano.md`

```markdown
# GPT-4.1 nano by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-4.1-nano
description: Test and compare GPT-4.1 nano by OpenAI
---
```

### 377. `playground/openai_gpt-4.1.md`

```markdown
# GPT-4.1 by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-4.1
description: Test and compare GPT-4.1 by OpenAI
---
```

### 378. `playground/openai_gpt-4.5-preview.md`

```markdown
# GPT-4.5 Preview by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-4.5-preview
description: Test and compare GPT-4.5 Preview by OpenAI
---
```

### 379. `playground/openai_gpt-4o-mini.md`

```markdown
# GPT-4o mini by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-4o-mini
description: Test and compare GPT-4o mini by OpenAI
---
```

### 380. `playground/openai_gpt-4o.md`

```markdown
# GPT-4o by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:gpt-4o
description: Test and compare GPT-4o by OpenAI
---
```

### 381. `playground/openai_o3-mini-high.md`

```markdown
# o3-mini (High) by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:o3-mini-high
description: Test and compare o3-mini (High) by OpenAI
---
```

### 382. `playground/openai_o3-mini-low.md`

```markdown
# o3-mini (Low) by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:o3-mini-low
description: Test and compare o3-mini (Low) by OpenAI
---
```

### 383. `playground/openai_o3-mini-medium.md`

```markdown
# o3-mini (Medium) by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:o3-mini-medium
description: Test and compare o3-mini (Medium) by OpenAI
---
```

### 384. `playground/openai_o3-mini.md`

```markdown
# o3-mini by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:o3-mini
description: Test and compare o3-mini by OpenAI
---
```

### 385. `playground/openai_o3.md`

```markdown
# o3 by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:o3
description: Test and compare o3 by OpenAI
---
```

### 386. `playground/openai_o4-mini.md`

```markdown
# o4-mini by OpenAI on the AI Playground


---
url: https://ai-sdk.dev/playground/openai:o4-mini
description: Test and compare o4-mini by OpenAI
---
```

### 387. `playground/perplexity_sonar-pro.md`

```markdown
# Sonar Pro by Perplexity on the AI Playground


---
url: https://ai-sdk.dev/playground/perplexity:sonar-pro
description: Test and compare Sonar Pro by Perplexity
---
```

### 388. `playground/perplexity_sonar-reasoning-pro.md`

```markdown
# Sonar Reasoning Pro by Perplexity on the AI Playground


---
url: https://ai-sdk.dev/playground/perplexity:sonar-reasoning-pro
description: Test and compare Sonar Reasoning Pro by Perplexity
---
```

### 389. `playground/perplexity_sonar-reasoning.md`

```markdown
# Sonar Reasoning by Perplexity on the AI Playground


---
url: https://ai-sdk.dev/playground/perplexity:sonar-reasoning
description: Test and compare Sonar Reasoning by Perplexity
---
```

### 390. `playground/perplexity_sonar.md`

```markdown
# Sonar by Perplexity on the AI Playground


---
url: https://ai-sdk.dev/playground/perplexity:sonar
description: Test and compare Sonar by Perplexity
---
```

### 391. `playground/vertex_claude-3-5-haiku-20241022.md`

```markdown
# Claude 3.5 Haiku (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:claude-3-5-haiku-20241022
description: Test and compare Claude 3.5 Haiku (Vertex) by Vertex
---
```

### 392. `playground/vertex_claude-3-5-sonnet-20240620.md`

```markdown
# Claude 3.5 Sonnet (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:claude-3-5-sonnet-20240620
description: Test and compare Claude 3.5 Sonnet (Vertex) by Vertex
---
```

### 393. `playground/vertex_claude-3-5-sonnet-v2-20241022.md`

```markdown
# Claude 3.5 Sonnet v2 (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:claude-3-5-sonnet-v2-20241022
description: Test and compare Claude 3.5 Sonnet v2 (Vertex) by Vertex
---
```

### 394. `playground/vertex_claude-3-7-sonnet-20250219.md`

```markdown
# Claude 3.7 Sonnet (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:claude-3-7-sonnet-20250219
description: Test and compare Claude 3.7 Sonnet (Vertex) by Vertex
---
```

### 395. `playground/vertex_claude-3-haiku-20240307.md`

```markdown
# Claude 3 Haiku (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:claude-3-haiku-20240307
description: Test and compare Claude 3 Haiku (Vertex) by Vertex
---
```

### 396. `playground/vertex_claude-3-opus-20240229.md`

```markdown
# Claude 3 Opus (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:claude-3-opus-20240229
description: Test and compare Claude 3 Opus (Vertex) by Vertex
---
```

### 397. `playground/vertex_claude-4-opus-20250514.md`

```markdown
# Claude 4 Opus (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:claude-4-opus-20250514
description: Test and compare Claude 4 Opus (Vertex) by Vertex
---
```

### 398. `playground/vertex_claude-4-sonnet-20250514.md`

```markdown
# Claude 4 Sonnet (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:claude-4-sonnet-20250514
description: Test and compare Claude 4 Sonnet (Vertex) by Vertex
---
```

### 399. `playground/vertex_gemini-2.0-flash-001.md`

```markdown
# Gemini 2.0 Flash (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:gemini-2.0-flash-001
description: Test and compare Gemini 2.0 Flash (Vertex) by Vertex
---
```

### 400. `playground/vertex_gemini-2.0-flash-lite-001.md`

```markdown
# Gemini 2.0 Flash Lite (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:gemini-2.0-flash-lite-001
description: Test and compare Gemini 2.0 Flash Lite (Vertex) by Vertex
---
```

### 401. `playground/vertex_gemini-2.5-flash-preview-04-17.md`

```markdown
# Gemini 2.5 Flash Preview (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:gemini-2.5-flash-preview-04-17
description: Test and compare Gemini 2.5 Flash Preview (Vertex) by Vertex
---
```

### 402. `playground/vertex_gemini-2.5-pro-preview-05-06.md`

```markdown
# Gemini 2.5 Pro Preview (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:gemini-2.5-pro-preview-05-06
description: Test and compare Gemini 2.5 Pro Preview (Vertex) by Vertex
---
```

### 403. `playground/vertex_llama-3.3-70b-instruct-maas.md`

```markdown
# Llama 3.3 70B (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:llama-3.3-70b-instruct-maas
description: Test and compare Llama 3.3 70B (Vertex) by Vertex
---
```

### 404. `playground/vertex_llama-4-maverick-17b-128e-instruct-maas.md`

```markdown
# Llama 4 Maverick 17B 128E Instruct (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:llama-4-maverick-17b-128e-instruct-maas
description: Test and compare Llama 4 Maverick 17B 128E Instruct (Vertex) by Vertex
---
```

### 405. `playground/vertex_llama-4-scout-17b-16e-instruct-maas.md`

```markdown
# Llama 4 Scout 17B 16E Instruct (Vertex) by Vertex on the AI Playground


---
url: https://ai-sdk.dev/playground/vertex:llama-4-scout-17b-16e-instruct-maas
description: Test and compare Llama 4 Scout 17B 16E Instruct (Vertex) by Vertex
---
```

### 406. `playground/xai_grok-2-1212.md`

```markdown
# Grok 2 by xAI on the AI Playground


---
url: https://ai-sdk.dev/playground/xai:grok-2-1212
description: Test and compare Grok 2 by xAI
---
```

### 407. `playground/xai_grok-2-vision-1212.md`

```markdown
# Grok 2 Vision by xAI on the AI Playground


---
url: https://ai-sdk.dev/playground/xai:grok-2-vision-1212
description: Test and compare Grok 2 Vision by xAI
---
```

### 408. `playground/xai_grok-3-beta.md`

```markdown
# Grok 3 Beta by xAI on the AI Playground


---
url: https://ai-sdk.dev/playground/xai:grok-3-beta
description: Test and compare Grok 3 Beta by xAI
---
```

### 409. `playground/xai_grok-3-fast-beta.md`

```markdown
# Grok 3 Fast Beta by xAI on the AI Playground


---
url: https://ai-sdk.dev/playground/xai:grok-3-fast-beta
description: Test and compare Grok 3 Fast Beta by xAI
---
```

### 410. `playground/xai_grok-3-mini-beta.md`

```markdown
# Grok 3 Mini Beta by xAI on the AI Playground


---
url: https://ai-sdk.dev/playground/xai:grok-3-mini-beta
description: Test and compare Grok 3 Mini Beta by xAI
---
```

### 411. `playground/xai_grok-3-mini-fast-beta.md`

```markdown
# Grok 3 Mini Fast Beta by xAI on the AI Playground


---
url: https://ai-sdk.dev/playground/xai:grok-3-mini-fast-beta
description: Test and compare Grok 3 Mini Fast Beta by xAI
---
```

### 412. `playground/xai_grok-beta.md`

```markdown
# Grok Beta by xAI on the AI Playground


---
url: https://ai-sdk.dev/playground/xai:grok-beta
description: Test and compare Grok Beta by xAI
---
```

### 413. `playground/xai_grok-vision-beta.md`

```markdown
# Grok Vision Beta by xAI on the AI Playground


---
url: https://ai-sdk.dev/playground/xai:grok-vision-beta
description: Test and compare Grok Vision Beta by xAI
---
```

### 414. `playground.md`

```markdown
# AI Playground | Compare top AI models side-by-side


---
url: https://ai-sdk.dev/playground
description: Chat and compare OpenAI GPT, Anthropic Claude, Google Gemini, Llama, Mistral, and more.
---
```

### 415. `providers/adapters/langchain.md`

```markdown
# LangChain


---
url: https://ai-sdk.dev/providers/adapters/langchain
description: Learn how to use LangChain with the AI SDK.
---


# [LangChain](#langchain)


[LangChain](https://js.langchain.com/docs/) is a framework for developing applications powered by language models. It provides tools and abstractions for working with AI models, agents, vector stores, and other data sources for retrieval augmented generation (RAG). However, LangChain does not provide a way to easily build UIs or a standard way to stream data to the client.


## [Example: Completion](#example-completion)


Here is a basic example that uses both the AI SDK and LangChain together with the [Next.js](https://nextjs.org/docs) App Router.

The AI SDK [`LangChainAdapter`](/docs/reference/stream-helpers/langchain-adapter) uses the result from [LangChain ExpressionLanguage streaming](https://js.langchain.com/docs/expression_language/streaming) to pipe text to the client. `LangChainAdapter.toDataStreamResponse()` is compatible with the LangChain Expression Language `.stream()` function response.

app/api/completion/route.ts

```
import{ChatOpenAI}from'@langchain/openai';import{LangChainAdapter}from'ai';exportconst maxDuration =60;exportasyncfunctionPOST(req: Request){const{ prompt }=await req.json();const model =newChatOpenAI({    model:'gpt-3.5-turbo-0125',    temperature:0,});const stream =await model.stream(prompt);returnLangChainAdapter.toDataStreamResponse(stream);}
```

Then, we use the AI SDK's [`useCompletion`](/docs/ai-sdk-ui/completion) method in the page component to handle the completion:

app/page.tsx

```
'use client';import{ useCompletion }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ completion, input, handleInputChange, handleSubmit }=useCompletion();return(<div>{completion}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}/></form></div>);}
```


## [More Examples](#more-examples)


You can find additional examples in the AI SDK [examples/next-langchain](https://github.com/vercel/ai/tree/main/examples/next-langchain) folder.
```

### 416. `providers/adapters/llamaindex.md`

```markdown
# LlamaIndex


---
url: https://ai-sdk.dev/providers/adapters/llamaindex
description: Learn how to use LlamaIndex with the AI SDK.
---


# [LlamaIndex](#llamaindex)


[LlamaIndex](https://ts.llamaindex.ai/) is a framework for building LLM-powered applications. LlamaIndex helps you ingest, structure, and access private or domain-specific data. LlamaIndex.TS offers the core features of LlamaIndex for Python for popular runtimes like Node.js (official support), Vercel Edge Functions (experimental), and Deno (experimental).


## [Example: Completion](#example-completion)


Here is a basic example that uses both AI SDK and LlamaIndex together with the [Next.js](https://nextjs.org/docs) App Router.

The AI SDK [`LlamaIndexAdapter`](/docs/reference/stream-helpers/llamaindex-adapter) uses the stream result from calling the `chat` method on a [LlamaIndex ChatEngine](https://ts.llamaindex.ai/modules/chat_engine) or the `query` method on a [LlamaIndex QueryEngine](https://ts.llamaindex.ai/modules/query_engines) to pipe text to the client.

app/api/completion/route.ts

```
import{OpenAI,SimpleChatEngine}from'llamaindex';import{LlamaIndexAdapter}from'ai';exportconst maxDuration =60;exportasyncfunctionPOST(req: Request){const{ prompt }=await req.json();const llm =newOpenAI({ model:'gpt-4o'});const chatEngine =newSimpleChatEngine({ llm });const stream =await chatEngine.chat({    message: prompt,    stream:true,});returnLlamaIndexAdapter.toDataStreamResponse(stream);}
```

Then, we use the AI SDK's [`useCompletion`](/docs/ai-sdk-ui/completion) method in the page component to handle the completion:

app/page.tsx

```
'use client';import{ useCompletion }from'@ai-sdk/react';exportdefaultfunctionChat(){const{ completion, input, handleInputChange, handleSubmit }=useCompletion();return(<div>{completion}<formonSubmit={handleSubmit}><inputvalue={input}onChange={handleInputChange}/></form></div>);}
```


## [More Examples](#more-examples)


[create-llama](https://github.com/run-llama/create-llama) is the easiest way to get started with LlamaIndex. It uses the AI SDK to connect to LlamaIndex in all its generated code.
```

### 417. `providers/adapters.md`

```markdown
# Adapters


---
url: https://ai-sdk.dev/providers/adapters
description: Learn how to use AI SDK Adapters.
---


# [Adapters](#adapters)


Adapters are lightweight integrations that enable you to use the AI SDK UI functions (`useChat` and `useCompletion`) with 3rd party libraries.

The following adapters are currently available:

-   [LangChain](/providers/adapters/langchain)
-   [LlamaIndex](/providers/adapters/llamaindex)
```

### 418. `providers/ai-sdk-providers/amazon-bedrock.md`

```markdown
# Amazon Bedrock Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock
description: Learn how to use the Amazon Bedrock provider.
---


# [Amazon Bedrock Provider](#amazon-bedrock-provider)


The Amazon Bedrock provider for the [AI SDK](/docs) contains language model support for the [Amazon Bedrock](https://aws.amazon.com/bedrock) APIs.


## [Setup](#setup)


The Bedrock provider is available in the `@ai-sdk/amazon-bedrock` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/amazon-bedrock


### [Prerequisites](#prerequisites)


Access to Amazon Bedrock foundation models isn't granted by default. In order to gain access to a foundation model, an IAM user with sufficient permissions needs to request access to it through the console. Once access is provided to a model, it is available for all users in the account.

See the [Model Access Docs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) for more information.


### [Authentication](#authentication)



#### [Using IAM Access Key and Secret Key](#using-iam-access-key-and-secret-key)


**Step 1: Creating AWS Access Key and Secret Key**

To get started, you'll need to create an AWS access key and secret key. Here's how:

**Login to AWS Management Console**

-   Go to the [AWS Management Console](https://console.aws.amazon.com/) and log in with your AWS account credentials.

**Create an IAM User**

-   Navigate to the [IAM dashboard](https://console.aws.amazon.com/iam/home) and click on "Users" in the left-hand navigation menu.
-   Click on "Create user" and fill in the required details to create a new IAM user.
-   Make sure to select "Programmatic access" as the access type.
-   The user account needs the `AmazonBedrockFullAccess` policy attached to it.

**Create Access Key**

-   Click on the "Security credentials" tab and then click on "Create access key".
-   Click "Create access key" to generate a new access key pair.
-   Download the `.csv` file containing the access key ID and secret access key.

**Step 2: Configuring the Access Key and Secret Key**

Within your project add a `.env` file if you don't already have one. This file will be used to set the access key and secret key as environment variables. Add the following lines to the `.env` file:

```
AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_IDAWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEYAWS_REGION=YOUR_REGION
```

Many frameworks such as [Next.js](https://nextjs.org/) load the `.env` file automatically. If you're using a different framework, you may need to load the `.env` file manually using a package like [`dotenv`](https://github.com/motdotla/dotenv).

Remember to replace `YOUR_ACCESS_KEY_ID`, `YOUR_SECRET_ACCESS_KEY`, and `YOUR_REGION` with the actual values from your AWS account.


#### [Using AWS SDK Credentials Chain (instance profiles, instance roles, ECS roles, EKS Service Accounts, etc.)](#using-aws-sdk-credentials-chain-instance-profiles-instance-roles-ecs-roles-eks-service-accounts-etc)


When using AWS SDK, the SDK will automatically use the credentials chain to determine the credentials to use. This includes instance profiles, instance roles, ECS roles, EKS Service Accounts, etc. A similar behavior is possible using the AI SDK by not specifying the `accessKeyId` and `secretAccessKey`, `sessionToken` properties in the provider settings and instead passing a `credentialProvider` property.

*Usage:*

`@aws-sdk/credential-providers` package provides a set of credential providers that can be used to create a credential provider chain.

pnpm

npm

yarn

pnpm add @aws-sdk/credential-providers

```
import{ createAmazonBedrock }from'@ai-sdk/amazon-bedrock';import{ fromNodeProviderChain }from'@aws-sdk/credential-providers';const bedrock =createAmazonBedrock({  region:'us-east-1',  credentialProvider:fromNodeProviderChain(),});
```


## [Provider Instance](#provider-instance)


You can import the default provider instance `bedrock` from `@ai-sdk/amazon-bedrock`:

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';
```

If you need a customized setup, you can import `createAmazonBedrock` from `@ai-sdk/amazon-bedrock` and create a provider instance with your settings:

```
import{ createAmazonBedrock }from'@ai-sdk/amazon-bedrock';const bedrock =createAmazonBedrock({  region:'us-east-1',  accessKeyId:'xxxxxxxxx',  secretAccessKey:'xxxxxxxxx',  sessionToken:'xxxxxxxxx',});
```

The credentials settings fall back to environment variable defaults described below. These may be set by your serverless environment without your awareness, which can lead to merged/conflicting credential values and provider errors around failed authentication. If you're experiencing issues be sure you are explicitly specifying all settings (even if `undefined`) to avoid any defaults.

You can use the following optional settings to customize the Amazon Bedrock provider instance:

-   **region** *string*

    The AWS region that you want to use for the API calls. It uses the `AWS_REGION` environment variable by default.

-   **accessKeyId** *string*

    The AWS access key ID that you want to use for the API calls. It uses the `AWS_ACCESS_KEY_ID` environment variable by default.

-   **secretAccessKey** *string*

    The AWS secret access key that you want to use for the API calls. It uses the `AWS_SECRET_ACCESS_KEY` environment variable by default.

-   **sessionToken** *string*

    Optional. The AWS session token that you want to use for the API calls. It uses the `AWS_SESSION_TOKEN` environment variable by default.

-   **credentialProvider** *() => Promise<{ accessKeyId: string; secretAccessKey: string; sessionToken?: string; }>*

    Optional. The AWS credential provider chain that you want to use for the API calls. It uses the specified credentials by default.



## [Language Models](#language-models)


You can create models that call the Bedrock API using the provider instance. The first argument is the model id, e.g. `meta.llama3-70b-instruct-v1:0`.

```
const model =bedrock('meta.llama3-70b-instruct-v1:0');
```

Amazon Bedrock models also support some model specific settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model =bedrock('anthropic.claude-3-sonnet-20240229-v1:0',{  additionalModelRequestFields:{ top_k:350},});
```

Documentation for additional settings based on the selected model can be found within the [Amazon Bedrock Inference Parameter Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html).

You can use Amazon Bedrock language models to generate text with the `generateText` function:

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:bedrock('meta.llama3-70b-instruct-v1:0'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Amazon Bedrock language models can also be used in the `streamText` function (see [AI SDK Core](/docs/ai-sdk-core)).


### [File Inputs](#file-inputs)


Amazon Bedrock supports file inputs on in combination with specific models, e.g. `anthropic.claude-3-haiku-20240307-v1:0`.

The Amazon Bedrock provider supports file inputs, e.g. PDF files.

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ generateText }from'ai';const result =awaitgenerateText({  model:bedrock('anthropic.claude-3-haiku-20240307-v1:0'),  messages:[{      role:'user',      content:[{type:'text', text:'Describe the pdf in detail.'},{type:'file',          data: fs.readFileSync('./data/ai.pdf'),          mimeType:'application/pdf',},],},],});
```


### [Guardrails](#guardrails)


You can use the `bedrock` provider options to utilize [Amazon Bedrock Guardrails](https://aws.amazon.com/bedrock/guardrails/):

```
const result =awaitgenerateText({bedrock('anthropic.claude-3-sonnet-20240229-v1:0'),  providerOptions:{    bedrock:{      guardrailConfig:{        guardrailIdentifier:'1abcd2ef34gh',        guardrailVersion:'1',        trace:'enabled'asconst,        streamProcessingMode:'async',},},},});
```

Tracing information will be returned in the provider metadata if you have tracing enabled.

```
if(result.providerMetadata?.bedrock.trace){// ...}
```

See the [Amazon Bedrock Guardrails documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html) for more information.


### [Cache Points](#cache-points)


Amazon Bedrock prompt caching is currently in preview release. To request access, visit the [Amazon Bedrock prompt caching page](https://aws.amazon.com/bedrock/prompt-caching/).

In messages, you can use the `providerOptions` property to set cache points. Set the `bedrock` property in the `providerOptions` object to `{ cachePoint: { type: 'default' } }` to create a cache point.

Cache usage information is returned in the `providerMetadata` object\`. See examples below.

Cache points have model-specific token minimums and limits. For example, Claude 3.5 Sonnet v2 requires at least 1,024 tokens for a cache point and allows up to 4 cache points. See the [Amazon Bedrock prompt caching documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html) for details on supported models, regions, and limits.

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ generateText }from'ai';const cyberpunkAnalysis ='... literary analysis of cyberpunk themes and concepts ...';const result =awaitgenerateText({  model:bedrock('anthropic.claude-3-5-sonnet-20241022-v2:0'),  messages:[{      role:'system',      content:`You are an expert on William Gibson's cyberpunk literature and themes. You have access to the following academic analysis: ${cyberpunkAnalysis}`,      providerOptions:{        bedrock:{ cachePoint:{type:'default'}},},},{      role:'user',      content:'What are the key cyberpunk themes that Gibson explores in Neuromancer?',},],});console.log(result.text);console.log(result.providerMetadata?.bedrock?.usage);// Shows cache read/write token usage, e.g.:// {//   cacheReadInputTokens: 1337,//   cacheWriteInputTokens: 42,// }
```

Cache points also work with streaming responses:

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ streamText }from'ai';const cyberpunkAnalysis ='... literary analysis of cyberpunk themes and concepts ...';const result =streamText({  model:bedrock('anthropic.claude-3-5-sonnet-20241022-v2:0'),  messages:[{      role:'assistant',      content:[{type:'text', text:'You are an expert on cyberpunk literature.'},{type:'text', text:`Academic analysis: ${cyberpunkAnalysis}`},],      providerOptions:{ bedrock:{ cachePoint:{type:'default'}}},},{      role:'user',      content:'How does Gibson explore the relationship between humanity and technology?',},],});forawait(const textPart of result.textStream){  process.stdout.write(textPart);}console.log('Cache token usage:',(await result.providerMetadata)?.bedrock?.usage,);// Shows cache read/write token usage, e.g.:// {//   cacheReadInputTokens: 1337,//   cacheWriteInputTokens: 42,// }
```


## [Reasoning](#reasoning)


Amazon Bedrock has reasoning support for the `claude-3-7-sonnet-20250219` model.

You can enable it using the `reasoning_config` provider option and specifying a thinking budget in tokens (minimum: `1024`, maximum: `64000`).

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ generateText }from'ai';const{ text, reasoning, reasoningDetails }=awaitgenerateText({  model:bedrock('us.anthropic.claude-3-7-sonnet-20250219-v1:0'),  prompt:'How many people will live in the world in 2040?',  providerOptions:{    bedrock:{      reasoningConfig:{type:'enabled', budgetTokens:1024},},},});console.log(reasoning);// reasoning textconsole.log(reasoningDetails);// reasoning details including redacted reasoningconsole.log(text);// text response
```

See [AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details on how to integrate reasoning into your chatbot.


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`amazon.titan-tg1-large`

`amazon.titan-text-express-v1`

`amazon.nova-micro-v1:0`

`amazon.nova-lite-v1:0`

`amazon.nova-pro-v1:0`

`anthropic.claude-3-7-sonnet-20250219-v1:0`

`anthropic.claude-3-5-sonnet-20241022-v2:0`

`anthropic.claude-3-5-sonnet-20240620-v1:0`

`anthropic.claude-3-5-haiku-20241022-v1:0`

`anthropic.claude-3-opus-20240229-v1:0`

`anthropic.claude-3-sonnet-20240229-v1:0`

`anthropic.claude-3-haiku-20240307-v1:0`

`anthropic.claude-v2:1`

`cohere.command-r-v1:0`

`cohere.command-r-plus-v1:0`

`deepseek.r1-v1:0`

`meta.llama2-13b-chat-v1`

`meta.llama2-70b-chat-v1`

`meta.llama3-8b-instruct-v1:0`

`meta.llama3-70b-instruct-v1:0`

`meta.llama3-1-8b-instruct-v1:0`

`meta.llama3-1-70b-instruct-v1:0`

`meta.llama3-1-405b-instruct-v1:0`

`meta.llama3-2-1b-instruct-v1:0`

`meta.llama3-2-3b-instruct-v1:0`

`meta.llama3-2-11b-instruct-v1:0`

`meta.llama3-2-90b-instruct-v1:0`

`mistral.mistral-7b-instruct-v0:2`

`mistral.mixtral-8x7b-instruct-v0:1`

`mistral.mistral-large-2402-v1:0`

`mistral.mistral-small-2402-v1:0`

The table above lists popular models. Please see the [Amazon Bedrock docs](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.


## [Embedding Models](#embedding-models)


You can create models that call the Bedrock API [Bedrock API](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html) using the `.embedding()` factory method.

```
const model = bedrock.embedding('amazon.titan-embed-text-v1');
```

Bedrock Titan embedding model amazon.titan-embed-text-v2:0 supports several additional settings. You can pass them as an options argument:

```
const model = bedrock.embedding('amazon.titan-embed-text-v2:0',{  dimensions:512// optional, number of dimensions for the embedding  normalize:true// optional  normalize the output embeddings})
```

The following optional settings are available for Bedrock Titan embedding models:

-   **dimensions**: *number*

    The number of dimensions the output embeddings should have. The following values are accepted: 1024 (default), 512, 256.

-   **normalize** *boolean*

    Flag indicating whether or not to normalize the output embeddings. Defaults to true.



### [Model Capabilities](#model-capabilities-1)


Model

Default Dimensions

Custom Dimensions

`amazon.titan-embed-text-v1`

1536

`amazon.titan-embed-text-v2:0`

1024


## [Image Models](#image-models)


You can create models that call the Bedrock API [Bedrock API](https://docs.aws.amazon.com/nova/latest/userguide/image-generation.html) using the `.image()` factory method.

For more on the Amazon Nova Canvas image model, see the [Nova Canvas Overview](https://docs.aws.amazon.com/ai/responsible-ai/nova-canvas/overview.html).

The `amazon.nova-canvas-v1:0` model is available in the `us-east-1` region.

```
const model = bedrock.image('amazon.nova-canvas-v1:0');
```

You can then generate images with the `experimental_generateImage` function:

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: bedrock.imageModel('amazon.nova-canvas-v1:0'),  prompt:'A beautiful sunset over a calm ocean',  size:'512x512',  seed:42,});
```

You can also pass the `providerOptions` object to the `generateImage` function to customize the generation behavior:

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: bedrock.imageModel('amazon.nova-canvas-v1:0'),  prompt:'A beautiful sunset over a calm ocean',  size:'512x512',  seed:42,  providerOptions:{ bedrock:{ quality:'premium'}},});
```

Documentation for additional settings can be found within the [Amazon Bedrock User Guide for Amazon Nova Documentation](https://docs.aws.amazon.com/nova/latest/userguide/image-gen-req-resp-structure.html).


### [Image Model Settings](#image-model-settings)


When creating an image model, you can customize the generation behavior with optional settings:

```
const model = bedrock.imageModel('amazon.nova-canvas-v1:0',{  maxImagesPerCall:1,// Maximum number of images to generate per API call});
```

-   **maxImagesPerCall** *number*

    Override the maximum number of images generated per API call. Default can vary by model, with 5 as a common default.



### [Model Capabilities](#model-capabilities-2)


The Amazon Nova Canvas model supports custom sizes with constraints as follows:

-   Each side must be between 320-4096 pixels, inclusive.
-   Each side must be evenly divisible by 16.
-   The aspect ratio must be between 1:4 and 4:1. That is, one side can't be more than 4 times longer than the other side.
-   The total pixel count must be less than 4,194,304.

For more, see [Image generation access and usage](https://docs.aws.amazon.com/nova/latest/userguide/image-gen-access.html).

Model

Sizes

`amazon.nova-canvas-v1:0`

Custom sizes: 320-4096px per side (must be divisible by 16), aspect ratio 1:4 to 4:1, max 4.2M pixels


## [Response Headers](#response-headers)


The Amazon Bedrock provider will return the response headers associated with network requests made of the Bedrock servers.

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:bedrock('meta.llama3-70b-instruct-v1:0'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});console.log(result.response.headers);
```

Below is sample output where you can see the `x-amzn-requestid` header. This can be useful for correlating Bedrock API calls with requests made by the AI SDK:

```
{  connection:'keep-alive','content-length':'2399','content-type':'application/json',  date:'Fri, 07 Feb 2025 04:28:30 GMT','x-amzn-requestid':'c9f3ace4-dd5d-49e5-9807-39aedfa47c8e'}
```

This information is also available with `streamText`:

```
import{ bedrock }from'@ai-sdk/amazon-bedrock';import{ streamText }from'ai';const result =streamText({  model:bedrock('meta.llama3-70b-instruct-v1:0'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});forawait(const textPart of result.textStream){  process.stdout.write(textPart);}console.log('Response headers:',(await result.response).headers);
```

With sample output as:

```
{  connection:'keep-alive','content-type':'application/vnd.amazon.eventstream',  date:'Fri, 07 Feb 2025 04:33:37 GMT','transfer-encoding':'chunked','x-amzn-requestid':'a976e3fc-0e45-4241-9954-b9bdd80ab407'}
```


## [Migrating to `@ai-sdk/amazon-bedrock` 2.x](#migrating-to-ai-sdkamazon-bedrock-2x)


The Amazon Bedrock provider was rewritten in version 2.x to remove the dependency on the `@aws-sdk/client-bedrock-runtime` package.

The `bedrockOptions` provider setting previously available has been removed. If you were using the `bedrockOptions` object, you should now use the `region`, `accessKeyId`, `secretAccessKey`, and `sessionToken` settings directly instead.

Note that you may need to set all of these explicitly, e.g. even if you're not using `sessionToken`, set it to `undefined`. If you're running in a serverless environment, there may be default environment variables set by your containing environment that the Amazon Bedrock provider will then pick up and could conflict with the ones you're intending to use.
```

### 419. `providers/ai-sdk-providers/anthropic.md`

```markdown
# Anthropic Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/anthropic
description: Learn how to use the Anthropic provider for the AI SDK.
---


# [Anthropic Provider](#anthropic-provider)


The [Anthropic](https://www.anthropic.com/) provider contains language model support for the [Anthropic Messages API](https://docs.anthropic.com/claude/reference/messages_post).


## [Setup](#setup)


The Anthropic provider is available in the `@ai-sdk/anthropic` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/anthropic


## [Provider Instance](#provider-instance)


You can import the default provider instance `anthropic` from `@ai-sdk/anthropic`:

```
import{ anthropic }from'@ai-sdk/anthropic';
```

If you need a customized setup, you can import `createAnthropic` from `@ai-sdk/anthropic` and create a provider instance with your settings:

```
import{ createAnthropic }from'@ai-sdk/anthropic';const anthropic =createAnthropic({// custom settings});
```

You can use the following optional settings to customize the Anthropic provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.anthropic.com/v1`.

-   **apiKey** *string*

    API key that is being sent using the `x-api-key` header. It defaults to the `ANTHROPIC_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create models that call the [Anthropic Messages API](https://docs.anthropic.com/claude/reference/messages_post) using the provider instance. The first argument is the model id, e.g. `claude-3-haiku-20240307`. Some models have multi-modal capabilities.

```
const model =anthropic('claude-3-haiku-20240307');
```

You can use Anthropic language models to generate text with the `generateText` function:

```
import{ anthropic }from'@ai-sdk/anthropic';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:anthropic('claude-3-haiku-20240307'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Anthropic language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core)).

The Anthropic API returns streaming tool calls all at once after a delay. This causes the `streamObject` function to generate the object fully after a delay instead of streaming it incrementally.

The following optional settings are available for Anthropic models:

-   `sendReasoning` *boolean*

    Optional. Include reasoning content in requests sent to the model. Defaults to `true`.

    If you are experiencing issues with the model handling requests involving reasoning content, you can set this to `false` to omit them from the request.



### [Reasoning](#reasoning)


Anthropic has reasoning support for `claude-4-opus-20250514`, `claude-4-sonnet-20250514`, and `claude-3-7-sonnet-20250219` models.

You can enable it using the `thinking` provider option and specifying a thinking budget in tokens.

```
import{ anthropic,AnthropicProviderOptions}from'@ai-sdk/anthropic';import{ generateText }from'ai';const{ text, reasoning, reasoningDetails }=awaitgenerateText({  model:anthropic('claude-4-opus-20250514'),  prompt:'How many people will live in the world in 2040?',  providerOptions:{    anthropic:{      thinking:{type:'enabled', budgetTokens:12000},} satisfies AnthropicProviderOptions,},});console.log(reasoning);// reasoning textconsole.log(reasoningDetails);// reasoning details including redacted reasoningconsole.log(text);// text response
```

See [AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details on how to integrate reasoning into your chatbot.


### [Cache Control](#cache-control)


Anthropic cache control was originally a beta feature and required passing an opt-in `cacheControl` setting when creating the model instance. It is now Generally Available and enabled by default. The `cacheControl` setting is no longer needed and will be removed in a future release.

In the messages and message parts, you can use the `providerOptions` property to set cache control breakpoints. You need to set the `anthropic` property in the `providerOptions` object to `{ cacheControl: { type: 'ephemeral' } }` to set a cache control breakpoint.

The cache creation input tokens are then returned in the `providerMetadata` object for `generateText` and `generateObject`, again under the `anthropic` property. When you use `streamText` or `streamObject`, the response contains a promise that resolves to the metadata. Alternatively you can receive it in the `onFinish` callback.

```
import{ anthropic }from'@ai-sdk/anthropic';import{ generateText }from'ai';const errorMessage ='... long error message ...';const result =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-20240620'),  messages:[{      role:'user',      content:[{type:'text', text:'You are a JavaScript expert.'},{type:'text',          text:`Error message: ${errorMessage}`,          providerOptions:{            anthropic:{ cacheControl:{type:'ephemeral'}},},},{type:'text', text:'Explain the error message.'},],},],});console.log(result.text);console.log(result.providerMetadata?.anthropic);// e.g. { cacheCreationInputTokens: 2118, cacheReadInputTokens: 0 }
```

You can also use cache control on system messages by providing multiple system messages at the head of your messages array:

```
const result =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-20240620'),  messages:[{      role:'system',      content:'Cached system message part',      providerOptions:{        anthropic:{ cacheControl:{type:'ephemeral'}},},},{      role:'system',      content:'Uncached system message part',},{      role:'user',      content:'User prompt',},],});
```

The minimum cacheable prompt length is:

-   1024 tokens for Claude 3.7 Sonnet, Claude 3.5 Sonnet and Claude 3 Opus
-   2048 tokens for Claude 3.5 Haiku and Claude 3 Haiku

Shorter prompts cannot be cached, even if marked with `cacheControl`. Any requests to cache fewer than this number of tokens will be processed without caching.

For more on prompt caching with Anthropic, see [Anthropic's Cache Control documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching).

Because the `UIMessage` type (used by AI SDK UI hooks like `useChat`) does not support the `providerOptions` property, you can use `convertToCoreMessages` first before passing the messages to functions like `generateText` or `streamText`. For more details on `providerOptions` usage, see [here](/docs/foundations/prompts#provider-options).


### [Computer Use](#computer-use)


Anthropic provides three built-in tools that can be used to interact with external systems:

1.  **Bash Tool**: Allows running bash commands.
2.  **Text Editor Tool**: Provides functionality for viewing and editing text files.
3.  **Computer Tool**: Enables control of keyboard and mouse actions on a computer.

They are available via the `tools` property of the provider instance.


#### [Bash Tool](#bash-tool)


The Bash Tool allows running bash commands. Here's how to create and use it:

```
const bashTool = anthropic.tools.bash_20241022({execute:async({ command, restart })=>{// Implement your bash command execution logic here// Return the result of the command execution},});
```

Parameters:

-   `command` (string): The bash command to run. Required unless the tool is being restarted.
-   `restart` (boolean, optional): Specifying true will restart this tool.


#### [Text Editor Tool](#text-editor-tool)


The Text Editor Tool provides functionality for viewing and editing text files:

```
const textEditorTool = anthropic.tools.textEditor_20241022({execute:async({    command,    path,    file_text,    insert_line,    new_str,    old_str,    view_range,})=>{// Implement your text editing logic here// Return the result of the text editing operation},});
```

Parameters:

-   `command` ('view' | 'create' | 'str\_replace' | 'insert' | 'undo\_edit'): The command to run.
-   `path` (string): Absolute path to file or directory, e.g. `/repo/file.py` or `/repo`.
-   `file_text` (string, optional): Required for `create` command, with the content of the file to be created.
-   `insert_line` (number, optional): Required for `insert` command. The line number after which to insert the new string.
-   `new_str` (string, optional): New string for `str_replace` or `insert` commands.
-   `old_str` (string, optional): Required for `str_replace` command, containing the string to replace.
-   `view_range` (number\[\], optional): Optional for `view` command to specify line range to show.

When using the Text Editor Tool, make sure to name the key in the tools object `str_replace_editor`.

```
const response =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-20241022'),  prompt:"Create a new file called example.txt, write 'Hello World' to it, and run 'cat example.txt' in the terminal",  tools:{    str_replace_editor: textEditorTool,},});
```


#### [Computer Tool](#computer-tool)


The Computer Tool enables control of keyboard and mouse actions on a computer:

```
const computerTool = anthropic.tools.computer_20241022({  displayWidthPx:1920,  displayHeightPx:1080,  displayNumber:0,// Optional, for X11 environmentsexecute:async({ action, coordinate, text })=>{// Implement your computer control logic here// Return the result of the action// Example code:switch(action){case'screenshot':{// multipart result:return{type:'image',          data: fs.readFileSync('./data/screenshot-editor.png').toString('base64'),};}default:{console.log('Action:', action);console.log('Coordinate:', coordinate);console.log('Text:', text);return`executed ${action}`;}}},// map to tool result content for LLM consumption:experimental_toToolResultContent(result){returntypeof result ==='string'?[{type:'text', text: result }]:[{type:'image', data: result.data, mimeType:'image/png'}];},});
```

Parameters:

-   `action` ('key' | 'type' | 'mouse\_move' | 'left\_click' | 'left\_click\_drag' | 'right\_click' | 'middle\_click' | 'double\_click' | 'screenshot' | 'cursor\_position'): The action to perform.
-   `coordinate` (number\[\], optional): Required for `mouse_move` and `left_click_drag` actions. Specifies the (x, y) coordinates.
-   `text` (string, optional): Required for `type` and `key` actions.

These tools can be used in conjunction with the `sonnet-3-5-sonnet-20240620` model to enable more complex interactions and tasks.


### [PDF support](#pdf-support)


Anthropic Sonnet `claude-3-5-sonnet-20241022` supports reading PDF files. You can pass PDF files as part of the message content using the `file` type:

Option 1: URL-based PDF document

```
const result =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-20241022'),  messages:[{      role:'user',      content:[{type:'text',          text:'What is an embedding model according to this document?',},{type:'file',          data:newURL('https://github.com/vercel/ai/blob/main/examples/ai-core/data/ai.pdf?raw=true',),          mimeType:'application/pdf',},],},],});
```

Option 2: Base64-encoded PDF document

```
const result =awaitgenerateText({  model:anthropic('claude-3-5-sonnet-20241022'),  messages:[{      role:'user',      content:[{type:'text',          text:'What is an embedding model according to this document?',},{type:'file',          data: fs.readFileSync('./data/ai.pdf'),          mimeType:'application/pdf',},],},],});
```

The model will have access to the contents of the PDF file and respond to questions about it. The PDF file should be passed using the `data` field, and the `mimeType` should be set to `'application/pdf'`.


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Computer Use

`claude-4-opus-20250514`

`claude-4-sonnet-20250514`

`claude-3-7-sonnet-20250219`

`claude-3-5-sonnet-20241022`

`claude-3-5-sonnet-20240620`

`claude-3-5-haiku-20241022`

`claude-3-opus-20240229`

`claude-3-sonnet-20240229`

`claude-3-haiku-20240307`

The table above lists popular models. Please see the [Anthropic docs](https://docs.anthropic.com/en/docs/about-claude/models) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.
```

### 420. `providers/ai-sdk-providers/assemblyai.md`

```markdown
# AssemblyAI Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/assemblyai
description: Learn how to use the AssemblyAI provider for the AI SDK.
---


# [AssemblyAI Provider](#assemblyai-provider)


The [AssemblyAI](https://assemblyai.com/) provider contains language model support for the AssemblyAI transcription API.


## [Setup](#setup)


The AssemblyAI provider is available in the `@ai-sdk/assemblyai` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/assemblyai


## [Provider Instance](#provider-instance)


You can import the default provider instance `assemblyai` from `@ai-sdk/assemblyai`:

```
import{ assemblyai }from'@ai-sdk/assemblyai';
```

If you need a customized setup, you can import `createAssemblyAI` from `@ai-sdk/assemblyai` and create a provider instance with your settings:

```
import{ createAssemblyAI }from'@ai-sdk/assemblyai';const assemblyai =createAssemblyAI({// custom settings, e.g.  fetch: customFetch,});
```

You can use the following optional settings to customize the AssemblyAI provider instance:

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `ASSEMBLYAI_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Transcription Models](#transcription-models)


You can create models that call the [AssemblyAI transcription API](https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file/typescript) using the `.transcription()` factory method.

The first argument is the model id e.g. `best`.

```
const model = assemblyai.transcription('best');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the `contentSafety` option will enable content safety filtering.

```
import{ experimental_transcribe as transcribe }from'ai';import{ assemblyai }from'@ai-sdk/assemblyai';import{ readFile }from'fs/promises';const result =awaittranscribe({  model: assemblyai.transcription('best'),  audio:awaitreadFile('audio.mp3'),  providerOptions:{ assemblyai:{ contentSafety:true}},});
```

The following provider options are available:

-   **audioEndAt** *number*

    End time of the audio in milliseconds. Optional.

-   **audioStartFrom** *number*

    Start time of the audio in milliseconds. Optional.

-   **autoChapters** *boolean*

    Whether to automatically generate chapters for the transcription. Optional.

-   **autoHighlights** *boolean*

    Whether to automatically generate highlights for the transcription. Optional.

-   **boostParam** *enum*

    Boost parameter for the transcription. Allowed values: `'low'`, `'default'`, `'high'`. Optional.

-   **contentSafety** *boolean*

    Whether to enable content safety filtering. Optional.

-   **contentSafetyConfidence** *number*

    Confidence threshold for content safety filtering (25-100). Optional.

-   **customSpelling** *array of objects*

    Custom spelling rules for the transcription. Each object has `from` (array of strings) and `to` (string) properties. Optional.

-   **disfluencies** *boolean*

    Whether to include disfluencies (um, uh, etc.) in the transcription. Optional.

-   **entityDetection** *boolean*

    Whether to detect entities in the transcription. Optional.

-   **filterProfanity** *boolean*

    Whether to filter profanity in the transcription. Optional.

-   **formatText** *boolean*

    Whether to format the text in the transcription. Optional.

-   **iabCategories** *boolean*

    Whether to include IAB categories in the transcription. Optional.

-   **languageCode** *string*

    Language code for the audio. Supports numerous ISO-639-1 and ISO-639-3 language codes. Optional.

-   **languageConfidenceThreshold** *number*

    Confidence threshold for language detection. Optional.

-   **languageDetection** *boolean*

    Whether to enable language detection. Optional.

-   **multichannel** *boolean*

    Whether to process multiple audio channels separately. Optional.

-   **punctuate** *boolean*

    Whether to add punctuation to the transcription. Optional.

-   **redactPii** *boolean*

    Whether to redact personally identifiable information. Optional.

-   **redactPiiAudio** *boolean*

    Whether to redact PII in the audio file. Optional.

-   **redactPiiAudioQuality** *enum*

    Quality of the redacted audio file. Allowed values: `'mp3'`, `'wav'`. Optional.

-   **redactPiiPolicies** *array of enums*

    Policies for PII redaction, specifying which types of information to redact. Supports numerous types like `'person_name'`, `'phone_number'`, etc. Optional.

-   **redactPiiSub** *enum*

    Substitution method for redacted PII. Allowed values: `'entity_name'`, `'hash'`. Optional.

-   **sentimentAnalysis** *boolean*

    Whether to perform sentiment analysis on the transcription. Optional.

-   **speakerLabels** *boolean*

    Whether to label different speakers in the transcription. Optional.

-   **speakersExpected** *number*

    Expected number of speakers in the audio. Optional.

-   **speechThreshold** *number*

    Threshold for speech detection (0-1). Optional.

-   **summarization** *boolean*

    Whether to generate a summary of the transcription. Optional.

-   **summaryModel** *enum*

    Model to use for summarization. Allowed values: `'informative'`, `'conversational'`, `'catchy'`. Optional.

-   **summaryType** *enum*

    Type of summary to generate. Allowed values: `'bullets'`, `'bullets_verbose'`, `'gist'`, `'headline'`, `'paragraph'`. Optional.

-   **topics** *array of strings*

    List of topics to detect in the transcription. Optional.

-   **webhookAuthHeaderName** *string*

    Name of the authentication header for webhook requests. Optional.

-   **webhookAuthHeaderValue** *string*

    Value of the authentication header for webhook requests. Optional.

-   **webhookUrl** *string*

    URL to send webhook notifications to. Optional.

-   **wordBoost** *array of strings*

    List of words to boost in the transcription. Optional.



### [Model Capabilities](#model-capabilities)


Model

Transcription

Duration

Segments

Language

`best`

`nano`
```

### 421. `providers/ai-sdk-providers/azure.md`

```markdown
# Azure OpenAI Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/azure
description: Learn how to use the Azure OpenAI provider for the AI SDK.
---


# [Azure OpenAI Provider](#azure-openai-provider)


The [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service) provider contains language model support for the Azure OpenAI chat API.


## [Setup](#setup)


The Azure OpenAI provider is available in the `@ai-sdk/azure` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/azure


## [Provider Instance](#provider-instance)


You can import the default provider instance `azure` from `@ai-sdk/azure`:

```
import{ azure }from'@ai-sdk/azure';
```

If you need a customized setup, you can import `createAzure` from `@ai-sdk/azure` and create a provider instance with your settings:

```
import{ createAzure }from'@ai-sdk/azure';const azure =createAzure({  resourceName:'your-resource-name',// Azure resource name  apiKey:'your-api-key',});
```

You can use the following optional settings to customize the OpenAI provider instance:

-   **resourceName** *string*

    Azure resource name. It defaults to the `AZURE_RESOURCE_NAME` environment variable.

    The resource name is used in the assembled URL: `https://{resourceName}.openai.azure.com/openai/deployments/{modelId}{path}`. You can use `baseURL` instead to specify the URL prefix.

-   **apiKey** *string*

    API key that is being sent using the `api-key` header. It defaults to the `AZURE_API_KEY` environment variable.

-   **apiVersion** *string*

    Sets a custom [api version](https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation). Defaults to `2024-10-01-preview`.

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers.

    Either this or `resourceName` can be used. When a baseURL is provided, the resourceName is ignored.

    With a baseURL, the resolved URL is `{baseURL}/{modelId}{path}`.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


The Azure OpenAI provider instance is a function that you can invoke to create a language model:

```
const model =azure('your-deployment-name');
```

You need to pass your deployment name as the first argument.


### [Reasoning Models](#reasoning-models)


Azure exposes the thinking of `DeepSeek-R1` in the generated text using the `<think>` tag. You can use the `extractReasoningMiddleware` to extract this reasoning and expose it as a `reasoning` property on the result:

```
import{ azure }from'@ai-sdk/azure';import{ wrapLanguageModel, extractReasoningMiddleware }from'ai';const enhancedModel =wrapLanguageModel({  model:azure('your-deepseek-r1-deployment-name'),  middleware:extractReasoningMiddleware({ tagName:'think'}),});
```

You can then use that enhanced model in functions like `generateText` and `streamText`.


### [Example](#example)


You can use OpenAI language models to generate text with the `generateText` function:

```
import{ azure }from'@ai-sdk/azure';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:azure('your-deployment-name'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

OpenAI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core)).

Azure OpenAI sends larger chunks than OpenAI. This can lead to the perception that the response is slower. See [Troubleshooting: Azure OpenAI Slow To Stream](/docs/troubleshooting/common-issues/azure-stream-slow)


### [Provider Options](#provider-options)


When using OpenAI language models on Azure, you can configure provider-specific options using `providerOptions.openai`. More information on available configuration options are on [the OpenAI provider page](/providers/ai-sdk-providers/openai#language-models).

```
const messages =[{    role:'user',    content:[{type:'text',        text:'What is the capital of the moon?',},{type:'image',        image:'https://example.com/image.png',        providerOptions:{          openai:{ imageDetail:'low'},},},],},];const{ text }=awaitgenerateText({  model:azure('your-deployment-name'),  providerOptions:{    openai:{      reasoningEffort:'low',},},});
```


### [Chat Models](#chat-models)


The URL for calling Azure chat models will be constructed as follows: `https://RESOURCE_NAME.openai.azure.com/openai/deployments/DEPLOYMENT_NAME/chat/completions?api-version=API_VERSION`

Azure OpenAI chat models support also some model specific settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model =azure('your-deployment-name',{  logitBias:{// optional likelihood for specific tokens'50256':-100,},  user:'test-user',// optional unique user identifier});
```

The following optional settings are available for OpenAI chat models:

-   **logitBias** *Record<number, number>*

    Modifies the likelihood of specified tokens appearing in the completion.

    Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

    As an example, you can pass `{"50256": -100}` to prevent the token from being generated.

-   **logprobs** *boolean | number*

    Return the log probabilities of the tokens. Including logprobs will increase the response size and can slow down response times. However, it can be useful to better understand how the model is behaving.

    Setting to true will return the log probabilities of the tokens that were generated.

    Setting to a number will return the log probabilities of the top n tokens that were generated.

-   **parallelToolCalls** *boolean*

    Whether to enable parallel function calling during tool use. Default to true.

-   **user** *string*

    A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.



### [Responses Models](#responses-models)


You can use the Azure OpenAI responses API with the `azure.responses(deploymentName)` factory method.

```
const model = azure.responses('your-deployment-name');
```

Further configuration can be done using OpenAI provider options. You can validate the provider options using the `OpenAIResponsesProviderOptions` type.

```
import{ azure,OpenAIResponsesProviderOptions}from'@ai-sdk/azure';import{ generateText }from'ai';const result =awaitgenerateText({  model: azure.responses('your-deployment-name'),  providerOptions:{    openai:{      parallelToolCalls:false,      store:false,      user:'user_123',// ...} satisfies OpenAIResponsesProviderOptions,},// ...});
```

The following provider options are available:

-   **parallelToolCalls** *boolean* Whether to use parallel tool calls. Defaults to `true`.

-   **store** *boolean* Whether to store the generation. Defaults to `true`.

-   **metadata** *Record<string, string>* Additional metadata to store with the generation.

-   **previousResponseId** *string* The ID of the previous response. You can use it to continue a conversation. Defaults to `undefined`.

-   **instructions** *string* Instructions for the model. They can be used to change the system or developer message when continuing a conversation using the `previousResponseId` option. Defaults to `undefined`.

-   **user** *string* A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Defaults to `undefined`.

-   **reasoningEffort** *'low' | 'medium' | 'high'* Reasoning effort for reasoning models. Defaults to `medium`. If you use `providerOptions` to set the `reasoningEffort` option, this model setting will be ignored.

-   **strictSchemas** *boolean* Whether to use strict JSON schemas in tools and when generating JSON outputs. Defaults to `true`.


The Azure OpenAI responses provider also returns provider-specific metadata:

```
const{ providerMetadata }=awaitgenerateText({  model: azure.responses('your-deployment-name'),});const openaiMetadata = providerMetadata?.openai;
```

The following OpenAI-specific metadata is returned:

-   **responseId** *string* The ID of the response. Can be used to continue a conversation.

-   **cachedPromptTokens** *number* The number of prompt tokens that were a cache hit.

-   **reasoningTokens** *number* The number of reasoning tokens that the model generated.



#### [PDF support](#pdf-support)


The Azure OpenAI Responses API supports reading PDF files. You can pass PDF files as part of the message content using the `file` type:

```
const result =awaitgenerateText({  model: azure.responses('your-deployment-name'),  messages:[{      role:'user',      content:[{type:'text',          text:'What is an embedding model?',},{type:'file',          data: fs.readFileSync('./data/ai.pdf'),          mimeType:'application/pdf',          filename:'ai.pdf',// optional},],},],});
```

The model will have access to the contents of the PDF file and respond to questions about it. The PDF file should be passed using the `data` field, and the `mimeType` should be set to `'application/pdf'`.


### [Completion Models](#completion-models)


You can create models that call the completions API using the `.completion()` factory method. The first argument is the model id. Currently only `gpt-35-turbo-instruct` is supported.

```
const model = azure.completion('your-gpt-35-turbo-instruct-deployment');
```

OpenAI completion models support also some model specific settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model = azure.completion('your-gpt-35-turbo-instruct-deployment',{  echo:true,// optional, echo the prompt in addition to the completion  logitBias:{// optional likelihood for specific tokens'50256':-100,},  suffix:'some text',// optional suffix that comes after a completion of inserted text  user:'test-user',// optional unique user identifier});
```

The following optional settings are available for Azure OpenAI completion models:

-   **echo**: *boolean*

    Echo back the prompt in addition to the completion.

-   **logitBias** *Record<number, number>*

    Modifies the likelihood of specified tokens appearing in the completion.

    Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

    As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.

-   **logprobs** *boolean | number*

    Return the log probabilities of the tokens. Including logprobs will increase the response size and can slow down response times. However, it can be useful to better understand how the model is behaving.

    Setting to true will return the log probabilities of the tokens that were generated.

    Setting to a number will return the log probabilities of the top n tokens that were generated.

-   **suffix** *string*

    The suffix that comes after a completion of inserted text.

-   **user** *string*

    A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.



## [Embedding Models](#embedding-models)


You can create models that call the Azure OpenAI embeddings API using the `.embedding()` factory method.

```
const model = azure.embedding('your-embedding-deployment');
```

Azure OpenAI embedding models support several additional settings. You can pass them as an options argument:

```
const model = azure.embedding('your-embedding-deployment',{  dimensions:512// optional, number of dimensions for the embedding  user:'test-user'// optional unique user identifier})
```

The following optional settings are available for Azure OpenAI embedding models:

-   **dimensions**: *number*

    The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.

-   **user** *string*

    A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.



## [Image Models](#image-models)


You can create models that call the Azure OpenAI image generation API (DALL-E) using the `.imageModel()` factory method. The first argument is your deployment name for the DALL-E model.

```
const model = azure.imageModel('your-dalle-deployment-name');
```

Azure OpenAI image models support several additional settings. You can pass them as an options argument:

```
const model = azure.imageModel('your-dalle-deployment-name',{  user:'test-user',// optional unique user identifier  responseFormat:'url',// 'url' or 'b64_json', defaults to 'url'});
```


### [Example](#example-1)


You can use Azure OpenAI image models to generate images with the `generateImage` function:

```
import{ azure }from'@ai-sdk/azure';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: azure.imageModel('your-dalle-deployment-name'),  prompt:'A photorealistic image of a cat astronaut floating in space',  size:'1024x1024',// '1024x1024', '1792x1024', or '1024x1792' for DALL-E 3});// image contains the URL or base64 data of the generated imageconsole.log(image);
```


### [Model Capabilities](#model-capabilities)


Azure OpenAI supports DALL-E 2 and DALL-E 3 models through deployments. The capabilities depend on which model version your deployment is using:

Model Version

Sizes

DALL-E 3

1024x1024, 1792x1024, 1024x1792

DALL-E 2

256x256, 512x512, 1024x1024

DALL-E models do not support the `aspectRatio` parameter. Use the `size` parameter instead.

When creating your Azure OpenAI deployment, make sure to set the DALL-E model version you want to use.


## [Transcription Models](#transcription-models)


You can create models that call the Azure OpenAI transcription API using the `.transcription()` factory method.

The first argument is the model id e.g. `whisper-1`.

```
const model = azure.transcription('whisper-1');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format will improve accuracy and latency.

```
import{ experimental_transcribe as transcribe }from'ai';import{ azure }from'@ai-sdk/azure';import{ readFile }from'fs/promises';const result =awaittranscribe({  model: azure.transcription('whisper-1'),  audio:awaitreadFile('audio.mp3'),  providerOptions:{ azure:{ language:'en'}},});
```

The following provider options are available:

-   **timestampGranularities** *string\[\]* The granularity of the timestamps in the transcription. Defaults to `['segment']`. Possible values are `['word']`, `['segment']`, and `['word', 'segment']`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.

-   **language** *string* The language of the input audio. Supplying the input language in ISO-639-1 format (e.g. 'en') will improve accuracy and latency. Optional.

-   **prompt** *string* An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. Optional.

-   **temperature** *number* The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. Defaults to 0. Optional.

-   **include** *string\[\]* Additional information to include in the transcription response.



### [Model Capabilities](#model-capabilities-1)


Model

Transcription

Duration

Segments

Language

`whisper-1`

`gpt-4o-mini-transcribe`

`gpt-4o-transcribe`
```

### 422. `providers/ai-sdk-providers/cerebras.md`

```markdown
# Cerebras Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/cerebras
description: Learn how to use Cerebras's models with the AI SDK.
---


# [Cerebras Provider](#cerebras-provider)


The [Cerebras](https://cerebras.ai) provider offers access to powerful language models through the Cerebras API, including their high-speed inference capabilities powered by Wafer-Scale Engines and CS-3 systems.

API keys can be obtained from the [Cerebras Platform](https://cloud.cerebras.ai).


## [Setup](#setup)


The Cerebras provider is available via the `@ai-sdk/cerebras` module. You can install it with:

pnpm

npm

yarn

pnpm add @ai-sdk/cerebras


## [Provider Instance](#provider-instance)


You can import the default provider instance `cerebras` from `@ai-sdk/cerebras`:

```
import{ cerebras }from'@ai-sdk/cerebras';
```

For custom configuration, you can import `createCerebras` and create a provider instance with your settings:

```
import{ createCerebras }from'@ai-sdk/cerebras';const cerebras =createCerebras({  apiKey: process.env.CEREBRAS_API_KEY??'',});
```

You can use the following optional settings to customize the Cerebras provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls. The default prefix is `https://api.cerebras.ai/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `CEREBRAS_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.



## [Language Models](#language-models)


You can create language models using a provider instance:

```
import{ cerebras }from'@ai-sdk/cerebras';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:cerebras('llama3.1-8b'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Cerebras language models can be used in the `streamText` function (see [AI SDK Core](/docs/ai-sdk-core)).


## [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`llama3.1-8b`

`llama3.1-70b`

`llama3.3-70b`

Please see the [Cerebras docs](https://inference-docs.cerebras.ai/introduction) for more details about the available models. Note that context windows are temporarily limited to 8192 tokens in the Free Tier.
```

### 423. `providers/ai-sdk-providers/cohere.md`

```markdown
# Cohere Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/cohere
description: Learn how to use the Cohere provider for the AI SDK.
---


# [Cohere Provider](#cohere-provider)


The [Cohere](https://cohere.com/) provider contains language and embedding model support for the Cohere chat API.


## [Setup](#setup)


The Cohere provider is available in the `@ai-sdk/cohere` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/cohere


## [Provider Instance](#provider-instance)


You can import the default provider instance `cohere` from `@ai-sdk/cohere`:

```
import{ cohere }from'@ai-sdk/cohere';
```

If you need a customized setup, you can import `createCohere` from `@ai-sdk/cohere` and create a provider instance with your settings:

```
import{ createCohere }from'@ai-sdk/cohere';const cohere =createCohere({// custom settings});
```

You can use the following optional settings to customize the Cohere provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.cohere.com/v2`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `COHERE_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create models that call the [Cohere chat API](https://docs.cohere.com/v2/docs/chat-api) using a provider instance. The first argument is the model id, e.g. `command-r-plus`. Some Cohere chat models support tool calls.

```
const model =cohere('command-r-plus');
```


### [Example](#example)


You can use Cohere language models to generate text with the `generateText` function:

```
import{ cohere }from'@ai-sdk/cohere';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:cohere('command-r-plus'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Cohere language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core).


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`command-a-03-2025`

`command-r-plus`

`command-r`

`command-a-03-2025`

`command`

`command-light`

The table above lists popular models. Please see the [Cohere docs](https://docs.cohere.com/v2/docs/models#command) for a full list of available models. You can also pass any available provider model ID as a string if needed.


## [Embedding Models](#embedding-models)


You can create models that call the [Cohere embed API](https://docs.cohere.com/v2/reference/embed) using the `.embedding()` factory method.

```
const model = cohere.embedding('embed-english-v3.0');
```

Cohere embedding models support additional settings. You can pass them as an options argument:

```
const model = cohere.embedding('embed-english-v3.0',{  inputType:'search_document',});
```

The following optional settings are available for Cohere embedding models:

-   **inputType** *'search\_document' | 'search\_query' | 'classification' | 'clustering'*

    Specifies the type of input passed to the model. Default is `search_query`.

    -   `search_document`: Used for embeddings stored in a vector database for search use-cases.
    -   `search_query`: Used for embeddings of search queries run against a vector DB to find relevant documents.
    -   `classification`: Used for embeddings passed through a text classifier.
    -   `clustering`: Used for embeddings run through a clustering algorithm.
-   **truncate** *'NONE' | 'START' | 'END'*

    Specifies how the API will handle inputs longer than the maximum token length. Default is `END`.

    -   `NONE`: If selected, when the input exceeds the maximum input token length will return an error.
    -   `START`: Will discard the start of the input until the remaining input is exactly the maximum input token length for the model.
    -   `END`: Will discard the end of the input until the remaining input is exactly the maximum input token length for the model.


### [Model Capabilities](#model-capabilities-1)


Model

Embedding Dimensions

`embed-english-v3.0`

1024

`embed-multilingual-v3.0`

1024

`embed-english-light-v3.0`

384

`embed-multilingual-light-v3.0`

384

`embed-english-v2.0`

4096

`embed-english-light-v2.0`

1024

`embed-multilingual-v2.0`

768
```

### 424. `providers/ai-sdk-providers/deepgram.md`

```markdown
# Deepgram Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/deepgram
description: Learn how to use the Deepgram provider for the AI SDK.
---


# [Deepgram Provider](#deepgram-provider)


The [Deepgram](https://deepgram.com/) provider contains language model support for the Deepgram transcription API.


## [Setup](#setup)


The Deepgram provider is available in the `@ai-sdk/deepgram` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/deepgram


## [Provider Instance](#provider-instance)


You can import the default provider instance `deepgram` from `@ai-sdk/deepgram`:

```
import{ deepgram }from'@ai-sdk/deepgram';
```

If you need a customized setup, you can import `createDeepgram` from `@ai-sdk/deepgram` and create a provider instance with your settings:

```
import{ createDeepgram }from'@ai-sdk/deepgram';const deepgram =createDeepgram({// custom settings, e.g.  fetch: customFetch,});
```

You can use the following optional settings to customize the Deepgram provider instance:

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `DEEPGRAM_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Transcription Models](#transcription-models)


You can create models that call the [Deepgram transcription API](https://developers.deepgram.com/docs/pre-recorded-audio) using the `.transcription()` factory method.

The first argument is the model id e.g. `nova-3`.

```
const model = deepgram.transcription('nova-3');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the `summarize` option will enable summaries for sections of content.

```
import{ experimental_transcribe as transcribe }from'ai';import{ deepgram }from'@ai-sdk/deepgram';import{ readFile }from'fs/promises';const result =awaittranscribe({  model: deepgram.transcription('nova-3'),  audio:awaitreadFile('audio.mp3'),  providerOptions:{ deepgram:{ summarize:true}},});
```

The following provider options are available:

-   **language** *string*

    Language code for the audio. Supports numerous ISO-639-1 and ISO-639-3 language codes. Optional.

-   **smartFormat** *boolean*

    Whether to apply smart formatting to the transcription. Optional.

-   **punctuate** *boolean*

    Whether to add punctuation to the transcription. Optional.

-   **paragraphs** *boolean*

    Whether to format the transcription into paragraphs. Optional.

-   **summarize** *enum | boolean*

    Whether to generate a summary of the transcription. Allowed values: `'v2'`, `false`. Optional.

-   **topics** *boolean*

    Whether to detect topics in the transcription. Optional.

-   **intents** *boolean*

    Whether to detect intents in the transcription. Optional.

-   **sentiment** *boolean*

    Whether to perform sentiment analysis on the transcription. Optional.

-   **detectEntities** *boolean*

    Whether to detect entities in the transcription. Optional.

-   **redact** *string | array of strings*

    Specifies what content to redact from the transcription. Optional.

-   **replace** *string*

    Replacement string for redacted content. Optional.

-   **search** *string*

    Search term to find in the transcription. Optional.

-   **keyterm** *string*

    Key terms to identify in the transcription. Optional.

-   **diarize** *boolean*

    Whether to identify different speakers in the transcription. Defaults to `true`. Optional.

-   **utterances** *boolean*

    Whether to segment the transcription into utterances. Optional.

-   **uttSplit** *number*

    Threshold for splitting utterances. Optional.

-   **fillerWords** *boolean*

    Whether to include filler words (um, uh, etc.) in the transcription. Optional.



### [Model Capabilities](#model-capabilities)


Model

Transcription

Duration

Segments

Language

`nova-3` (+ [variants](https://developers.deepgram.com/docs/models-languages-overview#nova-3))

`nova-2` (+ [variants](https://developers.deepgram.com/docs/models-languages-overview#nova-2))

`nova` (+ [variants](https://developers.deepgram.com/docs/models-languages-overview#nova))

`enhanced` (+ [variants](https://developers.deepgram.com/docs/models-languages-overview#enhanced))

`base` (+ [variants](https://developers.deepgram.com/docs/models-languages-overview#base))
```

### 425. `providers/ai-sdk-providers/deepinfra.md`

```markdown
# DeepInfra Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra
description: Learn how to use DeepInfra's models with the AI SDK.
---


# [DeepInfra Provider](#deepinfra-provider)


The [DeepInfra](https://deepinfra.com) provider contains support for state-of-the-art models through the DeepInfra API, including Llama 3, Mixtral, Qwen, and many other popular open-source models.


## [Setup](#setup)


The DeepInfra provider is available via the `@ai-sdk/deepinfra` module. You can install it with:

pnpm

npm

yarn

pnpm add @ai-sdk/deepinfra


## [Provider Instance](#provider-instance)


You can import the default provider instance `deepinfra` from `@ai-sdk/deepinfra`:

```
import{ deepinfra }from'@ai-sdk/deepinfra';
```

If you need a customized setup, you can import `createDeepInfra` from `@ai-sdk/deepinfra` and create a provider instance with your settings:

```
import{ createDeepInfra }from'@ai-sdk/deepinfra';const deepinfra =createDeepInfra({  apiKey: process.env.DEEPINFRA_API_KEY??'',});
```

You can use the following optional settings to customize the DeepInfra provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.deepinfra.com/v1/openai`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `DEEPINFRA_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create language models using a provider instance. The first argument is the model ID, for example:

```
import{ deepinfra }from'@ai-sdk/deepinfra';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:deepinfra('meta-llama/Meta-Llama-3.1-70B-Instruct'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

DeepInfra language models can also be used in the `streamText` function (see [AI SDK Core](/docs/ai-sdk-core)).


## [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8`

`meta-llama/Llama-4-Scout-17B-16E-Instruct`

`meta-llama/Llama-3.3-70B-Instruct-Turbo`

`meta-llama/Llama-3.3-70B-Instruct`

`meta-llama/Meta-Llama-3.1-405B-Instruct`

`meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo`

`meta-llama/Meta-Llama-3.1-70B-Instruct`

`meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo`

`meta-llama/Meta-Llama-3.1-8B-Instruct`

`meta-llama/Llama-3.2-11B-Vision-Instruct`

`meta-llama/Llama-3.2-90B-Vision-Instruct`

`mistralai/Mixtral-8x7B-Instruct-v0.1`

`deepseek-ai/DeepSeek-V3`

`deepseek-ai/DeepSeek-R1`

`deepseek-ai/DeepSeek-R1-Distill-Llama-70B`

`deepseek-ai/DeepSeek-R1-Turbo`

`nvidia/Llama-3.1-Nemotron-70B-Instruct`

`Qwen/Qwen2-7B-Instruct`

`Qwen/Qwen2.5-72B-Instruct`

`Qwen/Qwen2.5-Coder-32B-Instruct`

`Qwen/QwQ-32B-Preview`

`google/codegemma-7b-it`

`google/gemma-2-9b-it`

`microsoft/WizardLM-2-8x22B`

The table above lists popular models. Please see the [DeepInfra docs](https://deepinfra.com) for a full list of available models. You can also pass any available provider model ID as a string if needed.


## [Image Models](#image-models)


You can create DeepInfra image models using the `.image()` factory method. For more on image generation with the AI SDK see [generateImage()](/docs/reference/ai-sdk-core/generate-image).

```
import{ deepinfra }from'@ai-sdk/deepinfra';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: deepinfra.image('stabilityai/sd3.5'),  prompt:'A futuristic cityscape at sunset',  aspectRatio:'16:9',});
```

Model support for `size` and `aspectRatio` parameters varies by model. Please check the individual model documentation on [DeepInfra's models page](https://deepinfra.com/models/text-to-image) for supported options and additional parameters.


### [Model-specific options](#model-specific-options)


You can pass model-specific parameters using the `providerOptions.deepinfra` field:

```
import{ deepinfra }from'@ai-sdk/deepinfra';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: deepinfra.image('stabilityai/sd3.5'),  prompt:'A futuristic cityscape at sunset',  aspectRatio:'16:9',  providerOptions:{    deepinfra:{      num_inference_steps:30,// Control the number of denoising steps (1-50)},},});
```


### [Model Capabilities](#model-capabilities-1)


For models supporting aspect ratios, the following ratios are typically supported: `1:1 (default), 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21`

For models supporting size parameters, dimensions must typically be:

-   Multiples of 32
-   Width and height between 256 and 1440 pixels
-   Default size is 1024x1024

Model

Dimensions Specification

Notes

`stabilityai/sd3.5`

Aspect Ratio

Premium quality base model, 8B parameters

`black-forest-labs/FLUX-1.1-pro`

Size

Latest state-of-art model with superior prompt following

`black-forest-labs/FLUX-1-schnell`

Size

Fast generation in 1-4 steps

`black-forest-labs/FLUX-1-dev`

Size

Optimized for anatomical accuracy

`black-forest-labs/FLUX-pro`

Size

Flagship Flux model

`stabilityai/sd3.5-medium`

Aspect Ratio

Balanced 2.5B parameter model

`stabilityai/sdxl-turbo`

Aspect Ratio

Optimized for fast generation

For more details and pricing information, see the [DeepInfra text-to-image models page](https://deepinfra.com/models/text-to-image).
```

### 426. `providers/ai-sdk-providers/deepseek.md`

```markdown
# DeepSeek Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/deepseek
description: Learn how to use DeepSeek's models with the AI SDK.
---


# [DeepSeek Provider](#deepseek-provider)


The [DeepSeek](https://www.deepseek.com) provider offers access to powerful language models through the DeepSeek API, including their [DeepSeek-V3 model](https://github.com/deepseek-ai/DeepSeek-V3).

API keys can be obtained from the [DeepSeek Platform](https://platform.deepseek.com/api_keys).


## [Setup](#setup)


The DeepSeek provider is available via the `@ai-sdk/deepseek` module. You can install it with:

pnpm

npm

yarn

pnpm add @ai-sdk/deepseek


## [Provider Instance](#provider-instance)


You can import the default provider instance `deepseek` from `@ai-sdk/deepseek`:

```
import{ deepseek }from'@ai-sdk/deepseek';
```

For custom configuration, you can import `createDeepSeek` and create a provider instance with your settings:

```
import{ createDeepSeek }from'@ai-sdk/deepseek';const deepseek =createDeepSeek({  apiKey: process.env.DEEPSEEK_API_KEY??'',});
```

You can use the following optional settings to customize the DeepSeek provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls. The default prefix is `https://api.deepseek.com/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `DEEPSEEK_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.



## [Language Models](#language-models)


You can create language models using a provider instance:

```
import{ deepseek }from'@ai-sdk/deepseek';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:deepseek('deepseek-chat'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

DeepSeek language models can be used in the `streamText` function (see [AI SDK Core](/docs/ai-sdk-core)).


### [Reasoning](#reasoning)


DeepSeek has reasoning support for the `deepseek-reasoner` model:

```
import{ deepseek }from'@ai-sdk/deepseek';import{ generateText }from'ai';const{ text, reasoning }=awaitgenerateText({  model:deepseek('deepseek-reasoner'),  prompt:'How many people will live in the world in 2040?',});console.log(reasoning);console.log(text);
```

See [AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details on how to integrate reasoning into your chatbot.


### [Cache Token Usage](#cache-token-usage)


DeepSeek provides context caching on disk technology that can significantly reduce token costs for repeated content. You can access the cache hit/miss metrics through the `providerMetadata` property in the response:

```
import{ deepseek }from'@ai-sdk/deepseek';import{ generateText }from'ai';const result =awaitgenerateText({  model:deepseek('deepseek-chat'),  prompt:'Your prompt here',});console.log(result.providerMetadata);// Example output: { deepseek: { promptCacheHitTokens: 1856, promptCacheMissTokens: 5 } }
```

The metrics include:

-   `promptCacheHitTokens`: Number of input tokens that were cached
-   `promptCacheMissTokens`: Number of input tokens that were not cached

For more details about DeepSeek's caching system, see the [DeepSeek caching documentation](https://api-docs.deepseek.com/guides/kv_cache#checking-cache-hit-status).


## [Model Capabilities](#model-capabilities)


Model

Text Generation

Object Generation

Image Input

Tool Usage

Tool Streaming

`deepseek-chat`

`deepseek-reasoner`

Please see the [DeepSeek docs](https://api-docs.deepseek.com) for a full list of available models. You can also pass any available provider model ID as a string if needed.
```

### 427. `providers/ai-sdk-providers/elevenlabs.md`

```markdown
# ElevenLabs Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs
description: Learn how to use the ElevenLabs provider for the AI SDK.
---


# [ElevenLabs Provider](#elevenlabs-provider)


The [ElevenLabs](https://elevenlabs.io/) provider contains language model support for the ElevenLabs transcription API.


## [Setup](#setup)


The ElevenLabs provider is available in the `@ai-sdk/elevenlabs` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/elevenlabs


## [Provider Instance](#provider-instance)


You can import the default provider instance `elevenlabs` from `@ai-sdk/elevenlabs`:

```
import{ elevenlabs }from'@ai-sdk/elevenlabs';
```

If you need a customized setup, you can import `createElevenLabs` from `@ai-sdk/elevenlabs` and create a provider instance with your settings:

```
import{ createElevenLabs }from'@ai-sdk/elevenlabs';const elevenlabs =createElevenLabs({// custom settings, e.g.  fetch: customFetch,});
```

You can use the following optional settings to customize the ElevenLabs provider instance:

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `ELEVENLABS_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Transcription Models](#transcription-models)


You can create models that call the [ElevenLabs transcription API](https://elevenlabs.io/speech-to-text) using the `.transcription()` factory method.

The first argument is the model id e.g. `scribe_v1`.

```
const model = elevenlabs.transcription('scribe_v1');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format can sometimes improve transcription performance if known beforehand.

```
import{ experimental_transcribe as transcribe }from'ai';import{ elevenlabs }from'@ai-sdk/elevenlabs';const result =awaittranscribe({  model: elevenlabs.transcription('scribe_v1'),  audio:newUint8Array([1,2,3,4]),  providerOptions:{ elevenlabs:{ languageCode:'en'}},});
```

The following provider options are available:

-   **languageCode** *string*

    An ISO-639-1 or ISO-639-3 language code corresponding to the language of the audio file. Can sometimes improve transcription performance if known beforehand. Defaults to `null`, in which case the language is predicted automatically.

-   **tagAudioEvents** *boolean*

    Whether to tag audio events like (laughter), (footsteps), etc. in the transcription. Defaults to `true`.

-   **numSpeakers** *integer*

    The maximum amount of speakers talking in the uploaded file. Can help with predicting who speaks when. The maximum amount of speakers that can be predicted is 32. Defaults to `null`, in which case the amount of speakers is set to the maximum value the model supports.

-   **timestampsGranularity** *enum*

    The granularity of the timestamps in the transcription. Defaults to `'word'`. Allowed values: `'none'`, `'word'`, `'character'`.

-   **diarize** *boolean*

    Whether to annotate which speaker is currently talking in the uploaded file. Defaults to `true`.

-   **fileFormat** *enum*

    The format of input audio. Defaults to `'other'`. Allowed values: `'pcm_s16le_16'`, `'other'`. For `'pcm_s16le_16'`, the input audio must be 16-bit PCM at a 16kHz sample rate, single channel (mono), and little-endian byte order. Latency will be lower than with passing an encoded waveform.



### [Model Capabilities](#model-capabilities)


Model

Transcription

Duration

Segments

Language

`scribe_v1`

`scribe_v1_experimental`
```

### 428. `providers/ai-sdk-providers/fal.md`

```markdown
# Fal Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/fal
description: Learn how to use Fal AI models with the AI SDK.
---


# [Fal Provider](#fal-provider)


[Fal AI](https://fal.ai/) provides a generative media platform for developers with lightning-fast inference capabilities. Their platform offers optimized performance for running diffusion models, with speeds up to 4x faster than alternatives.


## [Setup](#setup)


The Fal provider is available via the `@ai-sdk/fal` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/fal


## [Provider Instance](#provider-instance)


You can import the default provider instance `fal` from `@ai-sdk/fal`:

```
import{ fal }from'@ai-sdk/fal';
```

If you need a customized setup, you can import `createFal` and create a provider instance with your settings:

```
import{ createFal }from'@ai-sdk/fal';const fal =createFal({  apiKey:'your-api-key',// optional, defaults to FAL_API_KEY environment variable, falling back to FAL_KEY  baseURL:'custom-url',// optional  headers:{/* custom headers */},// optional});
```

You can use the following optional settings to customize the Fal provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://fal.run`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `FAL_API_KEY` environment variable, falling back to `FAL_KEY`.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Image Models](#image-models)


You can create Fal image models using the `.image()` factory method. For more on image generation with the AI SDK see [generateImage()](/docs/reference/ai-sdk-core/generate-image).


### [Basic Usage](#basic-usage)


```
import{ fal }from'@ai-sdk/fal';import{ experimental_generateImage as generateImage }from'ai';importfsfrom'fs';const{ image }=awaitgenerateImage({  model: fal.image('fal-ai/fast-sdxl'),  prompt:'A serene mountain landscape at sunset',});const filename =`image-${Date.now()}.png`;fs.writeFileSync(filename, image.uint8Array);console.log(`Image saved to ${filename}`);
```


### [Model Capabilities](#model-capabilities)


Fal offers many models optimized for different use cases. Here are a few popular examples. For a full list of models, see the [Fal AI documentation](https://fal.ai/models).

Model

Description

`fal-ai/fast-sdxl`

High-speed SDXL model optimized for quick inference with up to 4x faster speeds

`fal-ai/flux-pro/kontext`

FLUX.1 Kontext \[pro\] handles both text and reference images as inputs, seamlessly enabling targeted, local edits and complex transformations of entire scenes

`fal-ai/flux-pro/kontext/max`

FLUX.1 Kontext \[max\] with greatly improved prompt adherence and typography generation, meeting premium consistency for editing without compromise on speed

`fal-ai/flux-lora`

Super fast endpoint for the FLUX.1 \[dev\] model with LoRA support, enabling rapid and high-quality image generation using pre-trained LoRA adaptations.

`fal-ai/flux-pro/v1.1-ultra`

Professional-grade image generation with up to 2K resolution and enhanced photorealism

`fal-ai/ideogram/v2`

Specialized for high-quality posters and logos with exceptional typography handling

`fal-ai/recraft-v3`

SOTA in image generation with vector art and brand style capabilities

`fal-ai/stable-diffusion-3.5-large`

Advanced MMDiT model with improved typography and complex prompt understanding

`fal-ai/hyper-sdxl`

Performance-optimized SDXL variant with enhanced creative capabilities

Fal models support the following aspect ratios:

-   1:1 (square HD)
-   16:9 (landscape)
-   9:16 (portrait)
-   4:3 (landscape)
-   3:4 (portrait)
-   16:10 (1280x800)
-   10:16 (800x1280)
-   21:9 (2560x1080)
-   9:21 (1080x2560)

Key features of Fal models include:

-   Up to 4x faster inference speeds compared to alternatives
-   Optimized by the Fal Inference Engine™
-   Support for real-time infrastructure
-   Cost-effective scaling with pay-per-use pricing
-   LoRA training capabilities for model personalization


#### [Modify Image](#modify-image)


Transform existing images using text prompts.

```
// Example: Modify existing imageawaitgenerateImage({  model: fal.image('fal-ai/flux-pro/kontext'),  prompt:'Put a donut next to the flour.',  providerOptions:{    fal:{      image_url:'https://v3.fal.media/files/rabbit/rmgBxhwGYb2d3pl3x9sKf_output.png',},},});
```


### [Advanced Features](#advanced-features)


Fal's platform offers several advanced capabilities:

-   **Private Model Inference**: Run your own diffusion transformer models with up to 50% faster inference
-   **LoRA Training**: Train and personalize models in under 5 minutes
-   **Real-time Infrastructure**: Enable new user experiences with fast inference times
-   **Scalable Architecture**: Scale to thousands of GPUs when needed

For more details about Fal's capabilities and features, visit the [Fal AI documentation](https://fal.ai/docs).


## [Transcription Models](#transcription-models)


You can create models that call the [Fal transcription API](https://docs.fal.ai/guides/convert-speech-to-text) using the `.transcription()` factory method.

The first argument is the model id without the `fal-ai/` prefix e.g. `wizper`.

```
const model = fal.transcription('wizper');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the `batchSize` option will increase the number of audio chunks processed in parallel.

```
import{ experimental_transcribe as transcribe }from'ai';import{ fal }from'@ai-sdk/fal';import{ readFile }from'fs/promises';const result =awaittranscribe({  model: fal.transcription('wizper'),  audio:awaitreadFile('audio.mp3'),  providerOptions:{ fal:{ batchSize:10}},});
```

The following provider options are available:

-   **language** *string* Language of the audio file. If set to null, the language will be automatically detected. Accepts ISO language codes like 'en', 'fr', 'zh', etc. Optional.

-   **diarize** *boolean* Whether to diarize the audio file (identify different speakers). Defaults to true. Optional.

-   **chunkLevel** *string* Level of the chunks to return. Either 'segment' or 'word'. Default value: "word" Optional.

-   **version** *string* Version of the model to use. All models are Whisper large variants. Default value: "3" Optional.

-   **batchSize** *number* Batch size for processing. Default value: 64 Optional.

-   **numSpeakers** *number* Number of speakers in the audio file. If not provided, the number of speakers will be automatically detected. Optional.



### [Model Capabilities](#model-capabilities-1)


Model

Transcription

Duration

Segments

Language

`whisper`

`wizper`
```

### 429. `providers/ai-sdk-providers/fireworks.md`

```markdown
# Fireworks Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/fireworks
description: Learn how to use Fireworks models with the AI SDK.
---


# [Fireworks Provider](#fireworks-provider)


[Fireworks](https://fireworks.ai/) is a platform for running and testing LLMs through their [API](https://readme.fireworks.ai/).


## [Setup](#setup)


The Fireworks provider is available via the `@ai-sdk/fireworks` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/fireworks


## [Provider Instance](#provider-instance)


You can import the default provider instance `fireworks` from `@ai-sdk/fireworks`:

```
import{ fireworks }from'@ai-sdk/fireworks';
```

If you need a customized setup, you can import `createFireworks` from `@ai-sdk/fireworks` and create a provider instance with your settings:

```
import{ createFireworks }from'@ai-sdk/fireworks';const fireworks =createFireworks({  apiKey: process.env.FIREWORKS_API_KEY??'',});
```

You can use the following optional settings to customize the Fireworks provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.fireworks.ai/inference/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `FIREWORKS_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.



## [Language Models](#language-models)


You can create [Fireworks models](https://fireworks.ai/models) using a provider instance. The first argument is the model id, e.g. `accounts/fireworks/models/firefunction-v1`:

```
const model =fireworks('accounts/fireworks/models/firefunction-v1');
```


### [Reasoning Models](#reasoning-models)


Fireworks exposes the thinking of `deepseek-r1` in the generated text using the `<think>` tag. You can use the `extractReasoningMiddleware` to extract this reasoning and expose it as a `reasoning` property on the result:

```
import{ fireworks }from'@ai-sdk/fireworks';import{ wrapLanguageModel, extractReasoningMiddleware }from'ai';const enhancedModel =wrapLanguageModel({  model:fireworks('accounts/fireworks/models/deepseek-r1'),  middleware:extractReasoningMiddleware({ tagName:'think'}),});
```

You can then use that enhanced model in functions like `generateText` and `streamText`.


### [Example](#example)


You can use Fireworks language models to generate text with the `generateText` function:

```
import{ fireworks }from'@ai-sdk/fireworks';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:fireworks('accounts/fireworks/models/firefunction-v1'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Fireworks language models can also be used in the `streamText` function (see [AI SDK Core](/docs/ai-sdk-core)).


### [Completion Models](#completion-models)


You can create models that call the Fireworks completions API using the `.completion()` factory method:

```
const model = fireworks.completion('accounts/fireworks/models/firefunction-v1');
```


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`accounts/fireworks/models/deepseek-r1`

`accounts/fireworks/models/deepseek-v3`

`accounts/fireworks/models/llama-v3p1-405b-instruct`

`accounts/fireworks/models/llama-v3p1-8b-instruct`

`accounts/fireworks/models/llama-v3p2-3b-instruct`

`accounts/fireworks/models/llama-v3p3-70b-instruct`

`accounts/fireworks/models/mixtral-8x7b-instruct-hf`

`accounts/fireworks/models/mixtral-8x22b-instruct`

`accounts/fireworks/models/qwen2p5-coder-32b-instruct`

`accounts/fireworks/models/llama-v3p2-11b-vision-instruct`

`accounts/fireworks/models/yi-large`

The table above lists popular models. Please see the [Fireworks models page](https://fireworks.ai/models) for a full list of available models.


## [Embedding Models](#embedding-models)


You can create models that call the Fireworks embeddings API using the `.textEmbeddingModel()` factory method:

```
const model = fireworks.textEmbeddingModel('accounts/fireworks/models/nomic-embed-text-v1',);
```


## [Image Models](#image-models)


You can create Fireworks image models using the `.image()` factory method. For more on image generation with the AI SDK see [generateImage()](/docs/reference/ai-sdk-core/generate-image).

```
import{ fireworks }from'@ai-sdk/fireworks';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: fireworks.image('accounts/fireworks/models/flux-1-dev-fp8'),  prompt:'A futuristic cityscape at sunset',  aspectRatio:'16:9',});
```

Model support for `size` and `aspectRatio` parameters varies. See the [Model Capabilities](#model-capabilities-1) section below for supported dimensions, or check the model's documentation on [Fireworks models page](https://fireworks.ai/models) for more details.


### [Model Capabilities](#model-capabilities-1)


For all models supporting aspect ratios, the following aspect ratios are supported:

`1:1 (default), 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9`

For all models supporting size, the following sizes are supported:

`640 x 1536, 768 x 1344, 832 x 1216, 896 x 1152, 1024x1024 (default), 1152 x 896, 1216 x 832, 1344 x 768, 1536 x 640`

Model

Dimensions Specification

`accounts/fireworks/models/flux-1-dev-fp8`

Aspect Ratio

`accounts/fireworks/models/flux-1-schnell-fp8`

Aspect Ratio

`accounts/fireworks/models/playground-v2-5-1024px-aesthetic`

Size

`accounts/fireworks/models/japanese-stable-diffusion-xl`

Size

`accounts/fireworks/models/playground-v2-1024px-aesthetic`

Size

`accounts/fireworks/models/SSD-1B`

Size

`accounts/fireworks/models/stable-diffusion-xl-1024-v1-0`

Size

For more details, see the [Fireworks models page](https://fireworks.ai/models).


#### [Stability AI Models](#stability-ai-models)


Fireworks also presents several Stability AI models backed by Stability AI API keys and endpoint. The AI SDK Fireworks provider does not currently include support for these models:

Model ID

`accounts/stability/models/sd3-turbo`

`accounts/stability/models/sd3-medium`

`accounts/stability/models/sd3`
```

### 430. `providers/ai-sdk-providers/gladia.md`

```markdown
# Gladia Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/gladia
description: Learn how to use the Gladia provider for the AI SDK.
---


# [Gladia Provider](#gladia-provider)


The [Gladia](https://gladia.io/) provider contains language model support for the Gladia transcription API.


## [Setup](#setup)


The Gladia provider is available in the `@ai-sdk/gladia` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/gladia


## [Provider Instance](#provider-instance)


You can import the default provider instance `gladia` from `@ai-sdk/gladia`:

```
import{ gladia }from'@ai-sdk/gladia';
```

If you need a customized setup, you can import `createGladia` from `@ai-sdk/gladia` and create a provider instance with your settings:

```
import{ createGladia }from'@ai-sdk/gladia';const gladia =createGladia({// custom settings, e.g.  fetch: customFetch,});
```

You can use the following optional settings to customize the Gladia provider instance:

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `DEEPGRAM_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Transcription Models](#transcription-models)


You can create models that call the [Gladia transcription API](https://docs.gladia.io/chapters/pre-recorded-stt/getting-started) using the `.transcription()` factory method.

```
const model = gladia.transcription();
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the `summarize` option will enable summaries for sections of content.

```
import{ experimental_transcribe as transcribe }from'ai';import{ gladia }from'@ai-sdk/gladia';import{ readFile }from'fs/promises';const result =awaittranscribe({  model: gladia.transcription(),  audio:awaitreadFile('audio.mp3'),  providerOptions:{ gladia:{ summarize:true}},});
```

Gladia does not have various models, so you can omit the standard `model` id parameter.

The following provider options are available:

-   **contextPrompt** *string*

    Context to feed the transcription model with for possible better accuracy. Optional.

-   **customVocabulary** *boolean | any\[\]*

    Custom vocabulary to improve transcription accuracy. Optional.

-   **customVocabularyConfig** *object*

    Configuration for custom vocabulary. Optional.

    -   **vocabulary** *Array<string | { value: string, intensity?: number, pronunciations?: string\[\], language?: string }>*
    -   **defaultIntensity** *number*
-   **detectLanguage** *boolean*

    Whether to automatically detect the language. Optional.

-   **enableCodeSwitching** *boolean*

    Enable code switching for multilingual audio. Optional.

-   **codeSwitchingConfig** *object*

    Configuration for code switching. Optional.

    -   **languages** *string\[\]*
-   **language** *string*

    Specify the language of the audio. Optional.

-   **callback** *boolean*

    Enable callback when transcription is complete. Optional.

-   **callbackConfig** *object*

    Configuration for callback. Optional.

    -   **url** *string*
    -   **method** *'POST' | 'PUT'*
-   **subtitles** *boolean*

    Generate subtitles from the transcription. Optional.

-   **subtitlesConfig** *object*

    Configuration for subtitles. Optional.

    -   **formats** *Array<'srt' | 'vtt'>*
    -   **minimumDuration** *number*
    -   **maximumDuration** *number*
    -   **maximumCharactersPerRow** *number*
    -   **maximumRowsPerCaption** *number*
    -   **style** *'default' | 'compliance'*
-   **diarization** *boolean*

    Enable speaker diarization. Defaults to `true`. Optional.

-   **diarizationConfig** *object*

    Configuration for diarization. Optional.

    -   **numberOfSpeakers** *number*
    -   **minSpeakers** *number*
    -   **maxSpeakers** *number*
    -   **enhanced** *boolean*
-   **translation** *boolean*

    Enable translation of the transcription. Optional.

-   **translationConfig** *object*

    Configuration for translation. Optional.

    -   **targetLanguages** *string\[\]*
    -   **model** *'base' | 'enhanced'*
    -   **matchOriginalUtterances** *boolean*
-   **summarization** *boolean*

    Enable summarization of the transcription. Optional.

-   **summarizationConfig** *object*

    Configuration for summarization. Optional.

    -   **type** *'general' | 'bullet\_points' | 'concise'*
-   **moderation** *boolean*

    Enable content moderation. Optional.

-   **namedEntityRecognition** *boolean*

    Enable named entity recognition. Optional.

-   **chapterization** *boolean*

    Enable chapterization of the transcription. Optional.

-   **nameConsistency** *boolean*

    Enable name consistency in the transcription. Optional.

-   **customSpelling** *boolean*

    Enable custom spelling. Optional.

-   **customSpellingConfig** *object*

    Configuration for custom spelling. Optional.

    -   **spellingDictionary** *Record<string, string\[\]>*
-   **structuredDataExtraction** *boolean*

    Enable structured data extraction. Optional.

-   **structuredDataExtractionConfig** *object*

    Configuration for structured data extraction. Optional.

    -   **classes** *string\[\]*
-   **sentimentAnalysis** *boolean*

    Enable sentiment analysis. Optional.

-   **audioToLlm** *boolean*

    Enable audio to LLM processing. Optional.

-   **audioToLlmConfig** *object*

    Configuration for audio to LLM. Optional.

    -   **prompts** *string\[\]*
-   **customMetadata** *Record<string, any>*

    Custom metadata to include with the request. Optional.

-   **sentences** *boolean*

    Enable sentence detection. Optional.

-   **displayMode** *boolean*

    Enable display mode. Optional.

-   **punctuationEnhanced** *boolean*

    Enable enhanced punctuation. Optional.



### [Model Capabilities](#model-capabilities)


Model

Transcription

Duration

Segments

Language

`Default`
```

### 431. `providers/ai-sdk-providers/google-generative-ai.md`

```markdown
# Google Generative AI Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai
description: Learn how to use Google Generative AI Provider.
---


# [Google Generative AI Provider](#google-generative-ai-provider)


The [Google Generative AI](https://ai.google/discover/generativeai/) provider contains language and embedding model support for the [Google Generative AI](https://ai.google.dev/api/rest) APIs.


## [Setup](#setup)


The Google provider is available in the `@ai-sdk/google` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/google


## [Provider Instance](#provider-instance)


You can import the default provider instance `google` from `@ai-sdk/google`:

```
import{ google }from'@ai-sdk/google';
```

If you need a customized setup, you can import `createGoogleGenerativeAI` from `@ai-sdk/google` and create a provider instance with your settings:

```
import{ createGoogleGenerativeAI }from'@ai-sdk/google';const google =createGoogleGenerativeAI({// custom settings});
```

You can use the following optional settings to customize the Google Generative AI provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://generativelanguage.googleapis.com/v1beta`.

-   **apiKey** *string*

    API key that is being sent using the `x-goog-api-key` header. It defaults to the `GOOGLE_GENERATIVE_AI_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create models that call the [Google Generative AI API](https://ai.google.dev/api/rest) using the provider instance. The first argument is the model id, e.g. `gemini-1.5-pro-latest`. The models support tool calls and some have multi-modal capabilities.

```
const model =google('gemini-1.5-pro-latest');
```

You can use fine-tuned models by prefixing the model id with `tunedModels/`, e.g. `tunedModels/my-model`.

Google Generative AI also supports some model specific settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model =google('gemini-1.5-pro-latest',{  safetySettings:[{ category:'HARM_CATEGORY_UNSPECIFIED', threshold:'BLOCK_LOW_AND_ABOVE'},],});
```

The following optional settings are available for Google Generative AI models:

-   **cachedContent** *string*

    Optional. The name of the cached content used as context to serve the prediction. Format: cachedContents/{cachedContent}

-   **structuredOutputs** *boolean*

    Optional. Enable structured output. Default is true.

    This is useful when the JSON Schema contains elements that are not supported by the OpenAPI schema version that Google Generative AI uses. You can use this to disable structured outputs if you need to.

    See [Troubleshooting: Schema Limitations](#schema-limitations) for more details.

-   **safetySettings** *Array<{ category: string; threshold: string }>*

    Optional. Safety settings for the model.

    -   **category** *string*

        The category of the safety setting. Can be one of the following:

        -   `HARM_CATEGORY_HATE_SPEECH`
        -   `HARM_CATEGORY_DANGEROUS_CONTENT`
        -   `HARM_CATEGORY_HARASSMENT`
        -   `HARM_CATEGORY_SEXUALLY_EXPLICIT`
    -   **threshold** *string*

        The threshold of the safety setting. Can be one of the following:

        -   `HARM_BLOCK_THRESHOLD_UNSPECIFIED`
        -   `BLOCK_LOW_AND_ABOVE`
        -   `BLOCK_MEDIUM_AND_ABOVE`
        -   `BLOCK_ONLY_HIGH`
        -   `BLOCK_NONE`

Further configuration can be done using Google Generative AI provider options. You can validate the provider options using the `GoogleGenerativeAIProviderOptions` type.

```
import{ google }from'@ai-sdk/google';import{GoogleGenerativeAIProviderOptions}from'@ai-sdk/google';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:google('gemini-1.5-pro-latest'),  providerOptions:{    google:{      responseModalities:['TEXT','IMAGE'],} satisfies GoogleGenerativeAIProviderOptions,},// ...});
```

Another example showing the use of provider options to specify the thinking budget for a Google Generative AI thinking model:

```
import{ google }from'@ai-sdk/google';import{GoogleGenerativeAIProviderOptions}from'@ai-sdk/google';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:google('gemini-2.5-flash-preview-04-17'),  providerOptions:{    google:{      thinkingConfig:{        thinkingBudget:2048,},} satisfies GoogleGenerativeAIProviderOptions,},// ...});
```

The following provider options are available:

-   **responseModalities** *string\[\]* The modalities to use for the response. The following modalities are supported: `TEXT`, `IMAGE`. When not defined or empty, the model defaults to returning only text.

-   **thinkingConfig** *{ thinkingBudget: number; }*

    Optional. Configuration for the model's thinking process. Only supported by specific [Google Generative AI models](https://ai.google.dev/gemini-api/docs/thinking).

    -   **thinkingBudget** *number*

        Optional. Gives the model guidance on the number of thinking tokens it can use when generating a response. Must be an integer in the range 0 to 24576. Setting it to 0 disables thinking. Budgets from 1 to 1024 tokens will be set to 1024. For more information see [Google Generative AI documentation](https://ai.google.dev/gemini-api/docs/thinking).


You can use Google Generative AI language models to generate text with the `generateText` function:

```
import{ google }from'@ai-sdk/google';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:google('gemini-1.5-pro-latest'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Google Generative AI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core)).


### [File Inputs](#file-inputs)


The Google Generative AI provider supports file inputs, e.g. PDF files.

```
import{ google }from'@ai-sdk/google';import{ generateText }from'ai';const result =awaitgenerateText({  model:google('gemini-1.5-flash'),  messages:[{      role:'user',      content:[{type:'text',          text:'What is an embedding model according to this document?',},{type:'file',          data: fs.readFileSync('./data/ai.pdf'),          mimeType:'application/pdf',},],},],});
```

The AI SDK will automatically download URLs if you pass them as data, except for `https://generativelanguage.googleapis.com/v1beta/files/`. You can use the Google Generative AI Files API to upload larger files to that location.

See [File Parts](/docs/foundations/prompts#file-parts) for details on how to use files in prompts.


### [Cached Content](#cached-content)


You can use Google Generative AI language models to cache content:

```
import{ google }from'@ai-sdk/google';import{GoogleAICacheManager}from'@google/generative-ai/server';import{ generateText }from'ai';const cacheManager =newGoogleAICacheManager(  process.env.GOOGLE_GENERATIVE_AI_API_KEY,);// As of August 23rd, 2024, these are the only models that support cachingtypeGoogleModelCacheableId=|'models/gemini-1.5-flash-001'|'models/gemini-1.5-pro-001';const model:GoogleModelCacheableId='models/gemini-1.5-pro-001';const{ name: cachedContent }=await cacheManager.create({  model,  contents:[{      role:'user',      parts:[{ text:'1000 Lasanga Recipes...'}],},],  ttlSeconds:60*5,});const{ text: veggieLasangaRecipe }=awaitgenerateText({  model:google(model,{ cachedContent }),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});const{ text: meatLasangaRecipe }=awaitgenerateText({  model:google(model,{ cachedContent }),  prompt:'Write a meat lasagna recipe for 12 people.',});
```


### [Search Grounding](#search-grounding)


With [search grounding](https://ai.google.dev/gemini-api/docs/grounding), the model has access to the latest information using Google search. Search grounding can be used to provide answers around current events:

```
import{ google }from'@ai-sdk/google';import{GoogleGenerativeAIProviderMetadata}from'@ai-sdk/google';import{ generateText }from'ai';const{ text, providerMetadata }=awaitgenerateText({  model:google('gemini-1.5-pro',{    useSearchGrounding:true,}),  prompt:'List the top 5 San Francisco news from the past week.'+'You must include the date of each article.',});// access the grounding metadata. Casting to the provider metadata type// is optional but provides autocomplete and type safety.const metadata = providerMetadata?.google as|GoogleGenerativeAIProviderMetadata|undefined;const groundingMetadata = metadata?.groundingMetadata;const safetyRatings = metadata?.safetyRatings;
```

The grounding metadata includes detailed information about how search results were used to ground the model's response. Here are the available fields:

-   **`webSearchQueries`** (`string[] | null`)

    -   Array of search queries used to retrieve information
    -   Example: `["What's the weather in Chicago this weekend?"]`
-   **`searchEntryPoint`** (`{ renderedContent: string } | null`)

    -   Contains the main search result content used as an entry point
    -   The `renderedContent` field contains the formatted content
-   **`groundingSupports`** (Array of support objects | null)

    -   Contains details about how specific response parts are supported by search results
    -   Each support object includes:
        -   **`segment`**: Information about the grounded text segment
            -   `text`: The actual text segment
            -   `startIndex`: Starting position in the response
            -   `endIndex`: Ending position in the response
        -   **`groundingChunkIndices`**: References to supporting search result chunks
        -   **`confidenceScores`**: Confidence scores (0-1) for each supporting chunk

Example response:

```
{"groundingMetadata":{"webSearchQueries":["What's the weather in Chicago this weekend?"],"searchEntryPoint":{"renderedContent":"..."},"groundingSupports":[{"segment":{"startIndex":0,"endIndex":65,"text":"Chicago weather changes rapidly, so layers let you adjust easily."},"groundingChunkIndices":[0],"confidenceScores":[0.99]}]}}
```


#### [Dynamic Retrieval](#dynamic-retrieval)


With [dynamic retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-with-google-search#dynamic-retrieval), you can configure how the model decides when to turn on Grounding with Google Search. This gives you more control over when and how the model grounds its responses.

```
import{ google }from'@ai-sdk/google';import{ generateText }from'ai';const{ text, providerMetadata }=awaitgenerateText({  model:google('gemini-1.5-flash',{    useSearchGrounding:true,    dynamicRetrievalConfig:{      mode:'MODE_DYNAMIC',      dynamicThreshold:0.8,},}),  prompt:'Who won the latest F1 grand prix?',});
```

The `dynamicRetrievalConfig` describes the options to customize dynamic retrieval:

-   `mode`: The mode of the predictor to be used in dynamic retrieval. The following modes are supported:

    -   `MODE_DYNAMIC`: Run retrieval only when system decides it is necessary
    -   `MODE_UNSPECIFIED`: Always trigger retrieval
-   `dynamicThreshold`: The threshold to be used in dynamic retrieval (if not set, a system default value is used).


Dynamic retrieval is only available with Gemini 1.5 Flash models and is not supported with 8B variants.


### [Sources](#sources)


When you use [Search Grounding](#search-grounding), the model will include sources in the response. You can access them using the `sources` property of the result:

```
import{ google }from'@ai-sdk/google';import{ generateText }from'ai';const{ sources }=awaitgenerateText({  model:google('gemini-2.0-flash-exp',{ useSearchGrounding:true}),  prompt:'List the top 5 San Francisco news from the past week.',});
```


### [Image Outputs](#image-outputs)


The model `gemini-2.0-flash-exp` supports image generation. Images are exposed as files in the response. You need to enable image output in the provider options using the `responseModalities` option.

```
import{ google }from'@ai-sdk/google';import{ generateText }from'ai';const result =awaitgenerateText({  model:google('gemini-2.0-flash-exp'),  providerOptions:{    google:{ responseModalities:['TEXT','IMAGE']},},  prompt:'Generate an image of a comic cat',});for(const file of result.files){if(file.mimeType.startsWith('image/')){// show the image}}
```


### [Safety Ratings](#safety-ratings)


The safety ratings provide insight into the safety of the model's response. See [Google AI documentation on safety settings](https://ai.google.dev/gemini-api/docs/safety-settings).

Example response excerpt:

```
{"safetyRatings":[{"category":"HARM_CATEGORY_HATE_SPEECH","probability":"NEGLIGIBLE","probabilityScore":0.11027937,"severity":"HARM_SEVERITY_LOW","severityScore":0.28487435},{"category":"HARM_CATEGORY_DANGEROUS_CONTENT","probability":"HIGH","blocked":true,"probabilityScore":0.95422274,"severity":"HARM_SEVERITY_MEDIUM","severityScore":0.43398145},{"category":"HARM_CATEGORY_HARASSMENT","probability":"NEGLIGIBLE","probabilityScore":0.11085559,"severity":"HARM_SEVERITY_NEGLIGIBLE","severityScore":0.19027223},{"category":"HARM_CATEGORY_SEXUALLY_EXPLICIT","probability":"NEGLIGIBLE","probabilityScore":0.22901751,"severity":"HARM_SEVERITY_NEGLIGIBLE","severityScore":0.09089675}]}
```


### [Troubleshooting](#troubleshooting)



#### [Schema Limitations](#schema-limitations)


The Google Generative AI API uses a subset of the OpenAPI 3.0 schema, which does not support features such as unions. The errors that you get in this case look like this:

`GenerateContentRequest.generation_config.response_schema.properties[occupation].type: must be specified`

By default, structured outputs are enabled (and for tool calling they are required). You can disable structured outputs for object generation as a workaround:

```
const result =awaitgenerateObject({  model:google('gemini-1.5-pro-latest',{    structuredOutputs:false,}),  schema: z.object({    name: z.string(),    age: z.number(),    contact: z.union([      z.object({type: z.literal('email'),        value: z.string(),}),      z.object({type: z.literal('phone'),        value: z.string(),}),]),}),  prompt:'Generate an example person for testing.',});
```

The following Zod features are known to not work with Google Generative AI:

-   `z.union`
-   `z.record`


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`gemini-2.5-pro-preview-05-06`

`gemini-2.5-flash-preview-04-17`

`gemini-2.5-pro-exp-03-25`

`gemini-2.0-flash`

`gemini-1.5-pro`

`gemini-1.5-pro-latest`

`gemini-1.5-flash`

`gemini-1.5-flash-latest`

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-latest`

The table above lists popular models. Please see the [Google Generative AI docs](https://ai.google.dev/gemini-api/docs/models/gemini) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.


## [Embedding Models](#embedding-models)


You can create models that call the [Google Generative AI embeddings API](https://ai.google.dev/api/embeddings) using the `.textEmbeddingModel()` factory method.

```
const model = google.textEmbeddingModel('text-embedding-004');
```

Google Generative AI embedding models support aditional settings. You can pass them as an options argument:

```
const model = google.textEmbeddingModel('text-embedding-004',{  outputDimensionality:512,// optional, number of dimensions for the embedding  taskType:'SEMANTIC_SIMILARITY',// optional, specifies the task type for generating embeddings});
```

The following optional settings are available for Google Generative AI embedding models:

-   **outputDimensionality**: *number*

    Optional reduced dimension for the output embedding. If set, excessive values in the output embedding are truncated from the end.

-   **taskType**: *string*

    Optional. Specifies the task type for generating embeddings. Supported task types include:

    -   `SEMANTIC_SIMILARITY`: Optimized for text similarity.
    -   `CLASSIFICATION`: Optimized for text classification.
    -   `CLUSTERING`: Optimized for clustering texts based on similarity.
    -   `RETRIEVAL_DOCUMENT`: Optimized for document retrieval.
    -   `RETRIEVAL_QUERY`: Optimized for query-based retrieval.
    -   `QUESTION_ANSWERING`: Optimized for answering questions.
    -   `FACT_VERIFICATION`: Optimized for verifying factual information.
    -   `CODE_RETRIEVAL_QUERY`: Optimized for retrieving code blocks based on natural language queries.


### [Model Capabilities](#model-capabilities-1)


Model

Default Dimensions

Custom Dimensions

`text-embedding-004`

768
```

### 432. `providers/ai-sdk-providers/google-vertex.md`

```markdown
# Google Vertex Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex
description: Learn how to use the Google Vertex AI provider.
---


# [Google Vertex Provider](#google-vertex-provider)


The Google Vertex provider for the [AI SDK](/docs) contains language model support for the [Google Vertex AI](https://cloud.google.com/vertex-ai) APIs. This includes support for [Google's Gemini models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) and [Anthropic's Claude partner models](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude).

The Google Vertex provider is compatible with both Node.js and Edge runtimes. The Edge runtime is supported through the `@ai-sdk/google-vertex/edge` sub-module. More details can be found in the [Google Vertex Edge Runtime](#google-vertex-edge-runtime) and [Google Vertex Anthropic Edge Runtime](#google-vertex-anthropic-edge-runtime) sections below.


## [Setup](#setup)


The Google Vertex and Google Vertex Anthropic providers are both available in the `@ai-sdk/google-vertex` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/google-vertex


## [Google Vertex Provider Usage](#google-vertex-provider-usage)


The Google Vertex provider instance is used to create model instances that call the Vertex AI API. The models available with this provider include [Google's Gemini models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models). If you're looking to use [Anthropic's Claude models](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude), see the [Google Vertex Anthropic Provider](#google-vertex-anthropic-provider-usage) section below.


### [Provider Instance](#provider-instance)


You can import the default provider instance `vertex` from `@ai-sdk/google-vertex`:

```
import{ vertex }from'@ai-sdk/google-vertex';
```

If you need a customized setup, you can import `createVertex` from `@ai-sdk/google-vertex` and create a provider instance with your settings:

```
import{ createVertex }from'@ai-sdk/google-vertex';const vertex =createVertex({  project:'my-project',// optionallocation:'us-central1',// optional});
```

Google Vertex supports two different authentication implementations depending on your runtime environment.


#### [Node.js Runtime](#nodejs-runtime)


The Node.js runtime is the default runtime supported by the AI SDK. It supports all standard Google Cloud authentication options through the [`google-auth-library`](https://github.com/googleapis/google-auth-library-nodejs?tab=readme-ov-file#ways-to-authenticate). Typical use involves setting a path to a json credentials file in the `GOOGLE_APPLICATION_CREDENTIALS` environment variable. The credentials file can be obtained from the [Google Cloud Console](https://console.cloud.google.com/apis/credentials).

If you want to customize the Google authentication options you can pass them as options to the `createVertex` function, for example:

```
import{ createVertex }from'@ai-sdk/google-vertex';const vertex =createVertex({  googleAuthOptions:{    credentials:{      client_email:'my-email',      private_key:'my-private-key',},},});
```


##### [Optional Provider Settings](#optional-provider-settings)


You can use the following optional settings to customize the provider instance:

-   **project** *string*

    The Google Cloud project ID that you want to use for the API calls. It uses the `GOOGLE_VERTEX_PROJECT` environment variable by default.

-   **location** *string*

    The Google Cloud location that you want to use for the API calls, e.g. `us-central1`. It uses the `GOOGLE_VERTEX_LOCATION` environment variable by default.

-   **googleAuthOptions** *object*

    Optional. The Authentication options used by the [Google Auth Library](https://github.com/googleapis/google-auth-library-nodejs/). See also the [GoogleAuthOptions](https://github.com/googleapis/google-auth-library-nodejs/blob/08978822e1b7b5961f0e355df51d738e012be392/src/auth/googleauth.ts#L87C18-L87C35) interface.

    -   **authClient** *object* An `AuthClient` to use.

    -   **keyFilename** *string* Path to a .json, .pem, or .p12 key file.

    -   **keyFile** *string* Path to a .json, .pem, or .p12 key file.

    -   **credentials** *object* Object containing client\_email and private\_key properties, or the external account client options.

    -   **clientOptions** *object* Options object passed to the constructor of the client.

    -   **scopes** *string | string\[\]* Required scopes for the desired API request.

    -   **projectId** *string* Your project ID.

    -   **universeDomain** *string* The default service domain for a given Cloud universe.

-   **headers** *Resolvable<Record<string, string | undefined>>*

    Headers to include in the requests. Can be provided in multiple formats:

    -   A record of header key-value pairs: `Record<string, string | undefined>`
    -   A function that returns headers: `() => Record<string, string | undefined>`
    -   An async function that returns headers: `async () => Record<string, string | undefined>`
    -   A promise that resolves to headers: `Promise<Record<string, string | undefined>>`
-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.

-   **baseURL** *string*

    Optional. Base URL for the Google Vertex API calls e.g. to use proxy servers. By default, it is constructed using the location and project: `https://${location}-aiplatform.googleapis.com/v1/projects/${project}/locations/${location}/publishers/google`



#### [Edge Runtime](#edge-runtime)


Edge runtimes (like Vercel Edge Functions and Cloudflare Workers) are lightweight JavaScript environments that run closer to users at the network edge. They only provide a subset of the standard Node.js APIs. For example, direct file system access is not available, and many Node.js-specific libraries (including the standard Google Auth library) are not compatible.

The Edge runtime version of the Google Vertex provider supports Google's [Application Default Credentials](https://github.com/googleapis/google-auth-library-nodejs?tab=readme-ov-file#application-default-credentials) through environment variables. The values can be obtained from a json credentials file from the [Google Cloud Console](https://console.cloud.google.com/apis/credentials).

You can import the default provider instance `vertex` from `@ai-sdk/google-vertex/edge`:

```
import{ vertex }from'@ai-sdk/google-vertex/edge';
```

The `/edge` sub-module is included in the `@ai-sdk/google-vertex` package, so you don't need to install it separately. You must import from `@ai-sdk/google-vertex/edge` to differentiate it from the Node.js provider.

If you need a customized setup, you can import `createVertex` from `@ai-sdk/google-vertex/edge` and create a provider instance with your settings:

```
import{ createVertex }from'@ai-sdk/google-vertex/edge';const vertex =createVertex({  project:'my-project',// optionallocation:'us-central1',// optional});
```

For Edge runtime authentication, you'll need to set these environment variables from your Google Default Application Credentials JSON file:

-   `GOOGLE_CLIENT_EMAIL`
-   `GOOGLE_PRIVATE_KEY`
-   `GOOGLE_PRIVATE_KEY_ID` (optional)

These values can be obtained from a service account JSON file from the [Google Cloud Console](https://console.cloud.google.com/apis/credentials).


##### [Optional Provider Settings](#optional-provider-settings-1)


You can use the following optional settings to customize the provider instance:

-   **project** *string*

    The Google Cloud project ID that you want to use for the API calls. It uses the `GOOGLE_VERTEX_PROJECT` environment variable by default.

-   **location** *string*

    The Google Cloud location that you want to use for the API calls, e.g. `us-central1`. It uses the `GOOGLE_VERTEX_LOCATION` environment variable by default.

-   **googleCredentials** *object*

    Optional. The credentials used by the Edge provider for authentication. These credentials are typically set through environment variables and are derived from a service account JSON file.

    -   **clientEmail** *string* The client email from the service account JSON file. Defaults to the contents of the `GOOGLE_CLIENT_EMAIL` environment variable.

    -   **privateKey** *string* The private key from the service account JSON file. Defaults to the contents of the `GOOGLE_PRIVATE_KEY` environment variable.

    -   **privateKeyId** *string* The private key ID from the service account JSON file (optional). Defaults to the contents of the `GOOGLE_PRIVATE_KEY_ID` environment variable.

-   **headers** *Resolvable<Record<string, string | undefined>>*

    Headers to include in the requests. Can be provided in multiple formats:

    -   A record of header key-value pairs: `Record<string, string | undefined>`
    -   A function that returns headers: `() => Record<string, string | undefined>`
    -   An async function that returns headers: `async () => Record<string, string | undefined>`
    -   A promise that resolves to headers: `Promise<Record<string, string | undefined>>`
-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



### [Language Models](#language-models)


You can create models that call the Vertex API using the provider instance. The first argument is the model id, e.g. `gemini-1.5-pro`.

```
const model =vertex('gemini-1.5-pro');
```

If you are using [your own models](https://cloud.google.com/vertex-ai/docs/training-overview), the name of your model needs to start with `projects/`.

Google Vertex models support also some model specific settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model =vertex('gemini-1.5-pro',{  safetySettings:[{ category:'HARM_CATEGORY_UNSPECIFIED', threshold:'BLOCK_LOW_AND_ABOVE'},],});
```

The following optional settings are available for Google Vertex models:

-   **structuredOutputs** *boolean*

    Optional. Enable structured output. Default is true.

    This is useful when the JSON Schema contains elements that are not supported by the OpenAPI schema version that Google Vertex uses. You can use this to disable structured outputs if you need to.

    See [Troubleshooting: Schema Limitations](#schema-limitations) for more details.

-   **safetySettings** *Array<{ category: string; threshold: string }>*

    Optional. Safety settings for the model.

    -   **category** *string*

        The category of the safety setting. Can be one of the following:

        -   `HARM_CATEGORY_UNSPECIFIED`
        -   `HARM_CATEGORY_HATE_SPEECH`
        -   `HARM_CATEGORY_DANGEROUS_CONTENT`
        -   `HARM_CATEGORY_HARASSMENT`
        -   `HARM_CATEGORY_SEXUALLY_EXPLICIT`
        -   `HARM_CATEGORY_CIVIC_INTEGRITY`
    -   **threshold** *string*

        The threshold of the safety setting. Can be one of the following:

        -   `HARM_BLOCK_THRESHOLD_UNSPECIFIED`
        -   `BLOCK_LOW_AND_ABOVE`
        -   `BLOCK_MEDIUM_AND_ABOVE`
        -   `BLOCK_ONLY_HIGH`
        -   `BLOCK_NONE`
-   **useSearchGrounding** *boolean*

    Optional. When enabled, the model will [use Google search to ground the response](https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview).

-   **audioTimestamp** *boolean*

    Optional. Enables timestamp understanding for audio files. Defaults to false.

    This is useful for generating transcripts with accurate timestamps. Consult [Google's Documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/audio-understanding) for usage details.


You can use Google Vertex language models to generate text with the `generateText` function:

```
import{ vertex }from'@ai-sdk/google-vertex';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:vertex('gemini-1.5-pro'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Google Vertex language models can also be used in the `streamText` function (see [AI SDK Core](/docs/ai-sdk-core)).


#### [Reasoning (Thinking Tokens)](#reasoning-thinking-tokens)


Google Vertex AI, through its support for Gemini models, can also emit "thinking" tokens, representing the model's reasoning process. The AI SDK exposes these as reasoning information.

To enable thinking tokens for compatible Gemini models via Vertex, set `includeThoughts: true` in the `thinkingConfig` provider option. Since the Vertex provider uses the Google provider's underlying language model, these options are passed through `providerOptions.google`:

```
import{ vertex }from'@ai-sdk/google-vertex';import{GoogleGenerativeAIProviderOptions}from'@ai-sdk/google';// Note: importing from @ai-sdk/googleimport{ generateText, streamText }from'ai';// For generateText:const{ text, reasoning, reasoningDetails }=awaitgenerateText({  model:vertex('gemini-2.5-flash-preview-04-17'),// Or other supported model via Vertex  providerOptions:{    google:{// Options are nested under 'google' for Vertex provider      thinkingConfig:{        includeThoughts:true,// thinkingBudget: 2048, // Optional},} satisfies GoogleGenerativeAIProviderOptions,},  prompt:'Explain quantum computing in simple terms.',});console.log('Reasoning:', reasoning);console.log('Reasoning Details:', reasoningDetails);console.log('Final Text:', text);// For streamText:const result =streamText({  model:vertex('gemini-2.5-flash-preview-04-17'),// Or other supported model via Vertex  providerOptions:{    google:{// Options are nested under 'google' for Vertex provider      thinkingConfig:{        includeThoughts:true,// thinkingBudget: 2048, // Optional},} satisfies GoogleGenerativeAIProviderOptions,},  prompt:'Explain quantum computing in simple terms.',});forawait(const part of result.fullStream){if(part.type==='reasoning'){    process.stdout.write(`THOUGHT: ${part.textDelta}\n`);}elseif(part.type==='text-delta'){    process.stdout.write(part.textDelta);}}
```

When `includeThoughts` is true, parts of the API response marked with `thought: true` will be processed as reasoning.

-   In `generateText`, these contribute to the `reasoning` (string) and `reasoningDetails` (array) fields.
-   In `streamText`, these are emitted as `reasoning` stream parts.

Refer to the [Google Vertex AI documentation on "thinking"](https://cloud.google.com/vertex-ai/generative-ai/docs/thinking) for model compatibility and further details.


#### [File Inputs](#file-inputs)


The Google Vertex provider supports file inputs, e.g. PDF files.

```
import{ vertex }from'@ai-sdk/google-vertex';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:vertex('gemini-1.5-pro'),  messages:[{      role:'user',      content:[{type:'text',          text:'What is an embedding model according to this document?',},{type:'file',          data: fs.readFileSync('./data/ai.pdf'),          mimeType:'application/pdf',},],},],});
```

The AI SDK will automatically download URLs if you pass them as data, except for `gs://` URLs. You can use the Google Cloud Storage API to upload larger files to that location.

See [File Parts](/docs/foundations/prompts#file-parts) for details on how to use files in prompts.


#### [Search Grounding](#search-grounding)


With [search grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview), the model has access to the latest information using Google search. Search grounding can be used to provide answers around current events:

```
import{ vertex }from'@ai-sdk/google-vertex';import{GoogleGenerativeAIProviderMetadata}from'@ai-sdk/google';import{ generateText }from'ai';const{ text, providerMetadata }=awaitgenerateText({  model:vertex('gemini-1.5-pro',{    useSearchGrounding:true,}),  prompt:'List the top 5 San Francisco news from the past week.'+'You must include the date of each article.',});// access the grounding metadata. Casting to the provider metadata type// is optional but provides autocomplete and type safety.const metadata = providerMetadata?.google as|GoogleGenerativeAIProviderMetadata|undefined;const groundingMetadata = metadata?.groundingMetadata;const safetyRatings = metadata?.safetyRatings;
```

The grounding metadata includes detailed information about how search results were used to ground the model's response. Here are the available fields:

-   **`webSearchQueries`** (`string[] | null`)

    -   Array of search queries used to retrieve information
    -   Example: `["What's the weather in Chicago this weekend?"]`
-   **`searchEntryPoint`** (`{ renderedContent: string } | null`)

    -   Contains the main search result content used as an entry point
    -   The `renderedContent` field contains the formatted content
-   **`groundingSupports`** (Array of support objects | null)

    -   Contains details about how specific response parts are supported by search results
    -   Each support object includes:
        -   **`segment`**: Information about the grounded text segment
            -   `text`: The actual text segment
            -   `startIndex`: Starting position in the response
            -   `endIndex`: Ending position in the response
        -   **`groundingChunkIndices`**: References to supporting search result chunks
        -   **`confidenceScores`**: Confidence scores (0-1) for each supporting chunk

Example response excerpt:

```
{"groundingMetadata":{"retrievalQueries":["What's the weather in Chicago this weekend?"],"searchEntryPoint":{"renderedContent":"..."},"groundingSupports":[{"segment":{"startIndex":0,"endIndex":65,"text":"Chicago weather changes rapidly, so layers let you adjust easily."},"groundingChunkIndices":[0],"confidenceScores":[0.99]}]}}
```

The Google Vertex provider does not yet support [dynamic retrieval mode and threshold](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval).


### [Sources](#sources)


When you use [Search Grounding](#search-grounding), the model will include sources in the response. You can access them using the `sources` property of the result:

```
import{ vertex }from'@ai-sdk/google-vertex';import{ generateText }from'ai';const{ sources }=awaitgenerateText({  model:vertex('gemini-1.5-pro',{ useSearchGrounding:true}),  prompt:'List the top 5 San Francisco news from the past week.',});
```


### [Safety Ratings](#safety-ratings)


The safety ratings provide insight into the safety of the model's response. See [Google Vertex AI documentation on configuring safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters).

Example response excerpt:

```
{"safetyRatings":[{"category":"HARM_CATEGORY_HATE_SPEECH","probability":"NEGLIGIBLE","probabilityScore":0.11027937,"severity":"HARM_SEVERITY_LOW","severityScore":0.28487435},{"category":"HARM_CATEGORY_DANGEROUS_CONTENT","probability":"HIGH","blocked":true,"probabilityScore":0.95422274,"severity":"HARM_SEVERITY_MEDIUM","severityScore":0.43398145},{"category":"HARM_CATEGORY_HARASSMENT","probability":"NEGLIGIBLE","probabilityScore":0.11085559,"severity":"HARM_SEVERITY_NEGLIGIBLE","severityScore":0.19027223},{"category":"HARM_CATEGORY_SEXUALLY_EXPLICIT","probability":"NEGLIGIBLE","probabilityScore":0.22901751,"severity":"HARM_SEVERITY_NEGLIGIBLE","severityScore":0.09089675}]}
```

For more details, see the [Google Vertex AI documentation on grounding with Google Search](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#ground-to-search).


### [Troubleshooting](#troubleshooting)



#### [Schema Limitations](#schema-limitations)


The Google Vertex API uses a subset of the OpenAPI 3.0 schema, which does not support features such as unions. The errors that you get in this case look like this:

`GenerateContentRequest.generation_config.response_schema.properties[occupation].type: must be specified`

By default, structured outputs are enabled (and for tool calling they are required). You can disable structured outputs for object generation as a workaround:

```
const result =awaitgenerateObject({  model:vertex('gemini-1.5-pro',{    structuredOutputs:false,}),  schema: z.object({    name: z.string(),    age: z.number(),    contact: z.union([      z.object({type: z.literal('email'),        value: z.string(),}),      z.object({type: z.literal('phone'),        value: z.string(),}),]),}),  prompt:'Generate an example person for testing.',});
```

The following Zod features are known to not work with Google Vertex:

-   `z.union`
-   `z.record`


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`gemini-2.0-flash-001`

`gemini-2.0-flash-exp`

`gemini-1.5-flash`

`gemini-1.5-pro`

The table above lists popular models. Please see the [Google Vertex AI docs](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#supported-models) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.


### [Embedding Models](#embedding-models)


You can create models that call the Google Vertex AI embeddings API using the `.textEmbeddingModel()` factory method:

```
const model = vertex.textEmbeddingModel('text-embedding-004');
```

Google Vertex AI embedding models support additional settings. You can pass them as an options argument:

```
const model = vertex.textEmbeddingModel('text-embedding-004',{  outputDimensionality:512,// optional, number of dimensions for the embedding});
```

The following optional settings are available for Google Vertex AI embedding models:

-   **outputDimensionality**: *number*

    Optional reduced dimension for the output embedding. If set, excessive values in the output embedding are truncated from the end.



#### [Model Capabilities](#model-capabilities-1)


Model

Max Values Per Call

Parallel Calls

`text-embedding-004`

2048

The table above lists popular models. You can also pass any available provider model ID as a string if needed.


### [Image Models](#image-models)


You can create [Imagen](https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview) models that call the [Imagen on Vertex AI API](https://cloud.google.com/vertex-ai/generative-ai/docs/image/generate-images) using the `.image()` factory method. For more on image generation with the AI SDK see [generateImage()](/docs/reference/ai-sdk-core/generate-image).

```
import{ vertex }from'@ai-sdk/google-vertex';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: vertex.image('imagen-3.0-generate-002'),  prompt:'A futuristic cityscape at sunset',  aspectRatio:'16:9',});
```

Further configuration can be done using Google Vertex provider options. You can validate the provider options using the `GoogleVertexImageProviderOptions` type.

```
import{ vertex }from'@ai-sdk/google-vertex';import{GoogleVertexImageProviderOptions}from'@ai-sdk/google-vertex';import{ generateImage }from'ai';const{ image }=awaitgenerateImage({  model: vertex.image('imagen-3.0-generate-002'),  providerOptions:{    vertex:{      negativePrompt:'pixelated, blurry, low-quality',} satisfies GoogleVertexImageProviderOptions,},// ...});
```

The following provider options are available:

-   **negativePrompt** *string* A description of what to discourage in the generated images.

-   **personGeneration** `allow_adult` | `allow_all` | `dont_allow` Whether to allow person generation. Defaults to `allow_adult`.

-   **safetySetting** `block_low_and_above` | `block_medium_and_above` | `block_only_high` | `block_none` Whether to block unsafe content. Defaults to `block_medium_and_above`.

-   **addWatermark** *boolean* Whether to add an invisible watermark to the generated images. Defaults to `true`.

-   **storageUri** *string* Cloud Storage URI to store the generated images.


Imagen models do not support the `size` parameter. Use the `aspectRatio` parameter instead.


#### [Model Capabilities](#model-capabilities-2)


Model

Aspect Ratios

`imagen-3.0-generate-002`

1:1, 3:4, 4:3, 9:16, 16:9

`imagen-3.0-fast-generate-001`

1:1, 3:4, 4:3, 9:16, 16:9


## [Google Vertex Anthropic Provider Usage](#google-vertex-anthropic-provider-usage)


The Google Vertex Anthropic provider for the [AI SDK](/docs) offers support for Anthropic's Claude models through the Google Vertex AI APIs. This section provides details on how to set up and use the Google Vertex Anthropic provider.


### [Provider Instance](#provider-instance-1)


You can import the default provider instance `vertexAnthropic` from `@ai-sdk/google-vertex/anthropic`:

```
import{ vertexAnthropic }from'@ai-sdk/google-vertex/anthropic';
```

If you need a customized setup, you can import `createVertexAnthropic` from `@ai-sdk/google-vertex/anthropic` and create a provider instance with your settings:

```
import{ createVertexAnthropic }from'@ai-sdk/google-vertex/anthropic';const vertexAnthropic =createVertexAnthropic({  project:'my-project',// optionallocation:'us-central1',// optional});
```


#### [Node.js Runtime](#nodejs-runtime-1)


For Node.js environments, the Google Vertex Anthropic provider supports all standard Google Cloud authentication options through the `google-auth-library`. You can customize the authentication options by passing them to the `createVertexAnthropic` function:

```
import{ createVertexAnthropic }from'@ai-sdk/google-vertex/anthropic';const vertexAnthropic =createVertexAnthropic({  googleAuthOptions:{    credentials:{      client_email:'my-email',      private_key:'my-private-key',},},});
```


##### [Optional Provider Settings](#optional-provider-settings-2)


You can use the following optional settings to customize the Google Vertex Anthropic provider instance:

-   **project** *string*

    The Google Cloud project ID that you want to use for the API calls. It uses the `GOOGLE_VERTEX_PROJECT` environment variable by default.

-   **location** *string*

    The Google Cloud location that you want to use for the API calls, e.g. `us-central1`. It uses the `GOOGLE_VERTEX_LOCATION` environment variable by default.

-   **googleAuthOptions** *object*

    Optional. The Authentication options used by the [Google Auth Library](https://github.com/googleapis/google-auth-library-nodejs/). See also the [GoogleAuthOptions](https://github.com/googleapis/google-auth-library-nodejs/blob/08978822e1b7b5961f0e355df51d738e012be392/src/auth/googleauth.ts#L87C18-L87C35) interface.

    -   **authClient** *object* An `AuthClient` to use.

    -   **keyFilename** *string* Path to a .json, .pem, or .p12 key file.

    -   **keyFile** *string* Path to a .json, .pem, or .p12 key file.

    -   **credentials** *object* Object containing client\_email and private\_key properties, or the external account client options.

    -   **clientOptions** *object* Options object passed to the constructor of the client.

    -   **scopes** *string | string\[\]* Required scopes for the desired API request.

    -   **projectId** *string* Your project ID.

    -   **universeDomain** *string* The default service domain for a given Cloud universe.

-   **headers** *Resolvable<Record<string, string | undefined>>*

    Headers to include in the requests. Can be provided in multiple formats:

    -   A record of header key-value pairs: `Record<string, string | undefined>`
    -   A function that returns headers: `() => Record<string, string | undefined>`
    -   An async function that returns headers: `async () => Record<string, string | undefined>`
    -   A promise that resolves to headers: `Promise<Record<string, string | undefined>>`
-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



#### [Edge Runtime](#edge-runtime-1)


Edge runtimes (like Vercel Edge Functions and Cloudflare Workers) are lightweight JavaScript environments that run closer to users at the network edge. They only provide a subset of the standard Node.js APIs. For example, direct file system access is not available, and many Node.js-specific libraries (including the standard Google Auth library) are not compatible.

The Edge runtime version of the Google Vertex Anthropic provider supports Google's [Application Default Credentials](https://github.com/googleapis/google-auth-library-nodejs?tab=readme-ov-file#application-default-credentials) through environment variables. The values can be obtained from a json credentials file from the [Google Cloud Console](https://console.cloud.google.com/apis/credentials).

For Edge runtimes, you can import the provider instance from `@ai-sdk/google-vertex/anthropic/edge`:

```
import{ vertexAnthropic }from'@ai-sdk/google-vertex/anthropic/edge';
```

To customize the setup, use `createVertexAnthropic` from the same module:

```
import{ createVertexAnthropic }from'@ai-sdk/google-vertex/anthropic/edge';const vertexAnthropic =createVertexAnthropic({  project:'my-project',// optionallocation:'us-central1',// optional});
```

For Edge runtime authentication, set these environment variables from your Google Default Application Credentials JSON file:

-   `GOOGLE_CLIENT_EMAIL`
-   `GOOGLE_PRIVATE_KEY`
-   `GOOGLE_PRIVATE_KEY_ID` (optional)


##### [Optional Provider Settings](#optional-provider-settings-3)


You can use the following optional settings to customize the provider instance:

-   **project** *string*

    The Google Cloud project ID that you want to use for the API calls. It uses the `GOOGLE_VERTEX_PROJECT` environment variable by default.

-   **location** *string*

    The Google Cloud location that you want to use for the API calls, e.g. `us-central1`. It uses the `GOOGLE_VERTEX_LOCATION` environment variable by default.

-   **googleCredentials** *object*

    Optional. The credentials used by the Edge provider for authentication. These credentials are typically set through environment variables and are derived from a service account JSON file.

    -   **clientEmail** *string* The client email from the service account JSON file. Defaults to the contents of the `GOOGLE_CLIENT_EMAIL` environment variable.

    -   **privateKey** *string* The private key from the service account JSON file. Defaults to the contents of the `GOOGLE_PRIVATE_KEY` environment variable.

    -   **privateKeyId** *string* The private key ID from the service account JSON file (optional). Defaults to the contents of the `GOOGLE_PRIVATE_KEY_ID` environment variable.

-   **headers** *Resolvable<Record<string, string | undefined>>*

    Headers to include in the requests. Can be provided in multiple formats:

    -   A record of header key-value pairs: `Record<string, string | undefined>`
    -   A function that returns headers: `() => Record<string, string | undefined>`
    -   An async function that returns headers: `async () => Record<string, string | undefined>`
    -   A promise that resolves to headers: `Promise<Record<string, string | undefined>>`
-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



### [Language Models](#language-models-1)


You can create models that call the [Anthropic Messages API](https://docs.anthropic.com/claude/reference/messages_post) using the provider instance. The first argument is the model id, e.g. `claude-3-haiku-20240307`. Some models have multi-modal capabilities.

```
const model =anthropic('claude-3-haiku-20240307');
```

You can use Anthropic language models to generate text with the `generateText` function:

```
import{ vertexAnthropic }from'@ai-sdk/google-vertex/anthropic';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:vertexAnthropic('claude-3-haiku-20240307'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Anthropic language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core)).

The Anthropic API returns streaming tool calls all at once after a delay. This causes the `streamObject` function to generate the object fully after a delay instead of streaming it incrementally.

The following optional settings are available for Anthropic models:

-   `sendReasoning` *boolean*

    Optional. Include reasoning content in requests sent to the model. Defaults to `true`.

    If you are experiencing issues with the model handling requests involving reasoning content, you can set this to `false` to omit them from the request.



### [Reasoning](#reasoning)


Anthropic has reasoning support for the `claude-3-7-sonnet@20250219` model.

You can enable it using the `thinking` provider option and specifying a thinking budget in tokens.

```
import{ vertexAnthropic }from'@ai-sdk/google-vertex/anthropic';import{ generateText }from'ai';const{ text, reasoning, reasoningDetails }=awaitgenerateText({  model:vertexAnthropic('claude-3-7-sonnet@20250219'),  prompt:'How many people will live in the world in 2040?',  providerOptions:{    anthropic:{      thinking:{type:'enabled', budgetTokens:12000},},},});console.log(reasoning);// reasoning textconsole.log(reasoningDetails);// reasoning details including redacted reasoningconsole.log(text);// text response
```

See [AI SDK UI: Chatbot](/docs/ai-sdk-ui/chatbot#reasoning) for more details on how to integrate reasoning into your chatbot.


#### [Cache Control](#cache-control)


Anthropic cache control is in a Pre-Generally Available (GA) state on Google Vertex. For more see [Google Vertex Anthropic cache control documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude-prompt-caching).

In the messages and message parts, you can use the `providerOptions` property to set cache control breakpoints. You need to set the `anthropic` property in the `providerOptions` object to `{ cacheControl: { type: 'ephemeral' } }` to set a cache control breakpoint.

The cache creation input tokens are then returned in the `providerMetadata` object for `generateText` and `generateObject`, again under the `anthropic` property. When you use `streamText` or `streamObject`, the response contains a promise that resolves to the metadata. Alternatively you can receive it in the `onFinish` callback.

```
import{ vertexAnthropic }from'@ai-sdk/google-vertex/anthropic';import{ generateText }from'ai';const errorMessage ='... long error message ...';const result =awaitgenerateText({  model:vertexAnthropic('claude-3-5-sonnet-20240620'),  messages:[{      role:'user',      content:[{type:'text', text:'You are a JavaScript expert.'},{type:'text',          text:`Error message: ${errorMessage}`,          providerOptions:{            anthropic:{ cacheControl:{type:'ephemeral'}},},},{type:'text', text:'Explain the error message.'},],},],});console.log(result.text);console.log(result.providerMetadata?.anthropic);// e.g. { cacheCreationInputTokens: 2118, cacheReadInputTokens: 0 }
```

You can also use cache control on system messages by providing multiple system messages at the head of your messages array:

```
const result =awaitgenerateText({  model:vertexAnthropic('claude-3-5-sonnet-20240620'),  messages:[{      role:'system',      content:'Cached system message part',      providerOptions:{        anthropic:{ cacheControl:{type:'ephemeral'}},},},{      role:'system',      content:'Uncached system message part',},{      role:'user',      content:'User prompt',},],});
```

For more on prompt caching with Anthropic, see [Google Vertex AI's Claude prompt caching documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude-prompt-caching) and [Anthropic's Cache Control documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching).


### [Computer Use](#computer-use)


Anthropic provides three built-in tools that can be used to interact with external systems:

1.  **Bash Tool**: Allows running bash commands.
2.  **Text Editor Tool**: Provides functionality for viewing and editing text files.
3.  **Computer Tool**: Enables control of keyboard and mouse actions on a computer.

They are available via the `tools` property of the provider instance.

For more background see [Anthropic's Computer Use documentation](https://docs.anthropic.com/en/docs/build-with-claude/computer-use).


#### [Bash Tool](#bash-tool)


The Bash Tool allows running bash commands. Here's how to create and use it:

```
const bashTool = vertexAnthropic.tools.bash_20241022({execute:async({ command, restart })=>{// Implement your bash command execution logic here// Return the result of the command execution},});
```

Parameters:

-   `command` (string): The bash command to run. Required unless the tool is being restarted.
-   `restart` (boolean, optional): Specifying true will restart this tool.


#### [Text Editor Tool](#text-editor-tool)


The Text Editor Tool provides functionality for viewing and editing text files:

```
const textEditorTool = vertexAnthropic.tools.textEditor_20241022({execute:async({    command,    path,    file_text,    insert_line,    new_str,    old_str,    view_range,})=>{// Implement your text editing logic here// Return the result of the text editing operation},});
```

Parameters:

-   `command` ('view' | 'create' | 'str\_replace' | 'insert' | 'undo\_edit'): The command to run.
-   `path` (string): Absolute path to file or directory, e.g. `/repo/file.py` or `/repo`.
-   `file_text` (string, optional): Required for `create` command, with the content of the file to be created.
-   `insert_line` (number, optional): Required for `insert` command. The line number after which to insert the new string.
-   `new_str` (string, optional): New string for `str_replace` or `insert` commands.
-   `old_str` (string, optional): Required for `str_replace` command, containing the string to replace.
-   `view_range` (number\[\], optional): Optional for `view` command to specify line range to show.


#### [Computer Tool](#computer-tool)


The Computer Tool enables control of keyboard and mouse actions on a computer:

```
const computerTool = vertexAnthropic.tools.computer_20241022({  displayWidthPx:1920,  displayHeightPx:1080,  displayNumber:0,// Optional, for X11 environmentsexecute:async({ action, coordinate, text })=>{// Implement your computer control logic here// Return the result of the action// Example code:switch(action){case'screenshot':{// multipart result:return{type:'image',          data: fs.readFileSync('./data/screenshot-editor.png').toString('base64'),};}default:{console.log('Action:', action);console.log('Coordinate:', coordinate);console.log('Text:', text);return`executed ${action}`;}}},// map to tool result content for LLM consumption:experimental_toToolResultContent(result){returntypeof result ==='string'?[{type:'text', text: result }]:[{type:'image', data: result.data, mimeType:'image/png'}];},});
```

Parameters:

-   `action` ('key' | 'type' | 'mouse\_move' | 'left\_click' | 'left\_click\_drag' | 'right\_click' | 'middle\_click' | 'double\_click' | 'screenshot' | 'cursor\_position'): The action to perform.
-   `coordinate` (number\[\], optional): Required for `mouse_move` and `left_click_drag` actions. Specifies the (x, y) coordinates.
-   `text` (string, optional): Required for `type` and `key` actions.

These tools can be used in conjunction with the `claude-3-5-sonnet-v2@20241022` model to enable more complex interactions and tasks.


### [Model Capabilities](#model-capabilities-3)


The latest Anthropic model list on Vertex AI is available [here](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#model-list). See also [Anthropic Model Comparison](https://docs.anthropic.com/en/docs/about-claude/models#model-comparison).

Model

Image Input

Object Generation

Tool Usage

Tool Streaming

Computer Use

`claude-3-7-sonnet@20250219`

`claude-3-5-sonnet-v2@20241022`

`claude-3-5-sonnet@20240620`

`claude-3-5-haiku@20241022`

`claude-3-sonnet@20240229`

`claude-3-haiku@20240307`

`claude-3-opus@20240229`

The table above lists popular models. You can also pass any available provider model ID as a string if needed.
```

### 433. `providers/ai-sdk-providers/groq.md`

```markdown
# Groq Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/groq
description: Learn how to use Groq.
---


# [Groq Provider](#groq-provider)


The [Groq](https://groq.com/) provider contains language model support for the Groq API.


## [Setup](#setup)


The Groq provider is available via the `@ai-sdk/groq` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/groq


## [Provider Instance](#provider-instance)


You can import the default provider instance `groq` from `@ai-sdk/groq`:

```
import{ groq }from'@ai-sdk/groq';
```

If you need a customized setup, you can import `createGroq` from `@ai-sdk/groq` and create a provider instance with your settings:

```
import{ createGroq }from'@ai-sdk/groq';const groq =createGroq({// custom settings});
```

You can use the following optional settings to customize the Groq provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.groq.com/openai/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `GROQ_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create [Groq models](https://console.groq.com/docs/models) using a provider instance. The first argument is the model id, e.g. `gemma2-9b-it`.

```
const model =groq('gemma2-9b-it');
```


### [Reasoning Models](#reasoning-models)


Groq offers several reasoning models such as `qwen-qwq-32b` and `deepseek-r1-distill-llama-70b`. You can configure how the reasoning is exposed in the generated text by using the `reasoningFormat` option. It supports the options `parsed`, `hidden`, and `raw`.

```
import{ groq }from'@ai-sdk/groq';import{ generateText }from'ai';const result =awaitgenerateText({  model:groq('qwen-qwq-32b'),  providerOptions:{    groq:{ reasoningFormat:'parsed'},},  prompt:'How many "r"s are in the word "strawberry"?',});
```

Only Groq reasoning models support the `reasoningFormat` option.


### [Example](#example)


You can use Groq language models to generate text with the `generateText` function:

```
import{ groq }from'@ai-sdk/groq';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:groq('gemma2-9b-it'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```


## [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`meta-llama/llama-4-scout-17b-16e-instruct`

`gemma2-9b-it`

`llama-3.3-70b-versatile`

`llama-3.1-8b-instant`

`llama-guard-3-8b`

`llama3-70b-8192`

`llama3-8b-8192`

`mixtral-8x7b-32768`

`qwen-qwq-32b`

`mistral-saba-24b`

`qwen-2.5-32b`

`deepseek-r1-distill-qwen-32b`

`deepseek-r1-distill-llama-70b`

The table above lists popular models. Please see the [Groq docs](https://console.groq.com/docs/models) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.


## [Transcription Models](#transcription-models)


You can create models that call the [Groq transcription API](https://console.groq.com/docs/speech-to-text) using the `.transcription()` factory method.

The first argument is the model id e.g. `whisper-large-v3`.

```
const model = groq.transcription('whisper-large-v3');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format will improve accuracy and latency.

```
import{ experimental_transcribe as transcribe }from'ai';import{ groq }from'@ai-sdk/groq';import{ readFile }from'fs/promises';const result =awaittranscribe({  model: groq.transcription('whisper-large-v3'),  audio:awaitreadFile('audio.mp3'),  providerOptions:{ groq:{ language:'en'}},});
```

The following provider options are available:

-   **timestampGranularities** *string\[\]* The granularity of the timestamps in the transcription. Defaults to `['segment']`. Possible values are `['word']`, `['segment']`, and `['word', 'segment']`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.

-   **language** *string* The language of the input audio. Supplying the input language in ISO-639-1 format (e.g. 'en') will improve accuracy and latency. Optional.

-   **prompt** *string* An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. Optional.

-   **temperature** *number* The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. Defaults to 0. Optional.



### [Model Capabilities](#model-capabilities-1)


Model

Transcription

Duration

Segments

Language

`whisper-large-v3`

`whisper-large-v3-turbo`

`distil-whisper-large-v3-en`
```

### 434. `providers/ai-sdk-providers/hume.md`

```markdown
# Hume Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/hume
description: Learn how to use the Hume provider for the AI SDK.
---


# [Hume Provider](#hume-provider)


The [Hume](https://hume.ai/) provider contains language model support for the Hume transcription API.


## [Setup](#setup)


The Hume provider is available in the `@ai-sdk/hume` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/hume


## [Provider Instance](#provider-instance)


You can import the default provider instance `hume` from `@ai-sdk/hume`:

```
import{ hume }from'@ai-sdk/hume';
```

If you need a customized setup, you can import `createHume` from `@ai-sdk/hume` and create a provider instance with your settings:

```
import{ createHume }from'@ai-sdk/hume';const hume =createHume({// custom settings, e.g.  fetch: customFetch,});
```

You can use the following optional settings to customize the Hume provider instance:

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `HUME_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Speech Models](#speech-models)


You can create models that call the [Hume speech API](https://dev.hume.ai/docs/text-to-speech-tts/overview) using the `.speech()` factory method.

```
const model = hume.speech();
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying a voice to use for the generated audio.

```
import{ experimental_generateSpeech as generateSpeech }from'ai';import{ hume }from'@ai-sdk/hume';const result =awaitgenerateSpeech({  model: hume.speech(),  text:'Hello, world!',  voice:'d8ab67c6-953d-4bd8-9370-8fa53a0f1453',  providerOptions:{ hume:{}},});
```

The following provider options are available:

-   **context** *object*

    Either:

    -   `{ generationId: string }` - A generation ID to use for context.
    -   `{ utterances: HumeUtterance[] }` - An array of utterance objects for context.


### [Model Capabilities](#model-capabilities)


Model

Instructions

`default`
```

### 435. `providers/ai-sdk-providers/lmnt.md`

```markdown
# LMNT Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/lmnt
description: Learn how to use the LMNT provider for the AI SDK.
---


# [LMNT Provider](#lmnt-provider)


The [LMNT](https://lmnt.com/) provider contains language model support for the LMNT transcription API.


## [Setup](#setup)


The LMNT provider is available in the `@ai-sdk/lmnt` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/lmnt


## [Provider Instance](#provider-instance)


You can import the default provider instance `lmnt` from `@ai-sdk/lmnt`:

```
import{ lmnt }from'@ai-sdk/lmnt';
```

If you need a customized setup, you can import `createLMNT` from `@ai-sdk/lmnt` and create a provider instance with your settings:

```
import{ createLMNT }from'@ai-sdk/lmnt';const lmnt =createLMNT({// custom settings, e.g.  fetch: customFetch,});
```

You can use the following optional settings to customize the LMNT provider instance:

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `LMNT_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Speech Models](#speech-models)


You can create models that call the [LMNT speech API](https://docs.lmnt.com/api-reference/speech/synthesize-speech-bytes) using the `.speech()` factory method.

The first argument is the model id e.g. `aurora`.

```
const model = lmnt.speech('aurora');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying a voice to use for the generated audio.

```
import{ experimental_generateSpeech as generateSpeech }from'ai';import{ lmnt }from'@ai-sdk/lmnt';const result =awaitgenerateSpeech({  model: lmnt.speech('aurora'),  text:'Hello, world!',  providerOptions:{ lmnt:{ language:'en'}},});
```


### [Provider Options](#provider-options)


The LMNT provider accepts the following options:

-   **model** *'aurora' | 'blizzard'*

    The LMNT model to use. Defaults to `'aurora'`.

-   **language** *'auto' | 'en' | 'es' | 'pt' | 'fr' | 'de' | 'zh' | 'ko' | 'hi' | 'ja' | 'ru' | 'it' | 'tr'*

    The language to use for speech synthesis. Defaults to `'auto'`.

-   **format** *'aac' | 'mp3' | 'mulaw' | 'raw' | 'wav'*

    The audio format to return. Defaults to `'mp3'`.

-   **sampleRate** *number*

    The sample rate of the audio in Hz. Defaults to `24000`.

-   **speed** *number*

    The speed of the speech. Must be between 0.25 and 2. Defaults to `1`.

-   **seed** *number*

    An optional seed for deterministic generation.

-   **conversational** *boolean*

    Whether to use a conversational style. Defaults to `false`.

-   **length** *number*

    Maximum length of the audio in seconds. Maximum value is 300.

-   **topP** *number*

    Top-p sampling parameter. Must be between 0 and 1. Defaults to `1`.

-   **temperature** *number*

    Temperature parameter for sampling. Must be at least 0. Defaults to `1`.



### [Model Capabilities](#model-capabilities)


Model

Instructions

`aurora`

`blizzard`
```

### 436. `providers/ai-sdk-providers/luma.md`

```markdown
# Luma Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/luma
description: Learn how to use Luma AI models with the AI SDK.
---


# [Luma Provider](#luma-provider)


[Luma AI](https://lumalabs.ai/) provides state-of-the-art image generation models through their Dream Machine platform. Their models offer ultra-high quality image generation with superior prompt understanding and unique capabilities like character consistency and multi-image reference support.


## [Setup](#setup)


The Luma provider is available via the `@ai-sdk/luma` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/luma


## [Provider Instance](#provider-instance)


You can import the default provider instance `luma` from `@ai-sdk/luma`:

```
import{ luma }from'@ai-sdk/luma';
```

If you need a customized setup, you can import `createLuma` and create a provider instance with your settings:

```
import{ createLuma }from'@ai-sdk/luma';const luma =createLuma({  apiKey:'your-api-key',// optional, defaults to LUMA_API_KEY environment variable  baseURL:'custom-url',// optional  headers:{/* custom headers */},// optional});
```

You can use the following optional settings to customize the Luma provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.lumalabs.ai`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `LUMA_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Image Models](#image-models)


You can create Luma image models using the `.image()` factory method. For more on image generation with the AI SDK see [generateImage()](/docs/reference/ai-sdk-core/generate-image).


### [Basic Usage](#basic-usage)


```
import{ luma }from'@ai-sdk/luma';import{ experimental_generateImage as generateImage }from'ai';importfsfrom'fs';const{ image }=awaitgenerateImage({  model: luma.image('photon-1'),  prompt:'A serene mountain landscape at sunset',  aspectRatio:'16:9',});const filename =`image-${Date.now()}.png`;fs.writeFileSync(filename, image.uint8Array);console.log(`Image saved to ${filename}`);
```


### [Image Model Settings](#image-model-settings)


When creating an image model, you can customize the generation behavior with optional settings:

```
const model = luma.image('photon-1',{  maxImagesPerCall:1,// Maximum number of images to generate per API call  pollIntervalMillis:5000,// How often to check for completed images (in ms)  maxPollAttempts:10,// Maximum number of polling attempts before timeout});
```

Since Luma processes images through an asynchronous queue system, these settings allow you to tune the polling behavior:

-   **maxImagesPerCall** *number*

    Override the maximum number of images generated per API call. Defaults to 1.

-   **pollIntervalMillis** *number*

    Control how frequently the API is checked for completed images while they are being processed. Defaults to 500ms.

-   **maxPollAttempts** *number*

    Limit how long to wait for results before timing out, since image generation is queued asynchronously. Defaults to 120 attempts.



### [Model Capabilities](#model-capabilities)


Luma offers two main models:

Model

Description

`photon-1`

High-quality image generation with superior prompt understanding

`photon-flash-1`

Faster generation optimized for speed while maintaining quality

Both models support the following aspect ratios:

-   1:1
-   3:4
-   4:3
-   9:16
-   16:9 (default)
-   9:21
-   21:9

For more details about supported aspect ratios, see the [Luma Image Generation documentation](https://docs.lumalabs.ai/docs/image-generation).

Key features of Luma models include:

-   Ultra-high quality image generation
-   10x higher cost efficiency compared to similar models
-   Superior prompt understanding and adherence
-   Unique character consistency capabilities from single reference images
-   Multi-image reference support for precise style matching


### [Advanced Options](#advanced-options)


Luma models support several advanced features through the `providerOptions.luma` parameter.


#### [Image Reference](#image-reference)


Use up to 4 reference images to guide your generation. Useful for creating variations or visualizing complex concepts. Adjust the `weight` (0-1) to control the influence of reference images.

```
// Example: Generate a salamander with referenceawaitgenerateImage({  model: luma.image('photon-1'),  prompt:'A salamander at dusk in a forest pond, in the style of ukiyo-e',  providerOptions:{    luma:{      image_ref:[{          url:'https://example.com/reference.jpg',          weight:0.85,},],},},});
```


#### [Style Reference](#style-reference)


Apply specific visual styles to your generations using reference images. Control the style influence using the `weight` parameter.

```
// Example: Generate with style referenceawaitgenerateImage({  model: luma.image('photon-1'),  prompt:'A blue cream Persian cat launching its website on Vercel',  providerOptions:{    luma:{      style_ref:[{          url:'https://example.com/style.jpg',          weight:0.8,},],},},});
```


#### [Character Reference](#character-reference)


Create consistent and personalized characters using up to 4 reference images of the same subject. More reference images improve character representation.

```
// Example: Generate character-based imageawaitgenerateImage({  model: luma.image('photon-1'),  prompt:'A woman with a cat riding a broomstick in a forest',  providerOptions:{    luma:{      character_ref:{        identity0:{          images:['https://example.com/character.jpg'],},},},},});
```


#### [Modify Image](#modify-image)


Transform existing images using text prompts. Use the `weight` parameter to control how closely the result matches the input image (higher weight = closer to input but less creative).

For color changes, it's recommended to use a lower weight value (0.0-0.1).

```
// Example: Modify existing imageawaitgenerateImage({  model: luma.image('photon-1'),  prompt:'transform the bike to a boat',  providerOptions:{    luma:{      modify_image_ref:{        url:'https://example.com/image.jpg',        weight:1.0,},},},});
```

For more details about Luma's capabilities and features, visit the [Luma Image Generation documentation](https://docs.lumalabs.ai/docs/image-generation).
```

### 437. `providers/ai-sdk-providers/mistral.md`

```markdown
# Mistral AI Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/mistral
description: Learn how to use Mistral.
---


# [Mistral AI Provider](#mistral-ai-provider)


The [Mistral AI](https://mistral.ai/) provider contains language model support for the Mistral chat API.


## [Setup](#setup)


The Mistral provider is available in the `@ai-sdk/mistral` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/mistral


## [Provider Instance](#provider-instance)


You can import the default provider instance `mistral` from `@ai-sdk/mistral`:

```
import{ mistral }from'@ai-sdk/mistral';
```

If you need a customized setup, you can import `createMistral` from `@ai-sdk/mistral` and create a provider instance with your settings:

```
import{ createMistral }from'@ai-sdk/mistral';const mistral =createMistral({// custom settings});
```

You can use the following optional settings to customize the Mistral provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.mistral.ai/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `MISTRAL_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create models that call the [Mistral chat API](https://docs.mistral.ai/api/#operation/createChatCompletion) using a provider instance. The first argument is the model id, e.g. `mistral-large-latest`. Some Mistral chat models support tool calls.

```
const model =mistral('mistral-large-latest');
```

Mistral chat models also support additional model settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model =mistral('mistral-large-latest',{  safePrompt:true,// optional safety prompt injection});
```

The following optional settings are available for Mistral models:

-   **safePrompt** *boolean*

    Whether to inject a safety prompt before all conversations.

    Defaults to `false`.



### [Document OCR](#document-ocr)


Mistral chat models support document OCR for PDF files. You can optionally set image and page limits using the provider options.

```
const result =awaitgenerateText({  model:mistral('mistral-small-latest'),  messages:[{      role:'user',      content:[{type:'text',          text:'What is an embedding model according to this document?',},{type:'file',          data:newURL('https://github.com/vercel/ai/blob/main/examples/ai-core/data/ai.pdf?raw=true',),          mimeType:'application/pdf',},],},],// optional settings:  providerOptions:{    mistral:{      documentImageLimit:8,      documentPageLimit:64,},},});
```


### [Example](#example)


You can use Mistral language models to generate text with the `generateText` function:

```
import{ mistral }from'@ai-sdk/mistral';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:mistral('mistral-large-latest'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Mistral language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core)).


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`pixtral-large-latest`

`mistral-large-latest`

`mistral-small-latest`

`ministral-3b-latest`

`ministral-8b-latest`

`pixtral-12b-2409`

The table above lists popular models. Please see the [Mistral docs](https://docs.mistral.ai/getting-started/models/models_overview/) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.


## [Embedding Models](#embedding-models)


You can create models that call the [Mistral embeddings API](https://docs.mistral.ai/api/#operation/createEmbedding) using the `.embedding()` factory method.

```
const model = mistral.embedding('mistral-embed');
```


### [Model Capabilities](#model-capabilities-1)


Model

Default Dimensions

`mistral-embed`

1024
```

### 438. `providers/ai-sdk-providers/openai.md`

```markdown
# OpenAI Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/openai
description: Learn how to use the OpenAI provider for the AI SDK.
---


# [OpenAI Provider](#openai-provider)


The [OpenAI](https://openai.com/) provider contains language model support for the OpenAI responses, chat, and completion APIs, as well as embedding model support for the OpenAI embeddings API.


## [Setup](#setup)


The OpenAI provider is available in the `@ai-sdk/openai` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/openai


## [Provider Instance](#provider-instance)


You can import the default provider instance `openai` from `@ai-sdk/openai`:

```
import{ openai }from'@ai-sdk/openai';
```

If you need a customized setup, you can import `createOpenAI` from `@ai-sdk/openai` and create a provider instance with your settings:

```
import{ createOpenAI }from'@ai-sdk/openai';const openai =createOpenAI({// custom settings, e.g.  compatibility:'strict',// strict mode, enable when using the OpenAI API});
```

You can use the following optional settings to customize the OpenAI provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.openai.com/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `OPENAI_API_KEY` environment variable.

-   **name** *string*

    The provider name. You can set this when using OpenAI compatible providers to change the model provider property. Defaults to `openai`.

-   **organization** *string*

    OpenAI Organization.

-   **project** *string*

    OpenAI project.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.

-   **compatibility** *"strict" | "compatible"*

    OpenAI compatibility mode. Should be set to `strict` when using the OpenAI API, and `compatible` when using 3rd party providers. In `compatible` mode, newer information such as `streamOptions` are not being sent, resulting in `NaN` token usage. Defaults to 'compatible'.



## [Language Models](#language-models)


The OpenAI provider instance is a function that you can invoke to create a language model:

```
const model =openai('gpt-4-turbo');
```

It automatically selects the correct API based on the model id. You can also pass additional settings in the second argument:

```
const model =openai('gpt-4-turbo',{// additional settings});
```

The available options depend on the API that's automatically chosen for the model (see below). If you want to explicitly select a specific model API, you can use `.chat` or `.completion`.


### [Example](#example)


You can use OpenAI language models to generate text with the `generateText` function:

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:openai('gpt-4-turbo'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

OpenAI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core)).


### [Chat Models](#chat-models)


You can create models that call the [OpenAI chat API](https://platform.openai.com/docs/api-reference/chat) using the `.chat()` factory method. The first argument is the model id, e.g. `gpt-4`. The OpenAI chat models support tool calls and some have multi-modal capabilities.

```
const model = openai.chat('gpt-3.5-turbo');
```

OpenAI chat models support also some model specific settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model = openai.chat('gpt-3.5-turbo',{  logitBias:{// optional likelihood for specific tokens'50256':-100,},  user:'test-user',// optional unique user identifier});
```

The following optional settings are available for OpenAI chat models:

-   **logitBias** *Record<number, number>*

    Modifies the likelihood of specified tokens appearing in the completion.

    Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

    As an example, you can pass `{"50256": -100}` to prevent the token from being generated.

-   **logprobs** *boolean | number*

    Return the log probabilities of the tokens. Including logprobs will increase the response size and can slow down response times. However, it can be useful to better understand how the model is behaving.

    Setting to true will return the log probabilities of the tokens that were generated.

    Setting to a number will return the log probabilities of the top n tokens that were generated.

-   **parallelToolCalls** *boolean*

    Whether to enable parallel function calling during tool use. Defaults to `true`.

-   **useLegacyFunctionCalls** *boolean*

    Whether to use legacy function calling. Defaults to false.

    Required by some open source inference engines which do not support the `tools` API. May also provide a workaround for `parallelToolCalls` resulting in the provider buffering tool calls, which causes `streamObject` to be non-streaming.

    Prefer setting `parallelToolCalls: false` over this option.

-   **structuredOutputs** *boolean*

    Whether to use [structured outputs](#structured-outputs). Defaults to `false` for normal models, and `true` for reasoning models.

    When enabled, tool calls and object generation will be strict and follow the provided schema.

-   **user** *string*

    A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).

-   **downloadImages** *boolean*

    Automatically download images and pass the image as data to the model. OpenAI supports image URLs for public models, so this is only needed for private models or when the images are not publicly accessible. Defaults to `false`.

-   **simulateStreaming** *boolean*

    Simulates streaming by using a normal generate call and returning it as a stream. Enable this if the model that you are using does not support streaming. Defaults to `false`.

-   **reasoningEffort** *'low' | 'medium' | 'high'*

    Reasoning effort for reasoning models. Defaults to `medium`. If you use `providerOptions` to set the `reasoningEffort` option, this model setting will be ignored.



#### [Reasoning](#reasoning)


OpenAI has introduced the `o1`,`o3`, and `o4` series of [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently, `o4-mini`, `o3`, `o3-mini`, `o1`, `o1-mini`, and `o1-preview` are available.

Reasoning models currently only generate text, have several limitations, and are only supported using `generateText` and `streamText`.

They support additional settings and response metadata:

-   You can use `providerOptions` to set

    -   the `reasoningEffort` option (or alternatively the `reasoningEffort` model setting), which determines the amount of reasoning the model performs.
-   You can use response `providerMetadata` to access the number of reasoning tokens that the model generated.


```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const{ text, usage, providerMetadata }=awaitgenerateText({  model:openai('o3-mini'),  prompt:'Invent a new holiday and describe its traditions.',  providerOptions:{    openai:{      reasoningEffort:'low',},},});console.log(text);console.log('Usage:',{...usage,  reasoningTokens: providerMetadata?.openai?.reasoningTokens,});
```

System messages are automatically converted to OpenAI developer messages for reasoning models when supported. For models that do not support developer messages, such as `o1-preview`, system messages are removed and a warning is added.

Reasoning models like `o1-mini` and `o1-preview` require additional runtime inference to complete their reasoning phase before generating a response. This introduces longer latency compared to other models, with `o1-preview` exhibiting significantly more inference time than `o1-mini`.

`maxTokens` is automatically mapped to `max_completion_tokens` for reasoning models.


#### [Structured Outputs](#structured-outputs)


You can enable [OpenAI structured outputs](https://openai.com/index/introducing-structured-outputs-in-the-api/) by setting the `structuredOutputs` option to `true`. Structured outputs are a form of grammar-guided generation. The JSON schema is used as a grammar and the outputs will always conform to the schema.

```
import{ openai }from'@ai-sdk/openai';import{ generateObject }from'ai';import{ z }from'zod';const result =awaitgenerateObject({  model:openai('gpt-4o-2024-08-06',{    structuredOutputs:true,}),  schemaName:'recipe',  schemaDescription:'A recipe for lasagna.',  schema: z.object({    name: z.string(),    ingredients: z.array(      z.object({        name: z.string(),        amount: z.string(),}),),    steps: z.array(z.string()),}),  prompt:'Generate a lasagna recipe.',});console.log(JSON.stringify(result.object,null,2));
```

OpenAI structured outputs have several [limitations](https://openai.com/index/introducing-structured-outputs-in-the-api), in particular around the [supported schemas](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas), and are therefore opt-in.

For example, optional schema properties are not supported. You need to change Zod `.nullish()` and `.optional()` to `.nullable()`.


#### [PDF support](#pdf-support)


The OpenAI Chat API supports reading PDF files. You can pass PDF files as part of the message content using the `file` type:

```
const result =awaitgenerateText({  model:openai('gpt-4o'),  messages:[{      role:'user',      content:[{type:'text',          text:'What is an embedding model?',},{type:'file',          data: fs.readFileSync('./data/ai.pdf'),          mimeType:'application/pdf',          filename:'ai.pdf',// optional},],},],});
```

The model will have access to the contents of the PDF file and respond to questions about it. The PDF file should be passed using the `data` field, and the `mimeType` should be set to `'application/pdf'`.


#### [Predicted Outputs](#predicted-outputs)


OpenAI supports [predicted outputs](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs) for `gpt-4o` and `gpt-4o-mini`. Predicted outputs help you reduce latency by allowing you to specify a base text that the model should modify. You can enable predicted outputs by adding the `prediction` option to the `providerOptions.openai` object:

```
const result =streamText({  model:openai('gpt-4o'),  messages:[{      role:'user',      content:'Replace the Username property with an Email property.',},{      role:'user',      content: existingCode,},],  providerOptions:{    openai:{      prediction:{type:'content',        content: existingCode,},},},});
```

OpenAI provides usage information for predicted outputs (`acceptedPredictionTokens` and `rejectedPredictionTokens`). You can access it in the `providerMetadata` object.

```
const openaiMetadata =(await result.providerMetadata)?.openai;const acceptedPredictionTokens = openaiMetadata?.acceptedPredictionTokens;const rejectedPredictionTokens = openaiMetadata?.rejectedPredictionTokens;
```

OpenAI Predicted Outputs have several [limitations](https://platform.openai.com/docs/guides/predicted-outputs#limitations), e.g. unsupported API parameters and no tool calling support.


#### [Image Detail](#image-detail)


You can use the `openai` provider option to set the [image input detail](https://platform.openai.com/docs/guides/images-vision?api-mode=responses#specify-image-input-detail-level) to `high`, `low`, or `auto`:

```
const result =awaitgenerateText({  model:openai('gpt-4o'),  messages:[{      role:'user',      content:[{type:'text', text:'Describe the image in detail.'},{type:'image',          image:'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true',// OpenAI specific options - image detail:          providerOptions:{            openai:{ imageDetail:'low'},},},],},],});
```

Because the `UIMessage` type (used by AI SDK UI hooks like `useChat`) does not support the `providerOptions` property, you can use `convertToCoreMessages` first before passing the messages to functions like `generateText` or `streamText`. For more details on `providerOptions` usage, see [here](/docs/foundations/prompts#provider-options).


#### [Distillation](#distillation)


OpenAI supports model distillation for some models. If you want to store a generation for use in the distillation process, you can add the `store` option to the `providerOptions.openai` object. This will save the generation to the OpenAI platform for later use in distillation.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';import'dotenv/config';asyncfunctionmain(){const{ text, usage }=awaitgenerateText({    model:openai('gpt-4o-mini'),    prompt:'Who worked on the original macintosh?',    providerOptions:{      openai:{        store:true,        metadata:{          custom:'value',},},},});console.log(text);console.log();console.log('Usage:', usage);}main().catch(console.error);
```


#### [Prompt Caching](#prompt-caching)


OpenAI has introduced [Prompt Caching](https://platform.openai.com/docs/guides/prompt-caching) for supported models including `gpt-4o`, `gpt-4o-mini`, `o1-preview`, and `o1-mini`.

-   Prompt caching is automatically enabled for these models, when the prompt is 1024 tokens or longer. It does not need to be explicitly enabled.
-   You can use response `providerMetadata` to access the number of prompt tokens that were a cache hit.
-   Note that caching behavior is dependent on load on OpenAI's infrastructure. Prompt prefixes generally remain in the cache following 5-10 minutes of inactivity before they are evicted, but during off-peak periods they may persist for up to an hour.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const{ text, usage, providerMetadata }=awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:`A 1024-token or longer prompt...`,});console.log(`usage:`,{...usage,  cachedPromptTokens: providerMetadata?.openai?.cachedPromptTokens,});
```


#### [Audio Input](#audio-input)


With the `gpt-4o-audio-preview` model, you can pass audio files to the model.

The `gpt-4o-audio-preview` model is currently in preview and requires at least some audio inputs. It will not work with non-audio data.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const result =awaitgenerateText({  model:openai('gpt-4o-audio-preview'),  messages:[{      role:'user',      content:[{type:'text', text:'What is the audio saying?'},{type:'file',          mimeType:'audio/mpeg',          data: fs.readFileSync('./data/galileo.mp3'),},],},],});
```


### [Responses Models](#responses-models)


You can use the OpenAI responses API with the `openai.responses(modelId)` factory method.

```
const model = openai.responses('gpt-4o-mini');
```

Further configuration can be done using OpenAI provider options. You can validate the provider options using the `OpenAIResponsesProviderOptions` type.

```
import{ openai,OpenAIResponsesProviderOptions}from'@ai-sdk/openai';import{ generateText }from'ai';const result =awaitgenerateText({  model: openai.responses('gpt-4o-mini'),  providerOptions:{    openai:{      parallelToolCalls:false,      store:false,      user:'user_123',// ...} satisfies OpenAIResponsesProviderOptions,},// ...});
```

The following provider options are available:

-   **parallelToolCalls** *boolean* Whether to use parallel tool calls. Defaults to `true`.

-   **store** *boolean* Whether to store the generation. Defaults to `true`.

-   **metadata** *Record<string, string>* Additional metadata to store with the generation.

-   **previousResponseId** *string* The ID of the previous response. You can use it to continue a conversation. Defaults to `undefined`.

-   **instructions** *string* Instructions for the model. They can be used to change the system or developer message when continuing a conversation using the `previousResponseId` option. Defaults to `undefined`.

-   **user** *string* A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Defaults to `undefined`.

-   **reasoningEffort** *'low' | 'medium' | 'high'* Reasoning effort for reasoning models. Defaults to `medium`. If you use `providerOptions` to set the `reasoningEffort` option, this model setting will be ignored.

-   **reasoningSummary** *'auto' | 'detailed'* Controls whether the model returns its reasoning process. Set to `'auto'` for a condensed summary, `'detailed'` for more comprehensive reasoning. Defaults to `undefined` (no reasoning summaries). When enabled, reasoning summaries appear in the stream as events with type `'reasoning'` and in non-streaming responses within the `reasoning` field.

-   **strictSchemas** *boolean* Whether to use strict JSON schemas in tools and when generating JSON outputs. Defaults to `true`.


The OpenAI responses provider also returns provider-specific metadata:

```
const{ providerMetadata }=awaitgenerateText({  model: openai.responses('gpt-4o-mini'),});const openaiMetadata = providerMetadata?.openai;
```

The following OpenAI-specific metadata is returned:

-   **responseId** *string* The ID of the response. Can be used to continue a conversation.

-   **cachedPromptTokens** *number* The number of prompt tokens that were a cache hit.

-   **reasoningTokens** *number* The number of reasoning tokens that the model generated.



#### [Web Search](#web-search)


The OpenAI responses provider supports web search through the `openai.tools.webSearchPreview` tool.

You can force the use of the web search tool by setting the `toolChoice` parameter to `{ type: 'tool', toolName: 'web_search_preview' }`.

```
const result =awaitgenerateText({  model: openai.responses('gpt-4o-mini'),  prompt:'What happened in San Francisco last week?',  tools:{    web_search_preview: openai.tools.webSearchPreview({// optional configuration:      searchContextSize:'high',      userLocation:{type:'approximate',        city:'San Francisco',        region:'California',},}),},// Force web search tool:  toolChoice:{type:'tool', toolName:'web_search_preview'},});// URL sourcesconst sources = result.sources;
```


#### [Reasoning Summaries](#reasoning-summaries)


For reasoning models like `o3-mini`, `o3`, and `o4-mini`, you can enable reasoning summaries to see the model's thought process. Different models support different summarizers—for example, `o4-mini` supports detailed summaries. Set `reasoningSummary: "auto"` to automatically receive the richest level available.

```
import{ openai }from'@ai-sdk/openai';import{ streamText }from'ai';const result =streamText({  model: openai.responses('o4-mini'),  prompt:'Tell me about the Mission burrito debate in San Francisco.',  providerOptions:{    openai:{      reasoningSummary:'detailed',// 'auto' for condensed or 'detailed' for comprehensive},},});forawait(const part of result.fullStream){if(part.type==='reasoning'){console.log(`Reasoning: ${part.textDelta}`);}elseif(part.type==='text-delta'){    process.stdout.write(part.textDelta);}}
```

For non-streaming calls with `generateText`, the reasoning summaries are available in the `reasoning` field of the response:

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const result =awaitgenerateText({  model: openai.responses('o3-mini'),  prompt:'Tell me about the Mission burrito debate in San Francisco.',  providerOptions:{    openai:{      reasoningSummary:'auto',},},});console.log('Reasoning:', result.reasoning);
```

Learn more about reasoning summaries in the [OpenAI documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries).


#### [PDF support](#pdf-support-1)


The OpenAI Responses API supports reading PDF files. You can pass PDF files as part of the message content using the `file` type:

```
const result =awaitgenerateText({  model: openai.responses('gpt-4o'),  messages:[{      role:'user',      content:[{type:'text',          text:'What is an embedding model?',},{type:'file',          data: fs.readFileSync('./data/ai.pdf'),          mimeType:'application/pdf',          filename:'ai.pdf',// optional},],},],});
```

The model will have access to the contents of the PDF file and respond to questions about it. The PDF file should be passed using the `data` field, and the `mimeType` should be set to `'application/pdf'`.


#### [Structured Outputs](#structured-outputs-1)


The OpenAI Responses API supports structured outputs. You can enforce structured outputs using `generateObject` or `streamObject`, which expose a `schema` option. Additionally, you can pass a Zod or JSON Schema object to the `experimental_output` option when using `generateText` or `streamText`.

```
// Using generateObjectconst result =awaitgenerateObject({  model: openai.responses('gpt-4.1'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(        z.object({          name: z.string(),          amount: z.string(),}),),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});// Using generateTextconst result =awaitgenerateText({  model: openai.responses('gpt-4.1'),  prompt:'How do I make a pizza?',  experimental_output:Output.object({    schema: z.object({      ingredients: z.array(z.string()),      steps: z.array(z.string()),}),}),});
```


### [Completion Models](#completion-models)


You can create models that call the [OpenAI completions API](https://platform.openai.com/docs/api-reference/completions) using the `.completion()` factory method. The first argument is the model id. Currently only `gpt-3.5-turbo-instruct` is supported.

```
const model = openai.completion('gpt-3.5-turbo-instruct');
```

OpenAI completion models support also some model specific settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model = openai.completion('gpt-3.5-turbo-instruct',{  echo:true,// optional, echo the prompt in addition to the completion  logitBias:{// optional likelihood for specific tokens'50256':-100,},  suffix:'some text',// optional suffix that comes after a completion of inserted text  user:'test-user',// optional unique user identifier});
```

The following optional settings are available for OpenAI completion models:

-   **echo**: *boolean*

    Echo back the prompt in addition to the completion.

-   **logitBias** *Record<number, number>*

    Modifies the likelihood of specified tokens appearing in the completion.

    Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

    As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.

-   **logprobs** *boolean | number*

    Return the log probabilities of the tokens. Including logprobs will increase the response size and can slow down response times. However, it can be useful to better understand how the model is behaving.

    Setting to true will return the log probabilities of the tokens that were generated.

    Setting to a number will return the log probabilities of the top n tokens that were generated.

-   **suffix** *string*

    The suffix that comes after a completion of inserted text.

-   **user** *string*

    A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).



### [Model Capabilities](#model-capabilities)


Model

Image Input

Audio Input

Object Generation

Tool Usage

`gpt-4.1`

`gpt-4.1-mini`

`gpt-4.1-nano`

`gpt-4o`

`gpt-4o-mini`

`gpt-4o-audio-preview`

`gpt-4-turbo`

`gpt-4`

`gpt-3.5-turbo`

`o1`

`o1-mini`

`o1-preview`

`o3-mini`

`o3`

`o4-mini`

`chatgpt-4o-latest`

The table above lists popular models. Please see the [OpenAI docs](https://platform.openai.com/docs/models) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.


## [Embedding Models](#embedding-models)


You can create models that call the [OpenAI embeddings API](https://platform.openai.com/docs/api-reference/embeddings) using the `.embedding()` factory method.

```
const model = openai.embedding('text-embedding-3-large');
```

OpenAI embedding models support several additional settings. You can pass them as an options argument:

```
const model = openai.embedding('text-embedding-3-large',{  dimensions:512// optional, number of dimensions for the embedding  user:'test-user'// optional unique user identifier})
```

The following optional settings are available for OpenAI embedding models:

-   **dimensions**: *number*

    The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.

-   **user** *string*

    A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).



### [Model Capabilities](#model-capabilities-1)


Model

Default Dimensions

Custom Dimensions

`text-embedding-3-large`

3072

`text-embedding-3-small`

1536

`text-embedding-ada-002`

1536


## [Image Models](#image-models)


You can create models that call the [OpenAI image generation API](https://platform.openai.com/docs/api-reference/images) using the `.image()` factory method.

```
const model = openai.image('dall-e-3');
```

Dall-E models do not support the `aspectRatio` parameter. Use the `size` parameter instead.


### [Model Capabilities](#model-capabilities-2)


Model

Sizes

`gpt-image-1`

1024x1024, 1536x1024, 1024x1536

`dall-e-3`

1024x1024, 1792x1024, 1024x1792

`dall-e-2`

256x256, 512x512, 1024x1024

You can pass optional `providerOptions` to the image model. These are prone to change by OpenAI and are model dependent. For example, the `gpt-image-1` model supports the `quality` option:

```
const{ image }=awaitgenerateImage({  model: openai.image('gpt-image-1'),  prompt:'A salamander at sunrise in a forest pond in the Seychelles.',  providerOptions:{    openai:{ quality:'high'},},});
```

For more on `generateImage()` see [Image Generation](/docs/ai-sdk-core/image-generation).

For more information on the available OpenAI image model options, see the [OpenAI API reference](https://platform.openai.com/docs/api-reference/images/create).


## [Transcription Models](#transcription-models)


You can create models that call the [OpenAI transcription API](https://platform.openai.com/docs/api-reference/audio/transcribe) using the `.transcription()` factory method.

The first argument is the model id e.g. `whisper-1`.

```
const model = openai.transcription('whisper-1');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format will improve accuracy and latency.

```
import{ experimental_transcribe as transcribe }from'ai';import{ openai }from'@ai-sdk/openai';const result =awaittranscribe({  model: openai.transcription('whisper-1'),  audio:newUint8Array([1,2,3,4]),  providerOptions:{ openai:{ language:'en'}},});
```

The following provider options are available:

-   **timestampGranularities** *string\[\]* The granularity of the timestamps in the transcription. Defaults to `['segment']`. Possible values are `['word']`, `['segment']`, and `['word', 'segment']`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.

-   **language** *string* The language of the input audio. Supplying the input language in ISO-639-1 format (e.g. 'en') will improve accuracy and latency. Optional.

-   **prompt** *string* An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. Optional.

-   **temperature** *number* The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. Defaults to 0. Optional.

-   **include** *string\[\]* Additional information to include in the transcription response.



### [Model Capabilities](#model-capabilities-3)


Model

Transcription

Duration

Segments

Language

`whisper-1`

`gpt-4o-mini-transcribe`

`gpt-4o-transcribe`


## [Speech Models](#speech-models)


You can create models that call the [OpenAI speech API](https://platform.openai.com/docs/api-reference/audio/speech) using the `.speech()` factory method.

The first argument is the model id e.g. `tts-1`.

```
const model = openai.speech('tts-1');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying a voice to use for the generated audio.

```
import{ experimental_generateSpeech as generateSpeech }from'ai';import{ openai }from'@ai-sdk/openai';const result =awaitgenerateSpeech({  model: openai.speech('tts-1'),  text:'Hello, world!',  providerOptions:{ openai:{}},});
```

-   **instructions** *string* Control the voice of your generated audio with additional instructions e.g. "Speak in a slow and steady tone". Does not work with `tts-1` or `tts-1-hd`. Optional.

-   **response\_format** *string* The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`. Defaults to `mp3`. Optional.

-   **speed** *number* The speed of the generated audio. Select a value from 0.25 to 4.0. Defaults to 1.0. Optional.



### [Model Capabilities](#model-capabilities-4)


Model

Instructions

`tts-1`

`tts-1-hd`

`gpt-4o-mini-tts`
```

### 439. `providers/ai-sdk-providers/perplexity.md`

```markdown
# Perplexity Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/perplexity
description: Learn how to use Perplexity's Sonar API with the AI SDK.
---


# [Perplexity Provider](#perplexity-provider)


The [Perplexity](https://sonar.perplexity.ai) provider offers access to Sonar API - a language model that uniquely combines real-time web search with natural language processing. Each response is grounded in current web data and includes detailed citations, making it ideal for research, fact-checking, and obtaining up-to-date information.

API keys can be obtained from the [Perplexity Platform](https://docs.perplexity.ai).


## [Setup](#setup)


The Perplexity provider is available via the `@ai-sdk/perplexity` module. You can install it with:

pnpm

npm

yarn

pnpm add @ai-sdk/perplexity


## [Provider Instance](#provider-instance)


You can import the default provider instance `perplexity` from `@ai-sdk/perplexity`:

```
import{ perplexity }from'@ai-sdk/perplexity';
```

For custom configuration, you can import `createPerplexity` and create a provider instance with your settings:

```
import{ createPerplexity }from'@ai-sdk/perplexity';const perplexity =createPerplexity({  apiKey: process.env.PERPLEXITY_API_KEY??'',});
```

You can use the following optional settings to customize the Perplexity provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls. The default prefix is `https://api.perplexity.ai`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `PERPLEXITY_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.



## [Language Models](#language-models)


You can create Perplexity models using a provider instance:

```
import{ perplexity }from'@ai-sdk/perplexity';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:perplexity('sonar-pro'),  prompt:'What are the latest developments in quantum computing?',});
```


### [Sources](#sources)


Websites that have been used to generate the response are included in the `sources` property of the result:

```
import{ perplexity }from'@ai-sdk/perplexity';import{ generateText }from'ai';const{ text, sources }=awaitgenerateText({  model:perplexity('sonar-pro'),  prompt:'What are the latest developments in quantum computing?',});console.log(sources);
```


### [Provider Options & Metadata](#provider-options--metadata)


The Perplexity provider includes additional metadata in the response through `providerMetadata`. Additional configuration options are available through `providerOptions`.

```
const result =awaitgenerateText({  model:perplexity('sonar-pro'),  prompt:'What are the latest developments in quantum computing?',  providerOptions:{    perplexity:{      return_images:true,// Enable image responses (Tier-2 Perplexity users only)},},});console.log(result.providerMetadata);// Example output:// {//   perplexity: {//     usage: { citationTokens: 5286, numSearchQueries: 1 },//     images: [//       { imageUrl: "https://example.com/image1.jpg", originUrl: "https://elsewhere.com/page1", height: 1280, width: 720 },//       { imageUrl: "https://example.com/image2.jpg", originUrl: "https://elsewhere.com/page2", height: 1280, width: 720 }//     ]//   },// }
```

The metadata includes:

-   `usage`: Object containing `citationTokens` and `numSearchQueries` metrics
-   `images`: Array of image URLs when `return_images` is enabled (Tier-2 users only)

You can enable image responses by setting `return_images: true` in the provider options. This feature is only available to Perplexity Tier-2 users and above.

For more details about Perplexity's capabilities, see the [Perplexity chat completion docs](https://docs.perplexity.ai/api-reference/chat-completions).


## [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`sonar-pro`

`sonar`

`sonar-deep-research`

Please see the [Perplexity docs](https://docs.perplexity.ai) for detailed API documentation and the latest updates.
```

### 440. `providers/ai-sdk-providers/replicate.md`

```markdown
# Replicate Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/replicate
description: Learn how to use Replicate models with the AI SDK.
---


# [Replicate Provider](#replicate-provider)


[Replicate](https://replicate.com/) is a platform for running open-source AI models. It is a popular choice for running image generation models.


## [Setup](#setup)


The Replicate provider is available via the `@ai-sdk/replicate` module. You can install it with

pnpm

npm

yarn

pnpm add ai @ai-sdk/replicate


## [Provider Instance](#provider-instance)


You can import the default provider instance `replicate` from `@ai-sdk/replicate`:

```
import{ replicate }from'@ai-sdk/replicate';
```

If you need a customized setup, you can import `createReplicate` from `@ai-sdk/replicate` and create a provider instance with your settings:

```
import{ createReplicate }from'@ai-sdk/replicate';const replicate =createReplicate({  apiToken: process.env.REPLICATE_API_TOKEN??'',});
```

You can use the following optional settings to customize the Replicate provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.replicate.com/v1`.

-   **apiToken** *string*

    API token that is being sent using the `Authorization` header. It defaults to the `REPLICATE_API_TOKEN` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.



## [Image Models](#image-models)


You can create Replicate image models using the `.image()` factory method. For more on image generation with the AI SDK see [generateImage()](/docs/reference/ai-sdk-core/generate-image).

Model support for `size` and other parameters varies by model. Check the model's documentation on [Replicate](https://replicate.com/explore) for supported options and additional parameters that can be passed via `providerOptions.replicate`.


### [Supported Image Models](#supported-image-models)


The following image models are currently supported by the Replicate provider:

-   [black-forest-labs/flux-1.1-pro-ultra](https://replicate.com/black-forest-labs/flux-1.1-pro-ultra)
-   [black-forest-labs/flux-1.1-pro](https://replicate.com/black-forest-labs/flux-1.1-pro)
-   [black-forest-labs/flux-dev](https://replicate.com/black-forest-labs/flux-dev)
-   [black-forest-labs/flux-pro](https://replicate.com/black-forest-labs/flux-pro)
-   [black-forest-labs/flux-schnell](https://replicate.com/black-forest-labs/flux-schnell)
-   [ideogram-ai/ideogram-v2-turbo](https://replicate.com/ideogram-ai/ideogram-v2-turbo)
-   [ideogram-ai/ideogram-v2](https://replicate.com/ideogram-ai/ideogram-v2)
-   [luma/photon-flash](https://replicate.com/luma/photon-flash)
-   [luma/photon](https://replicate.com/luma/photon)
-   [recraft-ai/recraft-v3-svg](https://replicate.com/recraft-ai/recraft-v3-svg)
-   [recraft-ai/recraft-v3](https://replicate.com/recraft-ai/recraft-v3)
-   [stability-ai/stable-diffusion-3.5-large-turbo](https://replicate.com/stability-ai/stable-diffusion-3.5-large-turbo)
-   [stability-ai/stable-diffusion-3.5-large](https://replicate.com/stability-ai/stable-diffusion-3.5-large)
-   [stability-ai/stable-diffusion-3.5-medium](https://replicate.com/stability-ai/stable-diffusion-3.5-medium)

You can also use [versioned models](https://replicate.com/docs/topics/models/versions). The id for versioned models is the Replicate model id followed by a colon and the version ID (`$modelId:$versionId`), e.g. `bytedance/sdxl-lightning-4step:5599ed30703defd1d160a25a63321b4dec97101d98b4674bcc56e41f62f35637`.


### [Basic Usage](#basic-usage)


```
import{ replicate }from'@ai-sdk/replicate';import{ experimental_generateImage as generateImage }from'ai';import{ writeFile }from'node:fs/promises';const{ image }=awaitgenerateImage({  model: replicate.image('black-forest-labs/flux-schnell'),  prompt:'The Loch Ness Monster getting a manicure',  aspectRatio:'16:9',});awaitwriteFile('image.webp', image.uint8Array);console.log('Image saved as image.webp');
```


### [Model-specific options](#model-specific-options)


```
import{ replicate }from'@ai-sdk/replicate';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: replicate.image('recraft-ai/recraft-v3'),  prompt:'The Loch Ness Monster getting a manicure',  size:'1365x1024',  providerOptions:{    replicate:{      style:'realistic_image',},},});
```


### [Versioned Models](#versioned-models)


```
import{ replicate }from'@ai-sdk/replicate';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: replicate.image('bytedance/sdxl-lightning-4step:5599ed30703defd1d160a25a63321b4dec97101d98b4674bcc56e41f62f35637',),  prompt:'The Loch Ness Monster getting a manicure',});
```

For more details, see the [Replicate models page](https://replicate.com/explore).
```

### 441. `providers/ai-sdk-providers/revai.md`

```markdown
# Rev.ai Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/revai
description: Learn how to use the Rev.ai provider for the AI SDK.
---


# [Rev.ai Provider](#revai-provider)


The [Rev.ai](https://www.rev.ai/) provider contains language model support for the Rev.ai transcription API.


## [Setup](#setup)


The Rev.ai provider is available in the `@ai-sdk/revai` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/revai


## [Provider Instance](#provider-instance)


You can import the default provider instance `revai` from `@ai-sdk/revai`:

```
import{ revai }from'@ai-sdk/revai';
```

If you need a customized setup, you can import `createRevai` from `@ai-sdk/revai` and create a provider instance with your settings:

```
import{ createRevai }from'@ai-sdk/revai';const revai =createRevai({// custom settings, e.g.  fetch: customFetch,});
```

You can use the following optional settings to customize the Rev.ai provider instance:

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `REVAI_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Transcription Models](#transcription-models)


You can create models that call the [Rev.ai transcription API](https://www.rev.ai/docs/api/transcription) using the `.transcription()` factory method.

The first argument is the model id e.g. `machine`.

```
const model = revai.transcription('machine');
```

You can also pass additional provider-specific options using the `providerOptions` argument. For example, supplying the input language in ISO-639-1 (e.g. `en`) format can sometimes improve transcription performance if known beforehand.

```
import{ experimental_transcribe as transcribe }from'ai';import{ revai }from'@ai-sdk/revai';import{ readFile }from'fs/promises';const result =awaittranscribe({  model: revai.transcription('machine'),  audio:awaitreadFile('audio.mp3'),  providerOptions:{ revai:{ language:'en'}},});
```

The following provider options are available:

-   **metadata** *string*

    Optional metadata that was provided during job submission.

-   **notification\_config** *object*

    Optional configuration for a callback url to invoke when processing is complete.

    -   **url** *string* - Callback url to invoke when processing is complete.
    -   **auth\_headers** *object* - Optional authorization headers, if needed to invoke the callback.
-   **delete\_after\_seconds** *integer*

    Amount of time after job completion when job is auto-deleted.

-   **verbatim** *boolean*

    Configures the transcriber to transcribe every syllable, including all false starts and disfluencies.

-   **rush** *boolean*

    \[HIPAA Unsupported\] Only available for human transcriber option. When set to true, your job is given higher priority.

-   **skip\_diarization** *boolean*

    Specify if speaker diarization will be skipped by the speech engine.

-   **skip\_postprocessing** *boolean*

    Only available for English and Spanish languages. User-supplied preference on whether to skip post-processing operations.

-   **skip\_punctuation** *boolean*

    Specify if "punct" type elements will be skipped by the speech engine.

-   **remove\_disfluencies** *boolean*

    When set to true, disfluencies (like 'ums' and 'uhs') will not appear in the transcript.

-   **remove\_atmospherics** *boolean*

    When set to true, atmospherics (like `<laugh>`, `<affirmative>`) will not appear in the transcript.

-   **filter\_profanity** *boolean*

    When enabled, profanities will be filtered by replacing characters with asterisks except for the first and last.

-   **speaker\_channels\_count** *integer*

    Only available for English, Spanish and French languages. Specify the total number of unique speaker channels in the audio.

-   **speakers\_count** *integer*

    Only available for English, Spanish and French languages. Specify the total number of unique speakers in the audio.

-   **diarization\_type** *string*

    Specify diarization type. Possible values: "standard" (default), "premium".

-   **custom\_vocabulary\_id** *string*

    Supply the id of a pre-completed custom vocabulary submitted through the Custom Vocabularies API.

-   **custom\_vocabularies** *Array*

    Specify a collection of custom vocabulary to be used for this job.

-   **strict\_custom\_vocabulary** *boolean*

    If true, only exact phrases will be used as custom vocabulary.

-   **summarization\_config** *object*

    Specify summarization options.

    -   **model** *string* - Model type for summarization. Possible values: "standard" (default), "premium".
    -   **type** *string* - Summarization formatting type. Possible values: "paragraph" (default), "bullets".
    -   **prompt** *string* - Custom prompt for flexible summaries (mutually exclusive with type).
-   **translation\_config** *object*

    Specify translation options.

    -   **target\_languages** *Array* - Array of target languages for translation.
    -   **model** *string* - Model type for translation. Possible values: "standard" (default), "premium".
-   **language** *string*

    Language is provided as a ISO 639-1 language code. Default is "en".

-   **forced\_alignment** *boolean*

    When enabled, provides improved accuracy for per-word timestamps for a transcript. Default is `false`.

    Currently supported languages:

    -   English (en, en-us, en-gb)
    -   French (fr)
    -   Italian (it)
    -   German (de)
    -   Spanish (es)

    Note: This option is not available in low-cost environment.



### [Model Capabilities](#model-capabilities)


Model

Transcription

Duration

Segments

Language

`machine`

`human`

`low_cost`

`fusion`
```

### 442. `providers/ai-sdk-providers/togetherai.md`

```markdown
# Together.ai Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/togetherai
description: Learn how to use Together.ai's models with the AI SDK.
---


# [Together.ai Provider](#togetherai-provider)


The [Together.ai](https://together.ai) provider contains support for 200+ open-source models through the [Together.ai API](https://docs.together.ai/reference).


## [Setup](#setup)


The Together.ai provider is available via the `@ai-sdk/togetherai` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/togetherai


## [Provider Instance](#provider-instance)


You can import the default provider instance `togetherai` from `@ai-sdk/togetherai`:

```
import{ togetherai }from'@ai-sdk/togetherai';
```

If you need a customized setup, you can import `createTogetherAI` from `@ai-sdk/togetherai` and create a provider instance with your settings:

```
import{ createTogetherAI }from'@ai-sdk/togetherai';const togetherai =createTogetherAI({  apiKey: process.env.TOGETHER_AI_API_KEY??'',});
```

You can use the following optional settings to customize the Together.ai provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.together.xyz/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `TOGETHER_AI_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create [Together.ai models](https://docs.together.ai/docs/serverless-models) using a provider instance. The first argument is the model id, e.g. `google/gemma-2-9b-it`.

```
const model =togetherai('google/gemma-2-9b-it');
```


### [Reasoning Models](#reasoning-models)


Together.ai exposes the thinking of `deepseek-ai/DeepSeek-R1` in the generated text using the `<think>` tag. You can use the `extractReasoningMiddleware` to extract this reasoning and expose it as a `reasoning` property on the result:

```
import{ togetherai }from'@ai-sdk/togetherai';import{ wrapLanguageModel, extractReasoningMiddleware }from'ai';const enhancedModel =wrapLanguageModel({  model:togetherai('deepseek-ai/DeepSeek-R1'),  middleware:extractReasoningMiddleware({ tagName:'think'}),});
```

You can then use that enhanced model in functions like `generateText` and `streamText`.


### [Example](#example)


You can use Together.ai language models to generate text with the `generateText` function:

```
import{ togetherai }from'@ai-sdk/togetherai';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:togetherai('meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Together.ai language models can also be used in the `streamText` function (see [AI SDK Core](/docs/ai-sdk-core)).

The Together.ai provider also supports [completion models](https://docs.together.ai/docs/serverless-models#language-models) via (following the above example code) `togetherai.completionModel()` and [embedding models](https://docs.together.ai/docs/serverless-models#embedding-models) via `togetherai.textEmbeddingModel()`.


## [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo`

`meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo`

`mistralai/Mixtral-8x22B-Instruct-v0.1`

`mistralai/Mistral-7B-Instruct-v0.3`

`deepseek-ai/DeepSeek-V3`

`google/gemma-2b-it`

`Qwen/Qwen2.5-72B-Instruct-Turbo`

`databricks/dbrx-instruct`

The table above lists popular models. Please see the [Together.ai docs](https://docs.together.ai/docs/serverless-models) for a full list of available models. You can also pass any available provider model ID as a string if needed.


## [Image Models](#image-models)


You can create Together.ai image models using the `.image()` factory method. For more on image generation with the AI SDK see [generateImage()](/docs/reference/ai-sdk-core/generate-image).

```
import{ togetherai }from'@ai-sdk/togetherai';import{ experimental_generateImage as generateImage }from'ai';const{ images }=awaitgenerateImage({  model: togetherai.image('black-forest-labs/FLUX.1-dev'),  prompt:'A delighted resplendent quetzal mid flight amidst raindrops',});
```

You can pass optional provider-specific request parameters using the `providerOptions` argument.

```
import{ togetherai }from'@ai-sdk/togetherai';import{ experimental_generateImage as generateImage }from'ai';const{ images }=awaitgenerateImage({  model: togetherai.image('black-forest-labs/FLUX.1-dev'),  prompt:'A delighted resplendent quetzal mid flight amidst raindrops',  size:'512x512',// Optional additional provider-specific request parameters  providerOptions:{    togetherai:{      steps:40,},},});
```

For a complete list of available provider-specific options, see the [Together.ai Image Generation API Reference](https://docs.together.ai/reference/post_images-generations).


### [Model Capabilities](#model-capabilities-1)


Together.ai image models support various image dimensions that vary by model. Common sizes include 512x512, 768x768, and 1024x1024, with some models supporting up to 1792x1792. The default size is 1024x1024.

Available Models

`stabilityai/stable-diffusion-xl-base-1.0`

`black-forest-labs/FLUX.1-dev`

`black-forest-labs/FLUX.1-dev-lora`

`black-forest-labs/FLUX.1-schnell`

`black-forest-labs/FLUX.1-canny`

`black-forest-labs/FLUX.1-depth`

`black-forest-labs/FLUX.1-redux`

`black-forest-labs/FLUX.1.1-pro`

`black-forest-labs/FLUX.1-pro`

`black-forest-labs/FLUX.1-schnell-Free`

Please see the [Together.ai models page](https://docs.together.ai/docs/serverless-models#image-models) for a full list of available image models and their capabilities.
```

### 443. `providers/ai-sdk-providers/vercel.md`

```markdown
# Vercel Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/vercel
description: Learn how to use Vercel's v0 models with the AI SDK.
---


# [Vercel Provider](#vercel-provider)


The [Vercel](https://vercel.com) provider gives you access to the [v0 API](https://vercel.com/docs/v0/api), designed for building modern web applications. The `v0-1.0-md` model supports text and image inputs and provides fast streaming responses.

You can create your Vercel API key at [v0.dev](https://v0.dev/chat/settings/keys).

The v0 API is currently in beta and requires a Premium or Team plan with usage-based billing enabled. For details, visit the [pricing page](https://v0.dev/pricing). To request a higher limit, contact Vercel at [support@v0.dev](mailto:support@v0.dev).


## [Features](#features)


-   **Framework aware completions**: Evaluated on modern stacks like Next.js and Vercel
-   **Auto-fix**: Identifies and corrects common coding issues during generation
-   **Quick edit**: Streams inline edits as they're available
-   **OpenAI compatible**: Can be used with any tool or SDK that supports OpenAI's API format
-   **Multimodal**: Supports both text and image inputs


## [Setup](#setup)


The Vercel provider is available via the `@ai-sdk/vercel` module. You can install it with:

pnpm

npm

yarn

pnpm add @ai-sdk/vercel


## [Provider Instance](#provider-instance)


You can import the default provider instance `vercel` from `@ai-sdk/vercel`:

```
import{ vercel }from'@ai-sdk/vercel';
```

If you need a customized setup, you can import `createVercel` from `@ai-sdk/vercel` and create a provider instance with your settings:

```
import{ createVercel }from'@ai-sdk/vercel';const vercel =createVercel({  apiKey: process.env.VERCEL_API_KEY??'',});
```

You can use the following optional settings to customize the Vercel provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls. The default prefix is `https://api.v0.dev/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `VERCEL_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create language models using a provider instance. The first argument is the model ID, for example:

```
import{ vercel }from'@ai-sdk/vercel';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:vercel('v0-1.0-md'),  prompt:'Create a Next.js AI chatbot',});
```

Vercel language models can also be used in the `streamText` function (see [AI SDK Core](/docs/ai-sdk-core)).


## [Example with AI SDK](#example-with-ai-sdk)


```
import{ generateText }from'ai';import{ createVercel }from'@ai-sdk/vercel';const vercel =createVercel({  baseURL:'https://api.v0.dev/v1',  apiKey: process.env.VERCEL_API_KEY,});const{ text }=awaitgenerateText({  model:vercel('v0-1.0-md'),  prompt:'Create a Next.js AI chatbot with authentication',});
```


## [Models](#models)



### [v0-1.0-md](#v0-10-md)


The `v0-1.0-md` model is the default model served by the v0 API.

Capabilities:

-   Supports text and image inputs (multimodal)
-   Supports function/tool calls
-   Streaming responses with low latency
-   Optimized for frontend and full-stack web development


## [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`v0-1.0-md`
```

### 444. `providers/ai-sdk-providers/xai.md`

```markdown
# xAI Grok Provider


---
url: https://ai-sdk.dev/providers/ai-sdk-providers/xai
description: Learn how to use xAI Grok.
---


# [xAI Grok Provider](#xai-grok-provider)


The [xAI Grok](https://x.ai) provider contains language model support for the [xAI API](https://x.ai/api).


## [Setup](#setup)


The xAI Grok provider is available via the `@ai-sdk/xai` module. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/xai


## [Provider Instance](#provider-instance)


You can import the default provider instance `xai` from `@ai-sdk/xai`:

```
import{ xai }from'@ai-sdk/xai';
```

If you need a customized setup, you can import `createXai` from `@ai-sdk/xai` and create a provider instance with your settings:

```
import{ createXai }from'@ai-sdk/xai';const xai =createXai({  apiKey:'your-api-key',});
```

You can use the following optional settings to customize the xAI provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.x.ai/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `XAI_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create [xAI models](https://console.x.ai) using a provider instance. The first argument is the model id, e.g. `grok-beta`.

```
const model =xai('grok-3');
```


### [Example](#example)


You can use xAI language models to generate text with the `generateText` function:

```
import{ xai }from'@ai-sdk/xai';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:xai('grok-3'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

xAI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core)).


### [Chat Models](#chat-models)


xAI chat models also support some model specific settings that are not part of the [standard call settings](/docs/ai-sdk-core/settings). You can pass them as an options argument:

```
const model =xai('grok-3',{  user:'test-user',// optional unique user identifier});
```

The following optional settings are available for xAI chat models:

-   **user** *string*

    A unique identifier representing your end-user, which can help xAI to monitor and detect abuse.


xAI chat models also support some model specific provider options. You can pass them in `providerOptions` argument:

```
const model =xai('grok-3');awaitgenerateText({  model,  providerOptions:{    xai:{      reasoningEffort:'high',},},});
```

The following optional provider options are available for xAI chat models:

-   **reasoningEffort** *'low' | 'medium' | 'high'*

    Reasoning effort for reasoning models. Defaults to `medium`. If you use `providerOptions` to set the `reasoningEffort` option, this model setting will be ignored.



## [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`grok-3`

`grok-3-fast`

`grok-3-mini`

`grok-3-mini-fast`

`grok-2-1212`

`grok-2-vision-1212`

`grok-beta`

`grok-vision-beta`

The table above lists popular models. Please see the [xAI docs](https://docs.x.ai/docs#models) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.


## [Image Models](#image-models)


You can create xAI image models using the `.imageModel()` factory method. For more on image generation with the AI SDK see [generateImage()](/docs/reference/ai-sdk-core/generate-image).

```
import{ xai }from'@ai-sdk/xai';import{ experimental_generateImage as generateImage }from'ai';const{ image }=awaitgenerateImage({  model: xai.image('grok-2-image'),  prompt:'A futuristic cityscape at sunset',});
```

The xAI image model does not currently support the `aspectRatio` or `size` parameters. Image size defaults to 1024x768.


### [Model-specific options](#model-specific-options)


You can customize the image generation behavior with model-specific settings:

```
import{ xai }from'@ai-sdk/xai';import{ experimental_generateImage as generateImage }from'ai';const{ images }=awaitgenerateImage({  model: xai.image('grok-2-image',{    maxImagesPerCall:5,// Default is 10}),  prompt:'A futuristic cityscape at sunset',  n:2,// Generate 2 images});
```


### [Model Capabilities](#model-capabilities-1)


Model

Sizes

Notes

`grok-2-image`

1024x768 (default)

xAI's text-to-image generation model, designed to create high-quality images from text prompts. It's trained on a diverse dataset and can generate images across various styles, subjects, and settings.
```

### 445. `providers/ai-sdk-providers.md`

```markdown
# AI SDK Providers


---
url: https://ai-sdk.dev/providers/ai-sdk-providers
description: Learn how to use AI SDK providers.
---


# [AI SDK Providers](#ai-sdk-providers)


The AI SDK comes with several providers that you can use to interact with different language models:

[

xAI Grok

Image InputImage GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/xai)[

OpenAI

Image InputImage GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/openai)[

Azure

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/azure)[

Anthropic

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/anthropic)[

Amazon Bedrock

Image InputImage GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/amazon-bedrock)[

Groq

Object GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/groq)[

Fal AI

Image Generation

](/providers/ai-sdk-providers/fal)[

DeepInfra

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/deepinfra)[

Google Generative AI

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/google-generative-ai)[

Google Vertex AI

Image InputImage GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/google-vertex)[

Mistral

Image InputObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/mistral)[

Together.ai

Object GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/togetherai)[

Cohere

Tool UsageTool Streaming

](/providers/ai-sdk-providers/cohere)[

Fireworks

Image GenerationObject GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/fireworks)[

DeepSeek

Object GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/deepseek)[

Cerebras

Object GenerationTool UsageTool Streaming

](/providers/ai-sdk-providers/cerebras)[

Perplexity

](/providers/ai-sdk-providers/perplexity)[

Luma AI

Image Generation

](/providers/ai-sdk-providers/luma)

There are also [community providers](./community-providers) that have been created using the [Language Model Specification](./community-providers/custom-providers).

[

LLamaCpp

Provider Dependent

](/providers/community-providers/llama-cpp)[

Ollama

Provider Dependent

](/providers/community-providers/ollama)[

Chrome AI

Provider Dependent

](/providers/community-providers/chrome-ai)[

Anthropic Vertex

Provider Dependent

](/providers/community-providers/anthropic-vertex-ai)[

Portkey

Provider Dependent

](/providers/community-providers/portkey)[

Cloudflare Workers AI

Provider Dependent

](/providers/community-providers/cloudflare-workers-ai)[

Write your own

Provider Dependent

](/providers/community-providers/custom-providers)


## [Provider support](#provider-support)


Not all providers support all AI SDK features. Here's a quick comparison of the capabilities of popular models:

Provider

Model

Image Input

Object Generation

Tool Usage

Tool Streaming

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-3`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-3-fast`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-3-mini`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-3-mini-fast`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-2-1212`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-2-vision-1212`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-beta`

[xAI Grok](/providers/ai-sdk-providers/xai)

`grok-vision-beta`

[Vercel](/providers/ai-sdk-providers/vercel)

`v0-1.0-md`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4.1`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4.1-mini`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4.1-nano`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4o`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4o-mini`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4-turbo`

[OpenAI](/providers/ai-sdk-providers/openai)

`gpt-4`

[OpenAI](/providers/ai-sdk-providers/openai)

`o1`

[OpenAI](/providers/ai-sdk-providers/openai)

`o1-mini`

[OpenAI](/providers/ai-sdk-providers/openai)

`o1-preview`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-3-7-sonnet-20250219`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-3-5-sonnet-20241022`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-3-5-sonnet-20240620`

[Anthropic](/providers/ai-sdk-providers/anthropic)

`claude-3-5-haiku-20241022`

[Groq](/providers/ai-sdk-providers/groq)

`meta-llama/llama-4-scout-17b-16e-instruct`

[Groq](/providers/ai-sdk-providers/groq)

`deepseek-r1-distill-llama-70b`

[Groq](/providers/ai-sdk-providers/groq)

`llama-3.3-70b-versatile`

[Groq](/providers/ai-sdk-providers/groq)

`llama-3.1-8b-instant`

[Groq](/providers/ai-sdk-providers/groq)

`mistral-saba-24b`

[Groq](/providers/ai-sdk-providers/groq)

`qwen-qwq-32b`

[Groq](/providers/ai-sdk-providers/groq)

`mixtral-8x7b-32768`

[Groq](/providers/ai-sdk-providers/groq)

`gemma2-9b-it`

[DeepInfra](/providers/ai-sdk-providers/deepinfra)

`meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8`

[DeepInfra](/providers/ai-sdk-providers/deepinfra)

`meta-llama/Llama-4-Scout-17B-16E-Instruct`

[DeepInfra](/providers/ai-sdk-providers/deepinfra)

`meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo`

[DeepInfra](/providers/ai-sdk-providers/deepinfra)

`meta-llama/Llama-3.3-70B-Instruct`

[DeepInfra](/providers/ai-sdk-providers/deepinfra)

`deepseek-ai/DeepSeek-V3`

[DeepInfra](/providers/ai-sdk-providers/deepinfra)

`deepseek-ai/DeepSeek-R1`

[DeepInfra](/providers/ai-sdk-providers/deepinfra)

`deepseek-ai/DeepSeek-R1-Distill-Llama-70B`

[DeepInfra](/providers/ai-sdk-providers/deepinfra)

`deepseek-ai/DeepSeek-R1-Turbo`

[Mistral](/providers/ai-sdk-providers/mistral)

`pixtral-large-latest`

[Mistral](/providers/ai-sdk-providers/mistral)

`mistral-large-latest`

[Mistral](/providers/ai-sdk-providers/mistral)

`mistral-small-latest`

[Mistral](/providers/ai-sdk-providers/mistral)

`pixtral-12b-2409`

[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai)

`gemini-2.0-flash-exp`

[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai)

`gemini-1.5-flash`

[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai)

`gemini-1.5-pro`

[Google Vertex](/providers/ai-sdk-providers/google-vertex)

`gemini-2.0-flash-exp`

[Google Vertex](/providers/ai-sdk-providers/google-vertex)

`gemini-1.5-flash`

[Google Vertex](/providers/ai-sdk-providers/google-vertex)

`gemini-1.5-pro`

[DeepSeek](/providers/ai-sdk-providers/deepseek)

`deepseek-chat`

[DeepSeek](/providers/ai-sdk-providers/deepseek)

`deepseek-reasoner`

[Cerebras](/providers/ai-sdk-providers/cerebras)

`llama3.1-8b`

[Cerebras](/providers/ai-sdk-providers/cerebras)

`llama3.3-70b`

This table is not exhaustive. Additional models can be found in the provider documentation pages and on the provider websites.
```

### 446. `providers/community-providers/anthropic-vertex-ai.md`

```markdown
# AnthropicVertex Provider


---
url: https://ai-sdk.dev/providers/community-providers/anthropic-vertex-ai
description: Learn how to use the Anthropic Vertex provider for the AI SDK.
---


# [AnthropicVertex Provider](#anthropicvertex-provider)


Anthropic for Google Vertex is also support by the [AI SDK Google Vertex provider](/providers/ai-sdk-providers/google-vertex).

[nalaso/anthropic-vertex-ai](https://github.com/nalaso/anthropic-vertex-ai) is a community provider that uses Anthropic models through Vertex AI to provide language model support for the AI SDK.


## [Setup](#setup)


The AnthropicVertex provider is available in the `anthropic-vertex-ai` module. You can install it with:

pnpm

npm

yarn

pnpm add anthropic-vertex-ai


## [Provider Instance](#provider-instance)


You can import the default provider instance `anthropicVertex` from `anthropic-vertex-ai`:

```
import{ anthropicVertex }from'anthropic-vertex-ai';
```

If you need a customized setup, you can import `createAnthropicVertex` from `anthropic-vertex-ai` and create a provider instance with your settings:

```
import{ createAnthropicVertex }from'anthropic-vertex-ai';const anthropicVertex =createAnthropicVertex({  region:'us-central1',  projectId:'your-project-id',// other options});
```

You can use the following optional settings to customize the AnthropicVertex provider instance:

-   **region** *string*

    Your Google Vertex region. Defaults to the `GOOGLE_VERTEX_REGION` environment variable.

-   **projectId** *string*

    Your Google Vertex project ID. Defaults to the `GOOGLE_VERTEX_PROJECT_ID` environment variable.

-   **googleAuth** *GoogleAuth*

    Optional. The Authentication options provided by google-auth-library.

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g., to use proxy servers. The default prefix is `https://{region}-aiplatform.googleapis.com/v1`.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom fetch implementation. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g., testing.



## [Language Models](#language-models)


You can create models that call the Anthropic API through Vertex AI using the provider instance. The first argument is the model ID, e.g., `claude-3-sonnet@20240229`:

```
const model =anthropicVertex('claude-3-sonnet@20240229');
```


### [Example: Generate Text](#example-generate-text)


You can use AnthropicVertex language models to generate text with the `generateText` function:

```
import{ anthropicVertex }from'anthropic-vertex-ai';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:anthropicVertex('claude-3-sonnet@20240229'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

AnthropicVertex language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core) for more information).


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`claude-3-5-sonnet@20240620`

`claude-3-opus@20240229`

`claude-3-sonnet@20240229`

`claude-3-haiku@20240307`


## [Environment Variables](#environment-variables)


To use the AnthropicVertex provider, you need to set up the following environment variables:

-   `GOOGLE_VERTEX_REGION`: Your Google Vertex region (e.g., 'us-central1')
-   `GOOGLE_VERTEX_PROJECT_ID`: Your Google Cloud project ID

Make sure to set these variables in your environment or in a `.env` file in your project root.


## [Authentication](#authentication)


The AnthropicVertex provider uses Google Cloud authentication. Make sure you have set up your Google Cloud credentials properly. You can either use a service account key file or default application credentials.

For more information on setting up authentication, refer to the [Google Cloud Authentication guide](https://cloud.google.com/docs/authentication).
```

### 447. `providers/community-providers/azure-ai.md`

```markdown
# Azure Custom Provider for AI SDK


---
url: https://ai-sdk.dev/providers/community-providers/azure-ai
description: Learn how to use the @quail-ai/azure-ai-provider for the AI SDK.
---


# [Azure Custom Provider for AI SDK](#azure-custom-provider-for-ai-sdk)


The **[Quail-AI/azure-ai-provider](https://github.com/QuailAI/azure-ai-provider)** enables unofficial integration with Azure-hosted language models that use Azure's native APIs instead of the standard OpenAI API format.


## [Language Models](#language-models)


This provider works with any model in the Azure AI Foundry that is compatible with the Azure-Rest AI-inference API. **Note:** This provider is not compatible with the Azure OpenAI models. For those, please use the [Azure OpenAI Provider](/providers/ai-sdk-providers/azure).


### [Models Tested:](#models-tested)


-   DeepSeek-R1
-   LLama 3.3-70B Instruct
-   Cohere-command-r-08-2024


## [Setup](#setup)



### [Installation](#installation)


Install the provider via npm:

```
npm i @quail-ai/azure-ai-provider
```


## [Provider Instance](#provider-instance)


Create an Azure AI resource and set up your endpoint URL and API key. Add the following to your `.env` file:

```
AZURE_API_ENDPOINT=https://<your-resource>.services.ai.azure.com/modelsAZURE_API_KEY=<your-api-key>
```

Import `createAzure` from the package to create your provider instance:

```
import{ createAzure }from'@quail-ai/azure-ai-provider';const azure =createAzure({  endpoint: process.env.AZURE_API_ENDPOINT,  apiKey: process.env.AZURE_API_KEY,});
```


## [Basic Usage](#basic-usage)


Generate text using the Azure custom provider:

```
import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:azure('your-deployment-name'),  prompt:'Write a story about a robot.',});
```


## [Status](#status)


> ✅ Chat Completions: Working with both streaming and non-streaming responses ⚠️ Tool Calling: Functionality highly dependent on model choice ⚠️ Embeddings: Implementation present but untested
```

### 448. `providers/community-providers/chrome-ai.md`

```markdown
# ChromeAI


---
url: https://ai-sdk.dev/providers/community-providers/chrome-ai
description: Learn how to use the Chrome AI provider for the AI SDK.
---


# [ChromeAI](#chromeai)


[jeasonstudio/chrome-ai](https://github.com/jeasonstudio/chrome-ai) is a community provider that uses [Chrome Built-in AI](https://developer.chrome.com/docs/ai/built-in) to provide language model support for the AI SDK.

This module is under development and may contain errors and frequent incompatible changes.


## [Setup](#setup)


The ChromeAI provider is available in the `chrome-ai` module. You can install it with:

pnpm

npm

yarn

pnpm add chrome-ai


### [Enabling AI in Chrome](#enabling-ai-in-chrome)


Chrome's implementation of [built-in AI with Gemini Nano](https://developer.chrome.com/docs/ai/built-in) is experimental and will change as they test and address feedback.

Chrome built-in AI is a preview feature, you need to use chrome version 127 or greater, now in [dev](https://www.google.com/chrome/dev/?extra=devchannel) or [canary](https://www.google.com/chrome/canary/) channel, [may release on stable chanel at Jul 17, 2024](https://chromestatus.com/roadmap).

After then, you should turn on these flags:

-   [chrome://flags/#prompt-api-for-gemini-nano](chrome://flags/#prompt-api-for-gemini-nano): `Enabled`
-   [chrome://flags/#optimization-guide-on-device-model](chrome://flags/#optimization-guide-on-device-model): `Enabled BypassPrefRequirement`
-   [chrome://components/](chrome://components/): Click `Optimization Guide On Device Model` to download the model.


## [Language Models](#language-models)


The `chromeai` provider instance is a function that you can invoke to create a language model:

```
import{ chromeai }from'chrome-ai';const model =chromeai();
```

It automatically selects the correct model id. You can also pass additional settings in the second argument:

```
import{ chromeai }from'chrome-ai';const model =chromeai('generic',{// additional settings  temperature:0.5,  topK:5,});
```

You can use the following optional settings to customize:

-   **modelId** *'text' | 'generic'*

    Used to distinguish models of Gemini Nano, there is no difference in the current version.

-   **temperature** *number*

    The value is passed through to the provider. The range depends on the provider and model. For most providers, `0` means almost deterministic results, and higher values mean more randomness.

-   **topK** *number*

    Only sample from the top K options for each subsequent token.

    Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.



## [Examples](#examples)


You can use Chrome built-in language models to generate text with the `generateText` or `streamText` function:

```
import{ generateText }from'ai';import{ chromeai }from'chrome-ai';const{ text }=awaitgenerateText({  model:chromeai(),  prompt:'Who are you?',});console.log(text);//  I am a large language model, trained by Google.
```

```
import{ streamText }from'ai';import{ chromeai }from'chrome-ai';const{ textStream }=streamText({  model:chromeai(),  prompt:'Who are you?',});let result ='';forawait(const textPart of textStream){  result = textPart;}console.log(result);//  I am a large language model, trained by Google.
```

Chrome built-in language models can also be used in the `generateObject` function:

```
import{ generateObject }from'ai';import{ chromeai }from'chrome-ai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model:chromeai('text'),  schema: z.object({    recipe: z.object({      name: z.string(),      ingredients: z.array(        z.object({          name: z.string(),          amount: z.string(),}),),      steps: z.array(z.string()),}),}),  prompt:'Generate a lasagna recipe.',});console.log(object);// { recipe: {...} }
```

Due to model reasons, `toolCall` and `streamObject` are not supported. We are making an effort to implement these functions by prompt engineering.
```

### 449. `providers/community-providers/cloudflare-ai-gateway.md`

```markdown
# AI Gateway Provider


---
url: https://ai-sdk.dev/providers/community-providers/cloudflare-ai-gateway
description: Learn how to use the Cloudflare AI Gateway provider for the AI SDK.
---


# [AI Gateway Provider](#ai-gateway-provider)


The AI Gateway Provider is a library that integrates Cloudflare's AI Gateway with the Vercel AI SDK. It enables seamless access to multiple AI models from various providers through a unified interface, with automatic fallback for high availability.


## [Features](#features)


-   **Runtime Agnostic**: Compatible with Node.js, Edge Runtime, and other JavaScript runtimes supported by the Vercel AI SDK.
-   **Automatic Fallback**: Automatically switches to the next available model if one fails, ensuring resilience.
-   **Multi-Provider Support**: Supports models from OpenAI, Anthropic, DeepSeek, Google AI Studio, Grok, Mistral, Perplexity AI, Replicate, and Groq.
-   **AI Gateway Integration**: Leverages Cloudflare's AI Gateway for request management, caching, and rate limiting.
-   **Simplified Configuration**: Easy setup with support for API key authentication or Cloudflare AI bindings.


## [Setup](#setup)


The AI Gateway Provider is available in the `ai-gateway-provider` module. Install it with:

pnpm

npm

yarn

pnpm add ai-gateway-provider


## [Provider Instance](#provider-instance)


Create an `aigateway` provider instance using the `createAiGateway` function. You can authenticate using an API key or a Cloudflare AI binding.


### [API Key Authentication](#api-key-authentication)


```
import{ createAiGateway }from'ai-gateway-provider';const aigateway =createAiGateway({  accountId:'your-cloudflare-account-id',  gateway:'your-gateway-name',  apiKey:'your-cloudflare-api-key',// Only required if your gateway has authentication enabled  options:{    skipCache:true,// Optional request-level settings},});
```


### [Cloudflare AI Binding](#cloudflare-ai-binding)


This method is only available inside Cloudflare Workers.

Configure an AI binding in your `wrangler.toml`:

```
[AI]binding ="AI"
```

In your worker, create a new instance using the binding:

```
import{ createAiGateway }from'ai-gateway-provider';const aigateway =createAiGateway({  binding: env.AI.gateway('my-gateway'),  options:{    skipCache:true,// Optional request-level settings},});
```


## [Language Models](#language-models)


Create a model instance by passing an array of models to the `aigateway` provider. The provider will attempt to use the models in order, falling back to the next if one fails.

```
import{ createAiGateway }from'ai-gateway-provider';import{ createOpenAI }from'@ai-sdk/openai';import{ createAnthropic }from'@ai-sdk/anthropic';const aigateway =createAiGateway({  accountId:'your-cloudflare-account-id',  gateway:'your-gateway-name',  apiKey:'your-cloudflare-api-key',});const openai =createOpenAI({ apiKey:'openai-api-key'});const anthropic =createAnthropic({ apiKey:'anthropic-api-key'});const model =aigateway([anthropic('claude-3-5-haiku-20241022'),// Primary modelopenai('gpt-4o-mini'),// Fallback model]);
```


### [Request Options](#request-options)


Customize AI Gateway settings per request:

-   `cacheKey`: Custom cache key for the request.
-   `cacheTtl`: Cache time-to-live in seconds.
-   `skipCache`: Bypass caching.
-   `metadata`: Custom metadata for the request.
-   `collectLog`: Enable/disable log collection.
-   `eventId`: Custom event identifier.
-   `requestTimeoutMs`: Request timeout in milliseconds.
-   `retries`:
    -   `maxAttempts`: Number of retry attempts (1-5).
    -   `retryDelayMs`: Delay between retries.
    -   `backoff`: Retry strategy (`constant`, `linear`, `exponential`).

Example:

```
const aigateway =createAiGateway({  accountId:'your-cloudflare-account-id',  gateway:'your-gateway-name',  apiKey:'your-cloudflare-api-key',  options:{    cacheTtl:3600,// Cache for 1 hour    metadata:{ userId:'user123'},    retries:{      maxAttempts:3,      retryDelayMs:1000,      backoff:'exponential',},},});
```


## [Examples](#examples)



### [`generateText`](#generatetext)


Generate non-streaming text using the AI Gateway Provider:

```
import{ createAiGateway }from'ai-gateway-provider';import{ createOpenAI }from'@ai-sdk/openai';import{ generateText }from'ai';const aigateway =createAiGateway({  accountId:'your-cloudflare-account-id',  gateway:'your-gateway-name',  apiKey:'your-cloudflare-api-key',});const openai =createOpenAI({ apiKey:'openai-api-key'});const{ text }=awaitgenerateText({  model:aigateway([openai('gpt-4o-mini')]),  prompt:'Write a greeting.',});console.log(text);// Output: "Hello"
```


### [`streamText`](#streamtext)


Stream text responses using the AI Gateway Provider:

```
import{ createAiGateway }from'ai-gateway-provider';import{ createOpenAI }from'@ai-sdk/openai';import{ streamText }from'ai';const aigateway =createAiGateway({  accountId:'your-cloudflare-account-id',  gateway:'your-gateway-name',  apiKey:'your-cloudflare-api-key',});const openai =createOpenAI({ apiKey:'openai-api-key'});const result =awaitstreamText({  model:aigateway([openai('gpt-4o-mini')]),  prompt:'Write a multi-part greeting.',});let accumulatedText ='';forawait(const chunk of result.textStream){  accumulatedText += chunk;}console.log(accumulatedText);// Output: "Hello world!"
```


## [Supported Providers](#supported-providers)


-   OpenAI
-   Anthropic
-   DeepSeek
-   Google AI Studio
-   Grok
-   Mistral
-   Perplexity AI
-   Replicate
-   Groq


## [Error Handling](#error-handling)


The provider throws the following custom errors:

-   `AiGatewayUnauthorizedError`: Invalid or missing API key when authentication is enabled.
-   `AiGatewayDoesNotExist`: Specified AI Gateway does not exist.
```

### 450. `providers/community-providers/cloudflare-workers-ai.md`

```markdown
# Cloudflare Workers AI


---
url: https://ai-sdk.dev/providers/community-providers/cloudflare-workers-ai
description: Learn how to use the Cloudflare Workers AI provider for the AI SDK.
---


# [Cloudflare Workers AI](#cloudflare-workers-ai)


[workers-ai-provider](https://github.com/cloudflare/ai/tree/main/packages/workers-ai-provider) is a community provider that allows you to use Cloudflare's [Workers AI](https://ai.cloudflare.com/) models with the AI SDK.


## [Setup](#setup)


The Cloudflare Workers AI provider is available in the `workers-ai-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add workers-ai-provider

Then, setup an AI binding in your Cloudflare Workers project `wrangler.toml` file:

wrangler.toml

```
[ai]binding ="AI"
```


## [Provider Instance](#provider-instance)


To create a `workersai` provider instance, use the `createWorkersAI` function, passing in the AI binding as an option:

```
import{ createWorkersAI }from'workers-ai-provider';const workersai =createWorkersAI({ binding: env.AI});
```


## [Language Models](#language-models)


To create a model instance, call the provider instance and specify the model you would like to use as the first argument. You can also pass additional settings in the second argument:

```
import{ createWorkersAI }from'workers-ai-provider';const workersai =createWorkersAI({ binding: env.AI});const model =workersai('@cf/meta/llama-3.1-8b-instruct',{// additional settings  safePrompt:true,});
```

You can use the following optional settings to customize:

-   **safePrompt** *boolean*

    Whether to inject a safety prompt before all conversations. Defaults to `false`



### [Examples](#examples)


You can use Cloudflare Workers AI language models to generate text with the **`generateText`** or **`streamText`** function:


#### [`generateText`](#generatetext)


```
import{ createWorkersAI }from'workers-ai-provider';import{ generateText }from'ai';typeEnv={AI:Ai;};exportdefault{asyncfetch(_:Request, env:Env){const workersai =createWorkersAI({ binding: env.AI});const result =awaitgenerateText({      model:workersai('@cf/meta/llama-2-7b-chat-int8'),      prompt:'Write a 50-word essay about hello world.',});returnnewResponse(result.text);},};
```


#### [`streamText`](#streamtext)


```
import{ createWorkersAI }from'workers-ai-provider';import{ streamText }from'ai';typeEnv={AI:Ai;};exportdefault{asyncfetch(_:Request, env:Env){const workersai =createWorkersAI({ binding: env.AI});const result =streamText({      model:workersai('@cf/meta/llama-2-7b-chat-int8'),      prompt:'Write a 50-word essay about hello world.',});return result.toTextStreamResponse({      headers:{// add these headers to ensure that the// response is chunked and streamed'Content-Type':'text/x-unknown','content-encoding':'identity','transfer-encoding':'chunked',},});},};
```


#### [`generateObject`](#generateobject)


Some Cloudflare Workers AI language models can also be used with the `generateObject` function:

```
import{ createWorkersAI }from'workers-ai-provider';import{ generateObject }from'ai';import{ z }from'zod';typeEnv={AI:Ai;};exportdefault{asyncfetch(_:Request, env:Env){const workersai =createWorkersAI({ binding: env.AI});const result =awaitgenerateObject({      model:workersai('@cf/meta/llama-3.1-8b-instruct'),      prompt:'Generate a Lasagna recipe',      schema: z.object({        recipe: z.object({          ingredients: z.array(z.string()),          description: z.string(),}),}),});returnResponse.json(result.object);},};
```
```

### 451. `providers/community-providers/crosshatch.md`

```markdown
# Crosshatch Provider


---
url: https://ai-sdk.dev/providers/community-providers/crosshatch
description: Learn how to use the Crosshatch provider for the AI SDK.
---


# [Crosshatch Provider](#crosshatch-provider)


The [Crosshatch](https://crosshatch.io) provider supports secure inference from popular language models with permissioned access to data users share, giving responses personalized with complete user context.

It creates language model objects that can be used with the `generateText`, `streamText`, `generateObject` and `streamObject` functions.


## [Setup](#setup)


The Crosshatch provider is available via the `@crosshatch/ai-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add @crosshatch/ai-provider

The [Crosshatch](https://crosshatch.io/) provider supports all of their available models such as OpenAI's GPT and Anthropic's Claude. This provider also supports the querying interface for controlling Crosshatch's custom data integration behaviors. This provider wraps the existing underlying providers ([@ai-sdk/openai](/providers/ai-sdk-providers/openai), [@ai-sdk/anthropic](/providers/ai-sdk-providers/openai).


### [Credentials](#credentials)


The Crosshatch provider is authenticated by user-specific tokens, enabling permissioned access to personalized inference.

You can obtain synthetic and test user tokens from the [your Crosshatch developer dashboard](https://platform.crosshatch.io/).

Production user tokens are provisioned and accessed with the [Link SDK](https://www.npmjs.com/package/@crosshatch/link) using your Crosshatch developer client id.


## [Provider Instance](#provider-instance)


To create a Crosshatch provider instance, use the `createCrosshatch` function:

```
importcreateCrosshatchfrom'@crosshatch/ai-provider';
```


## [Language Models](#language-models)


You can create [Crosshatch models](https://docs.crosshatch.io/endpoints/ai#supported-model-providers) using a provider instance.

```
import{ createCrosshatch }from'@crosshatch/ai-provider';const crosshatch =createCrosshatch();
```

To create a model instance, call the provider instance and specify the model you would like to use in the first argument. In the second argument, specify the user auth token, desired context, and model arguments. You can use Crosshatch to get generated text based on permissioned user context and your favorite language model.


### [Example: Generate Text with Context](#example-generate-text-with-context)


This example uses `gpt-4o-mini` to generate text.

```
import{ generateText }from'ai';importcreateCrosshatchfrom'@crosshatch/ai-provider':const crosshatch =createCrosshatch();const{ text }=awaitgenerateText({  model: crosshatch.languageModel("gpt-4o-mini",{    token:'YOUR_ACCESS_TOKEN',    replace:{      restaurants:{        select:["entity_name","entity_city","entity_region"],from:"personalTimeline",        where:[{ field:"event", op:"=", value:"confirmed"},{ field:"entity_subtype2", op:"=", value:"RESTAURANTS"}],        groupby:["entity_name","entity_city","entity_region"],        orderby:"count DESC",        limit:5}}}),  system:`The user recently ate at these restaurants: {restaurants}`,  messages:[{role:"user", content:"Where should I stay in Paris?"}]});
```


### [Example: Recommend Items based on Context](#example-recommend-items-based-on-context)


Use crosshatch to re-rank items based on recent user purchases.

```
import{ streamObject }from'ai';importcreateCrosshatchfrom`@crosshatch/ai-provider`const crosshatch =createCrosshatch();const itemSummaries =[...];// list of itemsconst ids =(itemSummaries?.map(({ itemId })=> itemId)??[])asstring[];const{ elementStream }=streamObject({  output:"array",  mode:"json",  model: crosshatch.languageModel("gpt-4o-mini",{    token,    replace:{"orders":{        select:["originalTimestamp","entity_name","order_total","order_summary"],from:"personalTimeline",        where:[{ field:"event", op:"=", value:"purchased"}],        orderBy:[{ field:"originalTimestamp", dir:"desc"}],        limit:5,},},}),  system:`Rerank the following items based on alignment with users recent purchases {orders}`,  messages:[{role:"user", content:"Heres a list of item: ${JSON.stringify(itemSummaries)"},],  schema:jsonSchema<{ id:string; reason:string}>({type:"object",    properties:{      id:{type:"string",enum: ids },      reason:{type:"string", description:"Explain your ranking."},},}),})
```
```

### 452. `providers/community-providers/custom-providers.md`

```markdown
# Writing a Custom Provider


---
url: https://ai-sdk.dev/providers/community-providers/custom-providers
description: Learn how to write a custom provider for the AI SDK
---


# [Writing a Custom Provider](#writing-a-custom-provider)


The AI SDK provides a [Language Model Specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v1). You can write your own provider that adheres to the specification and it will be compatible with the AI SDK.

You can find the Language Model Specification in the [AI SDK repository](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v1). It can be imported from `'@ai-sdk/provider'`. We also provide utilities that make it easier to implement a custom provider. You can find them in the `@ai-sdk/provider-utils` package ([source code](https://github.com/vercel/ai/tree/main/packages/provider-utils)).

If you open-source a provider, we'd love to promote it here. Please send us a PR to add it to the [Community Providers](/providers/community-providers) section.


## [Provider Implementation Guide](#provider-implementation-guide)


Implementing a custom language model provider involves several steps:

-   Creating an entry point
-   Adding a language model implementation
-   Mapping the input (prompt, tools, settings)
-   Processing the results (generate, streaming, tool calls)
-   Supporting object generation

The best way to get started is to copy a reference implementation and modify it to fit your needs. Check out the [Mistral reference implementation](https://github.com/vercel/ai/tree/main/packages/mistral) to see how the project is structured, and feel free to copy the setup.


### [Creating an Entry Point](#creating-an-entry-point)


Each AI SDK provider should follow the pattern of using a factory function that returns a provider instance and provide a default instance.

custom-provider.ts

```
import{  generateId,  loadApiKey,  withoutTrailingSlash,}from'@ai-sdk/provider-utils';import{CustomChatLanguageModel}from'./custom-chat-language-model';import{CustomChatModelId,CustomChatSettings}from'./custom-chat-settings';// model factory function with additional methods and propertiesexportinterfaceCustomProvider{(    modelId:CustomChatModelId,    settings?:CustomChatSettings,):CustomChatLanguageModel;// explicit method for targeting a specific API in case there are severalchat(    modelId:CustomChatModelId,    settings?:CustomChatSettings,):CustomChatLanguageModel;}// optional settings for the providerexportinterfaceCustomProviderSettings{/**Use a different URL prefix for API calls, e.g. to use proxy servers.   */  baseURL?:string;/**API key.   */  apiKey?:string;/**Custom headers to include in the requests.     */  headers?:Record<string,string>;}// provider factory functionexportfunctioncreateCustomProvider(  options:CustomProviderSettings={},):CustomProvider{constcreateModel=(    modelId:CustomChatModelId,    settings:CustomChatSettings={},)=>newCustomChatLanguageModel(modelId, settings,{      provider:'custom.chat',      baseURL:withoutTrailingSlash(options.baseURL)??'https://custom.ai/api/v1',headers:()=>({Authorization:`Bearer ${loadApiKey({          apiKey: options.apiKey,          environmentVariableName:'CUSTOM_API_KEY',          description:'Custom Provider',})}`,...options.headers,}),      generateId: options.generateId?? generateId,});constprovider=function(    modelId:CustomChatModelId,    settings?:CustomChatSettings,){if(new.target){thrownewError('The model factory function cannot be called with the new keyword.',);}returncreateModel(modelId, settings);};  provider.chat= createModel;return provider;}/** * Default custom provider instance. */exportconst customProvider =createCustomProvider();
```


### [Implementing the Language Model](#implementing-the-language-model)


A [language model](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v1/language-model-v1.ts) needs to implement:

-   metadata fields
    -   `specificationVersion: 'v1'` - always `'v1'`
    -   `provider: string` - name of the provider
    -   `modelId: string` - unique identifier of the model
    -   `defaultObjectGenerationMode` - default object generation mode, e.g. "json"
-   `doGenerate` method
-   `doStream` method

Check out the [Mistral language model](https://github.com/vercel/ai/blob/main/packages/mistral/src/mistral-chat-language-model.ts) as an example.

At a high level, both `doGenerate` and `doStream` methods should:

1.  **Map the prompt and the settings to the format required by the provider API.** This can be extracted, e.g. the Mistral provider contains a `getArgs` method.
2.  **Call the provider API.** You could e.g. use fetch calls or a library offered by the provider.
3.  **Process the results.** You need to convert the response to the format required by the AI SDK.


### [Errors](#errors)


The AI SDK provides [standardized errors](https://github.com/vercel/ai/tree/main/packages/provider/src/errors) that should be used by providers where possible. This will make it easy for user to debug them.


### [Retries, timeouts, and abort signals](#retries-timeouts-and-abort-signals)


The AI SDK will handle retries, timeouts, and aborting requests in a unified way. The model classes should not implement retries or timeouts themselves. Instead, they should use the `abortSignal` parameter to determine when the call should be aborted, and they should throw `ApiCallErrors` (or similar) with a correct `isRetryable` flag when errors such as network errors occur.
```

### 453. `providers/community-providers/dify.md`

```markdown
# Dify Provider


---
url: https://ai-sdk.dev/providers/community-providers/dify
description: Learn how to use the Dify provider for the AI SDK.
---


# [Dify Provider](#dify-provider)


The **[Dify provider](https://github.com/warmwind/dify-ai-provider)** allows you to easily integrate Dify's application workflow with your applications using the AI SDK.


## [Setup](#setup)


The Dify provider is available in the `dify-ai-provider` module. You can install it with:

```
npminstall dify-ai-provider# pnpmpnpmadd dify-ai-provider# yarnyarnadd dify-ai-provider
```


## [Provider Instance](#provider-instance)


You can import `difyProvider` from `dify-ai-provider` to create a provider instance:

```
import{ difyProvider }from'dify-ai-provider';
```


## [Example](#example)



### [Use dify.ai](#use-difyai)


```
import{ generateText }from'ai';import{ difyProvider }from'dify-ai-provider';const dify =difyProvider('dify-application-id',{  responseMode:'blocking',  apiKey:'dify-api-key',});const{ text, providerMetadata }=awaitgenerateText({  model: dify,  messages:[{ role:'user', content:'Hello, how are you today?'}],  headers:{'user-id':'test-user'},});const{ conversationId, messageId }= providerMetadata.difyWorkflowData;console.log(text);console.log('conversationId', conversationId);console.log('messageId', messageId);
```


### [Use self-hosted Dify](#use-self-hosted-dify)


```
import{ createDifyProvider }from'dify-ai-provider';const difyProvider =createDifyProvider({  baseURL:'your-base-url',});const dify =difyProvider('dify-application-id',{  responseMode:'blocking',  apiKey:'dify-api-key',});
```


## [Documentation](#documentation)


Please refer to the **[Dify provider documentation](https://github.com/warmwind/dify-ai-provider)** for more detailed information.
```

### 454. `providers/community-providers/friendliai.md`

```markdown
# FriendliAI Provider


---
url: https://ai-sdk.dev/providers/community-providers/friendliai
description: Learn how to use the FriendliAI Provider for the AI SDK.
---


# [FriendliAI Provider](#friendliai-provider)


The [FriendliAI](https://friendli.ai/) provider supports both open-source LLMs via [Friendli Serverless Endpoints](https://friendli.ai/products/serverless-endpoints) and custom models via [Dedicated Endpoints](https://friendli.ai/products/dedicated-endpoints).

It creates language model objects that can be used with the `generateText`, `streamText`, `generateObject`, and `streamObject` functions.


## [Setup](#setup)


The Friendli provider is available via the `@friendliai/ai-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add @friendliai/ai-provider


### [Credentials](#credentials)


The tokens required for model usage can be obtained from the [Friendli suite](https://suite.friendli.ai/).

To use the provider, you need to set the `FRIENDLI_TOKEN` environment variable with your personal access token.

```
exportFRIENDLI_TOKEN="YOUR_FRIENDLI_TOKEN"
```

Check the [FriendliAI documentation](https://friendli.ai/docs/guides/personal_access_tokens) for more information.


## [Provider Instance](#provider-instance)


You can import the default provider instance `friendliai` from `@friendliai/ai-provider`:

```
import{ friendli }from'@friendliai/ai-provider';
```


## [Language Models](#language-models)


You can create [FriendliAI models](https://friendli.ai/docs/guides/serverless_endpoints/text_generation#model-supports) using a provider instance. The first argument is the model id, e.g. `meta-llama-3.1-8b-instruct`.

```
const model =friendli('meta-llama-3.1-8b-instruct');
```


### [Example: Generating text](#example-generating-text)


You can use FriendliAI language models to generate text with the `generateText` function:

```
import{ friendli }from'@friendliai/ai-provider';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:friendli('meta-llama-3.1-8b-instruct'),  prompt:'What is the meaning of life?',});console.log(text);
```


### [Example: Reasoning](#example-reasoning)


FriendliAI exposes the thinking of `deepseek-r1` in the generated text using the `<think>` tag. You can use the `extractReasoningMiddleware` to extract this reasoning and expose it as a `reasoning` property on the result:

```
import{ friendli }from'@friendliai/ai-provider';import{ wrapLanguageModel, extractReasoningMiddleware }from'ai';const enhancedModel =wrapLanguageModel({  model:friendli('deepseek-r1'),  middleware:extractReasoningMiddleware({ tagName:'think'}),});const{ text, reasoning }=awaitgenerateText({  model: enhancedModel,  prompt:'Explain quantum entanglement.',});
```


### [Example: Structured Outputs (regex)](#example-structured-outputs-regex)


The regex option allows you to control the format of your LLM's output by specifying patterns. This can be particularly useful when you need:

-   Specific formats like CSV
-   Restrict output to specific characters such as Korean or Japanese

This feature is available with both `generateText` and `streamText` functions.

For a deeper understanding of how to effectively use regex patterns with LLMs, check out our detailed guide in the [Structured Output LLM Agents](https://friendli.ai/blog/structured-output-llm-agents) blog post.

```
import{ friendli }from'@friendliai/ai-provider';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:friendli('meta-llama-3.1-8b-instruct',{    regex:newRegExp('[\n ,.?!0-9\uac00-\ud7af]*'),}),  prompt:'Who is the first king of the Joseon Dynasty?',});console.log(text);
```


### [Example: Structured Outputs (json)](#example-structured-outputs-json)


Structured outputs are a form of guided generation. The JSON schema is used as a grammar and the outputs will always conform to the schema.

```
import{ friendli }from'@friendliai/ai-provider';import{ generateObject }from'ai';import{ z }from'zod';const{ object }=awaitgenerateObject({  model:friendli('meta-llama-3.3-70b-instruct'),  schemaName:'CalendarEvent',  schema: z.object({    name: z.string(),    date: z.string(),    participants: z.array(z.string()),}),  system:'Extract the event information.',  prompt:'Alice and Bob are going to a science fair on Friday.',});console.log(object);
```


### [Example: Using built-in tools](#example-using-built-in-tools)


Built-in tools are currently in beta.

If you use `@friendliai/ai-provider`, you can use the [built-in tools](https://friendli.ai/docs/guides/serverless_endpoints/tools/built_in_tools) via the `tools` option.

Built-in tools allow models to use tools to generate better answers. For example, a `web:search` tool can provide up-to-date answers to current questions.

```
import{ friendli }from'@friendliai/ai-provider';import{ streamText }from'ai';const result =streamText({  model:friendli('meta-llama-3.3-70b-instruct',{    tools:[{type:'web:search'},{type:'math:calculator'}],}),  prompt:'Find the current USD to CAD exchange rate and calculate how much $5,000 USD would be in Canadian dollars.',});forawait(const textPart of result.textStream){console.log(textPart);}
```


### [Example: Generating text with Dedicated Endpoints](#example-generating-text-with-dedicated-endpoints)


To use a custom model via a dedicated endpoint, you can use the `friendli` instance with the endpoint id, e.g. `zbimjgovmlcb`

```
import{ friendli }from'@friendliai/ai-provider';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:friendli('YOUR_ENDPOINT_ID'),  prompt:'What is the meaning of life?',});console.log(text);
```

You can use the code below to force requests to dedicated endpoints. By default, they are auto-detected.

```
import{ friendli }from'@friendliai/ai-provider';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:friendli("YOUR_ENDPOINT_ID",{    endpoint:"dedicated",});  prompt:'What is the meaning of life?',});console.log(text);
```

FriendliAI language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions. (see [AI SDK Core](/docs/ai-sdk-core)).


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`deepseek-r1`

`meta-llama-3.3-70b-instruct`

`meta-llama-3.1-8b-instruct`

To access [more models](https://friendli.ai/models), visit the [Friendli Dedicated Endpoints documentation](https://friendli.ai/docs/guides/dedicated_endpoints/quickstart) to deploy your custom models.


### [OpenAI Compatibility](#openai-compatibility)


You can also use `@ai-sdk/openai` as the APIs are OpenAI-compatible.

```
import{ createOpenAI }from'@ai-sdk/openai';const friendli =createOpenAI({  baseURL:'https://api.friendli.ai/serverless/v1',  apiKey: process.env.FRIENDLI_TOKEN,});
```

If you are using dedicated endpoints

```
import{ createOpenAI }from'@ai-sdk/openai';const friendli =createOpenAI({  baseURL:'https://api.friendli.ai/dedicated/v1',  apiKey: process.env.FRIENDLI_TOKEN,});
```
```

### 455. `providers/community-providers/inflection-ai.md`

```markdown
# Unofficial Community Provider for AI SDK - Inflection AI


---
url: https://ai-sdk.dev/providers/community-providers/inflection-ai
description: Learn how to use the unofficial Inflection AI provider for the AI SDK.
---


# [Unofficial Community Provider for AI SDK - Inflection AI](#unofficial-community-provider-for-ai-sdk---inflection-ai)


The **[unofficial Inflection AI provider](https://www.npmjs.com/package/inflection-ai-sdk-provider)** for the [AI SDK](/docs) contains language model support for the [Inflection AI API](https://developers.inflection.ai/).


## [Setup](#setup)


The Inflection AI provider is available in the [`inflection-ai-sdk-provider`](https://www.npmjs.com/package/inflection-ai-sdk-provider) module on npm. You can install it with

```
npm i inflection-ai-sdk-provider
```


## [Provider Instance](#provider-instance)


You can import the default provider instance `inflection` from `inflection-ai-sdk-provider`:

```
import{ inflection }from'inflection-ai-sdk-provider';
```


## [Example](#example)


```
import{ inflection }from'inflection-ai-sdk-provider';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:inflection('inflection_3_with_tools'),  prompt:'how can I make quick chicken pho?',});
```


## [Models](#models)


The following models are supported:

-   `inflection_3_pi` - "the model powering our Pi experience, including a backstory, emotional intelligence, productivity, and safety. It excels in scenarios such as customer support chatbots."
-   `inflection_3_productivity`\- "the model optimized for following instructions. It is better for tasks requiring JSON output or precise adherence to provided guidelines."
-   `inflection_3_with_tools` - This model seems to be in preview and it lacks an official description as of the writing of this README in 1.0.0.

Model

Text Generation

Streaming

Image Input

Object Generation

Tool Usage

Tool Streaming

`inflection_3_pi`

✓

✓

✗

✗

✗

✗

`inflection_3_productivity`

✓

✓

✗

✗

✗

✗

`inflection_3_with_tools`

✓

✓

✗

✗

✗

✗

There is limited API support for features other than text generation and streaming text at this time. Should that change, the table above will be updated and support will be added to this unofficial provider.


## [Documentation](#documentation)


Please check out Inflection AI's [API Documentation](https://developers.inflection.ai/docs/api-reference) for more information.

You can find the source code for this provider [here on GitHub](https://github.com/Umbrage-Studios/inflection-ai-sdk-provider).
```

### 456. `providers/community-providers/langdb.md`

```markdown
# LangDB


---
url: https://ai-sdk.dev/providers/community-providers/langdb
description: Learn how to use LangDB with the AI SDK
---


# [LangDB](#langdb)


[LangDB](https://langdb.ai) is a high-performance enterprise AI gateway built in Rust, designed to govern, secure, and optimize AI traffic.

LangDB provides OpenAI-compatible APIs, enabling developers to connect with multiple LLMs by changing just two lines of code. With LangDB, you can:

-   Provide access to all major LLMs
-   Enable plug-and-play functionality using any framework like Langchain, Vercel AI SDK, CrewAI, etc., for easy adoption.
-   Simplify implementation of tracing and cost optimization features, ensuring streamlined operations.
-   Dynamically route requests to the most suitable LLM based on predefined parameters.


## [Setup](#setup)


The LangDB provider is available via the `@langdb/vercel-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add @langdb/vercel-provider


## [Provider Instance](#provider-instance)


To create a LangDB provider instance, use the `createLangDB` function:

```
import{ createLangDB }from'@langdb/vercel-provider';const langdb =createLangDB({  apiKey: process.env.LANGDB_API_KEY,// Required  projectId:'your-project-id',// Required  threadId:uuidv4(),// Optional  runId:uuidv4(),// Optional  label:'code-agent',// Optional  headers:{'Custom-Header':'value'},// Optional});
```

You can find your LangDB API key in the [LangDB dashboard](https://app.langdb.ai).


## [Examples](#examples)


You can use LangDB with the `generateText` or `streamText` function:


### [`generateText`](#generatetext)


```
import{ createLangDB }from'@langdb/vercel-provider';import{ generateText }from'ai';const langdb =createLangDB({  apiKey: process.env.LANGDB_API_KEY,  projectId:'your-project-id',});exportasyncfunctiongenerateTextExample(){const{ text }=awaitgenerateText({    model:langdb('openai/gpt-4o-mini'),    prompt:'Write a Python function that sorts a list:',});console.log(text);}
```


### [generateImage](#generateimage)


```
import{ createLangDB }from'@langdb/vercel-provider';import{ experimental_generateImage as generateImage }from'ai';import fs from'fs';import path from'path';const langdb =createLangDB({  apiKey: process.env.LANGDB_API_KEY,  projectId:'your-project-id',});exportasyncfunctiongenerateImageExample(){const{ images }=awaitgenerateImage({    model: langdb.image('openai/dall-e-3'),    prompt:'A delighted resplendent quetzal mid-flight amidst raindrops',});const imagePath = path.join(__dirname,'generated-image.png');  fs.writeFileSync(imagePath, images[0].uint8Array);console.log(`Image saved to: ${imagePath}`);}
```


### [embed](#embed)


```
import{ createLangDB }from'@langdb/vercel-provider';import{ embed }from'ai';const langdb =createLangDB({  apiKey: process.env.LANGDB_API_KEY,  projectId:'your-project-id',});exportasyncfunctiongenerateEmbeddings(){const{ embedding }=awaitembed({    model: langdb.textEmbeddingModel('text-embedding-3-small'),    value:'sunny day at the beach',});console.log('Embedding:', embedding);}
```


## [Supported Models](#supported-models)


LangDB supports over 250+ models, enabling seamless interaction with a wide range of AI capabilities.

Checkout the [model list](https://app.langdb.ai/models) for more information.

For more information, visit the [LangDB documentation](https://docs.langdb.ai/).
```

### 457. `providers/community-providers/letta.md`

```markdown
# Letta Provider


---
url: https://ai-sdk.dev/providers/community-providers/letta
description: Learn how to use the Letta AI SDK provider for the AI SDK.
---


# [Letta Provider](#letta-provider)


The [Letta AI-SDK provider](https://github.com/letta-ai/vercel-ai-sdk-provider) is the official provider for the [Letta](https://docs.letta.com) platform. It allows you to integrate Letta's AI capabilities into your applications using the Vercel AI SDK.


## [Setup](#setup)


The Letta provider is available in the `@letta-ai/vercel-ai-sdk-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add @letta-ai/vercel-ai-sdk-provider


## [Setup](#setup-1)


The Letta provider is available in the `@letta-ai/vercel-ai-sdk-provider` module. You can install it with

```
npm i @letta-ai/vercel-ai-sdk-provider a
```


## [Provider Instance](#provider-instance)


You can import the default provider instance `letta` from `@letta-ai/vercel-ai-sdk-provider`:

```
import{ letta }from'@letta-ai/vercel-ai-sdk-provider';
```


## [Quick Start](#quick-start)



### [Using Letta Cloud (](#using-letta-cloud-httpsapilettacom)[https://api.letta.com](https://api.letta.com))


Create a file called `.env.local` and add your [API Key](https://app.letta.com/api-keys)

```
LETTA_API_KEY=<your api key>
```

```
import{ lettaCloud }from'@letta-ai/vercel-ai-sdk-provider';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:lettaCloud('your-agent-id'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```


### [Local instances (](#local-instances-httplocalhost8283)[http://localhost:8283](http://localhost:8283))


```
import{ lettaLocal }from'@letta-ai/vercel-ai-sdk-provider';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:lettaLocal('your-agent-id'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```


### [Custom setups](#custom-setups)


```
import{ createLetta }from'@letta-ai/vercel-ai-sdk-provider';import{ generateText }from'ai';const letta =createLetta({  baseUrl:'<your-base-url>',  token:'<your-access-token>',});const{ text }=awaitgenerateText({  model:letta('your-agent-id'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```


### [Using other Letta Client Functions](#using-other-letta-client-functions)


The `vercel-ai-sdk-provider` extends the [@letta-ai/letta-client](https://www.npmjs.com/package/@letta-ai/letta-client), you can access the operations directly by using `lettaCloud.client` or `lettaLocal.client` or your custom generated `letta.client`

```
import{ lettaCloud }from'@letta-ai/vercel-ai-sdk-provider';lettaCloud.agents.list();
```


### [More Information](#more-information)


For more information on the Letta API, please refer to the [Letta API documentation](https://docs.letta.com/api).
```

### 458. `providers/community-providers/llama-cpp.md`

```markdown
# LLamaCpp Provider


---
url: https://ai-sdk.dev/providers/community-providers/llama-cpp
description: Learn how to use Llama CPP.
---


# [LLamaCpp Provider](#llamacpp-provider)


The LlamaCpp provider is a prototype that is currently unmaintained.

[nnance/llamacpp-ai-provider](https://github.com/nnance/llamacpp-ai-provider) is a community provider that uses [Llama.cpp](https://github.com/ggerganov/llama.cpp) to provide language model support for the AI SDK.
```

### 459. `providers/community-providers/mem0.md`

```markdown
# Mem0 Provider


---
url: https://ai-sdk.dev/providers/community-providers/mem0
description: Learn how to use the Mem0 AI SDK provider for the AI SDK.
---


# [Mem0 Provider](#mem0-provider)


The [Mem0 Provider](https://github.com/mem0ai/mem0/tree/main/vercel-ai-sdk) is a library developed by [**Mem0**](https://mem0.ai) to integrate with the AI SDK. This library brings enhanced AI interaction capabilities to your applications by introducing persistent memory functionality.

🎉 Exciting news! Mem0 AI SDK now supports **Tools Call**.


## [Setup](#setup)


The Mem0 provider is available in the `@mem0/vercel-ai-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add @mem0/vercel-ai-provider


## [Provider Instance](#provider-instance)


First, get your **Mem0 API Key** from the [Mem0 Dashboard](https://app.mem0.ai/dashboard/api-keys).

Then initialize the `Mem0 Client` in your application:

```
import{ createMem0 }from'@mem0/vercel-ai-provider';const mem0 =createMem0({  provider:'openai',  mem0ApiKey:'m0-xxx',  apiKey:'provider-api-key',  config:{    compatibility:'strict',},// Optional Mem0 Global Config  mem0Config:{    user_id:'mem0-user-id',    org_id:'mem0-org-id',    project_id:'mem0-project-id',},});
```

The `openai` provider is set as default. Consider using `MEM0_API_KEY` and `OPENAI_API_KEY` as environment variables for security.

The `mem0Config` is optional. It is used to set the global config for the Mem0 Client (eg. `user_id`, `agent_id`, `app_id`, `run_id`, `org_id`, `project_id` etc).

-   Add Memories to Enhance Context:

```
import{LanguageModelV1Prompt}from'ai';import{ addMemories }from'@mem0/vercel-ai-provider';const messages:LanguageModelV1Prompt=[{ role:'user', content:[{type:'text', text:'I love red cars.'}]},];awaitaddMemories(messages,{ user_id:'borat'});
```


## [Features](#features)



### [Adding and Retrieving Memories](#adding-and-retrieving-memories)


-   `retrieveMemories()`: Retrieves memory context for prompts.
-   `getMemories()`: Get memories from your profile in array format.
-   `addMemories()`: Adds user memories to enhance contextual responses.

```
awaitaddMemories(messages,{  user_id:'borat',  mem0ApiKey:'m0-xxx',  org_id:'org_xx',  project_id:'proj_xx',});awaitretrieveMemories(prompt,{  user_id:'borat',  mem0ApiKey:'m0-xxx',  org_id:'org_xx',  project_id:'proj_xx',});awaitgetMemories(prompt,{  user_id:'borat',  mem0ApiKey:'m0-xxx',  org_id:'org_xx',  project_id:'proj_xx',});
```

For standalone features, such as `addMemories`, `retrieveMemories`, and `getMemories`, you must either set `MEM0_API_KEY` as an environment variable or pass it directly in the function call.

`getMemories` will return raw memories in the form of an array of objects, while `retrieveMemories` will return a response in string format with a system prompt ingested with the retrieved memories.


### [Generate Text with Memory Context](#generate-text-with-memory-context)


You can use language models from **OpenAI**, **Anthropic**, **Cohere**, and **Groq** to generate text with the `generateText` function:

```
import{ generateText }from'ai';import{ createMem0 }from'@mem0/vercel-ai-provider';const mem0 =createMem0();const{ text }=awaitgenerateText({  model:mem0('gpt-4-turbo',{ user_id:'borat'}),  prompt:'Suggest me a good car to buy!',});
```


### [Structured Message Format with Memory](#structured-message-format-with-memory)


```
import{ generateText }from'ai';import{ createMem0 }from'@mem0/vercel-ai-provider';const mem0 =createMem0();const{ text }=awaitgenerateText({  model:mem0('gpt-4-turbo',{ user_id:'borat'}),  messages:[{      role:'user',      content:[{type:'text', text:'Suggest me a good car to buy.'},{type:'text', text:'Why is it better than the other cars for me?'},],},],});
```


### [Streaming Responses with Memory Context](#streaming-responses-with-memory-context)


```
import{ streamText }from'ai';import{ createMem0 }from'@mem0/vercel-ai-provider';const mem0 =createMem0();const{ textStream }=awaitstreamText({  model:mem0('gpt-4-turbo',{    user_id:'borat',}),  prompt:'Suggest me a good car to buy! Why is it better than the other cars for me? Give options for every price range.',});forawait(const textPart of textStream){  process.stdout.write(textPart);}
```


### [Generate Responses with Tools Call](#generate-responses-with-tools-call)


```
import{ generateText }from'ai';import{ createMem0 }from'@mem0/vercel-ai-provider';import{ z }from'zod';const mem0 =createMem0({  provider:'anthropic',  apiKey:'anthropic-api-key',  mem0Config:{// Global User ID    user_id:'borat',},});const prompt ='What the temperature in the city that I live in?';const result =awaitgenerateText({  model:mem0('claude-3-5-sonnet-20240620'),  tools:{    weather:tool({      description:'Get the weather in a location',      parameters: z.object({location: z.string().describe('The location to get the weather for'),}),execute:async({location})=>({location,        temperature:72+Math.floor(Math.random()*21)-10,}),}),},  prompt: prompt,});console.log(result);
```


### [Get sources from memory](#get-sources-from-memory)


```
const{ text, sources }=awaitgenerateText({  model:mem0('gpt-4-turbo'),  prompt:'Suggest me a good car to buy!',});console.log(sources);
```

This same functionality is available in the `streamText` function.


## [Best Practices](#best-practices)


-   **User Identification**: Use a unique `user_id` for consistent memory retrieval.
-   **Memory Cleanup**: Regularly clean up unused memory data.

We also have support for `agent_id`, `app_id`, and `run_id`. Refer [Docs](https://docs.mem0.ai/api-reference/memory/add-memories).


## [Help](#help)


-   For more details on Vercel AI SDK, visit the [Vercel AI SDK documentation](/docs/introduction).
-   For Mem0 documentation, refer to the [Mem0 Platform](https://app.mem0.ai/).
-   If you need further assistance, please feel free to reach out to us through following methods:


## [References](#references)


-   [Mem0 AI SDK Docs](https://docs.mem0.ai/integrations/vercel-ai-sdk#getting-started)
-   [Mem0 documentation](https://docs.mem0.ai)
```

### 460. `providers/community-providers/mixedbread.md`

```markdown
# Mixedbread Provider


---
url: https://ai-sdk.dev/providers/community-providers/mixedbread
description: Learn how to use the Mixedbread provider.
---


# [Mixedbread Provider](#mixedbread-provider)


[patelvivekdev/mixedbread-ai-provider](https://github.com/patelvivekdev/mixedbread-ai-provider) is a community provider that uses [Mixedbread](https://www.mixedbread.ai/) to provide Embedding support for the AI SDK.


## [Setup](#setup)


The Mixedbread provider is available in the `mixedbread-ai-provider` module. You can install it with

pnpm

npm

yarn

pnpm add mixedbread-ai-provider


## [Provider Instance](#provider-instance)


You can import the default provider instance `mixedbread` from `mixedbread-ai-provider`:

```
import{ mixedbread }from'mixedbread-ai-provider';
```

If you need a customized setup, you can import `createMixedbread` from `mixedbread-ai-provider` and create a provider instance with your settings:

```
import{ createMixedbread }from'mixedbread-ai-provider';const mixedbread =createMixedbread({  baseURL:'https://api.mixedbread.ai/v1',  apiKey: process.env.MIXEDBREAD_API_KEY,});
```

You can use the following optional settings to customize the Mixedbread provider instance:

-   **baseURL** *string*

    The base URL of the Mixedbread API

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.



## [Embedding Models](#embedding-models)


You can create models that call the [Mixedbread embeddings API](https://www.mixedbread.ai/api-reference/endpoints/embeddings) using the `.embedding()` factory method.

```
import{ mixedbread }from'mixedbread-ai-provider';const embeddingModel = mixedbread.textEmbeddingModel('mixedbread-ai/mxbai-embed-large-v1',);
```


### [Model Capabilities](#model-capabilities)


Model

Default Dimensions

Context Length

Custom Dimensions

`mxbai-embed-large-v1`

1024

512

`deepset-mxbai-embed-de-large-v1`

1024

512

The table above lists popular models. Please see the [Mixedbread docs](https://www.mixedbread.ai/docs/embeddings/models) for a full list of available models.


### [Add settings to the model](#add-settings-to-the-model)


The settings object should contain the settings you want to add to the model.

```
import{ mixedbread }from'mixedbread-ai-provider';const embeddingModel = mixedbread.textEmbeddingModel('mixedbread-ai/mxbai-embed-large-v1',{    prompt:'Generate embeddings for text',// Max 256 characters    dimensions:512,// Max 1024 for embed-large-v1},);
```
```

### 461. `providers/community-providers/ollama.md`

```markdown
# Ollama Provider


---
url: https://ai-sdk.dev/providers/community-providers/ollama
description: Learn how to use the Ollama provider.
---


# [Ollama Provider](#ollama-provider)


[sgomez/ollama-ai-provider](https://github.com/sgomez/ollama-ai-provider) is a community provider that uses [Ollama](https://ollama.com/) to provide language model support for the AI SDK.


## [Setup](#setup)


The Ollama provider is available in the `ollama-ai-provider` module. You can install it with

pnpm

npm

yarn

pnpm add ollama-ai-provider


## [Provider Instance](#provider-instance)


You can import the default provider instance `ollama` from `ollama-ai-provider`:

```
import{ ollama }from'ollama-ai-provider';
```

If you need a customized setup, you can import `createOllama` from `ollama-ai-provider` and create a provider instance with your settings:

```
import{ createOllama }from'ollama-ai-provider';const ollama =createOllama({// optional settings, e.g.  baseURL:'https://api.ollama.com',});
```

You can use the following optional settings to customize the Ollama provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `http://localhost:11434/api`.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.



## [Language Models](#language-models)


You can create models that call the [Ollama Chat Completion API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion) using the provider instance. The first argument is the model id, e.g. `phi3`. Some models have multi-modal capabilities.

```
const model =ollama('phi3');
```

You can find more models on the [Ollama Library](https://ollama.com/library) homepage.


### [Model Capabilities](#model-capabilities)


This provider is capable of generating and streaming text and objects. Object generation may fail depending on the model used and the schema used.

The following models have been tested with image inputs:

-   llava
-   llava-llama3
-   llava-phi3
-   moondream


## [Embedding Models](#embedding-models)


You can create models that call the [Ollama embeddings API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings) using the `.embedding()` factory method.

```
const model = ollama.embedding('nomic-embed-text');
```
```

### 462. `providers/community-providers/openrouter.md`

```markdown
# OpenRouter


---
url: https://ai-sdk.dev/providers/community-providers/openrouter
description: OpenRouter Provider for the AI SDK
---


# [OpenRouter](#openrouter)


[OpenRouter](https://openrouter.ai/) is a unified API gateway that provides access to hundreds of AI models from leading providers like Anthropic, Google, Meta, Mistral, and more. The OpenRouter provider for the AI SDK enables seamless integration with all these models while offering unique advantages:

-   **Universal Model Access**: One API key for hundreds of models from multiple providers
-   **Cost-Effective**: Pay-as-you-go pricing with no monthly fees or commitments
-   **Transparent Pricing**: Clear per-token costs for all models
-   **High Availability**: Enterprise-grade infrastructure with automatic failover
-   **Simple Integration**: Standardized API across all models
-   **Latest Models**: Immediate access to new models as they're released

Learn more about OpenRouter's capabilities in the [OpenRouter Documentation](https://openrouter.ai/docs).


## [Setup](#setup)


The OpenRouter provider is available in the `@openrouter/ai-sdk-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add @openrouter/ai-sdk-provider


## [Provider Instance](#provider-instance)


To create an OpenRouter provider instance, use the `createOpenRouter` function:

```
import{ createOpenRouter }from'@openrouter/ai-sdk-provider';const openrouter =createOpenRouter({  apiKey:'YOUR_OPENROUTER_API_KEY',});
```

You can obtain your OpenRouter API key from the [OpenRouter Dashboard](https://openrouter.ai/keys).


## [Language Models](#language-models)


OpenRouter supports both chat and completion models. Use `openrouter.chat()` for chat models and `openrouter.completion()` for completion models:

```
// Chat models (recommended)const chatModel = openrouter.chat('anthropic/claude-3.5-sonnet');// Completion modelsconst completionModel = openrouter.completion('meta-llama/llama-3.1-405b-instruct',);
```

You can find the full list of available models in the [OpenRouter Models documentation](https://openrouter.ai/docs#models).


## [Examples](#examples)


Here are examples of using OpenRouter with the AI SDK:


### [`generateText`](#generatetext)


```
import{ createOpenRouter }from'@openrouter/ai-sdk-provider';import{ generateText }from'ai';const openrouter =createOpenRouter({  apiKey:'YOUR_OPENROUTER_API_KEY',});const{ text }=awaitgenerateText({  model: openrouter.chat('anthropic/claude-3.5-sonnet'),  prompt:'What is OpenRouter?',});console.log(text);
```


### [`streamText`](#streamtext)


```
import{ createOpenRouter }from'@openrouter/ai-sdk-provider';import{ streamText }from'ai';const openrouter =createOpenRouter({  apiKey:'YOUR_OPENROUTER_API_KEY',});const result =streamText({  model: openrouter.chat('meta-llama/llama-3.1-405b-instruct'),  prompt:'Write a short story about AI.',});forawait(const chunk of result){console.log(chunk);}
```


## [Advanced Features](#advanced-features)


OpenRouter offers several advanced features to enhance your AI applications:

1.  **Model Flexibility**: Switch between hundreds of models without changing your code or managing multiple API keys.

2.  **Cost Management**: Track usage and costs per model in real-time through the dashboard.

3.  **Enterprise Support**: Available for high-volume users with custom SLAs and dedicated support.

4.  **Cross-Provider Compatibility**: Use the same code structure across different model providers.

5.  **Regular Updates**: Automatic access to new models and features as they become available.


For more information about these features and advanced configuration options, visit the [OpenRouter Documentation](https://openrouter.ai/docs).


## [Additional Resources](#additional-resources)


-   [OpenRouter Provider Repository](https://github.com/OpenRouterTeam/ai-sdk-provider)
-   [OpenRouter Documentation](https://openrouter.ai/docs)
-   [OpenRouter Dashboard](https://openrouter.ai/dashboard)
-   [OpenRouter Discord Community](https://discord.gg/openrouter)
-   [OpenRouter Status Page](https://status.openrouter.ai)
```

### 463. `providers/community-providers/portkey.md`

```markdown
# Portkey Provider


---
url: https://ai-sdk.dev/providers/community-providers/portkey
description: Learn how to use the Portkey provider for the AI SDK.
---


# [Portkey Provider](#portkey-provider)


[Portkey](https://portkey.ai/?utm_source=vercel&utm_medium=docs&utm_campaign=integration) natively integrates with the AI SDK to make your apps production-ready and reliable. Import Portkey's Vercel package and use it as a provider in your Vercel AI app to enable all of Portkey's features:

-   Full-stack **observability** and **tracing** for all requests
-   Interoperability across **250+ LLMs**
-   Built-in **50+** state-of-the-art guardrails
-   Simple & semantic **caching** to save costs & time
-   Conditional request routing with fallbacks, load-balancing, automatic retries, and more
-   Continuous improvement based on user feedback

Learn more at [Portkey docs for the AI SDK](https://docs.portkey.ai/docs/integrations/libraries/vercel)


## [Setup](#setup)


The Portkey provider is available in the `@portkey-ai/vercel-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add @portkey-ai/vercel-provider


## [Provider Instance](#provider-instance)


To create a Portkey provider instance, use the `createPortkey` function:

```
import{ createPortkey }from'@portkey-ai/vercel-provider';const portkeyConfig ={  provider:'openai',//enter provider of choice  api_key:'OPENAI_API_KEY',//enter the respective provider's api key  override_params:{    model:'gpt-4',//choose from 250+ LLMs},};const portkey =createPortkey({  apiKey:'YOUR_PORTKEY_API_KEY',  config: portkeyConfig,});
```

You can find your Portkey API key in the [Portkey Dashboard](https://app.portkey.ai).


## [Language Models](#language-models)


Portkey supports both chat and completion models. Use `portkey.chatModel()` for chat models and `portkey.completionModel()` for completion models:

```
const chatModel = portkey.chatModel('');const completionModel = portkey.completionModel('');
```

Note: You can provide an empty string as the model name if you've defined it in the `portkeyConfig`.


## [Examples](#examples)


You can use Portkey language models with the `generateText` or `streamText` function:


### [`generateText`](#generatetext)


```
import{ createPortkey }from'@portkey-ai/vercel-provider';import{ generateText }from'ai';const portkey =createPortkey({  apiKey:'YOUR_PORTKEY_API_KEY',  config: portkeyConfig,});const{ text }=awaitgenerateText({  model: portkey.chatModel(''),  prompt:'What is Portkey?',});console.log(text);
```


### [`streamText`](#streamtext)


```
import{ createPortkey }from'@portkey-ai/vercel-provider';import{ streamText }from'ai';const portkey =createPortkey({  apiKey:'YOUR_PORTKEY_API_KEY',  config: portkeyConfig,});const result =streamText({  model: portkey.completionModel(''),  prompt:'Invent a new holiday and describe its traditions.',});forawait(const chunk of result){console.log(chunk);}
```

Note:

-   Portkey supports `Tool` use with the AI SDK
-   `generatObject` and `streamObject` are currently not supported.


## [Advanced Features](#advanced-features)


Portkey offers several advanced features to enhance your AI applications:

1.  **Interoperability**: Easily switch between 250+ AI models by changing the provider and model name in your configuration.

2.  **Observability**: Access comprehensive analytics and logs for all your requests.

3.  **Reliability**: Implement caching, fallbacks, load balancing, and conditional routing.

4.  **Guardrails**: Enforce LLM behavior in real-time with input and output checks.

5.  **Security and Compliance**: Set budget limits and implement fine-grained user roles and permissions.


For detailed information on these features and advanced configuration options, please refer to the [Portkey documentation](https://docs.portkey.ai/docs/integrations/libraries/vercel).


## [Additional Resources](#additional-resources)


-   [Portkey Documentation](https://docs.portkey.ai/docs/integrations/libraries/vercel)
-   [Twitter](https://twitter.com/portkeyai)
-   [Discord Community](https://discord.gg/JHPt4C7r)
-   [Portkey Dashboard](https://app.portkey.ai)
```

### 464. `providers/community-providers/qwen.md`

```markdown
# Qwen Provider


---
url: https://ai-sdk.dev/providers/community-providers/qwen
description: Learn how to use the Qwen provider.
---


# [Qwen Provider](#qwen-provider)


[younis-ahmed/qwen-ai-provider](https://github.com/younis-ahmed/qwen-ai-provider) is a community provider that uses [Qwen](https://www.alibabacloud.com/en/solutions/generative-ai/qwen) to provide language model support for the AI SDK.


## [Setup](#setup)


The Qwen provider is available in the `qwen-ai-provider` module. You can install it with

pnpm

npm

yarn

pnpm add qwen-ai-provider


## [Provider Instance](#provider-instance)


You can import the default provider instance `qwen` from `qwen-ai-provider`:

```
import{ qwen }from'qwen-ai-provider';
```

If you need a customized setup, you can import `createQwen` from `qwen-ai-provider` and create a provider instance with your settings:

```
import{ createQwen }from'qwen-ai-provider';const qwen =createQwen({// optional settings, e.g.// baseURL: 'https://qwen/api/v1',});
```

You can use the following optional settings to customize the Qwen provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://dashscope-intl.aliyuncs.com/compatible-mode/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `DASHSCOPE_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create models that call the [Qwen chat API](https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api) using a provider instance. The first argument is the model id, e.g. `qwen-plus`. Some Qwen chat models support tool calls.

```
const model =qwen('qwen-plus');
```


### [Example](#example)


You can use Qwen language models to generate text with the `generateText` function:

```
import{ qwen }from'qwen-ai-provider';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:qwen('qwen-plus'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```

Qwen language models can also be used in the `streamText`, `generateObject`, and `streamObject` functions (see [AI SDK Core](/docs/ai-sdk-core)).


### [Model Capabilities](#model-capabilities)


Model

Image Input

Object Generation

Tool Usage

Tool Streaming

`qwen-vl-max`

`qwen-plus-latest`

`qwen-max`

`qwen2.5-72b-instruct`

`qwen2.5-14b-instruct-1m`

`qwen2.5-vl-72b-instruct`

The table above lists popular models. Please see the [Qwen docs](https://www.alibabacloud.com/help/en/model-studio/getting-started/models) for a full list of available models. The table above lists popular models. You can also pass any available provider model ID as a string if needed.


## [Embedding Models](#embedding-models)


You can create models that call the [Qwen embeddings API](https://www.alibabacloud.com/help/en/model-studio/getting-started/models#cff6607866tsg) using the `.textEmbeddingModel()` factory method.

```
const model = qwen.textEmbeddingModel('text-embedding-v3');
```


### [Model Capabilities](#model-capabilities-1)


Model

Default Dimensions

Maximum number of rows

Maximum tokens per row

`text-embedding-v3`

1024

6

8,192
```

### 465. `providers/community-providers/sambanova.md`

```markdown
# SambaNova Provider


---
url: https://ai-sdk.dev/providers/community-providers/sambanova
description: Learn how to use the SambaNova provider for the AI SDK.
---


# [SambaNova Provider](#sambanova-provider)


[sambanova-ai-provider](https://github.com/sambanova/sambanova-ai-provider) contains language model support for the SambaNova API.

API keys can be obtained from the [SambaNova Cloud Platform](https://cloud.sambanova.ai/apis).


## [Setup](#setup)


The SambaNova provider is available via the `sambanova-ai-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add sambanova-ai-provider


### [Environment variables](#environment-variables)


Create a `.env` file with a `SAMBANOVA_API_KEY` variable.


## [Provider Instance](#provider-instance)


You can import the default provider instance `sambanova` from `sambanova-ai-provider`:

```
import{ sambanova }from'sambanova-ai-provider';
```

If you need a customized setup, you can import `createSambaNova` from `sambanova-ai-provider` and create a provider instance with your settings:

```
import{ createSambaNova }from'sambanova-ai-provider';const sambanova =createSambaNova({// Optional settings});
```

You can use the following optional settings to customize the SambaNova provider instance:

-   **baseURL** *string*

    Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://api.sambanova.ai/v1`.

-   **apiKey** *string*

    API key that is being sent using the `Authorization` header. It defaults to the `SAMBANOVA_API_KEY` environment variable.

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Models](#models)


You can use [SambaNova models](https://docs.sambanova.ai/cloud/docs/get-started/supported-models) on a provider instance. The first argument is the model id, e.g. `Meta-Llama-3.1-70B-Instruct`.

```
const model =sambanova('Meta-Llama-3.1-70B-Instruct');
```


## [Tested models and capabilities](#tested-models-and-capabilities)


This provider is capable of generating and streaming text, and interpreting image inputs.

At least it has been tested with the following features (which use the `/chat/completion` endpoint):

Chat completion

Image input


### [Image input](#image-input)


You need to use any of the following models for visual understanding:

-   Llama3.2-11B-Vision-Instruct
-   Llama3.2-90B-Vision-Instruct

SambaNova does not support URLs, but the ai-sdk is able to download the file and send it to the model.


## [Example Usage](#example-usage)


Basic demonstration of text generation using the SambaNova provider.

```
import{ createSambaNova }from'sambanova-ai-provider';import{ generateText }from'ai';const sambanova =createSambaNova({  apiKey:'YOUR_API_KEY',});const model =sambanova('Meta-Llama-3.1-70B-Instruct');const{ text }=awaitgenerateText({  model,  prompt:'Hello, nice to meet you.',});console.log(text);
```

You will get an output text similar to this one:

```
Hello. Nice to meet you too. Is there something I can help you with or would you like to chat?
```


## [Intercepting Fetch Requests](#intercepting-fetch-requests)


This provider supports [Intercepting Fetch Requests](/examples/providers/intercepting-fetch-requests).


### [Example](#example)


```
import{ createSambaNova }from'sambanova-ai-provider';import{ generateText }from'ai';const sambanovaProvider =createSambaNova({  apiKey:'YOUR_API_KEY',fetch:async(url, options)=>{console.log('URL', url);console.log('Headers',JSON.stringify(options.headers,null,2));console.log(`Body ${JSON.stringify(JSON.parse(options.body),null,2)}`);returnawaitfetch(url, options);},});const model =sambanovaProvider('Meta-Llama-3.1-70B-Instruct');const{ text }=awaitgenerateText({  model,  prompt:'Hello, nice to meet you.',});
```

And you will get an output like this:

```
URL https://api.sambanova.ai/v1/chat/completionsHeaders {"Content-Type":"application/json","Authorization":"Bearer YOUR_API_KEY"}Body {"model":"Meta-Llama-3.1-70B-Instruct","temperature":0,"messages":[{"role":"user","content":"Hello, nice to meet you."}]}
```
```

### 466. `providers/community-providers/sarvam.md`

```markdown
# Sarvam Provider


---
url: https://ai-sdk.dev/providers/community-providers/sarvam
description: Learn how to use the Sarvam AI provider for the AI SDK.
---


# [Sarvam Provider](#sarvam-provider)


The Sarvam AI Provider is a library developed to integrate with the AI SDK. This library brings Speech to Text (STT) capabilities to your applications, allowing for seamless interaction with audio and text data.


## [Setup](#setup)


The Sarvam provider is available in the `sarvam-ai-provider` module. You can install it with:

pnpm

npm

yarn

pnpm add sarvam-ai-provider


## [Provider Instance](#provider-instance)


First, get your **Sarvam API Key** from the [Sarvam Dashboard](https://dashboard.sarvam.ai/auth/signin).

Then initialize `Sarvam` in your application:

```
import{ createSarvam }from'sarvam-ai-provider';const sarvam =createSarvam({  headers:{'api-subscription-key':'YOUR_API_KEY',},});
```

The `api-subscription-key` needs to be passed in headers. Consider using `YOUR_API_KEY` as environment variables for security.

-   Transcribe speech to text

```
import{ experimental_transcribe as transcribe }from'ai';import{ readFile }from'fs/promises';awaittranscribe({  model: sarvam.transcription('saarika:v2'),  audio:awaitreadFile('./src/transcript-test.mp3'),  providerOptions:{    sarvam:{      language_code:'en-IN',},},});
```


## [Features](#features)



### [Changing parameters](#changing-parameters)


-   Change language\_code

```
providerOptions:{    sarvam:{      language_code:'en-IN',},},
```

`language_code` specifies the language of the input audio and is required for accurate transcription. • It is mandatory for the `saarika:v1` model (this model does not support `unknown`). • It is optional for the `saarika:v2` model. • Use `unknown` when the language is not known; in that case, the API will auto‑detect it. Available options: `unknown`, `hi-IN`, `bn-IN`, `kn-IN`, `ml-IN`, `mr-IN`, `od-IN`, `pa-IN`, `ta-IN`, `te-IN`, `en-IN`, `gu-IN`.

-   with\_timestamps?

```
providerOptions:{  sarvam:{    with_timestamps:true,},},
```

`with_timestamps` specifies whether to include start/end timestamps for each word/token. • Type: boolean • When true, each word/token will include start/end timestamps. • Default: false

-   with\_diarization?

```
providerOptions:{  sarvam:{    with_diarization:true,},},
```

`with_diarization` enables speaker diarization (Beta). • Type: boolean • When true, enables speaker diarization. • Default: false

-   num\_speakers?

```
providerOptions:{  sarvam:{    with_diarization:true,    num_speakers:2,},},
```

`num_speakers` sets the number of distinct speakers to detect (only when `with_diarization` is true). • Type: number | null • Number of distinct speakers to detect. • Default: null


## [References](#references)


-   [Sarvam API Docs](https://docs.sarvam.ai/api-reference-docs/endpoints/speech-to-text)
```

### 467. `providers/community-providers/spark.md`

```markdown
# Spark Provider


---
url: https://ai-sdk.dev/providers/community-providers/spark
description: Learn how to use the Spark provider for the AI SDK.
---


# [Spark Provider](#spark-provider)


The **[Spark provider](https://github.com/klren0312/spark-ai-provider)** contains language model support for the Spark API, giving you access to models like lite, generalv3, pro-128k, generalv3.5, max-32k and 4.0Ultra.


## [Setup](#setup)


The Spark provider is available in the `spark-ai-provider` module. You can install it with

```
npm i spark-ai-provider
```


## [Provider Instance](#provider-instance)


You can import `createSparkProvider` from `spark-ai-provider` to create a provider instance:

```
import{ createSparkProvider }from'spark-ai-provider';
```


## [Example](#example)


```
import{ createSparkProvider }from'./index.mjs';import{ generateText }from'ai';const spark =createSparkProvider({  apiKey:'',});const{ text }=awaitgenerateText({  model:spark('lite'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```


## [Documentation](#documentation)


Please check out the **[Spark provider documentation](https://github.com/klren0312/spark-ai-provider)** for more information.
```

### 468. `providers/community-providers/voyage-ai.md`

```markdown
# Voyage AI Provider


---
url: https://ai-sdk.dev/providers/community-providers/voyage-ai
description: Learn how to use the Voyage AI provider.
---


# [Voyage AI Provider](#voyage-ai-provider)


[patelvivekdev/voyage-ai-provider](https://github.com/patelvivekdev/voyageai-ai-provider) is a community provider that uses [Voyage AI](https://www.voyageai.com) to provide Embedding support for the AI SDK.


## [Setup](#setup)


The Voyage provider is available in the `voyage-ai-provider` module. You can install it with

pnpm

npm

yarn

pnpm add voyage-ai-provider


## [Provider Instance](#provider-instance)


You can import the default provider instance `voyage` from `voyage-ai-provider`:

```
import{ voyage }from'voyage-ai-provider';
```

If you need a customized setup, you can import `createVoyage` from `voyage-ai-provider` and create a provider instance with your settings:

```
import{ createVoyage }from'voyage-ai-provider';const voyage =createVoyage({  baseURL:'https://api.voyageai.com/v1',  apiKey: process.env.VOYAGE_API_KEY,});
```

You can use the following optional settings to customize the Voyage provider instance:

-   **baseURL** *string*

    The base URL of the voyage API

-   **headers** *Record<string,string>*

    Custom headers to include in the requests.



## [Embedding Models](#embedding-models)


You can create models that call the [Voyage embeddings API](https://docs.voyageai.com/reference/embeddings-api) using the `.embedding()` factory method.

```
import{ voyage }from'voyage-ai-provider';const embeddingModel = voyage.textEmbeddingModel('voyage-3');
```

You can find more models on the [Voyage Library](https://docs.voyageai.com/docs/embeddings) homepage.


### [Model Capabilities](#model-capabilities)


Model

Default Dimensions

Context Length

`voyage-3-large`

1024 (default), 256, 512, 2048

32,000

`voyage-3`

1024

32000

`voyage-code-3`

1024 (default), 256, 512, 2048

32000

`voyage-3-lite`

512

32000

`voyage-finance-2`

1024

32000

`voyage-multilingual-2`

1024

32000

`voyage-law-2`

1024

32000

`voyage-code-2`

1024

16000

The table above lists popular models. Please see the [Voyage docs](https://docs.voyageai.com/docs/embeddings) for a full list of available models.


### [Add settings to the model](#add-settings-to-the-model)


The settings object should contain the settings you want to add to the model.

```
import{ voyage }from'voyage-ai-provider';const embeddingModel = voyage.textEmbeddingModel('voyage-3',{  inputType:'document',// 'document' or 'query'  truncation:false,  outputDimension:1024,// the new model voyage-code-3, voyage-3-large has 4 different output dimensions: 256, 512, 1024 (default), 2048  outputDtype:'float',// output data types - int8, uint8, binary, ubinary are supported by the new model voyage-code-3, voyage-3-large});
```

Learn more about the [output data types.](https://docs.voyageai.com/docs/faq#what-is-quantization-and-output-data-types)
```

### 469. `providers/community-providers/zhipu.md`

```markdown
# Zhipu AI Provider


---
url: https://ai-sdk.dev/providers/community-providers/zhipu
description: Learn how to use the Zhipu provider.
---


# [Zhipu AI Provider](#zhipu-ai-provider)


[Zhipu AI Provider](https://github.com/Xiang-CH/zhipu-ai-provider) is a community provider for the [AI SDK](/). It enables seamless integration with **GLM** and Embedding Models provided on [bigmodel.cn](https://bigmodel.cn/) by [ZhipuAI](https://www.zhipuai.cn/).


## [Setup](#setup)


pnpm

npm

yarn

pnpm add zhipu-ai-provider

Set up your `.env` file / environment with your API key.

```
ZHIPU_API_KEY=<your-api-key>
```


## [Provider Instance](#provider-instance)


You can import the default provider instance `zhipu` from `zhipu-ai-provider` (This automatically reads the API key from the environment variable `ZHIPU_API_KEY`):

```
import{ zhipu }from'zhipu-ai-provider';
```

Alternatively, you can create a provider instance with custom configuration with `createZhipu`:

```
import{ createZhipu }from'zhipu-ai-provider';const zhipu =createZhipu({  baseURL:'https://open.bigmodel.cn/api/paas/v4',  apiKey:'your-api-key',});
```

You can use the following optional settings to customize the Zhipu provider instance:

-   **baseURL**: *string*

    -   Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is `https://open.bigmodel.cn/api/paas/v4`.
-   **apiKey**: *string*

    -   Your API key for Zhipu [BigModel Platform](https://bigmodel.cn/). If not provided, the provider will attempt to read the API key from the environment variable `ZHIPU_API_KEY`.
-   **headers**: *Record<string, string>*

    -   Custom headers to include in the requests.


## [Example](#example)


```
import{ zhipu }from'zhipu-ai-provider';const{ text }=awaitgenerateText({  model:zhipu('glm-4-plus'),  prompt:'Why is the sky blue?',});console.log(result);
```


## [Documentation](#documentation)


-   **[Zhipu documentation](https://bigmodel.cn/dev/welcome)**
```

### 470. `providers/community-providers.md`

```markdown
# Community Providers


---
url: https://ai-sdk.dev/providers/community-providers
description: Learn how to use Language Model Specification.
---


# [Community Providers](#community-providers)


The AI SDK provides a [Language Model Specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v1). You can [write your own provider](./community-providers/custom-providers) that adheres to the specification and it will be compatible with the AI SDK.

Here are the community providers that implement the Language Model Specification:

[

LLamaCpp

Provider Dependent

](/providers/community-providers/llama-cpp)[

Ollama

Provider Dependent

](/providers/community-providers/ollama)[

Chrome AI

Provider Dependent

](/providers/community-providers/chrome-ai)[

Anthropic Vertex

Provider Dependent

](/providers/community-providers/anthropic-vertex-ai)[

Portkey

Provider Dependent

](/providers/community-providers/portkey)[

Cloudflare Workers AI

Provider Dependent

](/providers/community-providers/cloudflare-workers-ai)[

Write your own

Provider Dependent

](/providers/community-providers/custom-providers)
```

### 471. `providers/observability/braintrust.md`

```markdown
# Braintrust Observability


---
url: https://ai-sdk.dev/providers/observability/braintrust
description: Monitoring and tracing LLM applications with Braintrust
---


# [Braintrust Observability](#braintrust-observability)


Braintrust is an end-to-end platform for building AI applications. When building with the AI SDK, you can integrate Braintrust to [log](https://www.braintrust.dev/docs/guides/logging), monitor, and take action on real-world interactions.


## [Setup](#setup)



### [OpenTelemetry](#opentelemetry)


Braintrust supports [AI SDK telemetry data](/docs/ai-sdk-core/telemetry). To set up Braintrust as an [OpenTelemetry](https://opentelemetry.io/docs/) backend, you'll need to route the traces to Braintrust's OpenTelemetry endpoint, set your API key, and specify a parent project or experiment.

Once you set up an [OpenTelemetry Protocol Exporter](https://opentelemetry.io/docs/languages/js/exporters/) (OTLP) to send traces to Braintrust, we automatically convert LLM calls into Braintrust `LLM` spans, which can be saved as [prompts](https://www.braintrust.dev/docs/guides/functions/prompts) and evaluated in the [playground](https://www.braintrust.dev/docs/guides/playground).

To use the AI SDK to send telemetry data to Braintrust, set these environment variables in your Next.js app's `.env` file:

```
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.braintrust.dev/otelOTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer <Your API Key>, x-bt-parent=project_id:<Your Project ID>"
```

You can then use the `experimental_telemetry` option to enable telemetry on supported AI SDK function calls:

```
import{ createOpenAI }from'@ai-sdk/openai';import{ generateText }from'ai';const openai =createOpenAI();asyncfunctionmain(){const result =awaitgenerateText({    model:openai('gpt-4o-mini'),    prompt:'What is 2 + 2?',    experimental_telemetry:{      isEnabled:true,      metadata:{        query:'weather',location:'San Francisco',},},});console.log(result);}main();
```

Traced LLM calls will appear under the Braintrust project or experiment provided in the `x-bt-parent` header.


### [Model Wrapping](#model-wrapping)


You can wrap AI SDK models in Braintrust to automatically log your requests.

```
import{ initLogger, wrapAISDKModel }from'braintrust';import{ openai }from'@ai-sdk/openai';const logger =initLogger({  projectName:'My Project',  apiKey: process.env.BRAINTRUST_API_KEY,});const model =wrapAISDKModel(openai.chat('gpt-3.5-turbo'));asyncfunctionmain(){// This will automatically log the request, response, and metrics to Braintrustconst response =await model.doGenerate({    inputFormat:'messages',    mode:{type:'regular',},    prompt:[{        role:'user',        content:[{type:'text', text:'What is the capital of France?'}],},],});console.log(response);}main();
```


## [Resources](#resources)


To see a step-by-step example, check out the Braintrust [cookbook](https://www.braintrust.dev/docs/cookbook/recipes/OTEL-logging).

After you log your application in Braintrust, explore other workflows like:

-   Adding [tools](https://www.braintrust.dev/docs/guides/functions/tools) to your library and using them in [experiments](https://www.braintrust.dev/docs/guides/evals) and the [playground](https://www.braintrust.dev/docs/guides/playground)
-   Creating [custom scorers](https://www.braintrust.dev/docs/guides/functions/scorers) to assess the quality of your LLM calls
-   Adding your logs to a [dataset](https://www.braintrust.dev/docs/guides/datasets) and running evaluations comparing models and prompts
```

### 472. `providers/observability/helicone.md`

```markdown
# Helicone Observability


---
url: https://ai-sdk.dev/providers/observability/helicone
description: Monitor and optimize your AI SDK applications with minimal configuration using Helicone
---


# [Helicone Observability](#helicone-observability)


[Helicone](https://helicone.ai) is an open-source LLM observability platform that helps you monitor, analyze, and optimize your AI applications through a proxy-based approach, requiring minimal setup and zero additional dependencies.


## [Setup](#setup)


Setting up Helicone:

1.  Create a Helicone account at [helicone.ai](https://helicone.ai)

2.  Set your API key as an environment variable:

    .env

    ```
    HELICONE_API_KEY=your-helicone-api-key
    ```

3.  Update your model provider configuration to use Helicone's proxy:

    ```
    import{ createOpenAI }from'@ai-sdk/openai';const openai =createOpenAI({  baseURL:'https://oai.helicone.ai/v1',  headers:{'Helicone-Auth':`Bearer ${process.env.HELICONE_API_KEY}`,},});// Use normally with AI SDKconst response =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Hello world',});
    ```


That's it! Your requests are now being logged and monitored through Helicone.

[→ Learn more about getting started with Helicone on AI SDK](https://docs.helicone.ai/getting-started/integration-method/vercelai)


## [Integration Approach](#integration-approach)


While other observability solutions require OpenTelemetry instrumentation, Helicone uses a simple proxy approach:

Helicone Proxy (3 lines)

Typical OTEL Setup (simplified)

```
const openai =createOpenAI({  baseURL:"https://oai.helicone.ai/v1",  headers:{"Helicone-Auth":`Bearer ${process.env.HELICONE_API_KEY}`},});
```

**Characteristics of Helicone's Proxy Approach:**

-   No additional packages required
-   Compatible with JavaScript environments
-   Minimal code changes to existing implementations
-   Supports features such as caching and rate limiting

[→ Learn more about Helicone's proxy approach](https://docs.helicone.ai/references/proxy-vs-async)


## [Core Features](#core-features)



### [User Tracking](#user-tracking)


Monitor how individual users interact with your AI application:

```
const response =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Hello world',  headers:{'Helicone-User-Id':'user@example.com',},});
```

[→ Learn more about User Metrics](https://docs.helicone.ai/features/advanced-usage/user-metrics)


### [Custom Properties](#custom-properties)


Add structured metadata to filter and analyze requests:

```
const response =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Translate this text to French',  headers:{'Helicone-Property-Feature':'translation','Helicone-Property-Source':'mobile-app','Helicone-Property-Language':'French',},});
```

[→ Learn more about Custom Properties](https://docs.helicone.ai/features/advanced-usage/custom-properties)


### [Session Tracking](#session-tracking)


Group related requests into coherent conversations:

```
const response =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Tell me more about that',  headers:{'Helicone-Session-Id':'convo-123','Helicone-Session-Name':'Travel Planning','Helicone-Session-Path':'/chats/travel',},});
```

[→ Learn more about Sessions](https://docs.helicone.ai/features/sessions)


## [Advanced Configuration](#advanced-configuration)



### [Request Caching](#request-caching)


Reduce costs by caching identical requests:

```
const response =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'What is the capital of France?',  headers:{'Helicone-Cache-Enabled':'true',},});
```

[→ Learn more about Caching](https://docs.helicone.ai/features/advanced-usage/caching)


### [Rate Limiting](#rate-limiting)


Control usage by adding a rate limit policy:

```
const response =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Generate creative content',  headers:{// Allow 10,000 requests per hour'Helicone-RateLimit-Policy':'10000;w=3600',// Optional: limit by user'Helicone-User-Id':'user@example.com',},});
```

Format: `[quota];w=[time_window];u=[unit];s=[segment]` where:

-   `quota`: Maximum requests allowed in the time window
-   `w`: Time window in seconds (minimum 60s)
-   `u`: Optional unit - "request" (default) or "cents"
-   `s`: Optional segment - "user", custom property, or global (default)

[→ Learn more about Rate Limiting](https://docs.helicone.ai/features/advanced-usage/custom-rate-limits)


### [LLM Security](#llm-security)


Protect against prompt injection, jailbreaking, and other LLM-specific threats:

```
const response =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt: userInput,  headers:{// Basic protection (Prompt Guard model)'Helicone-LLM-Security-Enabled':'true',// Optional: Advanced protection (Llama Guard model)'Helicone-LLM-Security-Advanced':'true',},});
```

Protects against multiple attack vectors in 8 languages with minimal latency. Advanced mode adds protection across 14 threat categories.

[→ Learn more about LLM Security](https://docs.helicone.ai/features/advanced-usage/llm-security)


## [Resources](#resources)


-   [Helicone Documentation](https://docs.helicone.ai)
-   [GitHub Repository](https://github.com/Helicone/helicone)
-   [Discord Community](https://discord.com/invite/2TkeWdXNPQ)
```

### 473. `providers/observability/laminar.md`

```markdown
# Laminar observability


---
url: https://ai-sdk.dev/providers/observability/laminar
description: Monitor your AI SDK applications with Laminar
---


# [Laminar observability](#laminar-observability)


[Laminar](https://www.lmnr.ai) is the open-source platform for tracing and evaluating AI applications.

Laminar features:

-   [Tracing compatible with AI SDK and more](https://docs.lmnr.ai/tracing/introduction),
-   [Evaluations](https://docs.lmnr.ai/evaluations/introduction),
-   [Browser agent observability](https://docs.lmnr.ai/tracing/browser-agent-observability)

A version of this guide is available in [Laminar's docs](https://docs.lmnr.ai/tracing/integrations/vercel-ai-sdk).


## [Setup](#setup)


Laminar's tracing is based on OpenTelemetry. It supports AI SDK [telemetry](/docs/ai-sdk-core/telemetry).


### [Installation](#installation)


To start with Laminar's tracing, first [install](https://docs.lmnr.ai/installation) the `@lmnr-ai/lmnr` package.

pnpm

npm

yarn

pnpm add @lmnr-ai/lmnr


### [Get your project API key and set in the environment](#get-your-project-api-key-and-set-in-the-environment)


Then, either sign up on [Laminar](https://www.lmnr.ai) or self-host an instance ([github](https://github.com/lmnr-ai/lmnr)) and create a new project.

In the project settings, create and copy the API key.

In your .env

```
LMNR_PROJECT_API_KEY=...
```


## [Next.js](#nextjs)



### [Initialize tracing](#initialize-tracing)


In Next.js, Laminar initialization should be done in `instrumentation.{ts,js}`:

```
exportasyncfunctionregister(){// prevent this from running in the edge runtimeif(process.env.NEXT_RUNTIME==='nodejs'){const{Laminar}=awaitimport('@lmnr-ai/lmnr');Laminar.initialize({      projectApiKey: process.env.LMNR_API_KEY,});}}
```


### [Add @lmnr-ai/lmnr to your next.config](#add-lmnr-ailmnr-to-your-nextconfig)


In your `next.config.js` (`.ts` / `.mjs`), add the following lines:

```
const nextConfig ={  serverExternalPackages:['@lmnr-ai/lmnr'],};exportdefault nextConfig;
```

This is because Laminar depends on OpenTelemetry, which uses some Node.js-specific functionality, and we need to inform Next.js about it. Learn more in the [Next.js docs](https://nextjs.org/docs/app/api-reference/config/next-config-js/serverExternalPackages).


### [Tracing AI SDK calls](#tracing-ai-sdk-calls)


Then, when you call AI SDK functions in any of your API routes, add the Laminar tracer to the `experimental_telemetry` option.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';import{ getTracer }from'@lmnr-ai/lmnr';const{ text }=awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'What is Laminar flow?',  experimental_telemetry:{    isEnabled:true,    tracer:getTracer(),},});
```

This will create spans for `ai.generateText`. Laminar collects and displays the following information:

-   LLM call input and output
-   Start and end time
-   Duration / latency
-   Provider and model used
-   Input and output tokens
-   Input and output price
-   Additional metadata and span attributes


### [Older versions of Next.js](#older-versions-of-nextjs)


If you are using 13.4 ≤ Next.js < 15, you will also need to enable the experimental instrumentation hook. Place the following in your `next.config.js`:

```
module.exports={  experimental:{    instrumentationHook:true,},};
```

For more information, see Laminar's [Next.js guide](https://docs.lmnr.ai/tracing/nextjs) and Next.js [instrumentation docs](https://nextjs.org/docs/app/api-reference/file-conventions/instrumentation). You can also learn how to enable all traces for Next.js in the docs.


### [Usage with `@vercel/otel`](#usage-with-vercelotel)


Laminar can live alongside `@vercel/otel` and trace AI SDK calls. The default Laminar setup will ensure that

-   regular Next.js traces are sent via `@vercel/otel` to your Telemetry backend configured with Vercel,
-   AI SDK and other LLM or browser agent traces are sent via Laminar.

```
import{ registerOTel }from'@vercel/otel';exportasyncfunctionregister(){registerOTel('my-service-name');if(process.env.NEXT_RUNTIME==='nodejs'){const{Laminar}=awaitimport('@lmnr-ai/lmnr');// Make sure to initialize Laminar **after** `@registerOTel`Laminar.initialize({      projectApiKey: process.env.LMNR_PROJECT_API_KEY,});}}
```

For an advanced configuration that allows you to trace all Next.js traces via Laminar, see an example [repo](https://github.com/lmnr-ai/lmnr-ts/tree/main/examples/nextjs).


### [Usage with `@sentry/node`](#usage-with-sentrynode)


Laminar can live alongside `@sentry/node` and trace AI SDK calls. Make sure to initialize Laminar **after** `Sentry.init`.

This will ensure that

-   Whatever is instrumented by Sentry is sent to your Sentry backend,
-   AI SDK and other LLM or browser agent traces are sent via Laminar.

```
exportasyncfunctionregister(){if(process.env.NEXT_RUNTIME==='nodejs'){constSentry=awaitimport('@sentry/node');const{Laminar}=awaitimport('@lmnr-ai/lmnr');Sentry.init({      dsn: process.env.SENTRY_DSN,});// Make sure to initialize Laminar **after** `Sentry.init`Laminar.initialize({      projectApiKey: process.env.LMNR_PROJECT_API_KEY,});}}
```


## [Node.js](#nodejs)



### [Initialize tracing](#initialize-tracing-1)


Then, initialize tracing in your application:

```
import{Laminar}from'@lmnr-ai/lmnr';Laminar.initialize();
```

This must be done once in your application, as early as possible, but *after* other tracing libraries (e.g. `@sentry/node`) are initialized.

Read more in Laminar [docs](https://docs.lmnr.ai/tracing/introduction).


### [Tracing AI SDK calls](#tracing-ai-sdk-calls-1)


Then, when you call AI SDK functions in any of your API routes, add the Laminar tracer to the `experimental_telemetry` option.

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';import{ getTracer }from'@lmnr-ai/lmnr';const{ text }=awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'What is Laminar flow?',  experimental_telemetry:{    isEnabled:true,    tracer:getTracer(),},});
```

This will create spans for `ai.generateText`. Laminar collects and displays the following information:

-   LLM call input and output
-   Start and end time
-   Duration / latency
-   Provider and model used
-   Input and output tokens
-   Input and output price
-   Additional metadata and span attributes


### [Usage with `@sentry/node`](#usage-with-sentrynode-1)


Laminar can work with `@sentry/node` to trace AI SDK calls. Make sure to initialize Laminar **after** `Sentry.init`:

```
constSentry=awaitimport('@sentry/node');const{Laminar}=awaitimport('@lmnr-ai/lmnr');Sentry.init({  dsn: process.env.SENTRY_DSN,});Laminar.initialize({  projectApiKey: process.env.LMNR_PROJECT_API_KEY,});
```

This will ensure that

-   Whatever is instrumented by Sentry is sent to your Sentry backend,
-   AI SDK and other LLM or browser agent traces are sent via Laminar.

The two libraries allow for additional advanced configuration, but the default setup above is recommended.


## [Additional configuration](#additional-configuration)



### [Nested spans](#nested-spans)


If you want to trace not just the AI SDK calls, but also other functions in your application, you can use Laminar's `observe` wrapper.

```
import{ getTracer, observe }from'@lmnr-ai/lmnr';const result =awaitobserve({ name:'my-function'},async()=>{// ... some workawaitgenerateText({//...});// ... some work});
```

This will create a span with the name "my-function" and trace the function call. Inside it, you will see the nested `ai.generateText` spans.

To trace input arguments of the function that you wrap in `observe`, pass them to the wrapper as additional arguments. The return value of the function will be returned from the wrapper and traced as the span's output.

```
const result =awaitobserve({ name:'poem writer'},async(topic: string, mood: string)=>{const{ text }=awaitgenerateText({      model:openai('gpt-4.1-nano'),      prompt:`Write a poem about ${topic} in ${mood} mood.`,});return text;},'Laminar flow','happy',);
```


### [Metadata](#metadata)


In Laminar, metadata is set on the trace level. Metadata contains key-value pairs and can be used to filter traces.

```
import{ getTracer }from'@lmnr-ai/lmnr';const{ text }=awaitgenerateText({  model:openai('gpt-4.1-nano'),  prompt:`Write a poem about Laminar flow.`,  experimental_telemetry:{    isEnabled:true,    tracer:getTracer(),    metadata:{'my-key':'my-value','another-key':'another-value',},},});
```

This is converted to Laminar's metadata and stored in the trace.
```

### 474. `providers/observability/langfuse.md`

```markdown
# Langfuse Observability


---
url: https://ai-sdk.dev/providers/observability/langfuse
description: Monitor, evaluate and debug your AI SDK application with Langfuse
---


# [Langfuse Observability](#langfuse-observability)


[Langfuse](https://langfuse.com/) ([GitHub](https://github.com/langfuse/langfuse)) is an open source LLM engineering platform that helps teams to collaboratively develop, monitor, and debug AI applications. Langfuse integrates with the AI SDK to provide:

-   [Application traces](https://langfuse.com/docs/tracing)
-   Usage patterns
-   Cost data by user and model
-   Replay sessions to debug issues
-   [Evaluations](https://langfuse.com/docs/scores/overview)


## [Setup](#setup)


The AI SDK supports tracing via OpenTelemetry. With the `LangfuseExporter` you can collect these traces in Langfuse. While telemetry is experimental ([docs](/docs/ai-sdk-core/telemetry#enabling-telemetry)), you can enable it by setting `experimental_telemetry` on each request that you want to trace.

```
const result =awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Write a short story about a cat.',  experimental_telemetry:{ isEnabled:true},});
```

To collect the traces in Langfuse, you need to add the `LangfuseExporter` to your application.

You can set the Langfuse credentials via environment variables or directly to the `LangfuseExporter` constructor.

To get your Langfuse API keys, you can [self-host Langfuse](https://langfuse.com/docs/deployment/self-host) or sign up for Langfuse Cloud [here](https://cloud.langfuse.com). Create a project in the Langfuse dashboard to get your `secretKey` and `publicKey.`

Environment Variables

Constructor

.env

```
LANGFUSE_SECRET_KEY="sk-lf-..."LANGFUSE_PUBLIC_KEY="pk-lf-..."LANGFUSE_BASEURL="https://cloud.langfuse.com"# 🇪🇺 EU region, use "https://us.cloud.langfuse.com" for US region
```

Now you need to register this exporter via the OpenTelemetry SDK.

Next.js

Node.js

Next.js has support for OpenTelemetry instrumentation on the framework level. Learn more about it in the [Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry).

Install dependencies:

```
npminstall @vercel/otel langfuse-vercel @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs
```

Add `LangfuseExporter` to your instrumentation:

instrumentation.ts

```
import{ registerOTel }from'@vercel/otel';import{LangfuseExporter}from'langfuse-vercel';exportfunctionregister(){registerOTel({    serviceName:'langfuse-vercel-ai-nextjs-example',    traceExporter:newLangfuseExporter(),});}
```

Done! All traces that contain AI SDK spans are automatically captured in Langfuse.


## [Example Application](#example-application)


Check out the sample repository ([langfuse/langfuse-vercel-ai-nextjs-example](https://github.com/langfuse/langfuse-vercel-ai-nextjs-example)) based on the [next-openai](https://github.com/vercel/ai/tree/main/examples/next-openai) template to showcase the integration of Langfuse with Next.js and AI SDK.


## [Configuration](#configuration)



### [Group multiple executions in one trace](#group-multiple-executions-in-one-trace)


You can open a Langfuse trace and pass the trace ID to AI SDK calls to group multiple execution spans under one trace. The passed name in `functionId` will be the root span name of the respective execution.

```
import{ randomUUID }from'crypto';import{Langfuse}from'langfuse';const langfuse =newLangfuse();const parentTraceId =randomUUID();langfuse.trace({  id: parentTraceId,  name:'holiday-traditions',});for(let i =0; i <3; i++){const result =awaitgenerateText({    model:openai('gpt-3.5-turbo'),    maxTokens:50,    prompt:'Invent a new holiday and describe its traditions.',    experimental_telemetry:{      isEnabled:true,      functionId:`holiday-tradition-${i}`,      metadata:{        langfuseTraceId: parentTraceId,        langfuseUpdateParent:false,// Do not update the parent trace with execution results},},});console.log(result.text);}await langfuse.flushAsync();await sdk.shutdown();
```

The resulting trace hierarchy will be:


### [Disable Tracking of Input/Output](#disable-tracking-of-inputoutput)


By default, the exporter captures the input and output of each request. You can disable this behavior by setting the `recordInputs` and `recordOutputs` options to `false`.


### [Link Langfuse prompts to traces](#link-langfuse-prompts-to-traces)


You can link Langfuse prompts to AI SDK generations by setting the `langfusePrompt` property in the `metadata` field:

```
import{ generateText }from'ai';import{Langfuse}from'langfuse';const langfuse =newLangfuse();const fetchedPrompt =await langfuse.getPrompt('my-prompt');const result =awaitgenerateText({  model:openai('gpt-4o'),  prompt: fetchedPrompt.prompt,  experimental_telemetry:{    isEnabled:true,    metadata:{      langfusePrompt: fetchedPrompt.toJSON(),},},});
```

The resulting generation will have the prompt linked to the trace in Langfuse. Learn more about prompts in Langfuse [here](https://langfuse.com/docs/prompts/get-started).


### [Pass Custom Attributes](#pass-custom-attributes)


All of the `metadata` fields are automatically captured by the exporter. You can also pass custom trace attributes to e.g. track users or sessions.

```
const result =awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Write a short story about a cat.',  experimental_telemetry:{    isEnabled:true,    functionId:'my-awesome-function',// Trace name    metadata:{      langfuseTraceId:'trace-123',// Langfuse trace      tags:['story','cat'],// Custom tags      userId:'user-123',// Langfuse user      sessionId:'session-456',// Langfuse session      foo:'bar',// Any custom attribute recorded in metadata},},});
```


## [Debugging](#debugging)


Enable the `debug` option to see the logs of the exporter.

```
newLangfuseExporter({ debug:true});
```


## [Troubleshooting](#troubleshooting)


-   If you deploy on Vercel, Vercel's OpenTelemetry Collector is only available on Pro and Enterprise Plans ([docs](https://vercel.com/docs/observability/otel-overview)).
-   You need to be on `"ai": "^3.3.0"` to use the telemetry feature. In case of any issues, please update to the latest version.
-   On NextJS, make sure that you only have a single instrumentation file.
-   If you use Sentry, make sure to either:
    -   set `skipOpenTelemetrySetup: true` in Sentry.init
    -   follow Sentry's docs on how to manually set up Sentry with OTEL


## [Learn more](#learn-more)


-   After setting up Langfuse Tracing for the AI SDK, you can utilize any of the other Langfuse [platform features](https://langfuse.com/docs):
    -   [Prompt Management](https://langfuse.com/docs/prompts): Collaboratively manage and iterate on prompts, use them with low-latency in production.
    -   [Evaluations](https://langfuse.com/docs/scores): Test the application holistically in development and production using user feedback, LLM-as-a-judge evaluators, manual reviews, or custom evaluation pipelines.
    -   [Experiments](https://langfuse.com/docs/datasets): Iterate on prompts, models, and application design in a structured manner with datasets and evaluations.
-   For more information, see the [telemetry documentation](/docs/ai-sdk-core/telemetry) of the AI SDK.
```

### 475. `providers/observability/langsmith.md`

```markdown
# LangSmith Observability


---
url: https://ai-sdk.dev/providers/observability/langsmith
description: Monitor and evaluate your AI SDK application with LangSmith
---


# [LangSmith Observability](#langsmith-observability)


[LangSmith](https://docs.smith.langchain.com) is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.

Use of LangChain's open-source frameworks is not necessary, and LangSmith integrates with the [AI SDK](/docs/introduction) via the `AISDKExporter` OpenTelemetry trace exporter.

A version of this guide is also available in the [LangSmith documentation](https://docs.smith.langchain.com/observability/how_to_guides/tracing/trace_with_vercel_ai_sdk).


## [Setup](#setup)


Install an [AI SDK model provider](/providers/ai-sdk-providers) and the [LangSmith client SDK](https://npmjs.com/package/langsmith). The code snippets below will use the [AI SDK's OpenAI provider](/providers/ai-sdk-providers/openai), but you can use any [other supported provider](/providers/ai-sdk-providers) as well.

pnpm

npm

yarn

pnpm add @ai-sdk/openai langsmith

The `AISDKExporter` class is only available in `langsmith` SDK version `>=0.2.1`.

Next, set required environment variables.

```
exportLANGCHAIN_TRACING_V2=trueexportLANGCHAIN_API_KEY=<your-api-key>exportOPENAI_API_KEY=<your-openai-api-key># The examples use OpenAI (replace with your selected provider)
```

You can also [see this guide](https://docs.smith.langchain.com/observability/how_to_guides/tracing/trace_without_env_vars) for other ways to configure LangSmith, or [the section below](#custom-langsmith-client) for how to pass in a custom LangSmith client instance.


## [Trace Logging](#trace-logging)



### [Next.js](#nextjs)


First, create an `instrumentation.js` file in your project root.

```
import{ registerOTel }from'@vercel/otel';import{AISDKExporter}from'langsmith/vercel';exportfunctionregister(){registerOTel({    serviceName:'langsmith-vercel-ai-sdk-example',    traceExporter:newAISDKExporter(),});}
```

You can learn more how to [setup OpenTelemetry instrumentation within your Next.js app here](https://nextjs.org/docs/app/api-reference/file-conventions/instrumentation).

If you are using Next.js version 14, you need to add the following configuration to your `next.config.js`:

```
module.exports={  experimental:{    instrumentationHook:true,},};
```

For more information, see the [Next.js documentation on instrumentationHook](https://nextjs.org/docs/14/pages/api-reference/next-config-js/instrumentationHook).

Afterwards, add the `experimental_telemetry` argument to your AI SDK calls that you want to trace. For convenience, we've included the `AISDKExporter.getSettings()` method which appends additional metadata for LangSmith.

```
import{AISDKExporter}from'langsmith/vercel';import{ streamText }from'ai';import{ openai }from'@ai-sdk/openai';awaitstreamText({  model:openai('gpt-4o-mini'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',  experimental_telemetry:AISDKExporter.getSettings(),});
```

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/a9d9521a-4f97-4843-b1e2-b87c3a125503/r).

You can also trace tool calls:

```
import{AISDKExporter}from'langsmith/vercel';import{ generateText, tool }from'ai';import{ openai }from'@ai-sdk/openai';import{ z }from'zod';awaitgenerateText({  model:openai('gpt-4o-mini'),  messages:[{      role:'user',      content:'What are my orders and where are they? My user ID is 123',},],  tools:{    listOrders:tool({      description:'list all orders',      parameters: z.object({ userId: z.string()}),execute:async({ userId })=>`User ${userId} has the following orders: 1`,}),    viewTrackingInformation:tool({      description:'view tracking information for a specific order',      parameters: z.object({ orderId: z.string()}),execute:async({ orderId })=>`Here is the tracking information for ${orderId}`,}),},  experimental_telemetry:AISDKExporter.getSettings(),  maxSteps:10,});
```

Which results in a trace like [this one](https://smith.langchain.com/public/4d3add36-756d-4c8c-845d-4ad701a315bb/r).


### [Node.js](#nodejs)


Add the `AISDKExporter` to the trace exporter to your OpenTelemetry setup.

```
import{AISDKExporter}from'langsmith/vercel';import{NodeSDK}from'@opentelemetry/sdk-node';import{ getNodeAutoInstrumentations }from'@opentelemetry/auto-instrumentations-node';const sdk =newNodeSDK({  traceExporter:newAISDKExporter(),  instrumentations:[getNodeAutoInstrumentations()],});sdk.start();
```

Afterwards, add the `experimental_telemetry` argument to your AI SDK calls that you want to trace.

Do not forget to call `await sdk.shutdown()` before your application shuts down in order to flush any remaining traces to LangSmith.

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';import{AISDKExporter}from'langsmith/vercel';const result =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',  experimental_telemetry:AISDKExporter.getSettings(),});await sdk.shutdown();
```

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/a9d9521a-4f97-4843-b1e2-b87c3a125503/r).


### [Sentry](#sentry)


If you're using Sentry, you can attach the LangSmith trace exporter to Sentry's default OpenTelemetry instrumentation as follows:

```
import*asSentryfrom'@sentry/node';import{BatchSpanProcessor}from'@opentelemetry/sdk-trace-base';import{AISDKExporter}from'langsmith/vercel';const client =Sentry.init({  dsn:'[Sentry DSN]',  tracesSampleRate:1.0,});client?.traceProvider?.addSpanProcessor(newBatchSpanProcessor(newAISDKExporter()),);
```


## [Configuration](#configuration)



### [Customize run name](#customize-run-name)


You can customize the run name by passing the `runName` argument to the `AISDKExporter.getSettings()` method.

```
import{AISDKExporter}from'langsmith/vercel';import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',  experimental_telemetry:AISDKExporter.getSettings({    runName:'my-custom-run-name',}),});
```


### [Customize run ID](#customize-run-id)


You can customize the run ID by passing the `runId` argument to the `AISDKExporter.getSettings()` method. This is especially useful if you want to know the run ID before the run has been completed.

The run ID has to be a valid UUID.

```
import{AISDKExporter}from'langsmith/vercel';import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';import{ v4 as uuidv4 }from'uuid';awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',  experimental_telemetry:AISDKExporter.getSettings({    runId:uuidv4(),}),});
```


### [Nesting runs](#nesting-runs)


You can also nest runs within other traced functions to create a hierarchy of associated runs. Here's an example using the [`traceable`](https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code#use-traceable--traceable) method:

```
import{AISDKExporter}from'langsmith/vercel';import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';import{ traceable }from'langsmith/traceable';const wrappedGenerateText =traceable(async(content:string)=>{const{ text }=awaitgenerateText({      model:openai('gpt-4o-mini'),      messages:[{ role:'user', content }],      experimental_telemetry:AISDKExporter.getSettings(),});const reverseText =traceable(async(text:string)=>{return text.split('').reverse().join('');},{        name:'reverseText',},);const reversedText =awaitreverseText(text);return{ text, reversedText };},{ name:'parentTraceable'},);const result =awaitwrappedGenerateText('What color is the sky? Respond with one word.',);
```

The resulting trace will look like [this one](https://smith.langchain.com/public/c0466ed5-3932-4140-83b1-cf11e998fa6a/r).


### [Custom LangSmith client](#custom-langsmith-client)


You can also pass a LangSmith client instance into the `AISDKExporter` constructor:

```
import{AISDKExporter}from'langsmith/vercel';import{Client}from'langsmith';import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';import{NodeSDK}from'@opentelemetry/sdk-node';import{ getNodeAutoInstrumentations }from'@opentelemetry/auto-instrumentations-node';const langsmithClient =newClient({});const sdk =newNodeSDK({  traceExporter:newAISDKExporter({ client: langsmithClient }),  instrumentations:[getNodeAutoInstrumentations()],});sdk.start();awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',  experimental_telemetry:AISDKExporter.getSettings(),});
```


### [Debugging Exporter](#debugging-exporter)


You can enable debug logs for the `AISDKExporter` by passing the `debug` argument to the constructor.

```
const traceExporter =newAISDKExporter({ debug:true});
```

Alternatively, you can set the `OTEL_LOG_LEVEL=DEBUG` environment variable to enable debug logs for the exporter as well as the rest of the OpenTelemetry stack.


### [Adding metadata](#adding-metadata)


You can add metadata to your traces to help organize and filter them in the LangSmith UI:

```
import{AISDKExporter}from'langsmith/vercel';import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',  experimental_telemetry:AISDKExporter.getSettings({    metadata:{ userId:'123', language:'english'},}),});
```

Metadata will be visible in your LangSmith dashboard and can be used to filter and search for specific traces.


## [Further reading](#further-reading)


-   [LangSmith docs](https://docs.smith.langchain.com)
-   [LangSmith guide on tracing with the AI SDK](https://docs.smith.langchain.com/observability/how_to_guides/tracing/trace_with_vercel_ai_sdk)
-   [LangSmith guide on tracing without environment variables](https://docs.smith.langchain.com/observability/how_to_guides/tracing/trace_without_env_vars)

Once you've set up LangSmith tracing for your project, try gathering a dataset and evaluating it:

-   [LangSmith evaluation](https://docs.smith.langchain.com/evaluation)
```

### 476. `providers/observability/langwatch.md`

```markdown
# LangWatch Observability


---
url: https://ai-sdk.dev/providers/observability/langwatch
description: Track, monitor, guardrail and evaluate your AI SDK applications with LangWatch.
---


# [LangWatch Observability](#langwatch-observability)


[LangWatch](https://langwatch.ai/) ([GitHub](https://github.com/langwatch/langwatch)) is an LLM Ops platform for monitoring, experimenting, measuring and improving LLM pipelines, with a fair-code distribution model.


## [Setup](#setup)


Obtain your `LANGWATCH_API_KEY` from the [LangWatch dashboard](https://app.langwatch.com/).

pnpm

npm

yarn

pnpm add langwatch

Ensure `LANGWATCH_API_KEY` is set:

Environment variables

Client parameters

.env

```
LANGWATCH_API_KEY='your_api_key_here'
```


## [Basic Concepts](#basic-concepts)


-   Each message triggering your LLM pipeline as a whole is captured with a [Trace](https://docs.langwatch.ai/concepts#traces).
-   A [Trace](https://docs.langwatch.ai/concepts#traces) contains multiple [Spans](https://docs.langwatch.ai/concepts#spans), which are the steps inside your pipeline.
    -   A span can be an LLM call, a database query for a RAG retrieval, or a simple function transformation.
    -   Different types of [Spans](https://docs.langwatch.ai/concepts#spans) capture different parameters.
    -   [Spans](https://docs.langwatch.ai/concepts#spans) can be nested to capture the pipeline structure.
-   [Traces](https://docs.langwatch.ai/concepts#traces) can be grouped together on LangWatch Dashboard by having the same [`thread_id`](https://docs.langwatch.ai/concepts#threads) in their metadata, making the individual messages become part of a conversation.
    -   It is also recommended to provide the [`user_id`](https://docs.langwatch.ai/concepts#user-id) metadata to track user analytics.


## [Configuration](#configuration)


The AI SDK supports tracing via Next.js OpenTelemetry integration. By using the `LangWatchExporter`, you can automatically collect those traces to LangWatch.

First, you need to install the necessary dependencies:

```
npminstall @vercel/otel langwatch @opentelemetry/api-logs @opentelemetry/instrumentation @opentelemetry/sdk-logs
```

Then, set up the OpenTelemetry for your application, follow one of the tabs below depending whether you are using AI SDK with Next.js or on Node.js:

Next.js

Node.js

You need to enable the `instrumentationHook` in your `next.config.js` file if you haven't already:

```
/** @type {import('next').NextConfig} */const nextConfig ={  experimental:{    instrumentationHook:true,},};module.exports= nextConfig;
```

Next, you need to create a file named `instrumentation.ts` (or `.js`) in the **root directory** of the project (or inside `src` folder if using one), with `LangWatchExporter` as the traceExporter:

```
import{ registerOTel }from'@vercel/otel';import{LangWatchExporter}from'langwatch';exportfunctionregister(){registerOTel({    serviceName:'next-app',    traceExporter:newLangWatchExporter(),});}
```

(Read more about Next.js OpenTelemetry configuration [on the official guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry#manual-opentelemetry-configuration))

Finally, enable `experimental_telemetry` tracking on the AI SDK calls you want to trace:

```
const result =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'Explain why a chicken would make a terrible astronaut, be creative and humorous about it.',  experimental_telemetry:{    isEnabled:true,// optional metadata    metadata:{      userId:'myuser-123',      threadId:'mythread-123',},},});
```

That's it! Your messages will now be visible on LangWatch:


### [Example Project](#example-project)


You can find a full example project with a more complex pipeline and AI SDK and LangWatch integration [on our GitHub](https://github.com/langwatch/langwatch/blob/main/typescript-sdk/example/lib/chat/vercel-ai.tsx).


### [Manual Integration](#manual-integration)


The docs from here below are for manual integration, in case you are not using the AI SDK OpenTelemetry integration, you can manually start a trace to capture your messages:

```
import{LangWatch}from'langwatch';const langwatch =newLangWatch();const trace = langwatch.getTrace({  metadata:{ threadId:'mythread-123', userId:'myuser-123'},});
```

Then, you can start an LLM span inside the trace with the input about to be sent to the LLM.

```
const span = trace.startLLMSpan({  name:'llm',  model: model,  input:{type:'chat_messages',    value: messages,},});
```

This will capture the LLM input and register the time the call started. Once the LLM call is done, end the span to get the finish timestamp to be registered, and capture the output and the token metrics, which will be used for cost calculation, e.g.:

```
span.end({  output:{type:'chat_messages',    value:[chatCompletion.choices[0]!.message],},  metrics:{    promptTokens: chatCompletion.usage?.prompt_tokens,    completionTokens: chatCompletion.usage?.completion_tokens,},});
```


## [Resources](#resources)


For more information and examples, you can read more below:

-   [LangWatch documentation](https://docs.langwatch.ai/)
-   [LangWatch GitHub](https://github.com/langwatch/langwatch)


## [Support](#support)


If you have questions or need help, join our community:

-   [LangWatch Discord](https://discord.gg/kT4PhDS2gH)
-   [Email support](mailto:support@langwatch.ai)
```

### 477. `providers/observability/patronus.md`

```markdown
# Patronus Observability


---
url: https://ai-sdk.dev/providers/observability/patronus
description: Monitor, evaluate and debug your AI SDK application with Patronus
---


# [Patronus Observability](#patronus-observability)


[Patronus AI](https://patronus.ai) provides an end-to-end system to evaluate, monitor and improve performance of an LLM system, enabling developers to ship AI products safely and confidently. Learn more [here](https://docs.patronus.ai/docs).

When you build with the AI SDK, you can stream OpenTelemetry (OTEL) traces straight into Patronus and pair every generation with rich automatic evaluations.


## [Setup](#setup)



### [1\. OpenTelemetry](#1-opentelemetry)


Patronus exposes a fully‑managed OTEL endpoint. Configure an **OTLP exporter** to point at it, pass your API key, and you’re done—Patronus will automatically convert LLM spans into prompt/response records you can explore and evaluate.


#### [Environment variables (recommended)](#environment-variables-recommended)


.env.local

```
OTEL_EXPORTER_OTLP_ENDPOINT=https://otel.patronus.ai/v1/tracesOTEL_EXPORTER_OTLP_HEADERS="x-api-key:<PATRONUS_API_KEY>"
```


#### [With `@vercel/otel`](#with-vercelotel)


instrumentation.ts

```
import{ registerOTel }from'@vercel/otel';import{OTLPTraceExporter}from'@opentelemetry/exporter-trace-otlp-http';import{BatchSpanProcessor}from'@opentelemetry/sdk-trace-node';exportfunctionregister(){registerOTel({    serviceName:'next-app',    additionalSpanProcessors:[newBatchSpanProcessor(newOTLPTraceExporter({          url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT,          headers:{'x-api-key': process.env.PATRONUS_API_KEY!,},}),),],});}
```

If you need gRPC instead of HTTP, swap the exporter for `@opentelemetry/exporter-trace-otlp-grpc` and use `https://otel.patronus.ai:4317`.


### [2\. Enable telemetry on individual calls](#2-enable-telemetry-on-individual-calls)


The AI SDK emits a span only when you opt in with `experimental_telemetry`:

```
import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';const result =awaitgenerateText({  model:openai('gpt-4o'),  prompt:'Write a haiku about spring.',  experimental_telemetry:{    isEnabled:true,    functionId:'spring-haiku',// span name    metadata:{      userId:'user-123',// custom attrs surface in Patronus UI},},});
```

Every attribute inside `metadata` becomes an OTEL attribute and is indexed by Patronus for filtering.


## [Example — tracing and automated evaluation](#example--tracing-and-automated-evaluation)


app/api/chat/route.ts

```
import{ trace }from'@opentelemetry/api';import{ generateText }from'ai';import{ openai }from'@ai-sdk/openai';exportasyncfunctionPOST(req:Request){const body =await req.json();const tracer = trace.getTracer('next-app');returnawait tracer.startActiveSpan('chat-evaluate',async span =>{try{/* 1️⃣ generate answer */const answer =awaitgenerateText({        model:openai('gpt-4o'),        prompt: body.prompt,        experimental_telemetry:{ isEnabled:true, functionId:'chat'},});/* 2️⃣ run Patronus evaluation inside the same trace */awaitfetch('https://api.patronus.ai/v1/evaluate',{        method:'POST',        headers:{'X-API-Key': process.env.PATRONUS_API_KEY!,'Content-Type':'application/json',},        body:JSON.stringify({          evaluators:[{ evaluator:'lynx', criteria:'patronus:hallucination'},],          evaluated_model_input: body.prompt,          evaluated_model_output: answer.text,          trace_id: span.spanContext().traceId,          span_id: span.spanContext().spanId,}),});returnnewResponse(answer.text);}finally{      span.end();}});}
```

Result: a single trace containing the root HTTP request, the LLM generation span, and your evaluation span—**all visible in Patronus** with the hallucination score attached.


## [Once you've traced](#once-youve-traced)


-   If you're tracing an agent, Patronus's AI assistant Percival will assist with error analysis and prompt optimization. Learn more [here](https://docs.patronus.ai/docs/percival/percival)
-   Get set up on production monitoring and alerting by viewing logs and traces on Patronus and configuring webhooks for alerting. Learn more [here](https://docs.patronus.ai/docs/real_time_monitoring/webhooks)


## [Resources](#resources)


-   [Patronus docs](https://docs.patronus.ai)
-   [OpenTelemetry SDK (JS)](https://opentelemetry.io/docs/instrumentation/js/)
```

### 478. `providers/observability/traceloop.md`

```markdown
# Traceloop


---
url: https://ai-sdk.dev/providers/observability/traceloop
description: Monitoring and evaluating LLM applications with Traceloop
---


# [Traceloop](#traceloop)


[Traceloop](https://www.traceloop.com/) is a development platform for building reliable AI applications. After integrating with the AI SDK, you can use Traceloop to trace, monitor, and experiment with LLM providers, prompts and flows.


## [Setup](#setup)


Traceloop supports [AI SDK telemetry data](/docs/ai-sdk-core/telemetry) through [OpenTelemetry](https://opentelemetry.io/docs/). You'll need to sign up at [https://app.traceloop.com](https://app.traceloop.com) and get an API Key.


### [Next.js](#nextjs)


To use the AI SDK to send telemetry data to Traceloop, set these environment variables in your Next.js app's `.env` file:

```
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.traceloop.comOTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer <Your API Key>"
```

You can then use the `experimental_telemetry` option to enable telemetry on supported AI SDK function calls:

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const result =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'What is 2 + 2?',  experimental_telemetry:{    isEnabled:true,    metadata:{      query:'weather',location:'San Francisco',},},});
```


## [Resources](#resources)


-   [Traceloop demo chatbot](https://www.traceloop.com/docs/demo)
-   [Traceloop docs](https://www.traceloop.com/docs)
```

### 479. `providers/observability/weave.md`

```markdown
# Weave Observability


---
url: https://ai-sdk.dev/providers/observability/weave
description: Monitor and evaluate LLM applications with Weave.
---


# [Weave Observability](#weave-observability)


[Weave](https://wandb.ai/site/weave) is a toolkit built by [Weights & Biases](https://wandb.ai/site/) for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications.

After integrating with the AI SDK, you can use Weave to view and interact with trace information for your AI SDK application including prompts, responses, flow, cost and more.


## [Setup](#setup)


To set up Weave as an [OpenTelemetry](https://opentelemetry.io/docs/) backend, you'll need to route the traces to Weave's OpenTelemetry endpoint, set your API key, and specify a team and project. In order to log your traces to Weave, you must you must have a [Weights & Biases account](https://wandb.ai/site/weave).


### [Authentication](#authentication)


First, go to [wandb.ai/authorize](https://wandb.ai/authorize), copy your API key and generate a base64-encoded authorization string by running:

```
echo -n "api:<YOUR_API_KEY>"| base64
```

Note the output. You'll use it in your environment configuration.


### [Project Configuration](#project-configuration)


Your W&B project ID identifies where your telemetry data will be logged. It follows the format `<YOUR_TEAM_NAME>/<YOUR_PROJECT_NAME>`.

1.  Navigate to the [Weights & Biases dashboard](https://wandb.ai/home).
2.  In the **Teams** section, select or create a team.
3.  Select an existing project or create a new one.
4.  Note `<YOUR_TEAM_NAME>/<YOUR_PROJECT_NAME>` for the next step.


### [Next.js](#nextjs)


In your Next.js app’s `.env` file, set the OTEL environment variables. Replace `<BASE64_AUTH_STRING>` and `<YOUR_TEAM_NAME>/<YOUR_PROJECT_NAME>` with your values from the previous steps:

```
OTEL_EXPORTER_OTLP_ENDPOINT="https://trace.wandb.ai/otel/v1/traces"OTEL_EXPORTER_OTLP_HEADERS="Authorization=Basic <BASE64_AUTH_STRING>,project_id=<YOUR_TEAM_NAME>/<YOUR_PROJECT_NAME>"
```

You can then use the `experimental_telemetry` option to enable telemetry on supported AI SDK function calls:

```
import{ openai }from'@ai-sdk/openai';import{ generateText }from'ai';const result =awaitgenerateText({  model:openai('gpt-4o-mini'),  prompt:'What is 2 + 2?',  experimental_telemetry:{    isEnabled:true,    metadata:{      query:'math',      difficulty:'easy',},},});
```


## [Resources](#resources)


-   [Weave Documentation](https://weave-docs.wandb.ai)
-   [OpenTelemetry Documentation](https://opentelemetry.io/docs/)
-   [AI SDK Telemetry Guide](/docs/ai-sdk-core/telemetry)
```

### 480. `providers/observability.md`

```markdown
# Observability Integrations


---
url: https://ai-sdk.dev/providers/observability
description: AI SDK Integration for monitoring and tracing LLM applications
---


# [Observability Integrations](#observability-integrations)


Several LLM observability providers offer integrations with the AI SDK telemetry data:

-   [Braintrust](/providers/observability/braintrust)
-   [Helicone](/providers/observability/helicone)
-   [Traceloop](/providers/observability/traceloop)
-   [Weave](/providers/observability/weave)
-   [Langfuse](/providers/observability/langfuse)
-   [LangSmith](/providers/observability/langsmith)
-   [Laminar](/providers/observability/laminar)
-   [LangWatch](/providers/observability/langwatch)
-   [HoneyHive](https://docs.honeyhive.ai/integrations/vercel)

There are also providers that provide monitoring and tracing for the AI SDK through model wrappers:

-   [Literal AI](https://docs.literalai.com/integrations/vercel-ai-sdk)

Do you have an observability integration that supports the AI SDK and has an integration guide? Please open a pull request to add it to the list.
```

### 481. `providers/openai-compatible-providers/baseten.md`

```markdown
# Baseten Provider


---
url: https://ai-sdk.dev/providers/openai-compatible-providers/baseten
description: Use a Baseten OpenAI compatible API with the AI SDK.
---


# [Baseten Provider](#baseten-provider)


[Baseten](https://baseten.co/) is a platform for running and testing LLMs. It allows you to deploy models that are OpenAI API compatible that you can use with the AI SDK.


## [Setup](#setup)


The Baseten provider is available via the `@ai-sdk/openai-compatible` module as it is compatible with the OpenAI API. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/openai-compatible


## [Provider Instance](#provider-instance)


To use Baseten, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';constBASETEN_MODEL_ID='<model-id>';// e.g. 5q3z8xcwconstBASETEN_MODEL_URL=`https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;const baseten =createOpenAICompatible({  name:'baseten',  baseURL:BASETEN_MODEL_URL,  headers:{Authorization:`Bearer ${process.env.BASETEN_API_KEY??''}`,},});
```

Be sure to have your `BASETEN_API_KEY` set in your environment and the model `<model-id>` ready. The `<model-id>` will be given after you have deployed the model on Baseten.


## [Language Models](#language-models)


You can create [Baseten models](https://www.baseten.co/library/) using a provider instance. The first argument is the served model name, e.g. `llama`.

```
const model =baseten('llama');
```


### [Example](#example)


You can use Baseten language models to generate text with the `generateText` function:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ generateText }from'ai';constBASETEN_MODEL_ID='<model-id>';// e.g. 5q3z8xcwconstBASETEN_MODEL_URL=`https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;const baseten =createOpenAICompatible({  name:'baseten',  baseURL:BASETEN_MODEL_URL,  headers:{Authorization:`Bearer ${process.env.BASETEN_API_KEY??''}`,},});const{ text }=awaitgenerateText({  model:baseten('llama'),  prompt:'Tell me about yourself in one sentence',});console.log(text);
```

Baseten language models are also able to generate text in a streaming fashion with the `streamText` function:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ streamText }from'ai';constBASETEN_MODEL_ID='<model-id>';// e.g. 5q3z8xcwconstBASETEN_MODEL_URL=`https://model-${BASETEN_MODEL_ID}.api.baseten.co/environments/production/sync/v1`;const baseten =createOpenAICompatible({  name:'baseten',  baseURL:BASETEN_MODEL_URL,  headers:{Authorization:`Bearer ${process.env.BASETEN_API_KEY??''}`,},});const result =streamText({  model:baseten('llama'),  prompt:'Tell me about yourself in one sentence',});forawait(const message of result.textStream){console.log(message);}
```

Baseten language models can also be used in the `generateObject`, and `streamObject` functions.
```

### 482. `providers/openai-compatible-providers/custom-providers.md`

```markdown
# Writing a Custom Provider


---
url: https://ai-sdk.dev/providers/openai-compatible-providers/custom-providers
description: Create a custom provider package for an OpenAI-compatible provider leveraging the AI SDK OpenAI Compatible package.
---


# [Writing a Custom Provider](#writing-a-custom-provider)


You can create your own provider package that leverages the AI SDK's [OpenAI Compatible package](https://www.npmjs.com/package/@ai-sdk/openai-compatible). Publishing your provider package to [npm](https://www.npmjs.com/) can give users an easy way to use the provider's models and stay up to date with any changes you may have. Here's an example structure:


### [File Structure](#file-structure)


```
packages/example/├── src/│   ├── example-chat-settings.ts       # Chat model types and settings│   ├── example-completion-settings.ts # Completion model types and settings│   ├── example-embedding-settings.ts  # Embedding model types and settings│   ├── example-image-settings.ts      # Image model types and settings│   ├── example-provider.ts            # Main provider implementation│   ├── example-provider.test.ts       # Provider tests│   └── index.ts                       # Public exports├── package.json├── tsconfig.json├── tsup.config.ts                     # Build configuration└── README.md
```


### [Key Files](#key-files)


1.  **example-chat-settings.ts** - Define chat model IDs and settings:

```
import{OpenAICompatibleChatSettings}from'@ai-sdk/openai-compatible';exporttypeExampleChatModelId=|'example/chat-model-1'|'example/chat-model-2'|(string&{});exportinterfaceExampleChatSettingsextendsOpenAICompatibleChatSettings{// Add any custom settings here}
```

The completion, embedding, and image settings are implemented similarly to the chat settings.

2.  **example-provider.ts** - Main provider implementation:

```
import{LanguageModelV1,EmbeddingModelV1}from'@ai-sdk/provider';import{OpenAICompatibleChatLanguageModel,OpenAICompatibleCompletionLanguageModel,OpenAICompatibleEmbeddingModel,OpenAICompatibleImageModel,}from'@ai-sdk/openai-compatible';import{FetchFunction,  loadApiKey,  withoutTrailingSlash,}from'@ai-sdk/provider-utils';// Import your model id and settings here.exportinterfaceExampleProviderSettings{/**Example API key.*/  apiKey?:string;/**Base URL for the API calls.*/  baseURL?:string;/**Custom headers to include in the requests.*/  headers?:Record<string,string>;/**Optional custom url query parameters to include in request urls.*/  queryParams?:Record<string,string>;/**Custom fetch implementation. You can use it as a middleware to intercept requests,or to provide a custom fetch implementation for e.g. testing.*/  fetch?:FetchFunction;}exportinterfaceExampleProvider{/**Creates a model for text generation.*/(    modelId:ExampleChatModelId,    settings?:ExampleChatSettings,):LanguageModelV1;/**Creates a chat model for text generation.*/chatModel(    modelId:ExampleChatModelId,    settings?:ExampleChatSettings,):LanguageModelV1;/**Creates a completion model for text generation.*/completionModel(    modelId:ExampleCompletionModelId,    settings?:ExampleCompletionSettings,):LanguageModelV1;/**Creates a text embedding model for text generation.*/textEmbeddingModel(    modelId:ExampleEmbeddingModelId,    settings?:ExampleEmbeddingSettings,):EmbeddingModelV1<string>;/**Creates an image model for image generation.*/imageModel(    modelId:ExampleImageModelId,    settings?:ExampleImageSettings,):ImageModelV1;}exportfunctioncreateExample(  options:ExampleProviderSettings={},):ExampleProvider{const baseURL =withoutTrailingSlash(    options.baseURL??'https://api.example.com/v1',);constgetHeaders=()=>({Authorization:`Bearer ${loadApiKey({      apiKey: options.apiKey,      environmentVariableName:'EXAMPLE_API_KEY',      description:'Example API key',})}`,...options.headers,});interfaceCommonModelConfig{    provider:string;url:({ path }:{ path:string})=>string;headers:()=>Record<string,string>;    fetch?:FetchFunction;}const getCommonModelConfig =(modelType:string):CommonModelConfig=>({    provider:`example.${modelType}`,url:({ path })=>{const url =newURL(`${baseURL}${path}`);if(options.queryParams){        url.search=newURLSearchParams(options.queryParams).toString();}return url.toString();},    headers: getHeaders,    fetch: options.fetch,});constcreateChatModel=(    modelId:ExampleChatModelId,    settings:ExampleChatSettings={},)=>{returnnewOpenAICompatibleChatLanguageModel(modelId, settings,{...getCommonModelConfig('chat'),      defaultObjectGenerationMode:'tool',});};constcreateCompletionModel=(    modelId:ExampleCompletionModelId,    settings:ExampleCompletionSettings={},)=>newOpenAICompatibleCompletionLanguageModel(      modelId,      settings,getCommonModelConfig('completion'),);constcreateTextEmbeddingModel=(    modelId:ExampleEmbeddingModelId,    settings:ExampleEmbeddingSettings={},)=>newOpenAICompatibleEmbeddingModel(      modelId,      settings,getCommonModelConfig('embedding'),);constcreateImageModel=(    modelId:ExampleImageModelId,    settings:ExampleImageSettings={},)=>newOpenAICompatibleImageModel(      modelId,      settings,getCommonModelConfig('image'),);constprovider=(    modelId:ExampleChatModelId,    settings?:ExampleChatSettings,)=>createChatModel(modelId, settings);  provider.completionModel= createCompletionModel;  provider.chatModel= createChatModel;  provider.textEmbeddingModel= createTextEmbeddingModel;  provider.imageModel= createImageModel;return provider;}// Export default instanceexportconst example =createExample();
```

3.  **index.ts** - Public exports:

```
export{ createExample, example }from'./example-provider';exporttype{ExampleProvider,ExampleProviderSettings,}from'./example-provider';
```

4.  **package.json** - Package configuration:

```
{"name":"@company-name/example","version":"0.0.1","dependencies":{"@ai-sdk/openai-compatible":"^0.0.7","@ai-sdk/provider":"^1.0.2","@ai-sdk/provider-utils":"^2.0.4",// ...additional dependencies},// ...additional scripts and module build configuration}
```


### [Usage](#usage)


Once published, users can use your provider like this:

```
import{ example }from'@company-name/example';import{ generateText }from'ai';const{ text }=awaitgenerateText({  model:example('example/chat-model-1'),  prompt:'Hello, how are you?',});
```

This structure provides a clean, type-safe implementation that leverages the OpenAI Compatible package while maintaining consistency with the usage of other AI SDK providers.


### [Internal API](#internal-api)


As you work on your provider you may need to use some of the internal API of the OpenAI Compatible package. You can import these from the `@ai-sdk/openai-compatible/internal` package, for example:

```
import{ convertToOpenAICompatibleChatMessages }from'@ai-sdk/openai-compatible/internal';
```

You can see the latest available exports in the AI SDK [GitHub repository](https://github.com/vercel/ai/blob/main/packages/openai-compatible/src/internal/index.ts).
```

### 483. `providers/openai-compatible-providers/lmstudio.md`

```markdown
# LM Studio Provider


---
url: https://ai-sdk.dev/providers/openai-compatible-providers/lmstudio
description: Use the LM Studio OpenAI compatible API with the AI SDK.
---


# [LM Studio Provider](#lm-studio-provider)


[LM Studio](https://lmstudio.ai/) is a user interface for running local models.

It contains an OpenAI compatible API server that you can use with the AI SDK. You can start the local server under the [Local Server tab](https://lmstudio.ai/docs/basics/server) in the LM Studio UI ("Start Server" button).


## [Setup](#setup)


The LM Studio provider is available via the `@ai-sdk/openai-compatible` module as it is compatible with the OpenAI API. You can install it with

pnpm

npm

yarn

pnpm add @ai-sdk/openai-compatible


## [Provider Instance](#provider-instance)


To use LM Studio, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';const lmstudio =createOpenAICompatible({  name:'lmstudio',  baseURL:'http://localhost:1234/v1',});
```

LM Studio uses port `1234` by default, but you can change in the [app's Local Server tab](https://lmstudio.ai/docs/basics/server).


## [Language Models](#language-models)


You can interact with local LLMs in [LM Studio](https://lmstudio.ai/docs/basics/server#endpoints-overview) using a provider instance. The first argument is the model id, e.g. `llama-3.2-1b`.

```
const model =lmstudio('llama-3.2-1b');
```


###### [To be able to use a model, you need to](#to-be-able-to-use-a-model-you-need-to-download-it-first) [download it first](https://lmstudio.ai/docs/basics/download-model).



### [Example](#example)


You can use LM Studio language models to generate text with the `generateText` function:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ generateText }from'ai';const lmstudio =createOpenAICompatible({  name:'lmstudio',  baseURL:'https://localhost:1234/v1',});const{ text }=awaitgenerateText({  model:lmstudio('llama-3.2-1b'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',  maxRetries:1,// immediately error if the server is not running});
```

LM Studio language models can also be used with `streamText`.


## [Embedding Models](#embedding-models)


You can create models that call the [LM Studio embeddings API](https://lmstudio.ai/docs/basics/server#endpoints-overview) using the `.embedding()` factory method.

```
const model = lmstudio.embedding('text-embedding-nomic-embed-text-v1.5');
```


### [Example - Embedding a Single Value](#example---embedding-a-single-value)


```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ embed }from'ai';const lmstudio =createOpenAICompatible({  name:'lmstudio',  baseURL:'https://localhost:1234/v1',});// 'embedding' is a single embedding object (number[])const{ embedding }=awaitembed({  model: lmstudio.textEmbeddingModel('text-embedding-nomic-embed-text-v1.5'),  value:'sunny day at the beach',});
```


### [Example - Embedding Many Values](#example---embedding-many-values)


When loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG), it is often useful to embed many values at once (batch embedding).

The AI SDK provides the [`embedMany`](/docs/reference/ai-sdk-core/embed-many) function for this purpose. Similar to `embed`, you can use it with embeddings models, e.g. `lmstudio.textEmbeddingModel('text-embedding-nomic-embed-text-v1.5')` or `lmstudio.textEmbeddingModel('text-embedding-bge-small-en-v1.5')`.

```
import{ createOpenAICompatible }from'@ai-sdk/openai';import{ embedMany }from'ai';const lmstudio =createOpenAICompatible({  name:'lmstudio',  baseURL:'https://localhost:1234/v1',});// 'embeddings' is an array of embedding objects (number[][]).// It is sorted in the same order as the input values.const{ embeddings }=awaitembedMany({  model: lmstudio.textEmbeddingModel('text-embedding-nomic-embed-text-v1.5'),  values:['sunny day at the beach','rainy afternoon in the city','snowy night in the mountains',],});
```
```

### 484. `providers/openai-compatible-providers/nim.md`

```markdown
# NVIDIA NIM Provider


---
url: https://ai-sdk.dev/providers/openai-compatible-providers/nim
description: Use NVIDIA NIM OpenAI compatible API with the AI SDK.
---


# [NVIDIA NIM Provider](#nvidia-nim-provider)


[NVIDIA NIM](https://www.nvidia.com/en-us/ai/) provides optimized inference microservices for deploying foundation models. It offers an OpenAI-compatible API that you can use with the AI SDK.


## [Setup](#setup)


The NVIDIA NIM provider is available via the `@ai-sdk/openai-compatible` module as it is compatible with the OpenAI API. You can install it with:

pnpm

npm

yarn

pnpm add @ai-sdk/openai-compatible


## [Provider Instance](#provider-instance)


To use NVIDIA NIM, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';const nim =createOpenAICompatible({  name:'nim',  baseURL:'https://integrate.api.nvidia.com/v1',  headers:{Authorization:`Bearer ${process.env.NIM_API_KEY}`,},});
```

You can obtain an API key and free credits by registering at [NVIDIA Build](https://build.nvidia.com/explore/discover). New users receive 1,000 inference credits to get started.


## [Language Models](#language-models)


You can interact with NIM models using a provider instance. For example, to use [DeepSeek-R1](https://build.nvidia.com/deepseek-ai/deepseek-r1), a powerful open-source language model:

```
const model = nim.chatModel('deepseek-ai/deepseek-r1');
```


### [Example - Generate Text](#example---generate-text)


You can use NIM language models to generate text with the `generateText` function:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ generateText }from'ai';const nim =createOpenAICompatible({  name:'nim',  baseURL:'https://integrate.api.nvidia.com/v1',  headers:{Authorization:`Bearer ${process.env.NIM_API_KEY}`,},});const{ text, usage, finishReason }=awaitgenerateText({  model: nim.chatModel('deepseek-ai/deepseek-r1'),  prompt:'Tell me the history of the San Francisco Mission-style burrito.',});console.log(text);console.log('Token usage:', usage);console.log('Finish reason:', finishReason);
```


### [Example - Stream Text](#example---stream-text)


NIM language models can also generate text in a streaming fashion with the `streamText` function:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ streamText }from'ai';const nim =createOpenAICompatible({  name:'nim',  baseURL:'https://integrate.api.nvidia.com/v1',  headers:{Authorization:`Bearer ${process.env.NIM_API_KEY}`,},});const result =streamText({  model: nim.chatModel('deepseek-ai/deepseek-r1'),  prompt:'Tell me the history of the Northern White Rhino.',});forawait(const textPart of result.textStream){  process.stdout.write(textPart);}console.log();console.log('Token usage:',await result.usage);console.log('Finish reason:',await result.finishReason);
```

NIM language models can also be used with other AI SDK functions like `generateObject` and `streamObject`.

Model support for tool calls and structured object generation varies. For example, the [`meta/llama-3.3-70b-instruct`](https://build.nvidia.com/meta/llama-3_3-70b-instruct) model supports object generation capabilities. Check each model's documentation on NVIDIA Build for specific supported features.
```

### 485. `providers/openai-compatible-providers.md`

```markdown
# OpenAI Compatible Providers


---
url: https://ai-sdk.dev/providers/openai-compatible-providers
description: Use OpenAI compatible providers with the AI SDK.
---


# [OpenAI Compatible Providers](#openai-compatible-providers)


You can use the [OpenAI Compatible Provider](https://www.npmjs.com/package/@ai-sdk/openai-compatible) package to use language model providers that implement the OpenAI API.

Below we focus on the general setup and provider instance creation. You can also [write a custom provider package leveraging the OpenAI Compatible package](/providers/openai-compatible-providers/custom-providers).

We provide detailed documentation for the following OpenAI compatible providers:

-   [LM Studio](/providers/openai-compatible-providers/lmstudio)
-   [NIM](/providers/openai-compatible-providers/nim)
-   [Baseten](/providers/openai-compatible-providers/baseten)

The general setup and provider instance creation is the same for all of these providers.


## [Setup](#setup)


The OpenAI Compatible provider is available via the `@ai-sdk/openai-compatible` module. You can install it with:

pnpm

npm

yarn

pnpm add @ai-sdk/openai-compatible


## [Provider Instance](#provider-instance)


To use an OpenAI compatible provider, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';const provider =createOpenAICompatible({  name:'provider-name',  apiKey: process.env.PROVIDER_API_KEY,  baseURL:'https://api.provider.com/v1',});
```

You can use the following optional settings to customize the provider instance:

-   **baseURL** *string*

    Set the URL prefix for API calls.

-   **apiKey** *string*

    API key for authenticating requests. If specified, adds an `Authorization` header to request headers with the value `Bearer <apiKey>`. This will be added before any headers potentially specified in the `headers` option.

-   **headers** *Record<string,string>*

    Optional custom headers to include in requests. These will be added to request headers after any headers potentially added by use of the `apiKey` option.

-   **queryParams** *Record<string,string>*

    Optional custom url query parameters to include in request urls.

-   **fetch** *(input: RequestInfo, init?: RequestInit) => Promise<Response>*

    Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation. Defaults to the global `fetch` function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.



## [Language Models](#language-models)


You can create provider models using a provider instance. The first argument is the model id, e.g. `model-id`.

```
const model =provider('model-id');
```


### [Example](#example)


You can use provider language models to generate text with the `generateText` function:

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ generateText }from'ai';const provider =createOpenAICompatible({  name:'provider-name',  apiKey: process.env.PROVIDER_API_KEY,  baseURL:'https://api.provider.com/v1',});const{ text }=awaitgenerateText({  model:provider('model-id'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```


### [Including model ids for auto-completion](#including-model-ids-for-auto-completion)


```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';import{ generateText }from'ai';typeExampleChatModelIds=|'meta-llama/Llama-3-70b-chat-hf'|'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'|(string&{});typeExampleCompletionModelIds=|'codellama/CodeLlama-34b-Instruct-hf'|'Qwen/Qwen2.5-Coder-32B-Instruct'|(string&{});typeExampleEmbeddingModelIds=|'BAAI/bge-large-en-v1.5'|'bert-base-uncased'|(string&{});const model =createOpenAICompatible<ExampleChatModelIds,ExampleCompletionModelIds,ExampleEmbeddingModelIds>({  name:'example',  apiKey: process.env.PROVIDER_API_KEY,  baseURL:'https://api.example.com/v1',});// Subsequent calls to e.g. `model.chatModel` will auto-complete the model id// from the list of `ExampleChatModelIds` while still allowing free-form// strings as well.const{ text }=awaitgenerateText({  model: model.chatModel('meta-llama/Llama-3-70b-chat-hf'),  prompt:'Write a vegetarian lasagna recipe for 4 people.',});
```


### [Custom query parameters](#custom-query-parameters)


Some providers may require custom query parameters. An example is the [Azure AI Model Inference API](https://learn.microsoft.com/en-us/azure/machine-learning/reference-model-inference-chat-completions?view=azureml-api-2) which requires an `api-version` query parameter.

You can set these via the optional `queryParams` provider setting. These will be added to all requests made by the provider.

```
import{ createOpenAICompatible }from'@ai-sdk/openai-compatible';const provider =createOpenAICompatible({  name:'provider-name',  apiKey: process.env.PROVIDER_API_KEY,  baseURL:'https://api.provider.com/v1',  queryParams:{'api-version':'1.0.0',},});
```

For example, with the above configuration, API requests would include the query parameter in the URL like: `https://api.provider.com/v1/chat/completions?api-version=1.0.0`.


## [Provider-specific options](#provider-specific-options)


The OpenAI Compatible provider supports adding provider-specific options to the request body. These are specified with the `providerOptions` field in the request body.

For example, if you create a provider instance with the name `provider-name`, you can add a `custom-option` field to the request body like this:

```
const provider =createOpenAICompatible({  name:'provider-name',  apiKey: process.env.PROVIDER_API_KEY,  baseURL:'https://api.provider.com/v1',});const{ text }=awaitgenerateText({  model:provider('model-id'),  prompt:'Hello',  providerOptions:{'provider-name':{ customOption:'magic-value'},},});
```

The request body sent to the provider will include the `customOption` field with the value `magic-value`. This gives you an easy way to add provider-specific options to requests without having to modify the provider or AI SDK code.


## [Custom Metadata Extraction](#custom-metadata-extraction)


The OpenAI Compatible provider supports extracting provider-specific metadata from API responses through metadata extractors. These extractors allow you to capture additional information returned by the provider beyond the standard response format.

Metadata extractors receive the raw, unprocessed response data from the provider, giving you complete flexibility to extract any custom fields or experimental features that the provider may include. This is particularly useful when:

-   Working with providers that include non-standard response fields
-   Experimenting with beta or preview features
-   Capturing provider-specific metrics or debugging information
-   Supporting rapid provider API evolution without SDK changes

Metadata extractors work with both streaming and non-streaming chat completions and consist of two main components:

1.  A function to extract metadata from complete responses
2.  A streaming extractor that can accumulate metadata across chunks in a streaming response

Here's an example metadata extractor that captures both standard and custom provider data:

```
const myMetadataExtractor:MetadataExtractor={// Process complete, non-streaming responsesextractMetadata:({ parsedBody })=>{// You have access to the complete raw response// Extract any fields the provider includesreturn{      myProvider:{        standardUsage: parsedBody.usage,        experimentalFeatures: parsedBody.beta_features,        customMetrics:{          processingTime: parsedBody.server_timing?.total_ms,          modelVersion: parsedBody.model_version,// ... any other provider-specific data},},};},// Process streaming responsescreateStreamExtractor:()=>{let accumulatedData ={      timing:[],      customFields:{},};return{// Process each chunk's raw dataprocessChunk: parsedChunk =>{if(parsedChunk.server_timing){          accumulatedData.timing.push(parsedChunk.server_timing);}if(parsedChunk.custom_data){Object.assign(accumulatedData.customFields, parsedChunk.custom_data);}},// Build final metadata from accumulated databuildMetadata:()=>({        myProvider:{          streamTiming: accumulatedData.timing,          customData: accumulatedData.customFields,},}),};},};
```

You can provide a metadata extractor when creating your provider instance:

```
const provider =createOpenAICompatible({  name:'my-provider',  apiKey: process.env.PROVIDER_API_KEY,  baseURL:'https://api.provider.com/v1',  metadataExtractor: myMetadataExtractor,});
```

The extracted metadata will be included in the response under the `providerMetadata` field:

```
const{ text, providerMetadata }=awaitgenerateText({  model:provider('model-id'),  prompt:'Hello',});console.log(providerMetadata.myProvider.customMetric);
```

This allows you to access provider-specific information while maintaining a consistent interface across different providers.
```

### 486. `scraping-summary.json`

```json
{
  "scrapingDate": "2025-05-31T12:53:37.700Z",
  "baseUrl": "https://ai-sdk.dev/docs/introduction",
  "outputFormat": "markdown",
  "totalPages": 487,
  "pages": [
    {
      "title": "AI SDK",
      "url": "https://ai-sdk.dev/",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/index.md",
      "metadata": {
        "url": "https://ai-sdk.dev/",
        "description": "The AI Toolkit for TypeScript, from the creators of Next.js."
      }
    },
    {
      "title": "Cookbook",
      "url": "https://ai-sdk.dev/cookbook",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook",
        "description": "Open-source collection of examples, guides, and templates for building with the AI SDK."
      }
    },
    {
      "title": "Caching Middleware",
      "url": "https://ai-sdk.dev/cookbook/next/caching-middleware",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/caching-middleware.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/caching-middleware",
        "description": "Learn how to create a caching middleware with Next.js and KV."
      }
    },
    {
      "title": "Call Tools",
      "url": "https://ai-sdk.dev/cookbook/next/call-tools",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/call-tools.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/call-tools",
        "description": "Learn how to call tools using the AI SDK and Next.js"
      }
    },
    {
      "title": "Call Tools in Parallel",
      "url": "https://ai-sdk.dev/cookbook/next/call-tools-in-parallel",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/call-tools-in-parallel.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/call-tools-in-parallel",
        "description": "Learn how to call tools in parallel using the AI SDK and Next.js"
      }
    },
    {
      "title": "Call Tools in Multiple Steps",
      "url": "https://ai-sdk.dev/cookbook/next/call-tools-multiple-steps",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/call-tools-multiple-steps.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/call-tools-multiple-steps",
        "description": "Learn how to call tools in multiple steps using the AI SDK and Next.js"
      }
    },
    {
      "title": "Chat with PDFs",
      "url": "https://ai-sdk.dev/cookbook/next/chat-with-pdf",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/chat-with-pdf.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/chat-with-pdf",
        "description": "Learn how to build a chatbot that can understand PDFs using the AI SDK and Next.js"
      }
    },
    {
      "title": "Generate Image with Chat Prompt",
      "url": "https://ai-sdk.dev/cookbook/next/generate-image-with-chat-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/generate-image-with-chat-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/generate-image-with-chat-prompt",
        "description": "Learn how to generate an image with a chat prompt using the AI SDK and Next.js"
      }
    },
    {
      "title": "Generate Object",
      "url": "https://ai-sdk.dev/cookbook/next/generate-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/generate-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/generate-object",
        "description": "Learn how to generate object using the AI SDK and Next.js"
      }
    },
    {
      "title": "Generate Object with File Prompt through Form Submission",
      "url": "https://ai-sdk.dev/cookbook/next/generate-object-with-file-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/generate-object-with-file-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/generate-object-with-file-prompt",
        "description": "Learn how to generate object with file prompt through form submission using the AI SDK and Next.js"
      }
    },
    {
      "title": "Generate Text",
      "url": "https://ai-sdk.dev/cookbook/next/generate-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/generate-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/generate-text",
        "description": "Learn how to generate text using the AI SDK and Next.js."
      }
    },
    {
      "title": "Generate Text with Chat Prompt",
      "url": "https://ai-sdk.dev/cookbook/next/generate-text-with-chat-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/generate-text-with-chat-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/generate-text-with-chat-prompt",
        "description": "Learn how to generate text with chat prompt using the AI SDK and Next.js"
      }
    },
    {
      "title": "Human-in-the-Loop with Next.js",
      "url": "https://ai-sdk.dev/cookbook/next/human-in-the-loop",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/human-in-the-loop.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/human-in-the-loop",
        "description": "Add a human approval step to your agentic system with Next.js and the AI SDK"
      }
    },
    {
      "title": "Markdown Chatbot with Memoization",
      "url": "https://ai-sdk.dev/cookbook/next/markdown-chatbot-with-memoization",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/markdown-chatbot-with-memoization.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/markdown-chatbot-with-memoization",
        "description": "Build a chatbot that renders and memoizes Markdown responses with Next.js and the AI SDK."
      }
    },
    {
      "title": "MCP Tools",
      "url": "https://ai-sdk.dev/cookbook/next/mcp-tools",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/mcp-tools.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/mcp-tools",
        "description": "Learn how to use MCP tools with the AI SDK and Next.js"
      }
    },
    {
      "title": "Render Visual Interface in Chat",
      "url": "https://ai-sdk.dev/cookbook/next/render-visual-interface-in-chat",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/render-visual-interface-in-chat.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/render-visual-interface-in-chat",
        "description": "Learn how to render visual interfaces in chat using the AI SDK and Next.js"
      }
    },
    {
      "title": "Send Custom Body from useChat",
      "url": "https://ai-sdk.dev/cookbook/next/send-custom-body-from-use-chat",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/send-custom-body-from-use-chat.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/send-custom-body-from-use-chat",
        "description": "Learn how to send a custom body from the useChat hook using the AI SDK and Next.js"
      }
    },
    {
      "title": "Stream Assistant Response",
      "url": "https://ai-sdk.dev/cookbook/next/stream-assistant-response",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/stream-assistant-response.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/stream-assistant-response",
        "description": "Learn how to stream OpenAI Assistant's response using the AI SDK and Next.js"
      }
    },
    {
      "title": "Stream Assistant Response with Tools",
      "url": "https://ai-sdk.dev/cookbook/next/stream-assistant-response-with-tools",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/stream-assistant-response-with-tools.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/stream-assistant-response-with-tools",
        "description": "Learn how to stream OpenAI Assistant's response using the AI SDK and Next.js"
      }
    },
    {
      "title": "Stream Object",
      "url": "https://ai-sdk.dev/cookbook/next/stream-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/stream-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/stream-object",
        "description": "Learn how to stream object using the AI SDK and Next.js"
      }
    },
    {
      "title": "Stream Text",
      "url": "https://ai-sdk.dev/cookbook/next/stream-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/stream-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/stream-text",
        "description": "Learn how to stream text using the AI SDK and Next.js"
      }
    },
    {
      "title": "Stream Text Multi-Step",
      "url": "https://ai-sdk.dev/cookbook/next/stream-text-multistep",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/stream-text-multistep.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/stream-text-multistep",
        "description": "Learn how to create several streamText steps with different settings"
      }
    },
    {
      "title": "Stream Text with Chat Prompt",
      "url": "https://ai-sdk.dev/cookbook/next/stream-text-with-chat-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/stream-text-with-chat-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/stream-text-with-chat-prompt",
        "description": "Learn how to generate text using the AI SDK and Next.js"
      }
    },
    {
      "title": "Stream Text with Image Prompt",
      "url": "https://ai-sdk.dev/cookbook/next/stream-text-with-image-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/next/stream-text-with-image-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/next/stream-text-with-image-prompt",
        "description": "Learn how to stream text with an image prompt using the AI SDK and Next.js"
      }
    },
    {
      "title": "Call Tools",
      "url": "https://ai-sdk.dev/cookbook/node/call-tools",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/call-tools.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/call-tools",
        "description": "Learn how to call tools using the AI SDK and Node"
      }
    },
    {
      "title": "Call Tools in Parallel",
      "url": "https://ai-sdk.dev/cookbook/node/call-tools-in-parallel",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/call-tools-in-parallel.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/call-tools-in-parallel",
        "description": "Learn how to call tools in parallel using the AI SDK and Node"
      }
    },
    {
      "title": "Call Tools in Multiple Steps",
      "url": "https://ai-sdk.dev/cookbook/node/call-tools-multiple-steps",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/call-tools-multiple-steps.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/call-tools-multiple-steps",
        "description": "Learn how to call tools with multiple steps using the AI SDK and Node"
      }
    },
    {
      "title": "Call Tools with Image Prompt",
      "url": "https://ai-sdk.dev/cookbook/node/call-tools-with-image-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/call-tools-with-image-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/call-tools-with-image-prompt",
        "description": "Learn how to call tools with image prompt using the AI SDK and Node"
      }
    },
    {
      "title": "Embed Text",
      "url": "https://ai-sdk.dev/cookbook/node/embed-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/embed-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/embed-text",
        "description": "Learn how to embed text using the AI SDK and Node"
      }
    },
    {
      "title": "Embed Text in Batch",
      "url": "https://ai-sdk.dev/cookbook/node/embed-text-batch",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/embed-text-batch.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/embed-text-batch",
        "description": "Learn how to embed multiple text using the AI SDK and Node"
      }
    },
    {
      "title": "Generate Object",
      "url": "https://ai-sdk.dev/cookbook/node/generate-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/generate-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/generate-object",
        "description": "Learn how to generate structured data using the AI SDK and Node"
      }
    },
    {
      "title": "Generate Object with a Reasoning Model",
      "url": "https://ai-sdk.dev/cookbook/node/generate-object-reasoning",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/generate-object-reasoning.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/generate-object-reasoning",
        "description": "Learn how to generate structured data with a reasoning model using the AI SDK and Node"
      }
    },
    {
      "title": "Generate Text",
      "url": "https://ai-sdk.dev/cookbook/node/generate-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/generate-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/generate-text",
        "description": "Learn how to generate text using the AI SDK and Node"
      }
    },
    {
      "title": "Generate Text with Chat Prompt",
      "url": "https://ai-sdk.dev/cookbook/node/generate-text-with-chat-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/generate-text-with-chat-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/generate-text-with-chat-prompt",
        "description": "Learn how to generate text with chat prompt using the AI SDK and Node"
      }
    },
    {
      "title": "Generate Text with Image Prompt",
      "url": "https://ai-sdk.dev/cookbook/node/generate-text-with-image-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/generate-text-with-image-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/generate-text-with-image-prompt",
        "description": "Learn how to generate text with image prompt using the AI SDK and Node"
      }
    },
    {
      "title": "Intercepting Fetch Requests",
      "url": "https://ai-sdk.dev/cookbook/node/intercept-fetch-requests",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/intercept-fetch-requests.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/intercept-fetch-requests",
        "description": "Learn how to intercept fetch requests using the AI SDK and Node"
      }
    },
    {
      "title": "Local Caching Middleware",
      "url": "https://ai-sdk.dev/cookbook/node/local-caching-middleware",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/local-caching-middleware.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/local-caching-middleware",
        "description": "Learn how to create a caching middleware for local development."
      }
    },
    {
      "title": "MCP Tools",
      "url": "https://ai-sdk.dev/cookbook/node/mcp-tools",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/mcp-tools.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/mcp-tools",
        "description": "Learn how to use MCP tools with the AI SDK and Node"
      }
    },
    {
      "title": "Retrieval Augmented Generation",
      "url": "https://ai-sdk.dev/cookbook/node/retrieval-augmented-generation",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/retrieval-augmented-generation.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/retrieval-augmented-generation",
        "description": "Learn how to use retrieval augmented generation using the AI SDK and Node"
      }
    },
    {
      "title": "Stream Object",
      "url": "https://ai-sdk.dev/cookbook/node/stream-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/stream-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/stream-object",
        "description": "Learn how to stream structured data using the AI SDK and Node"
      }
    },
    {
      "title": "Record Final Object after Streaming Object",
      "url": "https://ai-sdk.dev/cookbook/node/stream-object-record-final-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/stream-object-record-final-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/stream-object-record-final-object",
        "description": "Learn how to record the final object after streaming an object using the AI SDK and Node"
      }
    },
    {
      "title": "Record Token Usage After Streaming Object",
      "url": "https://ai-sdk.dev/cookbook/node/stream-object-record-token-usage",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/stream-object-record-token-usage.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/stream-object-record-token-usage",
        "description": "Learn how to record token usage when streaming structured data using the AI SDK and Node"
      }
    },
    {
      "title": "Stream Object with Image Prompt",
      "url": "https://ai-sdk.dev/cookbook/node/stream-object-with-image-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/stream-object-with-image-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/stream-object-with-image-prompt",
        "description": "Learn how to stream structured data with an image prompt using the AI SDK and Node"
      }
    },
    {
      "title": "Stream Text",
      "url": "https://ai-sdk.dev/cookbook/node/stream-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/stream-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/stream-text",
        "description": "Learn how to stream text using the AI SDK and Node"
      }
    },
    {
      "title": "Stream Text with Chat Prompt",
      "url": "https://ai-sdk.dev/cookbook/node/stream-text-with-chat-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/stream-text-with-chat-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/stream-text-with-chat-prompt",
        "description": "Learn how to stream text with chat prompt using the AI SDK and Node"
      }
    },
    {
      "title": "Stream Text with File Prompt",
      "url": "https://ai-sdk.dev/cookbook/node/stream-text-with-file-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/stream-text-with-file-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/stream-text-with-file-prompt",
        "description": "Learn how to stream text with file prompt using the AI SDK and Node"
      }
    },
    {
      "title": "Stream Text with Image Prompt",
      "url": "https://ai-sdk.dev/cookbook/node/stream-text-with-image-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/stream-text-with-image-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/stream-text-with-image-prompt",
        "description": "Learn how to stream text with image prompt using the AI SDK and Node"
      }
    },
    {
      "title": "Web Search Agent",
      "url": "https://ai-sdk.dev/cookbook/node/web-search-agent",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/node/web-search-agent.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/node/web-search-agent",
        "description": "Learn how to build an agent that has access to web with the AI SDK and Node"
      }
    },
    {
      "title": "Call Tools",
      "url": "https://ai-sdk.dev/cookbook/rsc/call-tools",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/call-tools.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/call-tools",
        "description": "Learn how to call tools using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Call Tools in Parallel",
      "url": "https://ai-sdk.dev/cookbook/rsc/call-tools-in-parallel",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/call-tools-in-parallel.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/call-tools-in-parallel",
        "description": "Learn how to tools in parallel text using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Generate Object",
      "url": "https://ai-sdk.dev/cookbook/rsc/generate-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/generate-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/generate-object",
        "description": "Learn how to generate object using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Generate Text",
      "url": "https://ai-sdk.dev/cookbook/rsc/generate-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/generate-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/generate-text",
        "description": "Learn how to generate text using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Generate Text with Chat Prompt",
      "url": "https://ai-sdk.dev/cookbook/rsc/generate-text-with-chat-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/generate-text-with-chat-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/generate-text-with-chat-prompt",
        "description": "Learn how to generate text with chat prompt using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Render Visual Interface in Chat",
      "url": "https://ai-sdk.dev/cookbook/rsc/render-visual-interface-in-chat",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/render-visual-interface-in-chat.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/render-visual-interface-in-chat",
        "description": "Learn how to generate text using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Restore Messages from Database",
      "url": "https://ai-sdk.dev/cookbook/rsc/restore-messages-from-database",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/restore-messages-from-database.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/restore-messages-from-database",
        "description": "Learn how to restore messages from an external database using the AI SDK and React Server Components"
      }
    },
    {
      "title": "Save Messages To Database",
      "url": "https://ai-sdk.dev/cookbook/rsc/save-messages-to-database",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/save-messages-to-database.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/save-messages-to-database",
        "description": "Learn how to save messages to an external database using the AI SDK and React Server Components"
      }
    },
    {
      "title": "Stream Assistant Responses",
      "url": "https://ai-sdk.dev/cookbook/rsc/stream-assistant-response",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/stream-assistant-response.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/stream-assistant-response",
        "description": "Learn how to generate text using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Stream Assistant Responses",
      "url": "https://ai-sdk.dev/cookbook/rsc/stream-assistant-response-with-tools",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/stream-assistant-response-with-tools.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/stream-assistant-response-with-tools",
        "description": "Learn how to generate text using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Stream Object",
      "url": "https://ai-sdk.dev/cookbook/rsc/stream-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/stream-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/stream-object",
        "description": "Learn how to stream object using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Stream Text",
      "url": "https://ai-sdk.dev/cookbook/rsc/stream-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/stream-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/stream-text",
        "description": "Learn how to stream text using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Stream Text with Chat Prompt",
      "url": "https://ai-sdk.dev/cookbook/rsc/stream-text-with-chat-prompt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/stream-text-with-chat-prompt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/stream-text-with-chat-prompt",
        "description": "Learn how to stream text with chat prompt using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Record Token Usage after Streaming User Interfaces",
      "url": "https://ai-sdk.dev/cookbook/rsc/stream-ui-record-token-usage",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/stream-ui-record-token-usage.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/stream-ui-record-token-usage",
        "description": "Learn how to record token usage after streaming user interfaces using the AI SDK and React Server Components"
      }
    },
    {
      "title": "Stream Updates to Visual Interfaces",
      "url": "https://ai-sdk.dev/cookbook/rsc/stream-updates-to-visual-interfaces",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/cookbook/rsc/stream-updates-to-visual-interfaces.md",
      "metadata": {
        "url": "https://ai-sdk.dev/cookbook/rsc/stream-updates-to-visual-interfaces",
        "description": "Learn how to generate text using the AI SDK and React Server Components."
      }
    },
    {
      "title": "Advanced",
      "url": "https://ai-sdk.dev/docs/advanced",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced",
        "description": "Learn how to use advanced functionality within the AI SDK and RSC API."
      }
    },
    {
      "title": "Stream Back-pressure and Cancellation",
      "url": "https://ai-sdk.dev/docs/advanced/backpressure",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/backpressure.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/backpressure",
        "description": "How to handle backpressure and cancellation when working with the AI SDK"
      }
    },
    {
      "title": "Caching Responses",
      "url": "https://ai-sdk.dev/docs/advanced/caching",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/caching.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/caching",
        "description": "How to handle caching when working with the AI SDK"
      }
    },
    {
      "title": "Generative User Interfaces",
      "url": "https://ai-sdk.dev/docs/advanced/model-as-router",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/model-as-router.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/model-as-router",
        "description": "Generative User Interfaces and Language Models as Routers"
      }
    },
    {
      "title": "Multiple Streams",
      "url": "https://ai-sdk.dev/docs/advanced/multiple-streamables",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/multiple-streamables.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/multiple-streamables",
        "description": "Learn to handle multiple streamables in your application."
      }
    },
    {
      "title": "Multistep Interfaces",
      "url": "https://ai-sdk.dev/docs/advanced/multistep-interfaces",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/multistep-interfaces.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/multistep-interfaces",
        "description": "Concepts behind building multistep interfaces"
      }
    },
    {
      "title": "Prompt Engineering",
      "url": "https://ai-sdk.dev/docs/advanced/prompt-engineering",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/prompt-engineering.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/prompt-engineering",
        "description": "Learn how to engineer prompts for LLMs with the AI SDK"
      }
    },
    {
      "title": "Rate Limiting",
      "url": "https://ai-sdk.dev/docs/advanced/rate-limiting",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/rate-limiting.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/rate-limiting",
        "description": "Learn how to rate limit your application."
      }
    },
    {
      "title": "Rendering User Interfaces with Language Models",
      "url": "https://ai-sdk.dev/docs/advanced/rendering-ui-with-language-models",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/rendering-ui-with-language-models.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/rendering-ui-with-language-models",
        "description": "Rendering UI with Language Models"
      }
    },
    {
      "title": "Sequential Generations",
      "url": "https://ai-sdk.dev/docs/advanced/sequential-generations",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/sequential-generations.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/sequential-generations",
        "description": "Learn how to implement sequential generations (\"chains\") with the AI SDK"
      }
    },
    {
      "title": "Stopping Streams",
      "url": "https://ai-sdk.dev/docs/advanced/stopping-streams",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/stopping-streams.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/stopping-streams",
        "description": "Learn how to cancel streams with the AI SDK"
      }
    },
    {
      "title": "Vercel Deployment Guide",
      "url": "https://ai-sdk.dev/docs/advanced/vercel-deployment-guide",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/advanced/vercel-deployment-guide.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/advanced/vercel-deployment-guide",
        "description": "Learn how to deploy an AI application to production on Vercel"
      }
    },
    {
      "title": "AI SDK Core",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core",
        "description": "Learn about AI SDK Core."
      }
    },
    {
      "title": "Embeddings",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/embeddings",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/embeddings.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/embeddings",
        "description": "Learn how to embed values with the AI SDK."
      }
    },
    {
      "title": "Error Handling",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/error-handling",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/error-handling.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/error-handling",
        "description": "Learn how to handle errors in the AI SDK Core"
      }
    },
    {
      "title": "Generating Structured Data",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/generating-structured-data.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/generating-structured-data",
        "description": "Learn how to generate structured data with the AI SDK."
      }
    },
    {
      "title": "Generating and Streaming Text",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/generating-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/generating-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/generating-text",
        "description": "Learn how to generate text with the AI SDK."
      }
    },
    {
      "title": "Image Generation",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/image-generation",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/image-generation.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/image-generation",
        "description": "Learn how to generate images with the AI SDK."
      }
    },
    {
      "title": "Language Model Middleware",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/middleware",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/middleware.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/middleware",
        "description": "Learn how to use middleware to enhance the behavior of language models"
      }
    },
    {
      "title": "AI SDK Core",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/overview",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/overview.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/overview",
        "description": "An overview of AI SDK Core."
      }
    },
    {
      "title": "Prompt Engineering",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/prompt-engineering.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/prompt-engineering",
        "description": "Learn how to develop prompts with AI SDK Core."
      }
    },
    {
      "title": "Provider & Model Management",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/provider-management",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/provider-management.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/provider-management",
        "description": "Learn how to work with multiple providers and models"
      }
    },
    {
      "title": "Settings",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/settings",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/settings.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/settings",
        "description": "Learn how to configure the AI SDK."
      }
    },
    {
      "title": "Speech",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/speech",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/speech.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/speech",
        "description": "Learn how to generate speech from text with the AI SDK."
      }
    },
    {
      "title": "Telemetry",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/telemetry",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/telemetry.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/telemetry",
        "description": "Using OpenTelemetry with AI SDK Core"
      }
    },
    {
      "title": "Testing",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/testing",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/testing.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/testing",
        "description": "Learn how to use AI SDK Core mock providers for testing."
      }
    },
    {
      "title": "Tool Calling",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/tools-and-tool-calling.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling",
        "description": "Learn about tool calling and multi-step calls (using maxSteps) with AI SDK Core."
      }
    },
    {
      "title": "Transcription",
      "url": "https://ai-sdk.dev/docs/ai-sdk-core/transcription",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-core/transcription.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-core/transcription",
        "description": "Learn how to transcribe audio with the AI SDK."
      }
    },
    {
      "title": "AI SDK RSC",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc",
        "description": "Learn about AI SDK RSC."
      }
    },
    {
      "title": "Authentication",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/authentication",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/authentication.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/authentication",
        "description": "Learn how to authenticate with the AI SDK."
      }
    },
    {
      "title": "Error Handling",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/error-handling",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/error-handling.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/error-handling",
        "description": "Learn how to handle errors with the AI SDK."
      }
    },
    {
      "title": "Managing Generative UI State",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/generative-ui-state",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/generative-ui-state.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/generative-ui-state",
        "description": "Overview of the AI and UI states"
      }
    },
    {
      "title": "Handling Loading State",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/loading-state",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/loading-state.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/loading-state",
        "description": "Overview of handling loading state with AI SDK RSC"
      }
    },
    {
      "title": "Migrating from RSC to UI",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/migrating-to-ui",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/migrating-to-ui.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/migrating-to-ui",
        "description": "Learn how to migrate from AI SDK RSC to AI SDK UI."
      }
    },
    {
      "title": "Designing Multistep Interfaces",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/multistep-interfaces",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/multistep-interfaces.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/multistep-interfaces",
        "description": "Overview of Building Multistep Interfaces with AI SDK RSC"
      }
    },
    {
      "title": "AI SDK RSC",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/overview",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/overview.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/overview",
        "description": "An overview of AI SDK RSC."
      }
    },
    {
      "title": "Saving and Restoring States",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/saving-and-restoring-states",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/saving-and-restoring-states.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/saving-and-restoring-states",
        "description": "Saving and restoring AI and UI states with onGetUIState and onSetAIState"
      }
    },
    {
      "title": "Streaming React Components",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/streaming-react-components",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/streaming-react-components.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/streaming-react-components",
        "description": "Overview of streaming RSCs"
      }
    },
    {
      "title": "Streaming Values",
      "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/streaming-values",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-rsc/streaming-values.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-rsc/streaming-values",
        "description": "Overview of streaming RSCs"
      }
    },
    {
      "title": "AI SDK UI",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui",
        "description": "Learn about the AI SDK UI."
      }
    },
    {
      "title": "Chatbot",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/chatbot.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot",
        "description": "Learn how to use the useChat hook."
      }
    },
    {
      "title": "Chatbot Message Persistence",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/chatbot-message-persistence.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence",
        "description": "Learn how to store and load chat messages in a chatbot."
      }
    },
    {
      "title": "Chatbot Tool Usage",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/chatbot-tool-usage.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
        "description": "Learn how to use tools with the useChat hook."
      }
    },
    {
      "title": "Completion",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/completion",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/completion.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/completion",
        "description": "Learn how to use the useCompletion hook."
      }
    },
    {
      "title": "Error Handling",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/error-handling",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/error-handling.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/error-handling",
        "description": "Learn how to handle errors in the AI SDK UI"
      }
    },
    {
      "title": "Generative User Interfaces",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/generative-user-interfaces.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces",
        "description": "Learn how to build Generative UI with AI SDK UI."
      }
    },
    {
      "title": "Object Generation",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/object-generation",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/object-generation.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/object-generation",
        "description": "Learn how to use the useObject hook."
      }
    },
    {
      "title": "OpenAI Assistants",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/openai-assistants",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/openai-assistants.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/openai-assistants",
        "description": "Learn how to use the useAssistant hook."
      }
    },
    {
      "title": "AI SDK UI",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/overview",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/overview.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/overview",
        "description": "An overview of AI SDK UI."
      }
    },
    {
      "title": "Smooth streaming chinese text",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/smooth-stream-chinese",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/smooth-stream-chinese.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/smooth-stream-chinese",
        "description": "Learn how to stream smooth stream chinese text"
      }
    },
    {
      "title": "Smooth streaming japanese text",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/smooth-stream-japanese",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/smooth-stream-japanese.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/smooth-stream-japanese",
        "description": "Learn how to stream smooth stream japanese text"
      }
    },
    {
      "title": "Stream Protocols",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/stream-protocol.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol",
        "description": "Learn more about the supported stream protocols in the AI SDK."
      }
    },
    {
      "title": "Streaming Custom Data",
      "url": "https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/ai-sdk-ui/streaming-data.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data",
        "description": "Learn how to stream custom data to the client."
      }
    },
    {
      "title": "Announcing AI SDK 5 Alpha",
      "url": "https://ai-sdk.dev/docs/announcing-ai-sdk-5-alpha",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/announcing-ai-sdk-5-alpha.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/announcing-ai-sdk-5-alpha",
        "description": "Get started with the Alpha version of AI SDK 5."
      }
    },
    {
      "title": "Foundations",
      "url": "https://ai-sdk.dev/docs/foundations",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/foundations.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/foundations",
        "description": "A section that covers foundational knowledge around LLMs and concepts crucial to the AI SDK"
      }
    },
    {
      "title": "Agents",
      "url": "https://ai-sdk.dev/docs/foundations/agents",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/foundations/agents.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/foundations/agents",
        "description": "Learn how to build agents with AI SDK Core."
      }
    },
    {
      "title": "Overview",
      "url": "https://ai-sdk.dev/docs/foundations/overview",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/foundations/overview.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/foundations/overview",
        "description": "An overview of foundational concepts critical to understanding the AI SDK"
      }
    },
    {
      "title": "Prompts",
      "url": "https://ai-sdk.dev/docs/foundations/prompts",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/foundations/prompts.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/foundations/prompts",
        "description": "Learn about the Prompt structure used in the AI SDK."
      }
    },
    {
      "title": "Providers and Models",
      "url": "https://ai-sdk.dev/docs/foundations/providers-and-models",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/foundations/providers-and-models.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/foundations/providers-and-models",
        "description": "Learn about the providers and models available in the AI SDK."
      }
    },
    {
      "title": "Streaming",
      "url": "https://ai-sdk.dev/docs/foundations/streaming",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/foundations/streaming.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/foundations/streaming",
        "description": "Why use streaming for AI applications?"
      }
    },
    {
      "title": "Tools",
      "url": "https://ai-sdk.dev/docs/foundations/tools",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/foundations/tools.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/foundations/tools",
        "description": "Learn about tools with the AI SDK."
      }
    },
    {
      "title": "Getting Started",
      "url": "https://ai-sdk.dev/docs/getting-started",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/getting-started.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/getting-started",
        "description": "Welcome to the AI SDK documentation!"
      }
    },
    {
      "title": "Expo Quickstart",
      "url": "https://ai-sdk.dev/docs/getting-started/expo",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/getting-started/expo.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/getting-started/expo",
        "description": "Welcome to the AI SDK quickstart guide for Expo!"
      }
    },
    {
      "title": "Navigating the Library",
      "url": "https://ai-sdk.dev/docs/getting-started/navigating-the-library",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/getting-started/navigating-the-library.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/getting-started/navigating-the-library",
        "description": "Learn how to navigate the AI SDK."
      }
    },
    {
      "title": "Next.js App Router Quickstart",
      "url": "https://ai-sdk.dev/docs/getting-started/nextjs-app-router",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/getting-started/nextjs-app-router.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/getting-started/nextjs-app-router",
        "description": "Welcome to the AI SDK quickstart guide for Next.js App Router!"
      }
    },
    {
      "title": "Next.js Pages Router Quickstart",
      "url": "https://ai-sdk.dev/docs/getting-started/nextjs-pages-router",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/getting-started/nextjs-pages-router.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/getting-started/nextjs-pages-router",
        "description": "Welcome to the AI SDK quickstart guide for Next.js Pages Router!"
      }
    },
    {
      "title": "Node.js Quickstart",
      "url": "https://ai-sdk.dev/docs/getting-started/nodejs",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/getting-started/nodejs.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/getting-started/nodejs",
        "description": "Welcome to the AI SDK quickstart guide for Node.js!"
      }
    },
    {
      "title": "Vue.js (Nuxt) Quickstart",
      "url": "https://ai-sdk.dev/docs/getting-started/nuxt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/getting-started/nuxt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/getting-started/nuxt",
        "description": "Welcome to the AI SDK quickstart guide for Vue.js (Nuxt)!"
      }
    },
    {
      "title": "Svelte Quickstart",
      "url": "https://ai-sdk.dev/docs/getting-started/svelte",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/getting-started/svelte.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/getting-started/svelte",
        "description": "Welcome to the AI SDK quickstart guide for Svelte!"
      }
    },
    {
      "title": "Guides",
      "url": "https://ai-sdk.dev/docs/guides",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides",
        "description": "Learn how to build AI applications with the AI SDK"
      }
    },
    {
      "title": "Get started with Claude 4",
      "url": "https://ai-sdk.dev/docs/guides/claude-4",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/claude-4.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/claude-4",
        "description": "Get started with Claude 4 using the AI SDK."
      }
    },
    {
      "title": "Get started with Computer Use",
      "url": "https://ai-sdk.dev/docs/guides/computer-use",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/computer-use.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/computer-use",
        "description": "Get started with Claude's Computer Use capabilities with the AI SDK"
      }
    },
    {
      "title": "Get started with OpenAI GPT-4.5",
      "url": "https://ai-sdk.dev/docs/guides/gpt-4-5",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/gpt-4-5.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/gpt-4-5",
        "description": "Get started with OpenAI GPT-4.5 using the AI SDK."
      }
    },
    {
      "title": "Get started with Llama 3.1",
      "url": "https://ai-sdk.dev/docs/guides/llama-3_1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/llama-3_1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/llama-3_1",
        "description": "Get started with Llama 3.1 using the AI SDK."
      }
    },
    {
      "title": "Multi-Modal Chatbot",
      "url": "https://ai-sdk.dev/docs/guides/multi-modal-chatbot",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/multi-modal-chatbot.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/multi-modal-chatbot",
        "description": "Learn how to build a multi-modal chatbot that can process images and PDFs with the AI SDK."
      }
    },
    {
      "title": "Natural Language Postgres Guide",
      "url": "https://ai-sdk.dev/docs/guides/natural-language-postgres",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/natural-language-postgres.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/natural-language-postgres",
        "description": "Learn how to build a Next.js app that lets you talk to a PostgreSQL database in natural language."
      }
    },
    {
      "title": "Get started with OpenAI o1",
      "url": "https://ai-sdk.dev/docs/guides/o1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/o1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/o1",
        "description": "Get started with OpenAI o1 using the AI SDK."
      }
    },
    {
      "title": "Get started with OpenAI o3-mini",
      "url": "https://ai-sdk.dev/docs/guides/o3",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/o3.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/o3",
        "description": "Get started with OpenAI o3-mini using the AI SDK."
      }
    },
    {
      "title": "Get started with OpenAI Responses API",
      "url": "https://ai-sdk.dev/docs/guides/openai-responses",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/openai-responses.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/openai-responses",
        "description": "Get started with the OpenAI Responses API using the AI SDK."
      }
    },
    {
      "title": "Get started with DeepSeek R1",
      "url": "https://ai-sdk.dev/docs/guides/r1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/r1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/r1",
        "description": "Get started with DeepSeek R1 using the AI SDK."
      }
    },
    {
      "title": "RAG Chatbot Guide",
      "url": "https://ai-sdk.dev/docs/guides/rag-chatbot",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/rag-chatbot.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/rag-chatbot",
        "description": "Learn how to build a RAG Chatbot with the AI SDK and Next.js"
      }
    },
    {
      "title": "Building a Slack AI Chatbot with the AI SDK",
      "url": "https://ai-sdk.dev/docs/guides/slackbot",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/slackbot.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/slackbot",
        "description": "Learn how to use the AI SDK to build an AI Slackbot."
      }
    },
    {
      "title": "Get started with Claude 3.7 Sonnet",
      "url": "https://ai-sdk.dev/docs/guides/sonnet-3-7",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/guides/sonnet-3-7.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/guides/sonnet-3-7",
        "description": "Get started with Claude 3.7 Sonnet using the AI SDK."
      }
    },
    {
      "title": "AI SDK",
      "url": "https://ai-sdk.dev/docs/introduction",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/introduction.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/introduction",
        "description": "The AI SDK is the TypeScript toolkit for building AI applications and agents with React, Next.js, Vue, Svelte, Node.js, and more."
      }
    },
    {
      "title": "Migration Guides",
      "url": "https://ai-sdk.dev/docs/migration-guides",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides",
        "description": "Learn how to upgrade between Vercel AI versions."
      }
    },
    {
      "title": "Migrate AI SDK 3.0 to 3.1",
      "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-3-1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides/migration-guide-3-1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-3-1",
        "description": "Learn how to upgrade AI SDK 3.0 to 3.1."
      }
    },
    {
      "title": "Migrate AI SDK 3.1 to 3.2",
      "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-3-2",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides/migration-guide-3-2.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-3-2",
        "description": "Learn how to upgrade AI SDK 3.1 to 3.2."
      }
    },
    {
      "title": "Migrate AI SDK 3.2 to 3.3",
      "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-3-3",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides/migration-guide-3-3.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-3-3",
        "description": "Learn how to upgrade AI SDK 3.2 to 3.3."
      }
    },
    {
      "title": "Migrate AI SDK 3.3 to 3.4",
      "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-3-4",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides/migration-guide-3-4.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-3-4",
        "description": "Learn how to upgrade AI SDK 3.3 to 3.4."
      }
    },
    {
      "title": "Migrate AI SDK 3.4 to 4.0",
      "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-4-0",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides/migration-guide-4-0.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-4-0",
        "description": "Learn how to upgrade AI SDK 3.4 to 4.0."
      }
    },
    {
      "title": "Migrate AI SDK 4.0 to 4.1",
      "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-4-1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides/migration-guide-4-1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-4-1",
        "description": "Learn how to upgrade AI SDK 4.0 to 4.1."
      }
    },
    {
      "title": "Migrate AI SDK 4.1 to 4.2",
      "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-4-2",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides/migration-guide-4-2.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides/migration-guide-4-2",
        "description": "Learn how to upgrade AI SDK 4.1 to 4.2."
      }
    },
    {
      "title": "Versioning",
      "url": "https://ai-sdk.dev/docs/migration-guides/versioning",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/migration-guides/versioning.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/migration-guides/versioning",
        "description": "Understand how the AI SDK approaches versioning."
      }
    },
    {
      "title": "API Reference",
      "url": "https://ai-sdk.dev/docs/reference",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference",
        "description": "Reference documentation for the AI SDK"
      }
    },
    {
      "title": "AI SDK Core",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core",
        "description": "Reference documentation for the AI SDK Core"
      }
    },
    {
      "title": "CoreMessage",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/core-message",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/core-message.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/core-message",
        "description": "Message types for AI SDK Core (API Reference)"
      }
    },
    {
      "title": "cosineSimilarity()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/cosine-similarity",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/cosine-similarity.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/cosine-similarity",
        "description": "Calculate the cosine similarity between two vectors (API Reference)"
      }
    },
    {
      "title": "createIdGenerator()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/create-id-generator",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/create-id-generator.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/create-id-generator",
        "description": "Create a customizable unique identifier generator (API Reference)"
      }
    },
    {
      "title": "experimental_createMCPClient()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/create-mcp-client",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/create-mcp-client.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/create-mcp-client",
        "description": "Create a client for connecting to MCP servers"
      }
    },
    {
      "title": "customProvider()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/custom-provider",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/custom-provider.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/custom-provider",
        "description": "Custom provider that uses models from a different provider (API Reference)"
      }
    },
    {
      "title": "defaultSettingsMiddleware()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/default-settings-middleware",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/default-settings-middleware.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/default-settings-middleware",
        "description": "Middleware that applies default settings for language models"
      }
    },
    {
      "title": "embed()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/embed",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/embed.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/embed",
        "description": "API Reference for embed."
      }
    },
    {
      "title": "embedMany()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/embed-many",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/embed-many.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/embed-many",
        "description": "API Reference for embedMany."
      }
    },
    {
      "title": "extractReasoningMiddleware()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/extract-reasoning-middleware",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/extract-reasoning-middleware.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/extract-reasoning-middleware",
        "description": "Middleware that extracts XML-tagged reasoning sections from generated text"
      }
    },
    {
      "title": "generateId()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-id",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/generate-id.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-id",
        "description": "Generate a unique identifier (API Reference)"
      }
    },
    {
      "title": "generateImage()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-image",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/generate-image.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-image",
        "description": "API Reference for generateImage."
      }
    },
    {
      "title": "generateObject()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/generate-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-object",
        "description": "API Reference for generateObject."
      }
    },
    {
      "title": "generateSpeech()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-speech",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/generate-speech.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-speech",
        "description": "API Reference for generateSpeech."
      }
    },
    {
      "title": "generateText()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/generate-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text",
        "description": "API Reference for generateText."
      }
    },
    {
      "title": "jsonSchema()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/json-schema",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/json-schema.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/json-schema",
        "description": "Helper function for creating JSON schemas"
      }
    },
    {
      "title": "LanguageModelV1Middleware",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/language-model-v1-middleware",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/language-model-v1-middleware.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/language-model-v1-middleware",
        "description": "Middleware for enhancing language model behavior (API Reference)"
      }
    },
    {
      "title": "Experimental_StdioMCPTransport",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/mcp-stdio-transport",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/mcp-stdio-transport.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/mcp-stdio-transport",
        "description": "Create a transport for Model Context Protocol (MCP) clients to communicate with MCP servers using standard input and output streams"
      }
    },
    {
      "title": "createProviderRegistry()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/provider-registry",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/provider-registry.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/provider-registry",
        "description": "Registry for managing multiple providers and models (API Reference)"
      }
    },
    {
      "title": "simulateReadableStream()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-readable-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/simulate-readable-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-readable-stream",
        "description": "Create a ReadableStream that emits values with configurable delays"
      }
    },
    {
      "title": "simulateStreamingMiddleware()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-streaming-middleware",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/simulate-streaming-middleware.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/simulate-streaming-middleware",
        "description": "Middleware that simulates streaming for non-streaming language models"
      }
    },
    {
      "title": "smoothStream()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/smooth-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/smooth-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/smooth-stream",
        "description": "Stream transformer for smoothing text output"
      }
    },
    {
      "title": "streamObject()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/stream-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-object",
        "description": "API Reference for streamObject"
      }
    },
    {
      "title": "streamText()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/stream-text.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text",
        "description": "API Reference for streamText."
      }
    },
    {
      "title": "tool()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/tool",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/tool.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/tool",
        "description": "Helper function for tool type inference"
      }
    },
    {
      "title": "transcribe()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/transcribe",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/transcribe.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/transcribe",
        "description": "API Reference for transcribe."
      }
    },
    {
      "title": "valibotSchema()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/valibot-schema",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/valibot-schema.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/valibot-schema",
        "description": "Helper function for creating Valibot schemas"
      }
    },
    {
      "title": "wrapLanguageModel()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/wrap-language-model",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/wrap-language-model.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/wrap-language-model",
        "description": "Function for wrapping a language model with middleware (API Reference)"
      }
    },
    {
      "title": "zodSchema()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/zod-schema",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-core/zod-schema.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-core/zod-schema",
        "description": "Helper function for creating Zod schemas"
      }
    },
    {
      "title": "AI SDK Errors",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors",
        "description": "Troubleshooting information for common AI SDK errors."
      }
    },
    {
      "title": "AI_APICallError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-api-call-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-api-call-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-api-call-error",
        "description": "Learn how to fix AI_APICallError"
      }
    },
    {
      "title": "AI_DownloadError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-download-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-download-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-download-error",
        "description": "Learn how to fix AI_DownloadError"
      }
    },
    {
      "title": "AI_EmptyResponseBodyError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-empty-response-body-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-empty-response-body-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-empty-response-body-error",
        "description": "Learn how to fix AI_EmptyResponseBodyError"
      }
    },
    {
      "title": "AI_InvalidArgumentError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-argument-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-invalid-argument-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-argument-error",
        "description": "Learn how to fix AI_InvalidArgumentError"
      }
    },
    {
      "title": "AI_InvalidDataContent",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-data-content",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-invalid-data-content.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-data-content",
        "description": "Learn how to fix AI_InvalidDataContent"
      }
    },
    {
      "title": "AI_InvalidDataContentError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-data-content-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-invalid-data-content-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-data-content-error",
        "description": "How to fix AI_InvalidDataContentError"
      }
    },
    {
      "title": "AI_InvalidMessageRoleError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-message-role-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-invalid-message-role-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-message-role-error",
        "description": "Learn how to fix AI_InvalidMessageRoleError"
      }
    },
    {
      "title": "AI_InvalidPromptError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-prompt-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-invalid-prompt-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-prompt-error",
        "description": "Learn how to fix AI_InvalidPromptError"
      }
    },
    {
      "title": "AI_InvalidResponseDataError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-response-data-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-invalid-response-data-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-response-data-error",
        "description": "Learn how to fix AI_InvalidResponseDataError"
      }
    },
    {
      "title": "AI_InvalidToolArgumentsError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-invalid-tool-arguments-error",
        "description": "Learn how to fix AI_InvalidToolArgumentsError"
      }
    },
    {
      "title": "AI_JSONParseError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-json-parse-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-json-parse-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-json-parse-error",
        "description": "Learn how to fix AI_JSONParseError"
      }
    },
    {
      "title": "AI_LoadAPIKeyError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-load-api-key-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-load-api-key-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-load-api-key-error",
        "description": "Learn how to fix AI_LoadAPIKeyError"
      }
    },
    {
      "title": "AI_LoadSettingError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-load-setting-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-load-setting-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-load-setting-error",
        "description": "Learn how to fix AI_LoadSettingError"
      }
    },
    {
      "title": "AI_MessageConversionError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-message-conversion-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-message-conversion-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-message-conversion-error",
        "description": "Learn how to fix AI_MessageConversionError"
      }
    },
    {
      "title": "AI_NoAudioGeneratedError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-audio-generated-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-audio-generated-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-audio-generated-error",
        "description": "Learn how to fix AI_NoAudioGeneratedError"
      }
    },
    {
      "title": "AI_NoContentGeneratedError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-content-generated-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-content-generated-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-content-generated-error",
        "description": "Learn how to fix AI_NoContentGeneratedError"
      }
    },
    {
      "title": "AI_NoImageGeneratedError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-image-generated-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-image-generated-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-image-generated-error",
        "description": "Learn how to fix AI_NoImageGeneratedError"
      }
    },
    {
      "title": "AI_NoObjectGeneratedError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-object-generated-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-object-generated-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-object-generated-error",
        "description": "Learn how to fix AI_NoObjectGeneratedError"
      }
    },
    {
      "title": "AI_NoOutputSpecifiedError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-output-specified-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-output-specified-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-output-specified-error",
        "description": "Learn how to fix AI_NoOutputSpecifiedError"
      }
    },
    {
      "title": "AI_NoSuchModelError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-model-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-such-model-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-model-error",
        "description": "Learn how to fix AI_NoSuchModelError"
      }
    },
    {
      "title": "AI_NoSuchProviderError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-provider-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-such-provider-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-provider-error",
        "description": "Learn how to fix AI_NoSuchProviderError"
      }
    },
    {
      "title": "AI_NoSuchToolError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-tool-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-such-tool-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-such-tool-error",
        "description": "Learn how to fix AI_NoSuchToolError"
      }
    },
    {
      "title": "AI_NoTranscriptGeneratedError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error",
        "description": "Learn how to fix AI_NoTranscriptGeneratedError"
      }
    },
    {
      "title": "AI_RetryError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-retry-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-retry-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-retry-error",
        "description": "Learn how to fix AI_RetryError"
      }
    },
    {
      "title": "AI_TooManyEmbeddingValuesForCallError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error",
        "description": "Learn how to fix AI_TooManyEmbeddingValuesForCallError"
      }
    },
    {
      "title": "ToolCallRepairError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-call-repair-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-tool-call-repair-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-call-repair-error",
        "description": "Learn how to fix AI SDK ToolCallRepairError"
      }
    },
    {
      "title": "AI_ToolExecutionError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-execution-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-tool-execution-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-tool-execution-error",
        "description": "Learn how to fix AI_ToolExecutionError"
      }
    },
    {
      "title": "AI_TypeValidationError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-type-validation-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-type-validation-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-type-validation-error",
        "description": "Learn how to fix AI_TypeValidationError"
      }
    },
    {
      "title": "AI_UnsupportedFunctionalityError",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error",
        "description": "Learn how to fix AI_UnsupportedFunctionalityError"
      }
    },
    {
      "title": "AI SDK RSC",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc",
        "description": "Reference documentation for the AI SDK UI"
      }
    },
    {
      "title": "createAI",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-ai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/create-ai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-ai",
        "description": "Reference for the createAI function from the AI SDK RSC"
      }
    },
    {
      "title": "createStreamableUI",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-ui",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/create-streamable-ui.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-ui",
        "description": "Reference for the createStreamableUI function from the AI SDK RSC"
      }
    },
    {
      "title": "createStreamableValue",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-value",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/create-streamable-value.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/create-streamable-value",
        "description": "Reference for the createStreamableValue function from the AI SDK RSC"
      }
    },
    {
      "title": "getAIState",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/get-ai-state",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/get-ai-state.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/get-ai-state",
        "description": "Reference for the getAIState function from the AI SDK RSC"
      }
    },
    {
      "title": "getMutableAIState",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/get-mutable-ai-state",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/get-mutable-ai-state.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/get-mutable-ai-state",
        "description": "Reference for the getMutableAIState function from the AI SDK RSC"
      }
    },
    {
      "title": "readStreamableValue",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/read-streamable-value",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/read-streamable-value.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/read-streamable-value",
        "description": "Reference for the readStreamableValue function from the AI SDK RSC"
      }
    },
    {
      "title": "render (Removed)",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/render",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/render.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/render",
        "description": "Reference for the render function from the AI SDK RSC"
      }
    },
    {
      "title": "streamUI",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/stream-ui",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/stream-ui.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/stream-ui",
        "description": "Reference for the streamUI function from the AI SDK RSC"
      }
    },
    {
      "title": "useActions",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-actions",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/use-actions.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-actions",
        "description": "Reference for the useActions function from the AI SDK RSC"
      }
    },
    {
      "title": "useAIState",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-ai-state",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/use-ai-state.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-ai-state",
        "description": "Reference for the useAIState function from the AI SDK RSC"
      }
    },
    {
      "title": "useStreamableValue",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-streamable-value",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/use-streamable-value.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-streamable-value",
        "description": "Reference for the useStreamableValue function from the AI SDK RSC"
      }
    },
    {
      "title": "useUIState",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-ui-state",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-rsc/use-ui-state.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-rsc/use-ui-state",
        "description": "Reference for the useUIState function from the AI SDK RSC"
      }
    },
    {
      "title": "AI SDK UI",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui",
        "description": "Reference documentation for the AI SDK UI"
      }
    },
    {
      "title": "appendClientMessage()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/append-client-message",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/append-client-message.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/append-client-message",
        "description": "Appends or updates a client Message to an existing array of UI messages for useChat (API Reference)"
      }
    },
    {
      "title": "appendResponseMessages()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/append-response-messages",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/append-response-messages.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/append-response-messages",
        "description": "Appends ResponseMessage[] from an AI response to an existing array of UI messages, generating timestamps and reusing IDs for useChat (API Reference)"
      }
    },
    {
      "title": "AssistantResponse",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/assistant-response",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/assistant-response.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/assistant-response",
        "description": "API reference for the AssistantResponse streaming helper."
      }
    },
    {
      "title": "convertToCoreMessages()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/convert-to-core-messages",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/convert-to-core-messages.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/convert-to-core-messages",
        "description": "Convert useChat messages to CoreMessages for AI core functions (API Reference)"
      }
    },
    {
      "title": "createDataStream",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/create-data-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/create-data-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/create-data-stream",
        "description": "Learn to use createDataStream helper function to stream additional data in your application."
      }
    },
    {
      "title": "createDataStreamResponse",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/create-data-stream-response",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/create-data-stream-response.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/create-data-stream-response",
        "description": "Learn to use createDataStreamResponse helper function to create a Response object with streaming data."
      }
    },
    {
      "title": "pipeDataStreamToResponse",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/pipe-data-stream-to-response",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/pipe-data-stream-to-response.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/pipe-data-stream-to-response",
        "description": "Learn to use pipeDataStreamToResponse helper function to pipe streaming data to a ServerResponse object."
      }
    },
    {
      "title": "StreamData",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/stream-data",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/stream-data.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/stream-data",
        "description": "Learn to use streamData helper function in your application."
      }
    },
    {
      "title": "useAssistant()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-assistant",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/use-assistant.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-assistant",
        "description": "API reference for the useAssistant hook."
      }
    },
    {
      "title": "useChat()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/use-chat.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat",
        "description": "API reference for the useChat hook."
      }
    },
    {
      "title": "useCompletion()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-completion",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/use-completion.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-completion",
        "description": "API reference for the useCompletion hook."
      }
    },
    {
      "title": "experimental_useObject()",
      "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/ai-sdk-ui/use-object.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-object",
        "description": "API reference for the useObject hook."
      }
    },
    {
      "title": "Reference: Stream Helpers",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers",
        "description": "Learn to use help functions that help stream generations from different providers."
      }
    },
    {
      "title": "AIStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/ai-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/ai-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/ai-stream",
        "description": "Learn to use AIStream helper function in your application."
      }
    },
    {
      "title": "AnthropicStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/anthropic-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/anthropic-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/anthropic-stream",
        "description": "Learn to use AnthropicStream helper function in your application."
      }
    },
    {
      "title": "AWSBedrockAnthropicStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-anthropic-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/aws-bedrock-anthropic-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-anthropic-stream",
        "description": "Learn to use AWSBedrockAnthropicStream helper function in your application."
      }
    },
    {
      "title": "AWSBedrockCohereStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-cohere-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/aws-bedrock-cohere-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-cohere-stream",
        "description": "Learn to use AWSBedrockCohereStream helper function in your application."
      }
    },
    {
      "title": "AWSBedrockLlama2Stream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-llama-2-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/aws-bedrock-llama-2-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-llama-2-stream",
        "description": "Learn to use AWSBedrockLlama2Stream helper function in your application."
      }
    },
    {
      "title": "AWSBedrockAnthropicMessagesStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-messages-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/aws-bedrock-messages-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-messages-stream",
        "description": "Learn to use AWSBedrockAnthropicMessagesStream helper function in your application."
      }
    },
    {
      "title": "AWSBedrockStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/aws-bedrock-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/aws-bedrock-stream",
        "description": "Learn to use AWSBedrockStream helper function in your application."
      }
    },
    {
      "title": "CohereStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/cohere-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/cohere-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/cohere-stream",
        "description": "Learn to use CohereStream helper function in your application."
      }
    },
    {
      "title": "GoogleGenerativeAIStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/google-generative-ai-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/google-generative-ai-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/google-generative-ai-stream",
        "description": "Learn to use GoogleGenerativeAIStream helper function in your application."
      }
    },
    {
      "title": "HuggingFaceStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/hugging-face-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/hugging-face-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/hugging-face-stream",
        "description": "Learn to use HuggingFaceStream helper function in your application."
      }
    },
    {
      "title": "InkeepStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/inkeep-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/inkeep-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/inkeep-stream",
        "description": "Learn to use InkeepStream helper function in your application."
      }
    },
    {
      "title": "LangChainAdapter",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/langchain-adapter",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/langchain-adapter.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/langchain-adapter",
        "description": "API Reference for LangChainAdapter."
      }
    },
    {
      "title": "LangChainStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/langchain-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/langchain-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/langchain-stream",
        "description": "API Reference for LangChainStream."
      }
    },
    {
      "title": "LlamaIndexAdapter",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/llamaindex-adapter",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/llamaindex-adapter.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/llamaindex-adapter",
        "description": "API Reference for LlamaIndexAdapter."
      }
    },
    {
      "title": "MistralStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/mistral-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/mistral-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/mistral-stream",
        "description": "Learn to use MistralStream helper function in your application."
      }
    },
    {
      "title": "OpenAIStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/openai-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/openai-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/openai-stream",
        "description": "Learn to use OpenAIStream helper function in your application."
      }
    },
    {
      "title": "ReplicateStream",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/replicate-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/replicate-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/replicate-stream",
        "description": "Learn to use ReplicateStream helper function in your application."
      }
    },
    {
      "title": "streamToResponse",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/stream-to-response",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/stream-to-response.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/stream-to-response",
        "description": "Learn to use streamToResponse helper function in your application."
      }
    },
    {
      "title": "StreamingTextResponse",
      "url": "https://ai-sdk.dev/docs/reference/stream-helpers/streaming-text-response",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/reference/stream-helpers/streaming-text-response.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/reference/stream-helpers/streaming-text-response",
        "description": "Learn to use StreamingTextResponse helper function in your application."
      }
    },
    {
      "title": "Troubleshooting",
      "url": "https://ai-sdk.dev/docs/troubleshooting",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting",
        "description": "Troubleshooting information for common issues encountered with the AI SDK."
      }
    },
    {
      "title": "Azure OpenAI Slow To Stream",
      "url": "https://ai-sdk.dev/docs/troubleshooting/azure-stream-slow",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/azure-stream-slow.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/azure-stream-slow",
        "description": "Learn to troubleshoot Azure OpenAI slow to stream issues."
      }
    },
    {
      "title": "Client-Side Function Calls Not Invoked",
      "url": "https://ai-sdk.dev/docs/troubleshooting/client-side-function-calls-not-invoked",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/client-side-function-calls-not-invoked.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/client-side-function-calls-not-invoked",
        "description": "Troubleshooting client-side function calls not being invoked."
      }
    },
    {
      "title": "\"Only plain objects can be passed from client components\" Server Action Error",
      "url": "https://ai-sdk.dev/docs/troubleshooting/client-stream-error",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/client-stream-error.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/client-stream-error",
        "description": "Troubleshooting errors related to using AI SDK Core functions with Server Actions."
      }
    },
    {
      "title": "Jest: cannot find module 'ai/rsc'",
      "url": "https://ai-sdk.dev/docs/troubleshooting/jest-cannot-find-module-ai-rsc",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/jest-cannot-find-module-ai-rsc.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/jest-cannot-find-module-ai-rsc",
        "description": "Troubleshooting AI SDK errors related to the Jest: cannot find module 'ai/rsc' error"
      }
    },
    {
      "title": "Model is not assignable to type \"LanguageModelV1\"",
      "url": "https://ai-sdk.dev/docs/troubleshooting/model-is-not-assignable-to-type",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/model-is-not-assignable-to-type.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/model-is-not-assignable-to-type",
        "description": "Troubleshooting errors related to incompatible models."
      }
    },
    {
      "title": "NaN token counts when using streamText with OpenAI models",
      "url": "https://ai-sdk.dev/docs/troubleshooting/nan-token-counts-openai-streaming",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/nan-token-counts-openai-streaming.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/nan-token-counts-openai-streaming",
        "description": "Troubleshooting errors related to NaN token counts in OpenAI streaming."
      }
    },
    {
      "title": "React error \"Maximum update depth exceeded\"",
      "url": "https://ai-sdk.dev/docs/troubleshooting/react-maximum-update-depth-exceeded",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/react-maximum-update-depth-exceeded.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/react-maximum-update-depth-exceeded",
        "description": "Troubleshooting errors related to the \"Maximum update depth exceeded\" error."
      }
    },
    {
      "title": "Server Actions in Client Components",
      "url": "https://ai-sdk.dev/docs/troubleshooting/server-actions-in-client-components",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/server-actions-in-client-components.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/server-actions-in-client-components",
        "description": "Troubleshooting errors related to server actions in client components."
      }
    },
    {
      "title": "useChat/useCompletion stream output contains 0:... instead of text",
      "url": "https://ai-sdk.dev/docs/troubleshooting/strange-stream-output",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/strange-stream-output.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/strange-stream-output",
        "description": "How to fix strange stream output in the UI"
      }
    },
    {
      "title": "streamText is not working",
      "url": "https://ai-sdk.dev/docs/troubleshooting/stream-text-not-working",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/stream-text-not-working.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/stream-text-not-working",
        "description": "Troubleshooting errors related to the streamText function not working."
      }
    },
    {
      "title": "Streamable UI Component Error",
      "url": "https://ai-sdk.dev/docs/troubleshooting/streamable-ui-errors",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/streamable-ui-errors.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/streamable-ui-errors",
        "description": "Troubleshooting errors related to streamable UI."
      }
    },
    {
      "title": "Streaming Not Working When Deployed",
      "url": "https://ai-sdk.dev/docs/troubleshooting/streaming-not-working-when-deployed",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/streaming-not-working-when-deployed.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/streaming-not-working-when-deployed",
        "description": "Troubleshooting streaming issues in deployed apps."
      }
    },
    {
      "title": "Streaming Not Working When Proxied",
      "url": "https://ai-sdk.dev/docs/troubleshooting/streaming-not-working-when-proxied",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/streaming-not-working-when-proxied.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/streaming-not-working-when-proxied",
        "description": "Troubleshooting streaming issues in proxied apps."
      }
    },
    {
      "title": "Getting Timeouts When Deploying on Vercel",
      "url": "https://ai-sdk.dev/docs/troubleshooting/timeout-on-vercel",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/timeout-on-vercel.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/timeout-on-vercel",
        "description": "Learn how to fix timeouts and cut off responses when deploying to Vercel."
      }
    },
    {
      "title": "Tool Invocation Missing Result Error",
      "url": "https://ai-sdk.dev/docs/troubleshooting/tool-invocation-missing-result",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/tool-invocation-missing-result.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/tool-invocation-missing-result",
        "description": "How to fix the \"ToolInvocation must have a result\" error when using tools without execute functions"
      }
    },
    {
      "title": "TypeScript error \"Cannot find namespace 'JSX'\"",
      "url": "https://ai-sdk.dev/docs/troubleshooting/typescript-cannot-find-namespace-jsx",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/typescript-cannot-find-namespace-jsx.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/typescript-cannot-find-namespace-jsx",
        "description": "Troubleshooting errors related to TypeScript and JSX."
      }
    },
    {
      "title": "Unclosed Streams",
      "url": "https://ai-sdk.dev/docs/troubleshooting/unclosed-streams",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/unclosed-streams.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/unclosed-streams",
        "description": "Troubleshooting errors related to unclosed streams."
      }
    },
    {
      "title": "useChat \"An error occurred\"",
      "url": "https://ai-sdk.dev/docs/troubleshooting/use-chat-an-error-occurred",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/use-chat-an-error-occurred.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/use-chat-an-error-occurred",
        "description": "Troubleshooting errors related to the \"An error occurred\" error in useChat."
      }
    },
    {
      "title": "useChat \"Failed to Parse Stream String\" Error",
      "url": "https://ai-sdk.dev/docs/troubleshooting/use-chat-failed-to-parse-stream",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/use-chat-failed-to-parse-stream.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/use-chat-failed-to-parse-stream",
        "description": "Troubleshooting errors related to the Use Chat Failed to Parse Stream error."
      }
    },
    {
      "title": "useChat No Response with maxSteps",
      "url": "https://ai-sdk.dev/docs/troubleshooting/use-chat-tools-no-response",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/docs/troubleshooting/use-chat-tools-no-response.md",
      "metadata": {
        "url": "https://ai-sdk.dev/docs/troubleshooting/use-chat-tools-no-response",
        "description": "Troubleshooting errors related to the Use Chat Failed to Parse Stream error."
      }
    },
    {
      "title": "Getting Started with the AI SDK",
      "url": "https://ai-sdk.dev/getting-started",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/getting-started.md",
      "metadata": {
        "url": "https://ai-sdk.dev/getting-started",
        "description": "The AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications with React, Next.js, Vue, Svelte, Node.js, and more."
      }
    },
    {
      "title": "AI Playground | Compare top AI models side-by-side",
      "url": "https://ai-sdk.dev/playground",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground",
        "description": "Chat and compare OpenAI GPT, Anthropic Claude, Google Gemini, Llama, Mistral, and more."
      }
    },
    {
      "title": "Claude 3.5 Haiku by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-3.5-haiku",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-3.5-haiku.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-3.5-haiku",
        "description": "Test and compare Claude 3.5 Haiku by Anthropic"
      }
    },
    {
      "title": "Claude 3.7 Sonnet by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-3.7-sonnet",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-3.7-sonnet.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-3.7-sonnet",
        "description": "Test and compare Claude 3.7 Sonnet by Anthropic"
      }
    },
    {
      "title": "Claude 3.7 Sonnet Reasoning by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-3.7-sonnet-reasoning",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-3.7-sonnet-reasoning.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-3.7-sonnet-reasoning",
        "description": "Test and compare Claude 3.7 Sonnet Reasoning by Anthropic"
      }
    },
    {
      "title": "Claude 4 Opus by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-4-opus-20250514",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-4-opus-20250514.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-4-opus-20250514",
        "description": "Test and compare Claude 4 Opus by Anthropic"
      }
    },
    {
      "title": "Claude 4 Sonnet by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-4-sonnet-20250514",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-4-sonnet-20250514.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-4-sonnet-20250514",
        "description": "Test and compare Claude 4 Sonnet by Anthropic"
      }
    },
    {
      "title": "Claude 2 by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-v2",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-v2.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-v2",
        "description": "Test and compare Claude 2 by Anthropic"
      }
    },
    {
      "title": "Claude 3 Haiku by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-v3-haiku",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-v3-haiku.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-v3-haiku",
        "description": "Test and compare Claude 3 Haiku by Anthropic"
      }
    },
    {
      "title": "Claude 3 Opus by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-v3-opus",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-v3-opus.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-v3-opus",
        "description": "Test and compare Claude 3 Opus by Anthropic"
      }
    },
    {
      "title": "Claude 3 Sonnet by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-v3-sonnet",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-v3-sonnet.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-v3-sonnet",
        "description": "Test and compare Claude 3 Sonnet by Anthropic"
      }
    },
    {
      "title": "Claude 3.5 Sonnet by Anthropic on the AI Playground",
      "url": "https://ai-sdk.dev/playground/anthropic:claude-v3.5-sonnet",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/anthropic_claude-v3.5-sonnet.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/anthropic:claude-v3.5-sonnet",
        "description": "Test and compare Claude 3.5 Sonnet by Anthropic"
      }
    },
    {
      "title": "Nova Lite by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:amazon.nova-lite-v1:0",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_amazon.nova-lite-v1_0.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:amazon.nova-lite-v1:0",
        "description": "Test and compare Nova Lite by Amazon"
      }
    },
    {
      "title": "Nova Micro by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:amazon.nova-micro-v1:0",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_amazon.nova-micro-v1_0.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:amazon.nova-micro-v1:0",
        "description": "Test and compare Nova Micro by Amazon"
      }
    },
    {
      "title": "Nova Pro by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:amazon.nova-pro-v1:0",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_amazon.nova-pro-v1_0.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:amazon.nova-pro-v1:0",
        "description": "Test and compare Nova Pro by Amazon"
      }
    },
    {
      "title": "Claude 3.5 Haiku (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:claude-3-5-haiku-20241022",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_claude-3-5-haiku-20241022.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:claude-3-5-haiku-20241022",
        "description": "Test and compare Claude 3.5 Haiku (Bedrock) by Amazon"
      }
    },
    {
      "title": "Claude 3.5 Sonnet (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:claude-3-5-sonnet-20240620-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_claude-3-5-sonnet-20240620-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:claude-3-5-sonnet-20240620-v1",
        "description": "Test and compare Claude 3.5 Sonnet (Bedrock) by Amazon"
      }
    },
    {
      "title": "Claude 3.5 Sonnet v2 (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:claude-3-5-sonnet-20241022-v2",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_claude-3-5-sonnet-20241022-v2.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:claude-3-5-sonnet-20241022-v2",
        "description": "Test and compare Claude 3.5 Sonnet v2 (Bedrock) by Amazon"
      }
    },
    {
      "title": "Claude 3.7 Sonnet (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:claude-3-7-sonnet-20250219",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_claude-3-7-sonnet-20250219.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:claude-3-7-sonnet-20250219",
        "description": "Test and compare Claude 3.7 Sonnet (Bedrock) by Amazon"
      }
    },
    {
      "title": "Claude 3 Haiku (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:claude-3-haiku-20240307-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_claude-3-haiku-20240307-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:claude-3-haiku-20240307-v1",
        "description": "Test and compare Claude 3 Haiku (Bedrock) by Amazon"
      }
    },
    {
      "title": "Claude 4 Opus (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:claude-4-opus-20250514-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_claude-4-opus-20250514-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:claude-4-opus-20250514-v1",
        "description": "Test and compare Claude 4 Opus (Bedrock) by Amazon"
      }
    },
    {
      "title": "Claude 4 Sonnet (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:claude-4-sonnet-20250514-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_claude-4-sonnet-20250514-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:claude-4-sonnet-20250514-v1",
        "description": "Test and compare Claude 4 Sonnet (Bedrock) by Amazon"
      }
    },
    {
      "title": "DeepSeek-R1 (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:deepseek.r1-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_deepseek.r1-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:deepseek.r1-v1",
        "description": "Test and compare DeepSeek-R1 (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 3.1 70B Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-1-70b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama3-1-70b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-1-70b-instruct-v1",
        "description": "Test and compare Llama 3.1 70B Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 3.1 8B Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-1-8b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama3-1-8b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-1-8b-instruct-v1",
        "description": "Test and compare Llama 3.1 8B Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 3.2 11B Vision Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-2-11b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama3-2-11b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-2-11b-instruct-v1",
        "description": "Test and compare Llama 3.2 11B Vision Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 3.2 1B Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-2-1b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama3-2-1b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-2-1b-instruct-v1",
        "description": "Test and compare Llama 3.2 1B Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 3.2 3B Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-2-3b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama3-2-3b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-2-3b-instruct-v1",
        "description": "Test and compare Llama 3.2 3B Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 3.2 90B Vision Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-2-90b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama3-2-90b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-2-90b-instruct-v1",
        "description": "Test and compare Llama 3.2 90B Vision Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 3.3 70B Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-3-70b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama3-3-70b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama3-3-70b-instruct-v1",
        "description": "Test and compare Llama 3.3 70B Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 4 Maverick 17B Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama4-maverick-17b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama4-maverick-17b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama4-maverick-17b-instruct-v1",
        "description": "Test and compare Llama 4 Maverick 17B Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "Llama 4 Scout 17B Instruct (Bedrock) by Amazon on the AI Playground",
      "url": "https://ai-sdk.dev/playground/bedrock:meta.llama4-scout-17b-instruct-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/bedrock_meta.llama4-scout-17b-instruct-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/bedrock:meta.llama4-scout-17b-instruct-v1",
        "description": "Test and compare Llama 4 Scout 17B Instruct (Bedrock) by Amazon"
      }
    },
    {
      "title": "DeepSeek R1 Distill Llama 70B by Cerebras on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cerebras:deepseek-r1-distill-llama-70b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cerebras_deepseek-r1-distill-llama-70b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cerebras:deepseek-r1-distill-llama-70b",
        "description": "Test and compare DeepSeek R1 Distill Llama 70B by Cerebras"
      }
    },
    {
      "title": "Llama 3.3 70B by Cerebras on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cerebras:llama-3.3-70b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cerebras_llama-3.3-70b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cerebras:llama-3.3-70b",
        "description": "Test and compare Llama 3.3 70B by Cerebras"
      }
    },
    {
      "title": "Llama 4 Scout by Cerebras on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cerebras:llama-4-scout-17b-16e-instruct",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cerebras_llama-4-scout-17b-16e-instruct.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cerebras:llama-4-scout-17b-16e-instruct",
        "description": "Test and compare Llama 4 Scout by Cerebras"
      }
    },
    {
      "title": "Llama 3.1 8B by Cerebras on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cerebras:llama3.1-8b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cerebras_llama3.1-8b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cerebras:llama3.1-8b",
        "description": "Test and compare Llama 3.1 8B by Cerebras"
      }
    },
    {
      "title": "Qwen 3.32B by Cerebras on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cerebras:qwen-3-32b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cerebras_qwen-3-32b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cerebras:qwen-3-32b",
        "description": "Test and compare Qwen 3.32B by Cerebras"
      }
    },
    {
      "title": "Command A by Cohere on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cohere:command-a",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cohere_command-a.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cohere:command-a",
        "description": "Test and compare Command A by Cohere"
      }
    },
    {
      "title": "Command Light Nightly by Cohere on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cohere:command-light-nightly",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cohere_command-light-nightly.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cohere:command-light-nightly",
        "description": "Test and compare Command Light Nightly by Cohere"
      }
    },
    {
      "title": "Command Nightly by Cohere on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cohere:command-nightly",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cohere_command-nightly.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cohere:command-nightly",
        "description": "Test and compare Command Nightly by Cohere"
      }
    },
    {
      "title": "Command R by Cohere on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cohere:command-r",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cohere_command-r.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cohere:command-r",
        "description": "Test and compare Command R by Cohere"
      }
    },
    {
      "title": "Command R+ by Cohere on the AI Playground",
      "url": "https://ai-sdk.dev/playground/cohere:command-r-plus",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/cohere_command-r-plus.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/cohere:command-r-plus",
        "description": "Test and compare Command R+ by Cohere"
      }
    },
    {
      "title": "Llama 4 Maverick 17B 128E Instruct FP8 by DeepInfra on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepinfra:llama-4-maverick-17b-128e-instruct-fp8",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepinfra_llama-4-maverick-17b-128e-instruct-fp8.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepinfra:llama-4-maverick-17b-128e-instruct-fp8",
        "description": "Test and compare Llama 4 Maverick 17B 128E Instruct FP8 by DeepInfra"
      }
    },
    {
      "title": "Llama 4 Scout 17B 16E Instruct by DeepInfra on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepinfra:llama-4-scout-17b-16e-instruct",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepinfra_llama-4-scout-17b-16e-instruct.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepinfra:llama-4-scout-17b-16e-instruct",
        "description": "Test and compare Llama 4 Scout 17B 16E Instruct by DeepInfra"
      }
    },
    {
      "title": "Qwen3-14B by DeepInfra on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepinfra:qwen3-14b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepinfra_qwen3-14b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepinfra:qwen3-14b",
        "description": "Test and compare Qwen3-14B by DeepInfra"
      }
    },
    {
      "title": "Qwen3-235B-A22B by DeepInfra on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepinfra:qwen3-235b-a22b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepinfra_qwen3-235b-a22b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepinfra:qwen3-235b-a22b",
        "description": "Test and compare Qwen3-235B-A22B by DeepInfra"
      }
    },
    {
      "title": "Qwen3-30B-A3B by DeepInfra on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepinfra:qwen3-30b-a3b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepinfra_qwen3-30b-a3b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepinfra:qwen3-30b-a3b",
        "description": "Test and compare Qwen3-30B-A3B by DeepInfra"
      }
    },
    {
      "title": "Qwen3-32B by DeepInfra on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepinfra:qwen3-32b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepinfra_qwen3-32b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepinfra:qwen3-32b",
        "description": "Test and compare Qwen3-32B by DeepInfra"
      }
    },
    {
      "title": "DeepSeek-V3 by DeepSeek on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepseek:chat",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepseek_chat.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepseek:chat",
        "description": "Test and compare DeepSeek-V3 by DeepSeek"
      }
    },
    {
      "title": "DeepSeek R1 by DeepSeek on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepseek:deepseek-r1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepseek_deepseek-r1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepseek:deepseek-r1",
        "description": "Test and compare DeepSeek R1 by DeepSeek"
      }
    },
    {
      "title": "DeepSeek R1 0528 by DeepSeek on the AI Playground",
      "url": "https://ai-sdk.dev/playground/deepseek:deepseek-r1-0528",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/deepseek_deepseek-r1-0528.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/deepseek:deepseek-r1-0528",
        "description": "Test and compare DeepSeek R1 0528 by DeepSeek"
      }
    },
    {
      "title": "DeepSeek R1 by Fireworks on the AI Playground",
      "url": "https://ai-sdk.dev/playground/fireworks:deepseek-r1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/fireworks_deepseek-r1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/fireworks:deepseek-r1",
        "description": "Test and compare DeepSeek R1 by Fireworks"
      }
    },
    {
      "title": "DeepSeek-V3 by Fireworks on the AI Playground",
      "url": "https://ai-sdk.dev/playground/fireworks:deepseek-v3",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/fireworks_deepseek-v3.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/fireworks:deepseek-v3",
        "description": "Test and compare DeepSeek-V3 by Fireworks"
      }
    },
    {
      "title": "FireFunction V1 by Fireworks on the AI Playground",
      "url": "https://ai-sdk.dev/playground/fireworks:firefunction-v1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/fireworks_firefunction-v1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/fireworks:firefunction-v1",
        "description": "Test and compare FireFunction V1 by Fireworks"
      }
    },
    {
      "title": "Mixtral MoE 8x22B Instruct by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/fireworks:mixtral-8x22b-instruct",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/fireworks_mixtral-8x22b-instruct.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/fireworks:mixtral-8x22b-instruct",
        "description": "Test and compare Mixtral MoE 8x22B Instruct by Mistral"
      }
    },
    {
      "title": "Mixtral MoE 8x7B Instruct by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/fireworks:mixtral-8x7b-instruct",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/fireworks_mixtral-8x7b-instruct.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/fireworks:mixtral-8x7b-instruct",
        "description": "Test and compare Mixtral MoE 8x7B Instruct by Mistral"
      }
    },
    {
      "title": "Qwen3-235B-A22B by Fireworks on the AI Playground",
      "url": "https://ai-sdk.dev/playground/fireworks:qwen3-235b-a22b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/fireworks_qwen3-235b-a22b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/fireworks:qwen3-235b-a22b",
        "description": "Test and compare Qwen3-235B-A22B by Fireworks"
      }
    },
    {
      "title": "QwQ-32B by Fireworks on the AI Playground",
      "url": "https://ai-sdk.dev/playground/fireworks:qwq-32b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/fireworks_qwq-32b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/fireworks:qwq-32b",
        "description": "Test and compare QwQ-32B by Fireworks"
      }
    },
    {
      "title": "Gemini 1.5 Flash 001 by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-1.5-flash",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-1.5-flash.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-1.5-flash",
        "description": "Test and compare Gemini 1.5 Flash 001 by Google"
      }
    },
    {
      "title": "Gemini 1.5 Flash 002 by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-1.5-flash-002",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-1.5-flash-002.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-1.5-flash-002",
        "description": "Test and compare Gemini 1.5 Flash 002 by Google"
      }
    },
    {
      "title": "Gemini 1.5 Flash 8b by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-1.5-flash-8b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-1.5-flash-8b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-1.5-flash-8b",
        "description": "Test and compare Gemini 1.5 Flash 8b by Google"
      }
    },
    {
      "title": "Gemini 1.5 Pro 001 by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-1.5-pro",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-1.5-pro.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-1.5-pro",
        "description": "Test and compare Gemini 1.5 Pro 001 by Google"
      }
    },
    {
      "title": "Gemini 1.5 Pro 002 by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-1.5-pro-002",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-1.5-pro-002.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-1.5-pro-002",
        "description": "Test and compare Gemini 1.5 Pro 002 by Google"
      }
    },
    {
      "title": "Gemini 2.0 Flash by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-2.0-flash-001",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-2.0-flash-001.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-2.0-flash-001",
        "description": "Test and compare Gemini 2.0 Flash by Google"
      }
    },
    {
      "title": "Gemini 2.0 Flash Lite Preview by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-2.0-flash-lite-preview-02-05",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-2.0-flash-lite-preview-02-05.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-2.0-flash-lite-preview-02-05",
        "description": "Test and compare Gemini 2.0 Flash Lite Preview by Google"
      }
    },
    {
      "title": "Gemini 2.5 Flash Preview by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-2.5-flash-preview-04-17",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-2.5-flash-preview-04-17.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-2.5-flash-preview-04-17",
        "description": "Test and compare Gemini 2.5 Flash Preview by Google"
      }
    },
    {
      "title": "Gemini 2.5 Pro Preview by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemini-2.5-pro-preview-03-25",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemini-2.5-pro-preview-03-25.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemini-2.5-pro-preview-03-25",
        "description": "Test and compare Gemini 2.5 Pro Preview by Google"
      }
    },
    {
      "title": "Gemma 3 27B by Google on the AI Playground",
      "url": "https://ai-sdk.dev/playground/google:gemma-3-27b-it",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/google_gemma-3-27b-it.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/google:gemma-3-27b-it",
        "description": "Test and compare Gemma 3 27B by Google"
      }
    },
    {
      "title": "DeepSeek R1 Distill Llama 70B by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:deepseek-r1-distill-llama-70b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_deepseek-r1-distill-llama-70b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:deepseek-r1-distill-llama-70b",
        "description": "Test and compare DeepSeek R1 Distill Llama 70B by Groq"
      }
    },
    {
      "title": "Gemma 2 9B IT by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:gemma2-9b-it",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_gemma2-9b-it.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:gemma2-9b-it",
        "description": "Test and compare Gemma 2 9B IT by Groq"
      }
    },
    {
      "title": "Llama 3 70B Instruct by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-3-70b-instruct",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-3-70b-instruct.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-3-70b-instruct",
        "description": "Test and compare Llama 3 70B Instruct by Groq"
      }
    },
    {
      "title": "Llama 3 8B Instruct by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-3-8b-instruct",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-3-8b-instruct.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-3-8b-instruct",
        "description": "Test and compare Llama 3 8B Instruct by Groq"
      }
    },
    {
      "title": "Llama 3.1 8B Instant by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-3.1-8b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-3.1-8b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-3.1-8b",
        "description": "Test and compare Llama 3.1 8B Instant by Groq"
      }
    },
    {
      "title": "Llama 3.2 11B by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-3.2-11b-vision-preview",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-3.2-11b-vision-preview.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-3.2-11b-vision-preview",
        "description": "Test and compare Llama 3.2 11B by Groq"
      }
    },
    {
      "title": "Llama 3.2 1B by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-3.2-1b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-3.2-1b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-3.2-1b",
        "description": "Test and compare Llama 3.2 1B by Groq"
      }
    },
    {
      "title": "Llama 3.2 3B by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-3.2-3b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-3.2-3b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-3.2-3b",
        "description": "Test and compare Llama 3.2 3B by Groq"
      }
    },
    {
      "title": "Llama 3.2 90B by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-3.2-90b-vision-preview",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-3.2-90b-vision-preview.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-3.2-90b-vision-preview",
        "description": "Test and compare Llama 3.2 90B by Groq"
      }
    },
    {
      "title": "Llama 3.3 70B Versatile by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-3.3-70b-versatile",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-3.3-70b-versatile.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-3.3-70b-versatile",
        "description": "Test and compare Llama 3.3 70B Versatile by Groq"
      }
    },
    {
      "title": "Llama 4 Scout 17B 16E Instruct by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:llama-4-scout-17b-16e-instruct",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_llama-4-scout-17b-16e-instruct.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:llama-4-scout-17b-16e-instruct",
        "description": "Test and compare Llama 4 Scout 17B 16E Instruct by Groq"
      }
    },
    {
      "title": "Mistral Saba 24B by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:mistral-saba-24b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_mistral-saba-24b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:mistral-saba-24b",
        "description": "Test and compare Mistral Saba 24B by Groq"
      }
    },
    {
      "title": "QWQ-32B by Groq on the AI Playground",
      "url": "https://ai-sdk.dev/playground/groq:qwen-qwq-32b",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/groq_qwen-qwq-32b.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/groq:qwen-qwq-32b",
        "description": "Test and compare QWQ-32B by Groq"
      }
    },
    {
      "title": "Mercury Coder Small Beta by Inception on the AI Playground",
      "url": "https://ai-sdk.dev/playground/inception:mercury-coder-small",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/inception_mercury-coder-small.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/inception:mercury-coder-small",
        "description": "Test and compare Mercury Coder Small Beta by Inception"
      }
    },
    {
      "title": "Mistral Codestral 25.01 by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/mistral:codestral-2501",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/mistral_codestral-2501.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/mistral:codestral-2501",
        "description": "Test and compare Mistral Codestral 25.01 by Mistral"
      }
    },
    {
      "title": "Ministral 3B by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/mistral:ministral-3b-latest",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/mistral_ministral-3b-latest.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/mistral:ministral-3b-latest",
        "description": "Test and compare Ministral 3B by Mistral"
      }
    },
    {
      "title": "Ministral 8B by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/mistral:ministral-8b-latest",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/mistral_ministral-8b-latest.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/mistral:ministral-8b-latest",
        "description": "Test and compare Ministral 8B by Mistral"
      }
    },
    {
      "title": "Mistral Large by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/mistral:mistral-large",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/mistral_mistral-large.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/mistral:mistral-large",
        "description": "Test and compare Mistral Large by Mistral"
      }
    },
    {
      "title": "Mistral Small by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/mistral:mistral-small",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/mistral_mistral-small.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/mistral:mistral-small",
        "description": "Test and compare Mistral Small by Mistral"
      }
    },
    {
      "title": "Mistral Small 2503 by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/mistral:mistral-small-2503",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/mistral_mistral-small-2503.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/mistral:mistral-small-2503",
        "description": "Test and compare Mistral Small 2503 by Mistral"
      }
    },
    {
      "title": "Pixtral 12B 2409 by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/mistral:pixtral-12b-2409",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/mistral_pixtral-12b-2409.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/mistral:pixtral-12b-2409",
        "description": "Test and compare Pixtral 12B 2409 by Mistral"
      }
    },
    {
      "title": "Pixtral Large by Mistral on the AI Playground",
      "url": "https://ai-sdk.dev/playground/mistral:pixtral-large-latest",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/mistral_pixtral-large-latest.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/mistral:pixtral-large-latest",
        "description": "Test and compare Pixtral Large by Mistral"
      }
    },
    {
      "title": "GPT-3.5 Turbo by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-3.5-turbo",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-3.5-turbo.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-3.5-turbo",
        "description": "Test and compare GPT-3.5 Turbo by OpenAI"
      }
    },
    {
      "title": "GPT-3.5 Turbo Instruct by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-3.5-turbo-instruct",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-3.5-turbo-instruct.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-3.5-turbo-instruct",
        "description": "Test and compare GPT-3.5 Turbo Instruct by OpenAI"
      }
    },
    {
      "title": "GPT-4 Turbo by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-4-turbo",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-4-turbo.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-4-turbo",
        "description": "Test and compare GPT-4 Turbo by OpenAI"
      }
    },
    {
      "title": "GPT-4.1 by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-4.1",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-4.1.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-4.1",
        "description": "Test and compare GPT-4.1 by OpenAI"
      }
    },
    {
      "title": "GPT-4.1 mini by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-4.1-mini",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-4.1-mini.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-4.1-mini",
        "description": "Test and compare GPT-4.1 mini by OpenAI"
      }
    },
    {
      "title": "GPT-4.1 nano by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-4.1-nano",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-4.1-nano.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-4.1-nano",
        "description": "Test and compare GPT-4.1 nano by OpenAI"
      }
    },
    {
      "title": "GPT-4.5 Preview by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-4.5-preview",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-4.5-preview.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-4.5-preview",
        "description": "Test and compare GPT-4.5 Preview by OpenAI"
      }
    },
    {
      "title": "GPT-4o by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-4o",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-4o.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-4o",
        "description": "Test and compare GPT-4o by OpenAI"
      }
    },
    {
      "title": "GPT-4o mini by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:gpt-4o-mini",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_gpt-4o-mini.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:gpt-4o-mini",
        "description": "Test and compare GPT-4o mini by OpenAI"
      }
    },
    {
      "title": "o3 by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:o3",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_o3.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:o3",
        "description": "Test and compare o3 by OpenAI"
      }
    },
    {
      "title": "o3-mini by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:o3-mini",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_o3-mini.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:o3-mini",
        "description": "Test and compare o3-mini by OpenAI"
      }
    },
    {
      "title": "o3-mini (High) by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:o3-mini-high",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_o3-mini-high.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:o3-mini-high",
        "description": "Test and compare o3-mini (High) by OpenAI"
      }
    },
    {
      "title": "o3-mini (Low) by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:o3-mini-low",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_o3-mini-low.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:o3-mini-low",
        "description": "Test and compare o3-mini (Low) by OpenAI"
      }
    },
    {
      "title": "o3-mini (Medium) by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:o3-mini-medium",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_o3-mini-medium.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:o3-mini-medium",
        "description": "Test and compare o3-mini (Medium) by OpenAI"
      }
    },
    {
      "title": "o4-mini by OpenAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/openai:o4-mini",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/openai_o4-mini.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/openai:o4-mini",
        "description": "Test and compare o4-mini by OpenAI"
      }
    },
    {
      "title": "Sonar by Perplexity on the AI Playground",
      "url": "https://ai-sdk.dev/playground/perplexity:sonar",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/perplexity_sonar.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/perplexity:sonar",
        "description": "Test and compare Sonar by Perplexity"
      }
    },
    {
      "title": "Sonar Pro by Perplexity on the AI Playground",
      "url": "https://ai-sdk.dev/playground/perplexity:sonar-pro",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/perplexity_sonar-pro.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/perplexity:sonar-pro",
        "description": "Test and compare Sonar Pro by Perplexity"
      }
    },
    {
      "title": "Sonar Reasoning by Perplexity on the AI Playground",
      "url": "https://ai-sdk.dev/playground/perplexity:sonar-reasoning",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/perplexity_sonar-reasoning.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/perplexity:sonar-reasoning",
        "description": "Test and compare Sonar Reasoning by Perplexity"
      }
    },
    {
      "title": "Sonar Reasoning Pro by Perplexity on the AI Playground",
      "url": "https://ai-sdk.dev/playground/perplexity:sonar-reasoning-pro",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/perplexity_sonar-reasoning-pro.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/perplexity:sonar-reasoning-pro",
        "description": "Test and compare Sonar Reasoning Pro by Perplexity"
      }
    },
    {
      "title": "Claude 3.5 Haiku (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:claude-3-5-haiku-20241022",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_claude-3-5-haiku-20241022.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:claude-3-5-haiku-20241022",
        "description": "Test and compare Claude 3.5 Haiku (Vertex) by Vertex"
      }
    },
    {
      "title": "Claude 3.5 Sonnet (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:claude-3-5-sonnet-20240620",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_claude-3-5-sonnet-20240620.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:claude-3-5-sonnet-20240620",
        "description": "Test and compare Claude 3.5 Sonnet (Vertex) by Vertex"
      }
    },
    {
      "title": "Claude 3.5 Sonnet v2 (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:claude-3-5-sonnet-v2-20241022",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_claude-3-5-sonnet-v2-20241022.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:claude-3-5-sonnet-v2-20241022",
        "description": "Test and compare Claude 3.5 Sonnet v2 (Vertex) by Vertex"
      }
    },
    {
      "title": "Claude 3.7 Sonnet (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:claude-3-7-sonnet-20250219",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_claude-3-7-sonnet-20250219.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:claude-3-7-sonnet-20250219",
        "description": "Test and compare Claude 3.7 Sonnet (Vertex) by Vertex"
      }
    },
    {
      "title": "Claude 3 Haiku (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:claude-3-haiku-20240307",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_claude-3-haiku-20240307.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:claude-3-haiku-20240307",
        "description": "Test and compare Claude 3 Haiku (Vertex) by Vertex"
      }
    },
    {
      "title": "Claude 3 Opus (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:claude-3-opus-20240229",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_claude-3-opus-20240229.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:claude-3-opus-20240229",
        "description": "Test and compare Claude 3 Opus (Vertex) by Vertex"
      }
    },
    {
      "title": "Claude 4 Opus (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:claude-4-opus-20250514",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_claude-4-opus-20250514.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:claude-4-opus-20250514",
        "description": "Test and compare Claude 4 Opus (Vertex) by Vertex"
      }
    },
    {
      "title": "Claude 4 Sonnet (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:claude-4-sonnet-20250514",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_claude-4-sonnet-20250514.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:claude-4-sonnet-20250514",
        "description": "Test and compare Claude 4 Sonnet (Vertex) by Vertex"
      }
    },
    {
      "title": "Gemini 2.0 Flash (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:gemini-2.0-flash-001",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_gemini-2.0-flash-001.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:gemini-2.0-flash-001",
        "description": "Test and compare Gemini 2.0 Flash (Vertex) by Vertex"
      }
    },
    {
      "title": "Gemini 2.0 Flash Lite (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:gemini-2.0-flash-lite-001",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_gemini-2.0-flash-lite-001.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:gemini-2.0-flash-lite-001",
        "description": "Test and compare Gemini 2.0 Flash Lite (Vertex) by Vertex"
      }
    },
    {
      "title": "Gemini 2.5 Flash Preview (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:gemini-2.5-flash-preview-04-17",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_gemini-2.5-flash-preview-04-17.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:gemini-2.5-flash-preview-04-17",
        "description": "Test and compare Gemini 2.5 Flash Preview (Vertex) by Vertex"
      }
    },
    {
      "title": "Gemini 2.5 Pro Preview (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:gemini-2.5-pro-preview-05-06",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_gemini-2.5-pro-preview-05-06.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:gemini-2.5-pro-preview-05-06",
        "description": "Test and compare Gemini 2.5 Pro Preview (Vertex) by Vertex"
      }
    },
    {
      "title": "Llama 3.3 70B (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:llama-3.3-70b-instruct-maas",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_llama-3.3-70b-instruct-maas.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:llama-3.3-70b-instruct-maas",
        "description": "Test and compare Llama 3.3 70B (Vertex) by Vertex"
      }
    },
    {
      "title": "Llama 4 Maverick 17B 128E Instruct (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:llama-4-maverick-17b-128e-instruct-maas",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_llama-4-maverick-17b-128e-instruct-maas.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:llama-4-maverick-17b-128e-instruct-maas",
        "description": "Test and compare Llama 4 Maverick 17B 128E Instruct (Vertex) by Vertex"
      }
    },
    {
      "title": "Llama 4 Scout 17B 16E Instruct (Vertex) by Vertex on the AI Playground",
      "url": "https://ai-sdk.dev/playground/vertex:llama-4-scout-17b-16e-instruct-maas",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/vertex_llama-4-scout-17b-16e-instruct-maas.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/vertex:llama-4-scout-17b-16e-instruct-maas",
        "description": "Test and compare Llama 4 Scout 17B 16E Instruct (Vertex) by Vertex"
      }
    },
    {
      "title": "Grok 2 by xAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/xai:grok-2-1212",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/xai_grok-2-1212.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/xai:grok-2-1212",
        "description": "Test and compare Grok 2 by xAI"
      }
    },
    {
      "title": "Grok 2 Vision by xAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/xai:grok-2-vision-1212",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/xai_grok-2-vision-1212.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/xai:grok-2-vision-1212",
        "description": "Test and compare Grok 2 Vision by xAI"
      }
    },
    {
      "title": "Grok 3 Beta by xAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/xai:grok-3-beta",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/xai_grok-3-beta.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/xai:grok-3-beta",
        "description": "Test and compare Grok 3 Beta by xAI"
      }
    },
    {
      "title": "Grok 3 Fast Beta by xAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/xai:grok-3-fast-beta",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/xai_grok-3-fast-beta.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/xai:grok-3-fast-beta",
        "description": "Test and compare Grok 3 Fast Beta by xAI"
      }
    },
    {
      "title": "Grok 3 Mini Beta by xAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/xai:grok-3-mini-beta",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/xai_grok-3-mini-beta.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/xai:grok-3-mini-beta",
        "description": "Test and compare Grok 3 Mini Beta by xAI"
      }
    },
    {
      "title": "Grok 3 Mini Fast Beta by xAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/xai:grok-3-mini-fast-beta",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/xai_grok-3-mini-fast-beta.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/xai:grok-3-mini-fast-beta",
        "description": "Test and compare Grok 3 Mini Fast Beta by xAI"
      }
    },
    {
      "title": "Grok Beta by xAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/xai:grok-beta",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/xai_grok-beta.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/xai:grok-beta",
        "description": "Test and compare Grok Beta by xAI"
      }
    },
    {
      "title": "Grok Vision Beta by xAI on the AI Playground",
      "url": "https://ai-sdk.dev/playground/xai:grok-vision-beta",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/playground/xai_grok-vision-beta.md",
      "metadata": {
        "url": "https://ai-sdk.dev/playground/xai:grok-vision-beta",
        "description": "Test and compare Grok Vision Beta by xAI"
      }
    },
    {
      "title": "Adapters",
      "url": "https://ai-sdk.dev/providers/adapters",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/adapters.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/adapters",
        "description": "Learn how to use AI SDK Adapters."
      }
    },
    {
      "title": "LangChain",
      "url": "https://ai-sdk.dev/providers/adapters/langchain",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/adapters/langchain.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/adapters/langchain",
        "description": "Learn how to use LangChain with the AI SDK."
      }
    },
    {
      "title": "LlamaIndex",
      "url": "https://ai-sdk.dev/providers/adapters/llamaindex",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/adapters/llamaindex.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/adapters/llamaindex",
        "description": "Learn how to use LlamaIndex with the AI SDK."
      }
    },
    {
      "title": "AI SDK Providers",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers",
        "description": "Learn how to use AI SDK providers."
      }
    },
    {
      "title": "Amazon Bedrock Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/amazon-bedrock.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/amazon-bedrock",
        "description": "Learn how to use the Amazon Bedrock provider."
      }
    },
    {
      "title": "Anthropic Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/anthropic",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/anthropic.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/anthropic",
        "description": "Learn how to use the Anthropic provider for the AI SDK."
      }
    },
    {
      "title": "AssemblyAI Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/assemblyai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/assemblyai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/assemblyai",
        "description": "Learn how to use the AssemblyAI provider for the AI SDK."
      }
    },
    {
      "title": "Azure OpenAI Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/azure",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/azure.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/azure",
        "description": "Learn how to use the Azure OpenAI provider for the AI SDK."
      }
    },
    {
      "title": "Cerebras Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/cerebras",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/cerebras.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/cerebras",
        "description": "Learn how to use Cerebras's models with the AI SDK."
      }
    },
    {
      "title": "Cohere Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/cohere",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/cohere.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/cohere",
        "description": "Learn how to use the Cohere provider for the AI SDK."
      }
    },
    {
      "title": "Deepgram Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/deepgram",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/deepgram.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/deepgram",
        "description": "Learn how to use the Deepgram provider for the AI SDK."
      }
    },
    {
      "title": "DeepInfra Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/deepinfra.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/deepinfra",
        "description": "Learn how to use DeepInfra's models with the AI SDK."
      }
    },
    {
      "title": "DeepSeek Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/deepseek",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/deepseek.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/deepseek",
        "description": "Learn how to use DeepSeek's models with the AI SDK."
      }
    },
    {
      "title": "ElevenLabs Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/elevenlabs.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/elevenlabs",
        "description": "Learn how to use the ElevenLabs provider for the AI SDK."
      }
    },
    {
      "title": "Fal Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/fal",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/fal.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/fal",
        "description": "Learn how to use Fal AI models with the AI SDK."
      }
    },
    {
      "title": "Fireworks Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/fireworks",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/fireworks.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/fireworks",
        "description": "Learn how to use Fireworks models with the AI SDK."
      }
    },
    {
      "title": "Gladia Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/gladia",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/gladia.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/gladia",
        "description": "Learn how to use the Gladia provider for the AI SDK."
      }
    },
    {
      "title": "Google Generative AI Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/google-generative-ai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai",
        "description": "Learn how to use Google Generative AI Provider."
      }
    },
    {
      "title": "Google Vertex Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/google-vertex.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/google-vertex",
        "description": "Learn how to use the Google Vertex AI provider."
      }
    },
    {
      "title": "Groq Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/groq",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/groq.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/groq",
        "description": "Learn how to use Groq."
      }
    },
    {
      "title": "Hume Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/hume",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/hume.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/hume",
        "description": "Learn how to use the Hume provider for the AI SDK."
      }
    },
    {
      "title": "LMNT Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/lmnt",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/lmnt.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/lmnt",
        "description": "Learn how to use the LMNT provider for the AI SDK."
      }
    },
    {
      "title": "Luma Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/luma",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/luma.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/luma",
        "description": "Learn how to use Luma AI models with the AI SDK."
      }
    },
    {
      "title": "Mistral AI Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/mistral",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/mistral.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/mistral",
        "description": "Learn how to use Mistral."
      }
    },
    {
      "title": "OpenAI Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/openai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/openai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/openai",
        "description": "Learn how to use the OpenAI provider for the AI SDK."
      }
    },
    {
      "title": "Perplexity Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/perplexity",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/perplexity.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/perplexity",
        "description": "Learn how to use Perplexity's Sonar API with the AI SDK."
      }
    },
    {
      "title": "Replicate Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/replicate",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/replicate.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/replicate",
        "description": "Learn how to use Replicate models with the AI SDK."
      }
    },
    {
      "title": "Rev.ai Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/revai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/revai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/revai",
        "description": "Learn how to use the Rev.ai provider for the AI SDK."
      }
    },
    {
      "title": "Together.ai Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/togetherai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/togetherai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/togetherai",
        "description": "Learn how to use Together.ai's models with the AI SDK."
      }
    },
    {
      "title": "Vercel Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/vercel",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/vercel.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/vercel",
        "description": "Learn how to use Vercel's v0 models with the AI SDK."
      }
    },
    {
      "title": "xAI Grok Provider",
      "url": "https://ai-sdk.dev/providers/ai-sdk-providers/xai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/ai-sdk-providers/xai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/ai-sdk-providers/xai",
        "description": "Learn how to use xAI Grok."
      }
    },
    {
      "title": "Community Providers",
      "url": "https://ai-sdk.dev/providers/community-providers",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers",
        "description": "Learn how to use Language Model Specification."
      }
    },
    {
      "title": "AnthropicVertex Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/anthropic-vertex-ai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/anthropic-vertex-ai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/anthropic-vertex-ai",
        "description": "Learn how to use the Anthropic Vertex provider for the AI SDK."
      }
    },
    {
      "title": "Azure Custom Provider for AI SDK",
      "url": "https://ai-sdk.dev/providers/community-providers/azure-ai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/azure-ai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/azure-ai",
        "description": "Learn how to use the @quail-ai/azure-ai-provider for the AI SDK."
      }
    },
    {
      "title": "ChromeAI",
      "url": "https://ai-sdk.dev/providers/community-providers/chrome-ai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/chrome-ai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/chrome-ai",
        "description": "Learn how to use the Chrome AI provider for the AI SDK."
      }
    },
    {
      "title": "AI Gateway Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/cloudflare-ai-gateway",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/cloudflare-ai-gateway.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/cloudflare-ai-gateway",
        "description": "Learn how to use the Cloudflare AI Gateway provider for the AI SDK."
      }
    },
    {
      "title": "Cloudflare Workers AI",
      "url": "https://ai-sdk.dev/providers/community-providers/cloudflare-workers-ai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/cloudflare-workers-ai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/cloudflare-workers-ai",
        "description": "Learn how to use the Cloudflare Workers AI provider for the AI SDK."
      }
    },
    {
      "title": "Crosshatch Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/crosshatch",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/crosshatch.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/crosshatch",
        "description": "Learn how to use the Crosshatch provider for the AI SDK."
      }
    },
    {
      "title": "Writing a Custom Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/custom-providers",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/custom-providers.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/custom-providers",
        "description": "Learn how to write a custom provider for the AI SDK"
      }
    },
    {
      "title": "Dify Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/dify",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/dify.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/dify",
        "description": "Learn how to use the Dify provider for the AI SDK."
      }
    },
    {
      "title": "FriendliAI Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/friendliai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/friendliai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/friendliai",
        "description": "Learn how to use the FriendliAI Provider for the AI SDK."
      }
    },
    {
      "title": "Unofficial Community Provider for AI SDK - Inflection AI",
      "url": "https://ai-sdk.dev/providers/community-providers/inflection-ai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/inflection-ai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/inflection-ai",
        "description": "Learn how to use the unofficial Inflection AI provider for the AI SDK."
      }
    },
    {
      "title": "LangDB",
      "url": "https://ai-sdk.dev/providers/community-providers/langdb",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/langdb.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/langdb",
        "description": "Learn how to use LangDB with the AI SDK"
      }
    },
    {
      "title": "Letta Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/letta",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/letta.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/letta",
        "description": "Learn how to use the Letta AI SDK provider for the AI SDK."
      }
    },
    {
      "title": "LLamaCpp Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/llama-cpp",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/llama-cpp.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/llama-cpp",
        "description": "Learn how to use Llama CPP."
      }
    },
    {
      "title": "Mem0 Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/mem0",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/mem0.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/mem0",
        "description": "Learn how to use the Mem0 AI SDK provider for the AI SDK."
      }
    },
    {
      "title": "Mixedbread Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/mixedbread",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/mixedbread.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/mixedbread",
        "description": "Learn how to use the Mixedbread provider."
      }
    },
    {
      "title": "Ollama Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/ollama",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/ollama.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/ollama",
        "description": "Learn how to use the Ollama provider."
      }
    },
    {
      "title": "OpenRouter",
      "url": "https://ai-sdk.dev/providers/community-providers/openrouter",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/openrouter.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/openrouter",
        "description": "OpenRouter Provider for the AI SDK"
      }
    },
    {
      "title": "Portkey Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/portkey",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/portkey.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/portkey",
        "description": "Learn how to use the Portkey provider for the AI SDK."
      }
    },
    {
      "title": "Qwen Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/qwen",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/qwen.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/qwen",
        "description": "Learn how to use the Qwen provider."
      }
    },
    {
      "title": "SambaNova Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/sambanova",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/sambanova.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/sambanova",
        "description": "Learn how to use the SambaNova provider for the AI SDK."
      }
    },
    {
      "title": "Sarvam Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/sarvam",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/sarvam.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/sarvam",
        "description": "Learn how to use the Sarvam AI provider for the AI SDK."
      }
    },
    {
      "title": "Spark Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/spark",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/spark.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/spark",
        "description": "Learn how to use the Spark provider for the AI SDK."
      }
    },
    {
      "title": "Voyage AI Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/voyage-ai",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/voyage-ai.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/voyage-ai",
        "description": "Learn how to use the Voyage AI provider."
      }
    },
    {
      "title": "Zhipu AI Provider",
      "url": "https://ai-sdk.dev/providers/community-providers/zhipu",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/community-providers/zhipu.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/community-providers/zhipu",
        "description": "Learn how to use the Zhipu provider."
      }
    },
    {
      "title": "Observability Integrations",
      "url": "https://ai-sdk.dev/providers/observability",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability",
        "description": "AI SDK Integration for monitoring and tracing LLM applications"
      }
    },
    {
      "title": "Braintrust Observability",
      "url": "https://ai-sdk.dev/providers/observability/braintrust",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/braintrust.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/braintrust",
        "description": "Monitoring and tracing LLM applications with Braintrust"
      }
    },
    {
      "title": "Helicone Observability",
      "url": "https://ai-sdk.dev/providers/observability/helicone",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/helicone.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/helicone",
        "description": "Monitor and optimize your AI SDK applications with minimal configuration using Helicone"
      }
    },
    {
      "title": "Laminar observability",
      "url": "https://ai-sdk.dev/providers/observability/laminar",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/laminar.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/laminar",
        "description": "Monitor your AI SDK applications with Laminar"
      }
    },
    {
      "title": "Langfuse Observability",
      "url": "https://ai-sdk.dev/providers/observability/langfuse",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/langfuse.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/langfuse",
        "description": "Monitor, evaluate and debug your AI SDK application with Langfuse"
      }
    },
    {
      "title": "LangSmith Observability",
      "url": "https://ai-sdk.dev/providers/observability/langsmith",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/langsmith.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/langsmith",
        "description": "Monitor and evaluate your AI SDK application with LangSmith"
      }
    },
    {
      "title": "LangWatch Observability",
      "url": "https://ai-sdk.dev/providers/observability/langwatch",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/langwatch.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/langwatch",
        "description": "Track, monitor, guardrail and evaluate your AI SDK applications with LangWatch."
      }
    },
    {
      "title": "Patronus Observability",
      "url": "https://ai-sdk.dev/providers/observability/patronus",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/patronus.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/patronus",
        "description": "Monitor, evaluate and debug your AI SDK application with Patronus"
      }
    },
    {
      "title": "Traceloop",
      "url": "https://ai-sdk.dev/providers/observability/traceloop",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/traceloop.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/traceloop",
        "description": "Monitoring and evaluating LLM applications with Traceloop"
      }
    },
    {
      "title": "Weave Observability",
      "url": "https://ai-sdk.dev/providers/observability/weave",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/observability/weave.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/observability/weave",
        "description": "Monitor and evaluate LLM applications with Weave."
      }
    },
    {
      "title": "OpenAI Compatible Providers",
      "url": "https://ai-sdk.dev/providers/openai-compatible-providers",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/openai-compatible-providers.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/openai-compatible-providers",
        "description": "Use OpenAI compatible providers with the AI SDK."
      }
    },
    {
      "title": "Baseten Provider",
      "url": "https://ai-sdk.dev/providers/openai-compatible-providers/baseten",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/openai-compatible-providers/baseten.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/openai-compatible-providers/baseten",
        "description": "Use a Baseten OpenAI compatible API with the AI SDK."
      }
    },
    {
      "title": "Writing a Custom Provider",
      "url": "https://ai-sdk.dev/providers/openai-compatible-providers/custom-providers",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/openai-compatible-providers/custom-providers.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/openai-compatible-providers/custom-providers",
        "description": "Create a custom provider package for an OpenAI-compatible provider leveraging the AI SDK OpenAI Compatible package."
      }
    },
    {
      "title": "LM Studio Provider",
      "url": "https://ai-sdk.dev/providers/openai-compatible-providers/lmstudio",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/openai-compatible-providers/lmstudio.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/openai-compatible-providers/lmstudio",
        "description": "Use the LM Studio OpenAI compatible API with the AI SDK."
      }
    },
    {
      "title": "NVIDIA NIM Provider",
      "url": "https://ai-sdk.dev/providers/openai-compatible-providers/nim",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/providers/openai-compatible-providers/nim.md",
      "metadata": {
        "url": "https://ai-sdk.dev/providers/openai-compatible-providers/nim",
        "description": "Use NVIDIA NIM OpenAI compatible API with the AI SDK."
      }
    },
    {
      "title": "AI SDK Showcase",
      "url": "https://ai-sdk.dev/showcase",
      "outputPath": "/Users/serbinov/Desktop/projects/personal/doc_scrapper/scraped-docs/ai-sdk-dev-docs/showcase.md",
      "metadata": {
        "url": "https://ai-sdk.dev/showcase",
        "description": "Collection of applications built with the AI SDK"
      }
    }
  ]
}
```

### 487. `showcase.md`

```markdown
# AI SDK Showcase


---
url: https://ai-sdk.dev/showcase
description: Collection of applications built with the AI SDK
---

[

AI SDK

](/)

Announcing AI SDK 5 Alpha!

[Learn more](https://ai-sdk.dev/docs/announcing-ai-sdk-5-alpha)

Menu


## Showcase


Check out these applications built with the AI SDK.

[

Perplexity

](https://perplexity.ai)[

v0

](https://v0.dev/chat)[

database.build

](https://database.build/)[

Midday

](https://www.midday.ai)[

Val Town

](https://val.town)[

Morphic

](https://morphic.sh)[

Dub.sh

](https://dub.sh)[

Chatbase

](https://chatbase.co)[

ChatPRD

](https://www.chatprd.ai)[

Ozone

](https://ozone.pro)[

2txt

](https://2txt.vercel.app)[

Vercel AI templates

](https://vercel.com/templates?type=ai)

Are you using the AI SDK?

[Add your company](https://github.com/vercel/ai/discussions/1914)
```

---

*Generated by Doc Scrapper AI - Your Documentation Consolidation Tool*
