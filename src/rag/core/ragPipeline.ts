import { RAGPipeline, ChatResponse, ProgressCallback, ProcessingStatus } from '../types/ragTypes.js';
import { DocumentLoader } from '../chunking/markdownChunker.js';
import { ChunkingStrategyFactory } from '../chunking/chunkingFactory.js';
import { ChromaVectorStore } from '../vectorstore/chromaStore.js';
import { RAGConfigService } from '../config/ragConfig.js';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

export class DocumentationRAGPipeline implements RAGPipeline {
  private vectorStore: ChromaVectorStore;
  private config: any;

  constructor(collectionName?: string) {
    this.vectorStore = new ChromaVectorStore(collectionName);
    this.config = RAGConfigService.getInstance().config;
  }

  async initialize(): Promise<void> {
    await this.vectorStore.initialize();
  }

  // NEW: Method to switch collection for session-specific queries
  async switchCollection(collectionName: string): Promise<void> {
    await this.vectorStore.switchCollection(collectionName);
  }

  // NEW: Method to get all available collections
  async listCollections(): Promise<Array<{name: string, count: number}>> {
    return await this.vectorStore.listCollections();
  }

  getCurrentCollectionName(): string {
    return this.vectorStore.getCurrentCollectionName();
  }

  async indexDocuments(documentsPath: string, progressCallback?: ProgressCallback): Promise<void> {
    console.log('üöÄ Starting document indexing...');
    
    try {
      // Initialize vector store
      await this.vectorStore.initialize();
      
      this.updateProgress(progressCallback, {
        stage: 'loading',
        progress: 10,
        message: 'Loading documents...',
        documentsProcessed: 0,
        totalDocuments: 0,
      });

      // Load documents
      console.log('üìÇ Loading documents from:', documentsPath);
      const documents = await DocumentLoader.loadFromDirectory(documentsPath);
      console.log(`üìÑ Loaded ${documents.length} documents`);

      if (documents.length === 0) {
        throw new Error('No documents found to index');
      }

      // Create optimal chunking strategy
      const chunkingStrategy = ChunkingStrategyFactory.createStrategy();

      this.updateProgress(progressCallback, {
        stage: 'chunking',
        progress: 30,
        message: 'Chunking documents with enhanced strategy...',
        documentsProcessed: 0,
        totalDocuments: documents.length,
      });

      // Chunk documents
      console.log('‚úÇÔ∏è Chunking documents...');
      const chunks = await chunkingStrategy.chunkDocuments(documents);
      console.log(`üß© Created ${chunks.length} chunks`);

      // Log chunking stats
      const avgTokens = chunks.reduce((sum, chunk) => sum + chunk.metadata.tokenCount, 0) / chunks.length;
      console.log(`üìä Average chunk size: ${Math.round(avgTokens)} tokens`);
      console.log(`üìè Chunk size range: ${Math.min(...chunks.map(c => c.metadata.tokenCount))} - ${Math.max(...chunks.map(c => c.metadata.tokenCount))} tokens`);

      this.updateProgress(progressCallback, {
        stage: 'embedding',
        progress: 50,
        message: 'Generating embeddings...',
        documentsProcessed: documents.length,
        totalDocuments: documents.length,
      });

      // Generate embeddings (this will be handled by ChromaDB)
      console.log('üî¢ Embeddings will be generated by ChromaDB...');

      this.updateProgress(progressCallback, {
        stage: 'indexing',
        progress: 80,
        message: 'Indexing chunks...',
        documentsProcessed: documents.length,
        totalDocuments: documents.length,
      });

      // Add to vector store with progress tracking ‚≠ê NEW
      console.log('üíæ Adding chunks to vector store...');
      await this.vectorStore.addDocuments(chunks, (batchProgress, current, total) => {
        // Map batch progress to final progress range 80-99%
        const progressRange = 99 - 80; // 19% range
        const progressPercent = 80 + Math.round((batchProgress / 100) * progressRange);
        
        this.updateProgress(progressCallback, {
          stage: 'indexing',
          progress: progressPercent,
          message: `–Ü–Ω–¥–µ–∫—Å—É—î–º–æ chunks –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É: ${current}/${total} (${batchProgress}%)`,
          documentsProcessed: documents.length,
          totalDocuments: documents.length,
        });
      });

      this.updateProgress(progressCallback, {
        stage: 'complete',
        progress: 100,
        message: 'Indexing complete!',
        documentsProcessed: documents.length,
        totalDocuments: documents.length,
      });

      // Show final stats
      const collectionInfo = await this.vectorStore.getCollectionInfo();
      console.log('\n‚úÖ Indexing completed successfully!');
      console.log(`üìä Collection: ${collectionInfo.name}`);
      console.log(`üìÑ Documents: ${documents.length}`);
      console.log(`üß© Chunks: ${chunks.length}`);
      console.log(`üíæ Total in store: ${collectionInfo.documentCount}`);

    } catch (error) {
      console.error('‚ùå Error during indexing:', error);
      throw error;
    }
  }

  async query(question: string): Promise<ChatResponse> {
    const startTime = Date.now();
    
    try {
      console.log(`üîç Searching for: "${question}"`);
      
      // Retrieve relevant chunks
      const retrievalResult = await this.vectorStore.similaritySearch(
        question, 
        this.config.retrieval.k
      );

      console.log(`üìã Found ${retrievalResult.chunks.length} relevant chunks`);
      
      if (retrievalResult.chunks.length === 0) {
        return {
          content: "Sorry, I couldn't find any relevant information to answer your question.",
          sources: [],
          model: this.config.llm.model,
          tokensUsed: 0,
          responseTime: Date.now() - startTime,
        };
      }

      // Build context from retrieved chunks
      const context = this.buildContext(retrievalResult.chunks);
      
      // Generate response with LLM
      const response = await this.generateResponse(question, context);
      
      // Extract sources
      const sources = retrievalResult.chunks.map((chunk, index) => ({
        title: chunk.metadata.title,
        url: chunk.metadata.sourceUrl,
        excerpt: this.truncateText(chunk.content, 150),
        score: retrievalResult.scores[index],
      }));

      const totalTime = Date.now() - startTime;

      return {
        content: response.text,
        sources,
        model: this.config.llm.model,
        tokensUsed: response.usage?.totalTokens || 0,
        responseTime: totalTime,
      };

    } catch (error) {
      console.error('‚ùå Error during query:', error);
      throw error;
    }
  }

  async chat(message: string, _sessionId?: string): Promise<ChatResponse> {
    // For now, treat chat the same as query
    // TODO: Implement session management and conversation history
    return this.query(message);
  }

  private buildContext(chunks: any[]): string {
    const contextParts = chunks.map((chunk, index) => {
      const section = chunk.metadata.section ? ` (${chunk.metadata.section})` : '';
      return `[${index + 1}] From "${chunk.metadata.title}"${section}:\n${chunk.content}`;
    });

    return contextParts.join('\n\n');
  }

  private async generateResponse(question: string, context: string): Promise<any> {
    const model = openai(this.config.llm.model);

    const prompt = this.buildPrompt(question, context);

    return await generateText({
      model,
      prompt,
      temperature: this.config.llm.temperature,
      maxTokens: this.config.llm.maxTokens,
    });
  }

  private buildPrompt(question: string, context: string): string {
    return `You are a helpful assistant that answers questions based on provided documentation context.

INSTRUCTIONS:
- Answer the question using ONLY the information provided in the context below
- If the context doesn't contain enough information to answer the question, say so
- Be specific and cite relevant sections when possible
- Keep your answer concise but comprehensive
- If you reference information, mention which source it comes from

CONTEXT:
${context}

QUESTION: ${question}

ANSWER:`;
  }

  private truncateText(text: string, maxLength: number): string {
    if (text.length <= maxLength) return text;
    return text.substring(0, maxLength - 3) + '...';
  }

  private updateProgress(callback: ProgressCallback | undefined, status: ProcessingStatus): void {
    if (callback) {
      callback(status);
    }
  }

  // Utility methods
  async getCollectionInfo() {
    return await this.vectorStore.getCollectionInfo();
  }

  async resetIndex(): Promise<void> {
    console.log('üîÑ Resetting vector store...');
    await this.vectorStore.reset();
    console.log('‚úÖ Vector store reset complete');
  }
} 